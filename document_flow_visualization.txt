DOCUMENT FLOW VISUALIZATION
================================================================================

ğŸ“– Attention Is All You Need (22 items)
  â†’ ğŸ“‘ Abstract (1 items)
  â†’ ğŸ“‘ 1 Introduction (8 items)
  â†’ ğŸ“‘ 2 Background (4 items)
  â†’ ğŸ“‘ 3 Model Architecture (2 items)
    â†’ ğŸ“ 3.1 Encoder and Decoder Stacks (5 items)
    â†’ ğŸ“ 3.2 Attention (1 items)
      â†’ â€¢ 3.2.1 Scaled Dot-Product Attention (6 items)
      â†’ â€¢ 3.2.2 Multi-Head Attention (7 items)
      â†’ â€¢ 3.2.3 Applications of Attention in our M... (2 items)
    â†’ ğŸ“ 3.3 Position-wise Feed-Forward Networks (2 items)
    â†’ ğŸ“ 3.4 Embeddings and Softma (1 items)
    â†’ ğŸ“ 3.5 Positional Encoding (6 items)
  â†’ ğŸ“‘ 4 Why Self-Attention (8 items)
  â†’ ğŸ“‘ 5 Training (1 items)
    â†’ ğŸ“ 5.1 Training Data and Batching (1 items)
    â†’ ğŸ“ 5.2 Hardware and Schedule (1 items)
    â†’ ğŸ“ 5.3 Optimizer (2 items)
    â†’ ğŸ“ 5.4 Regularization (4 items)
  â†’ ğŸ“‘ 6 Results
    â†’ ğŸ“ 6.1 Machine Translation (4 items)
    â†’ ğŸ“ 6.2 Model Variations (6 items)
  â†’ ğŸ“‘ 7 Conclusion (5 items)
  â†’ ğŸ“‘ References (34 items)
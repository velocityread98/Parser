{
  "id": "document_root",
  "label": "document",
  "text": "Document Root",
  "level": -1,
  "page": 0,
  "reading_order": -1,
  "bbox": null,
  "section_number": null,
  "summary": null,
  "embeddings": [],
  "children": [
    {
      "id": "page_1_order_1",
      "label": "sec",
      "text": "Natural Language\nProcessing with",
      "level": 1,
      "page": 1,
      "reading_order": 1,
      "bbox": [
        55,
        143,
        637,
        274
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_1_order_2",
          "label": "para",
          "text": "Python",
          "level": -1,
          "page": 1,
          "reading_order": 2,
          "bbox": [
            64,
            286,
            297,
            362
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_1_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_001_figure_003.png)",
          "level": -1,
          "page": 1,
          "reading_order": 3,
          "bbox": [
            0,
            369,
            664,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_1_order_4",
          "label": "para",
          "text": "O'REILLY",
          "level": -1,
          "page": 1,
          "reading_order": 4,
          "bbox": [
            46,
            823,
            181,
            851
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_1_order_5",
          "label": "para",
          "text": "Steven Bird, Euan Klein & Edward Loper",
          "level": -1,
          "page": 1,
          "reading_order": 5,
          "bbox": [
            252,
            824,
            637,
            852
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_2_order_2",
      "label": "sec",
      "text": "Natural Language Processing with Python",
      "level": 1,
      "page": 2,
      "reading_order": 2,
      "bbox": [
        46,
        98,
        530,
        125
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_2_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_002_figure_003.png)",
          "level": -1,
          "page": 2,
          "reading_order": 3,
          "bbox": [
            46,
            134,
            109,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_4",
          "label": "para",
          "text": "and translation. With it, you'll learn how to write Python pro-\ngrams that work with large collections of unstructured text.\nYou'll access richly annotated datasets using a comprehensive\nrange of linguistic data structures, and you'll understand the\nmain algorithms for analyzing the content and structure of\nwritten communication.",
          "level": -1,
          "page": 2,
          "reading_order": 4,
          "bbox": [
            46,
            206,
            413,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_5",
          "label": "para",
          "text": "Packed with examples and exercises, Natural Language\nProcessing with Python will help you:",
          "level": -1,
          "page": 2,
          "reading_order": 5,
          "bbox": [
            46,
            322,
            377,
            353
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_6",
          "label": "list_group",
          "text": "• Extract information from unstructured text, either to\nguess the topic or identify “ named entities\"\n• Analyze linguistic structure in text, including parsing\nand semantic analysis",
          "level": -1,
          "page": 2,
          "reading_order": 6,
          "bbox": [
            63,
            358,
            386,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "• Extract information from unstructured text, either to\nguess the topic or identify “ named entities\"",
              "bbox": [
                63,
                358,
                386,
                394
              ],
              "page": 2,
              "reading_order": 6
            },
            {
              "label": "list",
              "text": "• Analyze linguistic structure in text, including parsing\nand semantic analysis",
              "bbox": [
                63,
                403,
                386,
                431
              ],
              "page": 2,
              "reading_order": 7
            },
            {
              "label": "list",
              "text": "• Access popular linguistic databases, including WordNet\nand treebanks",
              "bbox": [
                63,
                439,
                396,
                474
              ],
              "page": 2,
              "reading_order": 8
            },
            {
              "label": "list",
              "text": "• Integrate techniques drawn from fields as diverse as\nlinguistics and artificial intelligence",
              "bbox": [
                63,
                483,
                377,
                515
              ],
              "page": 2,
              "reading_order": 9
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_2_order_10",
          "label": "para",
          "text": "This book will help you gain practical skills in natural language\nprocessing using the Python programming language and the\nNatural Language Toolkit (NLTK) open source library. If you're\ninterested in developing web applications, analyzing multi-\nlingual news sources, or documenting endangered languages—\nor if you're simply curious to have a programmer's perspective\non how human language works—you'll find Natural Language\nProcessing with Python both fascinating and immensely useful.",
          "level": -1,
          "page": 2,
          "reading_order": 10,
          "bbox": [
            46,
            528,
            423,
            664
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_11",
          "label": "para",
          "text": "“Rarely does a book tackle\nsuch a difficult computing\nsubject with such a clear\napproach and with such\nbeautifully clean code....\nThis is the book from\nwhich to learn natural\nlanguage processing.”",
          "level": -1,
          "page": 2,
          "reading_order": 11,
          "bbox": [
            448,
            152,
            630,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_12",
          "label": "para",
          "text": "— Ken Getz,\nSenior Consultant,\nMCW Technologies",
          "level": -1,
          "page": 2,
          "reading_order": 12,
          "bbox": [
            499,
            313,
            611,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_13",
          "label": "para",
          "text": "Steven Bird is Associate\nProfessor in the Department\nof Computer Science and\nSoftware Engineering at the\nUniversity of Melbourne, and\nSenior Research Associate in\nthe Linguistic Data Consor-\ntium at the University of\nPennsylvania.",
          "level": -1,
          "page": 2,
          "reading_order": 13,
          "bbox": [
            449,
            394,
            620,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_14",
          "label": "para",
          "text": "Ewan Klein is Professor of\nLanguage Technology in the\nSchool of Informatics at the\nUniversity of Edinburgh,",
          "level": -1,
          "page": 2,
          "reading_order": 14,
          "bbox": [
            449,
            564,
            611,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_15",
          "label": "para",
          "text": "Edward Loper recently con-\npleted a Ph.D. on machine\nlearning for natural language\nprocessing at the University\nof Pennsylvania, and is\nnow a researcher at BBN\nTechnologies in Boston.",
          "level": -1,
          "page": 2,
          "reading_order": 15,
          "bbox": [
            449,
            642,
            619,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_16",
          "label": "para",
          "text": "oreilly.com",
          "level": -1,
          "page": 2,
          "reading_order": 16,
          "bbox": [
            46,
            743,
            118,
            763
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_17",
          "label": "para",
          "text": "US $44.99 CAN $56.99",
          "level": -1,
          "page": 2,
          "reading_order": 17,
          "bbox": [
            46,
            770,
            207,
            800
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_18",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_002_figure_018.png)",
          "level": -1,
          "page": 2,
          "reading_order": 18,
          "bbox": [
            46,
            806,
            252,
            851
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_19",
          "label": "para",
          "text": "Safari",
          "level": -1,
          "page": 2,
          "reading_order": 19,
          "bbox": [
            350,
            788,
            485,
            835
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_2_order_20",
          "label": "para",
          "text": "Free online edition\nfor 45 days with\npurchase of this book.\nDetails on last page.",
          "level": -1,
          "page": 2,
          "reading_order": 20,
          "bbox": [
            503,
            788,
            637,
            851
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_3_order_0",
          "label": "para",
          "text": "Natural Language Processing with Python",
          "level": -1,
          "page": 3,
          "reading_order": 0,
          "bbox": [
            109,
            197,
            584,
            235
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_4_order_0",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 4,
          "reading_order": 0,
          "bbox": [
            153,
            161,
            494,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_5_order_0",
      "label": "sec",
      "text": "Natural Language Processing\nwith Python",
      "level": 1,
      "page": 5,
      "reading_order": 0,
      "bbox": [
        179,
        197,
        583,
        296
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_5_order_1",
          "label": "para",
          "text": "Steven Bird, Ewan Klein, and Edward Loper",
          "level": -1,
          "page": 5,
          "reading_order": 1,
          "bbox": [
            207,
            572,
            586,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_5_order_2",
          "label": "para",
          "text": "O’REILLY",
          "level": -1,
          "page": 5,
          "reading_order": 2,
          "bbox": [
            500,
            804,
            583,
            824
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_5_order_3",
          "label": "foot",
          "text": "Beijing • Cambridge • Farnham • Köln • Sebastopol • Taipei • Tokyo",
          "level": -1,
          "page": 5,
          "reading_order": 3,
          "bbox": [
            241,
            824,
            583,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_1",
          "label": "para",
          "text": "Copyright © 2009 Steven Bird, Ewan Klein, and Edward Loper. All rights reserved.\nPrinted in the United States of America.",
          "level": -1,
          "page": 6,
          "reading_order": 1,
          "bbox": [
            97,
            116,
            494,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_2",
          "label": "para",
          "text": "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472",
          "level": -1,
          "page": 6,
          "reading_order": 2,
          "bbox": [
            100,
            152,
            539,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_3",
          "label": "para",
          "text": "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions\nare also available for most titles ( http://my.safaribooksonline.com ). For more information, contact our\ncorporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com .",
          "level": -1,
          "page": 6,
          "reading_order": 3,
          "bbox": [
            97,
            178,
            585,
            217
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_4",
          "label": "para",
          "text": "Editor:\nJulie Steele",
          "level": -1,
          "page": 6,
          "reading_order": 4,
          "bbox": [
            97,
            232,
            190,
            245
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_5",
          "label": "para",
          "text": "Production Editor: Loranah Dimant",
          "level": -1,
          "page": 6,
          "reading_order": 5,
          "bbox": [
            99,
            247,
            270,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_6",
          "label": "para",
          "text": "Copyeditor: Genevieve d’Entremon",
          "level": -1,
          "page": 6,
          "reading_order": 6,
          "bbox": [
            97,
            259,
            270,
            275
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_7",
          "label": "para",
          "text": "Proofreader: Loranah Dimant",
          "level": -1,
          "page": 6,
          "reading_order": 7,
          "bbox": [
            99,
            275,
            245,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_8",
          "label": "para",
          "text": "Indexer:\nEllen Troutman Zaig",
          "level": -1,
          "page": 6,
          "reading_order": 8,
          "bbox": [
            341,
            232,
            494,
            246
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_9",
          "label": "para",
          "text": "Cover Designer: Karen Montgomery",
          "level": -1,
          "page": 6,
          "reading_order": 9,
          "bbox": [
            342,
            247,
            521,
            261
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_10",
          "label": "para",
          "text": "Interior Designer: David Futato",
          "level": -1,
          "page": 6,
          "reading_order": 10,
          "bbox": [
            341,
            261,
            495,
            275
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_11",
          "label": "para",
          "text": "Illustrator: Robert Romano",
          "level": -1,
          "page": 6,
          "reading_order": 11,
          "bbox": [
            342,
            275,
            477,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_12",
          "label": "para",
          "text": "Printing History",
          "level": -1,
          "page": 6,
          "reading_order": 12,
          "bbox": [
            97,
            304,
            171,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_13",
          "label": "para",
          "text": "June 2009:",
          "level": -1,
          "page": 6,
          "reading_order": 13,
          "bbox": [
            109,
            322,
            180,
            334
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_14",
          "label": "para",
          "text": "First Edition",
          "level": -1,
          "page": 6,
          "reading_order": 14,
          "bbox": [
            207,
            322,
            270,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_15",
          "label": "para",
          "text": "Nutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\nO'Reilly Media, Inc. Natural Language Processing with Python , the image of a right whale, and related\ntrade dress are trademarks of O'Reilly Media, Inc.",
          "level": -1,
          "page": 6,
          "reading_order": 15,
          "bbox": [
            97,
            473,
            584,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_16",
          "label": "para",
          "text": "Many of the designations used by manufacturers and sellers to distinguish their products are claimed as\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc. was aware of a\ntrademark claim, the designations have been printed in caps or initial caps.",
          "level": -1,
          "page": 6,
          "reading_order": 16,
          "bbox": [
            97,
            519,
            585,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_17",
          "label": "para",
          "text": "While every precaution has been taken in the preparation of this book, the publisher and authors assume\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\ntained herein.",
          "level": -1,
          "page": 6,
          "reading_order": 17,
          "bbox": [
            97,
            572,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_18",
          "label": "para",
          "text": "SBN: 978-0-596-51649-9",
          "level": -1,
          "page": 6,
          "reading_order": 18,
          "bbox": [
            100,
            758,
            219,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_19",
          "label": "para",
          "text": "M1",
          "level": -1,
          "page": 6,
          "reading_order": 19,
          "bbox": [
            100,
            779,
            118,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_6_order_20",
          "label": "para",
          "text": "1244726609",
          "level": -1,
          "page": 6,
          "reading_order": 20,
          "bbox": [
            99,
            797,
            156,
            815
          ],
          "section_number": "1244726609",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_7_order_0",
          "label": "table",
          "text": "Table of Contents [TABLE: <table><tr><td>1.</td><td>Language Processing and Python …… 1</td></tr><tr><td>1.1</td><td>Computing with Language: Texts and Words</td></tr><tr><td>1.2</td><td>A Closer Look at Python: Texts as Lists of Words</td></tr><tr><td>1.3</td><td>Computing with Language: Simple Statistics</td></tr><tr><td>1.4</td><td>Back to Python: Making Decisions and Taking Control</td></tr><tr><td>1.5</td><td>Automatic Natural Language Understanding</td></tr><tr><td>1.6</td><td>Summary</td></tr><tr><td>1.7</td><td>Further Reading</td></tr><tr><td>1.8</td><td>Exercises</td></tr></table>]",
          "level": -1,
          "page": 7,
          "reading_order": 0,
          "bbox": [
            97,
            259,
            585,
            806
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>1.</td><td>Language Processing and Python …… 1</td></tr><tr><td>1.1</td><td>Computing with Language: Texts and Words</td></tr><tr><td>1.2</td><td>A Closer Look at Python: Texts as Lists of Words</td></tr><tr><td>1.3</td><td>Computing with Language: Simple Statistics</td></tr><tr><td>1.4</td><td>Back to Python: Making Decisions and Taking Control</td></tr><tr><td>1.5</td><td>Automatic Natural Language Understanding</td></tr><tr><td>1.6</td><td>Summary</td></tr><tr><td>1.7</td><td>Further Reading</td></tr><tr><td>1.8</td><td>Exercises</td></tr></table>",
              "bbox": [
                97,
                259,
                585,
                806
              ],
              "page": 7,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Table of Contents",
              "bbox": [
                386,
                80,
                585,
                108
              ],
              "page": 7,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_8_order_0",
          "label": "para",
          "text": "3.10 Summary\n121\n3.11 Further Reading\n122\n3.12 Exercises\n123",
          "level": -1,
          "page": 8,
          "reading_order": 0,
          "bbox": [
            135,
            71,
            584,
            125
          ],
          "section_number": "3.10",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_8_order_1",
          "label": "para",
          "text": "4. Writing Structured Programs ...... 129\n4.1 Back to the Basics 130\n4.2 Sequences 133\n4.3 Questions of Style 138\n4.4 Functions: The Foundation of Structured Programming 142\n4.5 Doing More with Functions 149\n4.6 Program Development 154\n4.7 Algorithm Design 160\n4.8 A Sample of Python Libraries 167\n4.9 Summary 172\n4.10 Further Reading 173\n4.11 Exercises 173",
          "level": -1,
          "page": 8,
          "reading_order": 1,
          "bbox": [
            109,
            143,
            592,
            340
          ],
          "section_number": "4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_8_order_2",
          "label": "tab",
          "text": "<table><tr><td>5.</td><td>Categorizing and Tagging Words …… 179</td></tr><tr><td>5.1</td><td>Using a Tagger</td><td>179</td></tr><tr><td>5.2</td><td>Tagged Corpora</td><td>181</td></tr><tr><td>5.3</td><td>Mapping Words to Properties Using Python Dictionaries</td><td>189</td></tr><tr><td>5.4</td><td>Automatic Tagging</td><td>198</td></tr><tr><td>5.5</td><td>N-Gram Tagging</td><td>202</td></tr><tr><td>5.6</td><td>Transformation-Based Tagging</td><td>208</td></tr><tr><td>5.7</td><td>How to Determine the Category of a Word</td><td>210</td></tr><tr><td>5.8</td><td>Summary</td><td>213</td></tr><tr><td>5.9</td><td>Further Reading</td><td>214</td></tr><tr><td>5.10</td><td>Exercises</td><td>215</td></tr></table>",
          "level": -1,
          "page": 8,
          "reading_order": 2,
          "bbox": [
            100,
            358,
            592,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_8_order_3",
          "label": "foot",
          "text": "vi | Table of Content",
          "level": -1,
          "page": 8,
          "reading_order": 3,
          "bbox": [
            97,
            824,
            189,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_9_order_0",
          "label": "tab",
          "text": "<table><tr><td>7.2</td><td>Chunking</td><td>264</td></tr><tr><td>7.3</td><td>Developing and Evaluating Chunkers</td><td>270</td></tr><tr><td>7.4</td><td>Recursion in Linguistic Structure</td><td>277</td></tr><tr><td>7.5</td><td>Named Entity Recognition</td><td>281</td></tr><tr><td>7.6</td><td>Relation Extraction</td><td>284</td></tr><tr><td>7.7</td><td>Summary</td><td>285</td></tr><tr><td>7.8</td><td>Further Reading</td><td>286</td></tr><tr><td>7.9</td><td>Exercises</td><td>286</td></tr></table>",
          "level": -1,
          "page": 9,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            585,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_9_order_1",
          "label": "tab",
          "text": "<table><tr><td>8.1</td><td>Some Grammatical Dilemmas</td><td>292</td></tr><tr><td>8.2</td><td>What’s the Use of Syntax?</td><td>295</td></tr><tr><td>8.3</td><td>Context-Free Grammar</td><td>298</td></tr><tr><td>8.4</td><td>Parsing with Context-Free Grammar</td><td>302</td></tr><tr><td>8.5</td><td>Dependencies and Dependency Grammar</td><td>310</td></tr><tr><td>8.6</td><td>Grammar Development</td><td>315</td></tr><tr><td>8.7</td><td>Summary</td><td>321</td></tr><tr><td>8.8</td><td>Further Reading</td><td>322</td></tr><tr><td>8.9</td><td>Exercises</td><td>322</td></tr></table>",
          "level": -1,
          "page": 9,
          "reading_order": 1,
          "bbox": [
            109,
            224,
            585,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_9_order_2",
          "label": "tab",
          "text": "<table><tr><td>9.1</td><td>Grammatical Features</td><td>327</td></tr><tr><td>9.2</td><td>Processing Feature Structures</td><td>337</td></tr><tr><td>9.3</td><td>Extending a Feature-Based Grammar</td><td>344</td></tr><tr><td>9.4</td><td>Summary</td><td>356</td></tr><tr><td>9.5</td><td>Further Reading</td><td>357</td></tr><tr><td>9.6</td><td>Exercises</td><td>358</td></tr></table>",
          "level": -1,
          "page": 9,
          "reading_order": 2,
          "bbox": [
            109,
            403,
            585,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_9_order_3",
          "label": "tab",
          "text": "<table><tr><td>10.1</td><td>Natural Language Understanding</td><td>361</td></tr><tr><td>10.2</td><td>Propositional Logic</td><td>368</td></tr><tr><td>10.3</td><td>First-Order Logic</td><td>372</td></tr><tr><td>10.4</td><td>The Semantics of English Sentences</td><td>385</td></tr><tr><td>10.5</td><td>Discourse Semantics</td><td>397</td></tr><tr><td>10.6</td><td>Summary</td><td>402</td></tr><tr><td>10.7</td><td>Further Reading</td><td>403</td></tr><tr><td>10.8</td><td>Exercises</td><td>404</td></tr></table>",
          "level": -1,
          "page": 9,
          "reading_order": 3,
          "bbox": [
            100,
            546,
            584,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_9_order_4",
          "label": "tab",
          "text": "<table><tr><td>11.</td><td>Corpus Structure: A Case Study</td><td>407</td></tr><tr><td>11.2</td><td>The Life Cycle of a Corpus</td><td>412</td></tr><tr><td>11.3</td><td>Acquiring Data</td><td>416</td></tr><tr><td>11.4</td><td>Working with XML</td><td>425</td></tr></table>",
          "level": -1,
          "page": 9,
          "reading_order": 4,
          "bbox": [
            100,
            707,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_9_order_5",
          "label": "foot",
          "text": "Table of Contents | vii",
          "level": -1,
          "page": 9,
          "reading_order": 5,
          "bbox": [
            485,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_10_order_0",
          "label": "tab",
          "text": "<table><tr><td>11.5</td><td>Working with Toolbox Data</td><td>431</td></tr><tr><td>11.6</td><td>Describing Language Resources Using OLAC Metadata</td><td>435</td></tr><tr><td>11.7</td><td>Summary</td><td>437</td></tr><tr><td>11.8</td><td>Further Reading</td><td>437</td></tr><tr><td>11.9</td><td>Exercises</td><td>438</td></tr></table>",
          "level": -1,
          "page": 10,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_10_order_1",
          "label": "foot",
          "text": "viii | Table of Contents",
          "level": -1,
          "page": 10,
          "reading_order": 1,
          "bbox": [
            97,
            824,
            198,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_11_order_1",
          "label": "para",
          "text": "This is a book about Natural Language Processing. By “natural language” we mean a\nlanguage that is used for everyday communication by humans; languages such as Eng-\nlish, Hindi, or Portuguese. In contrast to artificial languages such as programming lan-\nguages and mathematical notations, natural languages have evolved as they pass from\ngeneration to generation, and are hard to pin down with explicit rules. We will take\nNatural Language Processing—or NLP for short—in a wide sense to cover any kind of\ncomputer manipulation of natural language. At one extreme, it could be as simple as\ncounting word frequencies to compare different writing styles. At the other extreme,\nNLP involves “understanding” complete human utterances, at least to the extent of\nbeing able to give useful responses to them.",
          "level": -1,
          "page": 11,
          "reading_order": 1,
          "bbox": [
            97,
            259,
            586,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_11_order_2",
          "label": "para",
          "text": "Technologies based on NLP are becoming increasingly widespread. For example,\nphones and handheld computers support predictive text and handwriting recognition;\nweb search engines give access to information locked up in unstructured text; machine\ntranslation allows us to retrieve texts written in Chinese and read them in Spanish. By\nproviding more natural human-machine interfaces, and more sophisticated access to\nstored information, language processing has come to play a central role in the multi-\nlingual information society.",
          "level": -1,
          "page": 11,
          "reading_order": 2,
          "bbox": [
            97,
            430,
            585,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_11_order_3",
          "label": "para",
          "text": "This book provides a highly accessible introduction to the field of NLP. It can be used\nfor individual study or as the textbook for a course on natural language processing or\ncomputational linguistics, or as a supplement to courses in artificial intelligence, text\nmining, or corpus linguistics. The book is intensely practical, containing hundreds of\nfully worked examples and graded exercises.",
          "level": -1,
          "page": 11,
          "reading_order": 3,
          "bbox": [
            97,
            555,
            586,
            639
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_11_order_4",
          "label": "para",
          "text": "The book is based on the Python programming language together with an open source\nlibrary called the Natural Language Toolkit (NLTK). NLTK includes extensive soft-\nware, data, and documentation, all freely downloadable from http://www.nltk.org/.\nDistributions are provided for Windows, Macintosh, and Unix platforms. We strongly\nencourage you to download Python and NLTK, and try out the examples and exercises\nalong the way.",
          "level": -1,
          "page": 11,
          "reading_order": 4,
          "bbox": [
            97,
            645,
            585,
            745
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_11_order_5",
          "label": "foot",
          "text": "ix",
          "level": -1,
          "page": 11,
          "reading_order": 5,
          "bbox": [
            574,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_12_order_0",
      "label": "sec",
      "text": "Audience",
      "level": 1,
      "page": 12,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        171,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_12_order_3",
          "label": "sub_sec",
          "text": "New to programming?",
          "level": 2,
          "page": 12,
          "reading_order": 3,
          "bbox": [
            97,
            304,
            225,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_12_order_4",
              "label": "para",
              "text": "The early chapters of the book are suitable for readers with no prior knowledge of\nprogramming, so long as you aren’t afraid to tackle new concepts and develop new\ncomputing skills. The book is full of examples that you can copy and try for your-\nself, together with hundreds of graded exercises. If you need a more general intro-\nduction to Python, see the list of Python resources at http://docs.python.org/ .",
              "level": -1,
              "page": 12,
              "reading_order": 4,
              "bbox": [
                121,
                322,
                586,
                403
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_12_order_5",
          "label": "sub_sec",
          "text": "New to Python",
          "level": 2,
          "page": 12,
          "reading_order": 5,
          "bbox": [
            100,
            410,
            180,
            422
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_12_order_6",
              "label": "para",
              "text": "Experienced programmers can quickly learn enough Python using this book to get\nimmersed in natural language processing. All relevant Python features are carefully\nexplained and exemplified, and you will quickly come to appreciate Python's suit-\nability for this application area. The language index will help you locate relevant\ndiscussions in the book.",
              "level": -1,
              "page": 12,
              "reading_order": 6,
              "bbox": [
                122,
                422,
                585,
                504
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_12_order_7",
          "label": "sub_sec",
          "text": "Already dreaming in Python?",
          "level": 2,
          "page": 12,
          "reading_order": 7,
          "bbox": [
            97,
            510,
            261,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_12_order_8",
              "label": "para",
              "text": "Skim the Python examples and dig into the interesting language analysis material\nthat starts in Chapter 1 . You'll soon be applying your skills to this fascinating\ndomain.",
              "level": -1,
              "page": 12,
              "reading_order": 8,
              "bbox": [
                122,
                528,
                585,
                574
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_12_order_1",
          "label": "para",
          "text": "NLP is important for scientific, economic, social, and cultural reasons. NLP is experi-\nencing rapid growth as its theories and methods are deployed in a variety of new lan-\nguage technologies. For this reason it is important for a wide range of people to have a\nworking knowledge of NLP. Within industry, this includes people in human-computer\ninteraction, business information analysis, and web software development. Within\nacademia, it includes people in areas from humanities computing and corpus linguistics\nthrough to computer science and artificial intelligence. (To many people in academia,\nNLP is known by the name of “Computational Linguistics.”)",
          "level": -1,
          "page": 12,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_12_order_2",
          "label": "para",
          "text": "This book is intended for a diverse range of people who want to learn how to write\nprograms that analyze written language, regardless of previous programming\nexperience:",
          "level": -1,
          "page": 12,
          "reading_order": 2,
          "bbox": [
            97,
            248,
            585,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_12_order_9",
      "label": "sec",
      "text": "Emphasis",
      "level": 1,
      "page": 12,
      "reading_order": 9,
      "bbox": [
        98,
        600,
        172,
        627
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_12_order_10",
          "label": "para",
          "text": "This book is a practical introduction to NLP. You will learn by example, write real\nprograms, and grasp the value of being able to test an idea through implementation. If\nyou haven't learned already, this book will teach you programming . Unlike other\nprogramming books, we provide extensive illustrations and exercises from NLP. The\napproach we have taken is also principled , in that we cover the theoretical underpin-\nnings and don't shy away from careful linguistic and computational analysis. We have\ntried to be pragmatic in striking a balance between theory and application, identifying\nthe connections and the tensions. Finally, we recognize that you won't get through this\nunless it is also pleasurable , so we have tried to include many applications and ex-\namples that are interesting and entertaining, and sometimes whimsical.",
          "level": -1,
          "page": 12,
          "reading_order": 10,
          "bbox": [
            97,
            635,
            586,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_12_order_11",
          "label": "foot",
          "text": "x | Preface",
          "level": -1,
          "page": 12,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            153,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_13_order_0",
          "label": "para",
          "text": "Note that this book is not a reference work. Its coverage of Python and NLP is selective,\nand presented in a tutorial style. For reference material, please consult the substantial\nquantity_of_searchable_resources available at http://python.org/ and http://www.nltk\n.org/.",
          "level": -1,
          "page": 13,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_13_order_1",
          "label": "para",
          "text": "This book is not an advanced computer science text. The content ranges from intro-\nductory to intermediate, and is directed at readers who want to learn how to analyze\ntext using Python and the Natural Language Toolkit. To learn about advanced algo-\nrithms implemented in NLTK, you can examine the Python code linked from http://\nwww.nltk.org/, and consult the other materials cited in this book.",
          "level": -1,
          "page": 13,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            585,
            228
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_13_order_2",
      "label": "sec",
      "text": "What You Will Learn",
      "level": 1,
      "page": 13,
      "reading_order": 2,
      "bbox": [
        97,
        250,
        261,
        277
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_13_order_3",
          "label": "para",
          "text": "By digging into the material presented here, you will learn:",
          "level": -1,
          "page": 13,
          "reading_order": 3,
          "bbox": [
            98,
            286,
            431,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_13_order_4",
          "label": "list_group",
          "text": "• How simple programs can help you manipulate and analyze language data, and\nhow to write these programs\n• How key concepts from NLP and linguistics are used to describe and analyze\nlanguage",
          "level": -1,
          "page": 13,
          "reading_order": 4,
          "bbox": [
            106,
            311,
            583,
            342
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "• How simple programs can help you manipulate and analyze language data, and\nhow to write these programs",
              "bbox": [
                106,
                311,
                583,
                342
              ],
              "page": 13,
              "reading_order": 4
            },
            {
              "label": "list",
              "text": "• How key concepts from NLP and linguistics are used to describe and analyze\nlanguage",
              "bbox": [
                106,
                348,
                585,
                379
              ],
              "page": 13,
              "reading_order": 5
            },
            {
              "label": "list",
              "text": "• How data structures and algorithms are used in NLP",
              "bbox": [
                106,
                385,
                423,
                403
              ],
              "page": 13,
              "reading_order": 6
            },
            {
              "label": "list",
              "text": "• How language data is stored in standard formats, and how data can be used to\nevaluate the performance of NLP techniques",
              "bbox": [
                106,
                403,
                584,
                439
              ],
              "page": 13,
              "reading_order": 7
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_13_order_8",
          "label": "para",
          "text": "Depending on your background, and your motivation for being interested in NLP, you\nwill gain different kinds of skills and knowledge from this book, as set out in Table P-1.",
          "level": -1,
          "page": 13,
          "reading_order": 8,
          "bbox": [
            97,
            448,
            584,
            478
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_13_order_9",
          "label": "table",
          "text": "Table P-1. Skills and knowledge to be gained from reading this book, depending on readers' goals and\nbackground [TABLE: <table><tr><td>Goals</td><td>Background in arts and humanities</td><td>Background in science and engineering</td></tr><tr><td>Language analysis</td><td>Manipulating large corpora, exploring linguistic models, and testing empirical claims.</td><td>Using techniques in data modeling, data mining, and knowledge discovery to analyze natural language.</td></tr><tr><td>Language technology</td><td>Building robustness of peerforming autistic tasks with technological applications.</td><td>Using linguistic algorithms and data structures in robust language processing software.</td></tr></table>]",
          "level": -1,
          "page": 13,
          "reading_order": 9,
          "bbox": [
            100,
            528,
            583,
            612
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Goals</td><td>Background in arts and humanities</td><td>Background in science and engineering</td></tr><tr><td>Language analysis</td><td>Manipulating large corpora, exploring linguistic models, and testing empirical claims.</td><td>Using techniques in data modeling, data mining, and knowledge discovery to analyze natural language.</td></tr><tr><td>Language technology</td><td>Building robustness of peerforming autistic tasks with technological applications.</td><td>Using linguistic algorithms and data structures in robust language processing software.</td></tr></table>",
              "bbox": [
                100,
                528,
                583,
                612
              ],
              "page": 13,
              "reading_order": 9
            },
            {
              "label": "cap",
              "text": "Table P-1. Skills and knowledge to be gained from reading this book, depending on readers' goals and\nbackground",
              "bbox": [
                97,
                492,
                585,
                520
              ],
              "page": 13,
              "reading_order": 10
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_13_order_11",
      "label": "sec",
      "text": "Organization",
      "level": 1,
      "page": 13,
      "reading_order": 11,
      "bbox": [
        97,
        636,
        200,
        663
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_13_order_12",
          "label": "para",
          "text": "The early chapters are organized in order of conceptual difficulty, starting with a prac-\ntical introduction to language processing that shows how to explore interesting bodies\nof text using tiny Python programs (Chapters 1 – 3 ). This is followed by a chapter on\nstructured programming ( Chapter 4 ) that consolidates the programming topics scat-\ntered across the preceding chapters. After this, the pace picks up, and we move on to\na series of chapters covering fundamental topics in language processing: tagging, clas-\nsification, and information extraction ( Chapters 5 – 7 ). The next three chapters look at",
          "level": -1,
          "page": 13,
          "reading_order": 12,
          "bbox": [
            97,
            671,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_13_order_13",
          "label": "foot",
          "text": "Preface | xi",
          "level": -1,
          "page": 13,
          "reading_order": 13,
          "bbox": [
            529,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_14_order_0",
          "label": "para",
          "text": "ways to parse a sentence, recognize its syntactic structure, and construct representa-\ntions of meaning (Chapters 8 – 10 ). The final chapter is devoted to linguistic data and\nhow it can be managed effectively ( Chapter 11 ). The book concludes with an After-\nword, briefly discussing the past and future of the field.",
          "level": -1,
          "page": 14,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_14_order_1",
          "label": "para",
          "text": "Within each chapter, we switch between different styles of presentation. In one style,\nnatural language is the driver. We analyze language, explore linguistic concepts, and\nuse programming examples to support the discussion. We often employ Python con-\nstructs that have not been introduced systematically, so you can see their purpose before\ndelving into the details of how and why they work. This is just like learning idiomatic\nexpressions in a foreign language: you’re able to buy a nice pastry without first having\nlearned the intricacies of question formation. In the other style of presentation, the\nprogramming language will be the driver. We’ll analyze programs, explore algorithms,\nand the linguistic examples will play a supporting role.",
          "level": -1,
          "page": 14,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            585,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_14_order_2",
          "label": "para",
          "text": "Each chapter ends with a series of graded exercises, which are useful for consolidating\nthe material. The exercises are graded according to the following scheme: $\\circ$ is for easy\nexercises that involve minor modifications to supplied code samples or other simple\nactivities; $\\bullet$ is for intermediate exercises that explore an aspect of the material in more\ndepth, requiring careful analysis and design; $\\bullet$ is for difficult, open-ended tasks that\nwill challenge your understanding of the material and force you to think independently\n(readers new to programming should skip these).",
          "level": -1,
          "page": 14,
          "reading_order": 2,
          "bbox": [
            97,
            304,
            585,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_14_order_3",
          "label": "para",
          "text": "Each chapter has a further reading section and an online “extras” section at http://www\n.nltk.org/, with pointers to more advanced materials and online resources. Online ver-\nsions of all the code examples are also available there.",
          "level": -1,
          "page": 14,
          "reading_order": 3,
          "bbox": [
            97,
            421,
            585,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_14_order_4",
      "label": "sec",
      "text": "Why Python?",
      "level": 1,
      "page": 14,
      "reading_order": 4,
      "bbox": [
        97,
        501,
        207,
        524
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_14_order_5",
          "label": "para",
          "text": "Python is a simple yet powerful programming language with excellent functionality for\nprocessing linguistic data. Python can be downloaded for free from http://www.python\n.org/. Installers are available for all platforms.",
          "level": -1,
          "page": 14,
          "reading_order": 5,
          "bbox": [
            97,
            528,
            585,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_14_order_6",
          "label": "para",
          "text": "Here is a five-line Python program that processes file.txt and prints all the words ending\nin ing:",
          "level": -1,
          "page": 14,
          "reading_order": 6,
          "bbox": [
            97,
            582,
            585,
            619
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_14_order_8",
          "label": "para",
          "text": "This program illustrates some of the main features of Python. First, whitespace is used\nto nest lines of code; thus the line starting with if falls inside the scope of the previous\nline starting with for ; this ensures that the ing test is performed for each word. Second,\nPython is object-oriented ; each variable is an entity that has certain defined attributes\nand methods. For example, the value of the variable line is more than a sequence of\ncharacters. It is a string object that has a “ method ” (or operation) called split() that",
          "level": -1,
          "page": 14,
          "reading_order": 8,
          "bbox": [
            97,
            689,
            586,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_14_order_9",
          "label": "foot",
          "text": "xii | Preface",
          "level": -1,
          "page": 14,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            155,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_15_order_0",
          "label": "para",
          "text": "we can use to break a line into its words. To apply a method to an object, we write the\nobject name, followed by a period, followed by the method name, i.e., line.split().\nThird, methods have arguments expressed inside parentheses. For instance, in the ex-\nample, word.endswith('ing') had the argument 'ing' to indicate that we wanted words\nending with ing and not something else. Finally—and most importantly—Python is\nhighly readable, so much so that it is fairly easy to guess what this program does even\nif you have never written a program before.",
          "level": -1,
          "page": 15,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_15_order_1",
          "label": "para",
          "text": "We chose Python because it has a shallow learning curve, its syntax and semantics are\ntransparent, and it has good string-handling functionality. As an interpreted language,\nPython facilitates interactive exploration. As an object-oriented language, Python per-\nmits data and methods to be encapsulated and re-used easily. As a dynamic language,\nPython permits attributes to be added to objects on the fly, and permits variables to be\ntyped dynamically, facilitating rapid development. Python comes with an extensive\nstandard library, including components for graphical programming, numerical pro-\ncessing, and web connectivity.",
          "level": -1,
          "page": 15,
          "reading_order": 1,
          "bbox": [
            97,
            197,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_15_order_2",
          "label": "para",
          "text": "Python is heavily used in industry, scientific research, and education around the world.\nPython is often praised for the way it facilitates productivity, quality, and main-\ntainability of software. A collection of Python success stories is posted at http://www\n.python.org/about/success/ .",
          "level": -1,
          "page": 15,
          "reading_order": 2,
          "bbox": [
            97,
            337,
            585,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_15_order_3",
          "label": "para",
          "text": "NLTK defines an infrastructure that can be used to build NLP programs in Python. It\nprovides basic classes for representing data relevant to natural language processing;\nstandard interfaces for performing tasks such as part-of-speech tagging, syntactic pars-\ning, and text classification; and standard implementations for each task that can be\ncombined to solve complex problems.",
          "level": -1,
          "page": 15,
          "reading_order": 3,
          "bbox": [
            97,
            410,
            585,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_15_order_4",
          "label": "para",
          "text": "NLTK comes with extensive documentation. In addition to this book, the website at\nhttp://www.nltk.org/ provides API documentation that covers every module, class, and\nfunction in the toolkit, specifying parameters and giving examples of usage. The website\nalso provides many HOWTOs with extensive examples and test cases, intended for\nusers, developers, and instructors.",
          "level": -1,
          "page": 15,
          "reading_order": 4,
          "bbox": [
            97,
            500,
            585,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_15_order_5",
      "label": "sec",
      "text": "Software Requirements",
      "level": 1,
      "page": 15,
      "reading_order": 5,
      "bbox": [
        97,
        607,
        288,
        630
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_15_order_7",
          "label": "sub_sec",
          "text": "Python",
          "level": 2,
          "page": 15,
          "reading_order": 7,
          "bbox": [
            98,
            680,
            137,
            692
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_15_order_8",
              "label": "para",
              "text": "The material presented in this book assumes that you are using Python version 2.4\nor 2.5. We are committed to porting NLTK to Python 3.0 once the libraries that\nNLTK depends on have been ported.",
              "level": -1,
              "page": 15,
              "reading_order": 8,
              "bbox": [
                122,
                696,
                585,
                743
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_15_order_9",
          "label": "sub_sec",
          "text": "NLTK",
          "level": 2,
          "page": 15,
          "reading_order": 9,
          "bbox": [
            100,
            751,
            133,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_15_order_10",
              "label": "para",
              "text": "The code examples in this book use NLTK version 2.0. Subsequent releases of\nNLTK will be backward-compatible.",
              "level": -1,
              "page": 15,
              "reading_order": 10,
              "bbox": [
                121,
                761,
                584,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_15_order_11",
              "label": "foot",
              "text": "Preface | xiii",
              "level": -1,
              "page": 15,
              "reading_order": 11,
              "bbox": [
                521,
                824,
                585,
                842
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_15_order_6",
          "label": "para",
          "text": "To get the most out of this book, you should install several free software packages.\nCurrent download pointers and instructions are available at http://www.nltk.org/.",
          "level": -1,
          "page": 15,
          "reading_order": 6,
          "bbox": [
            97,
            636,
            584,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_16_order_0",
      "label": "sec",
      "text": "NLTK-Date",
      "level": 1,
      "page": 16,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        163,
        89
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_16_order_1",
          "label": "para",
          "text": "This contains the linguistic corpora that are analyzed and processed in the book.",
          "level": -1,
          "page": 16,
          "reading_order": 1,
          "bbox": [
            118,
            89,
            583,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_16_order_2",
      "label": "sec",
      "text": "NumPy (recommended)",
      "level": 1,
      "page": 16,
      "reading_order": 2,
      "bbox": [
        97,
        107,
        234,
        126
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_16_order_3",
          "label": "para",
          "text": "This is a scientific computing library with support for multidimensional arrays and\nlinear algebra, required for certain probability, tagging, clustering, and classifica-\ntion tasks.",
          "level": -1,
          "page": 16,
          "reading_order": 3,
          "bbox": [
            121,
            126,
            585,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_16_order_4",
      "label": "sec",
      "text": "Matplotlib (recommended)",
      "level": 1,
      "page": 16,
      "reading_order": 4,
      "bbox": [
        97,
        179,
        245,
        197
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_16_order_5",
          "label": "para",
          "text": "This is a 2D plotting library for data visualization, and is used in some of the book’s\ncode samples that produce line graphs and bar charts.",
          "level": -1,
          "page": 16,
          "reading_order": 5,
          "bbox": [
            121,
            197,
            585,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_16_order_6",
      "label": "sec",
      "text": "NetworkX (optional",
      "level": 1,
      "page": 16,
      "reading_order": 6,
      "bbox": [
        97,
        232,
        210,
        250
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_16_order_7",
          "label": "para",
          "text": "This is a library for storing and manipulating network structures consisting of\nnodes and edges. For visualizing semantic networks, also install the Graphviz\nibrary.",
          "level": -1,
          "page": 16,
          "reading_order": 7,
          "bbox": [
            125,
            250,
            585,
            297
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_16_order_8",
      "label": "sec",
      "text": "Prover9 (optional)",
      "level": 1,
      "page": 16,
      "reading_order": 8,
      "bbox": [
        98,
        304,
        199,
        322
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_16_order_9",
          "label": "para",
          "text": "This is an automated theorem prover for first-order and equational logic, used to\nsupport inference in language processing.",
          "level": -1,
          "page": 16,
          "reading_order": 9,
          "bbox": [
            121,
            322,
            584,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_16_order_10",
      "label": "sec",
      "text": "Natural Language Toolkit (NLTK",
      "level": 1,
      "page": 16,
      "reading_order": 10,
      "bbox": [
        98,
        376,
        351,
        403
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_17_order_2",
          "label": "sub_sec",
          "text": "Simplicity",
          "level": 2,
          "page": 17,
          "reading_order": 2,
          "bbox": [
            97,
            159,
            153,
            173
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_17_order_3",
              "label": "para",
              "text": "To provide an intuitive framework along with substantial building blocks, giving\nusers a practical knowledge of NLP without getting bogged down in the tedious\nhouse-keeping usually associated with processing annotated language data",
              "level": -1,
              "page": 17,
              "reading_order": 3,
              "bbox": [
                121,
                173,
                585,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_17_order_4",
          "label": "sub_sec",
          "text": "Consistency",
          "level": 2,
          "page": 17,
          "reading_order": 4,
          "bbox": [
            98,
            230,
            163,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_17_order_5",
              "label": "para",
              "text": "To provide a uniform framework with consistent interfaces and data structures,\nand easily guessable method names",
              "level": -1,
              "page": 17,
              "reading_order": 5,
              "bbox": [
                121,
                241,
                584,
                277
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_17_order_6",
          "label": "sub_sec",
          "text": "Extensibility",
          "level": 2,
          "page": 17,
          "reading_order": 6,
          "bbox": [
            98,
            283,
            171,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_17_order_7",
              "label": "para",
              "text": "To provide a structure into which new software modules can be easily accommo-\ndated, including alternative implementations and competing approaches to the\nsame task",
              "level": -1,
              "page": 17,
              "reading_order": 7,
              "bbox": [
                121,
                295,
                585,
                349
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_17_order_8",
          "label": "sub_sec",
          "text": "Modularity",
          "level": 2,
          "page": 17,
          "reading_order": 8,
          "bbox": [
            97,
            349,
            162,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_17_order_9",
              "label": "para",
              "text": "To provide components that can be used independently without needing to un-\nderstand the rest of the toolkit",
              "level": -1,
              "page": 17,
              "reading_order": 9,
              "bbox": [
                121,
                367,
                584,
                403
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_17_order_10",
              "label": "para",
              "text": "Contrasting with these goals are three non-requirements—potentially useful qualities\nthat we have deliberately avoided. First, while the toolkit provides a wide range of\nfunctions, it is not encyclopedic; it is a toolkit, not a system, and it will continue to\nevolve with the field of NLP. Second, while the toolkit is efficient enough to support\nmeaningful tasks, it is not highly optimized for runtime performance; such optimiza-\ntions often involve more complex algorithms, or implementations in lower-level pro-\ngramming languages such as C or C++. This would make the software less readable\nand more difficult to install. Third, we have tried to avoid clever programming tricks,\nsince we believe that clear implementations are preferable to ingenious yet indecipher-\nable ones.",
              "level": -1,
              "page": 17,
              "reading_order": 10,
              "bbox": [
                97,
                411,
                586,
                573
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_16_order_11",
          "label": "para",
          "text": "NLTK was originally created in 2001 as part of a computational linguistics course in\nthe Department of Computer and Information Science at the University of Pennsylvania-\nnia. Since then it has been developed and expanded with the help of dozens of con-\ntributors. It has now been adopted in courses in dozens of universities, and serves as\nthe basis of many research projects. Table P-2 lists the most important NLTK modules.",
          "level": -1,
          "page": 16,
          "reading_order": 11,
          "bbox": [
            97,
            412,
            585,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_16_order_12",
          "label": "table",
          "text": "Table P-2. Language processing tasks and corresponding NLTK modules with examples of\nfunctionality [TABLE: <table><tr><td>Language processing task</td><td>NLTK modules</td><td>Functionality</td></tr><tr><td>Accessing corpora</td><td>nltk.corpus</td><td>Standardized interfaces to corpora and lexicons</td></tr><tr><td>String processing</td><td>nltk.tokenize, nltk.stem</td><td>Tokenizers, sentence tokenizers, stemmers</td></tr><tr><td>Collocation discovery</td><td>nltk.collocations</td><td>t-test, chi-squared, point-wise mutual information</td></tr><tr><td>Part-of-speech tagging</td><td>nltk.tag</td><td>n-gram, backoff, Brill, HMM, TnT</td></tr><tr><td>Classification</td><td>nltk.classify, nltk.cluster</td><td>Decision tree, maximum entropy, naive Bayes, EM, k-means</td></tr><tr><td>Chunking</td><td>nltk.chunk</td><td>Regular expression, n-gram, named entity</td></tr><tr><td>Parsing</td><td>nltk.parse</td><td>Chart, feature-based, unification, probabilistic, dependency</td></tr><tr><td>Semantic interpretation</td><td>nltk.sem, nltk.inference</td><td>Lambda calculus, first-order logic, model checking</td></tr><tr><td>Evaluation metrics</td><td>nltk.metrics</td><td>Precision, recall, agreement coefficients</td></tr><tr><td>Probability and estimation</td><td>nltk.probability</td><td>Frequency distributions, smoothed probability distributions</td></tr><tr><td>Applications</td><td>nltk.app, nltk.chat</td><td>Graphical concordancer, parsers, WordNet browser, chatbots</td></tr></table>]",
          "level": -1,
          "page": 16,
          "reading_order": 12,
          "bbox": [
            100,
            545,
            566,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Language processing task</td><td>NLTK modules</td><td>Functionality</td></tr><tr><td>Accessing corpora</td><td>nltk.corpus</td><td>Standardized interfaces to corpora and lexicons</td></tr><tr><td>String processing</td><td>nltk.tokenize, nltk.stem</td><td>Tokenizers, sentence tokenizers, stemmers</td></tr><tr><td>Collocation discovery</td><td>nltk.collocations</td><td>t-test, chi-squared, point-wise mutual information</td></tr><tr><td>Part-of-speech tagging</td><td>nltk.tag</td><td>n-gram, backoff, Brill, HMM, TnT</td></tr><tr><td>Classification</td><td>nltk.classify, nltk.cluster</td><td>Decision tree, maximum entropy, naive Bayes, EM, k-means</td></tr><tr><td>Chunking</td><td>nltk.chunk</td><td>Regular expression, n-gram, named entity</td></tr><tr><td>Parsing</td><td>nltk.parse</td><td>Chart, feature-based, unification, probabilistic, dependency</td></tr><tr><td>Semantic interpretation</td><td>nltk.sem, nltk.inference</td><td>Lambda calculus, first-order logic, model checking</td></tr><tr><td>Evaluation metrics</td><td>nltk.metrics</td><td>Precision, recall, agreement coefficients</td></tr><tr><td>Probability and estimation</td><td>nltk.probability</td><td>Frequency distributions, smoothed probability distributions</td></tr><tr><td>Applications</td><td>nltk.app, nltk.chat</td><td>Graphical concordancer, parsers, WordNet browser, chatbots</td></tr></table>",
              "bbox": [
                100,
                545,
                566,
                779
              ],
              "page": 16,
              "reading_order": 12
            },
            {
              "label": "cap",
              "text": "Table P-2. Language processing tasks and corresponding NLTK modules with examples of\nfunctionality",
              "bbox": [
                98,
                507,
                539,
                531
              ],
              "page": 16,
              "reading_order": 13
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_16_order_14",
          "label": "foot",
          "text": "xiv | Preface",
          "level": -1,
          "page": 16,
          "reading_order": 14,
          "bbox": [
            97,
            824,
            157,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_17_order_0",
          "label": "tab",
          "text": "<table><tr><td>Language processing task</td><td>NLTK modules</td><td>Functionality</td></tr><tr><td>Linguistic fieldwork</td><td>nltk.toolbox</td><td>Manipulate data in SIL Toolbox format</td></tr></table>",
          "level": -1,
          "page": 17,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            566,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_17_order_1",
          "label": "para",
          "text": "NLTK was designed with four primary goals in mind:",
          "level": -1,
          "page": 17,
          "reading_order": 1,
          "bbox": [
            98,
            134,
            404,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_17_order_11",
      "label": "sec",
      "text": "For Instructors",
      "level": 1,
      "page": 17,
      "reading_order": 11,
      "bbox": [
        98,
        600,
        213,
        619
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_17_order_12",
          "label": "para",
          "text": "Natural Language Processing is often taught within the confines of a single-semester\ncourse at the advanced undergraduate level or postgraduate level. Many instructors\nhave found that it is difficult to cover both the theoretical and practical sides of the\nsubject in such a short span of time. Some courses focus on theory to the exclusion of\npractical exercises, and deprive students of the challenge and excitement of writing\nprograms to automatically process language. Other courses are simply designed to\nteach programming for linguists, and do not manage to cover any significant NLP con-\ntent. NLTK was originally developed to address this problem, making it feasible to\ncover a substantial amount of theory and practice within a single-semester course, even\nif students have no prior programming experience.",
          "level": -1,
          "page": 17,
          "reading_order": 12,
          "bbox": [
            97,
            633,
            586,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_17_order_13",
          "label": "foot",
          "text": "Preface | xv",
          "level": -1,
          "page": 17,
          "reading_order": 13,
          "bbox": [
            527,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_18_order_0",
          "label": "para",
          "text": "A significant fraction of any NLP syllabus deals with algorithms and data structures.\nOn their own these can be rather dry, but NLTK brings them to life with the help of\ninteractive graphical user interfaces that make it possible to view algorithms step-by-\nstep. Most NLTK components include a demonstration that performs an interesting\ntask without requiring any special input from the user. An effective way to deliver the\nmaterials is through interactive presentation of the examples in this book, entering\nthem in a Python session, observing what they do, and modifying them to explore some\nempirical or theoretical issue.",
          "level": -1,
          "page": 18,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            586,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_18_order_1",
          "label": "para",
          "text": "This book contains hundreds of exercises that can be used as the basis for student\nassignments. The simplest exercises involve modifying a supplied program fragment in\na specified way in order to answer a concrete question. At the other end of the spectrum,\nNLTK provides a flexible framework for graduate-level research projects, with standard\nimplementations of all the basic data structures and algorithms, interfaces to dozens\nof widely used datasets (corpora), and a flexible and extensible architecture. Additional\nsupport for teaching using NLTK is available on the NLTK website.",
          "level": -1,
          "page": 18,
          "reading_order": 1,
          "bbox": [
            97,
            214,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_18_order_2",
          "label": "para",
          "text": "We believe this book is unique in providing a comprehensive framework for students\nto learn about NLP in the context of learning to program. What sets these materials\napart is the tight coupling of the chapters and exercises with NLTK, giving students —\neven those with no prior programming experience — a practical introduction to NLP.\nAfter completing these materials, students will be ready to attempt one of the more\nadvanced textbooks, such as Speech and Language Processing , by Jurafsky and Martin\n(Prentice Hall, 2008) .",
          "level": -1,
          "page": 18,
          "reading_order": 2,
          "bbox": [
            97,
            331,
            585,
            449
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_18_order_3",
          "label": "para",
          "text": "This book presents programming concepts in an unusual order, beginning with a non-\ntrivial data type—lists of strings—then introducing non-trivial control structures such\nas comprehensions and conditionals. These idioms permit us to do useful language\nprocessing from the start. Once this motivation is in place, we return to a systematic\npresentation of fundamental concepts such as strings, loops, files, and so forth. In this\nway, we cover the same ground as more conventional approaches, without expecting\nreaders to be interested in the programming language for its own sake.",
          "level": -1,
          "page": 18,
          "reading_order": 3,
          "bbox": [
            97,
            456,
            585,
            574
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_18_order_4",
          "label": "para",
          "text": "Two possible course plans are illustrated in Table P-3 . The first one presumes an arts/\nhumanities audience, whereas the second one presumes a science/engineering audi-\nence. Other course plans could cover the first five chapters, then devote the remaining\ntime to a single area, such as text classification (Chapters 6 and 7 ), syntax (Chapters\n8 and 9 ), semantics (Chapter 10 ), or linguistic data management (Chapter 11 ).",
          "level": -1,
          "page": 18,
          "reading_order": 4,
          "bbox": [
            97,
            582,
            585,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_18_order_5",
          "label": "table",
          "text": "Table P-3. Suggested course plans; approximate number of lectures per chapter [TABLE: <table><tr><td>Chapter</td><td>Arts and Humanities</td><td>Science and Engineering</td></tr><tr><td>Chapter 1, Language Processing and Python</td><td>2–4</td><td>2</td></tr><tr><td>Chapter 2, Accessing Text Corpora and Lexical Resources</td><td>2–4</td><td>2</td></tr><tr><td>Chapter 3, Processing Raw Text</td><td>2–4</td><td>2</td></tr><tr><td>Chapter 4, Writing Structured Programs</td><td>2–4</td><td>1–2</td></tr></table>]",
          "level": -1,
          "page": 18,
          "reading_order": 5,
          "bbox": [
            100,
            698,
            530,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Chapter</td><td>Arts and Humanities</td><td>Science and Engineering</td></tr><tr><td>Chapter 1, Language Processing and Python</td><td>2–4</td><td>2</td></tr><tr><td>Chapter 2, Accessing Text Corpora and Lexical Resources</td><td>2–4</td><td>2</td></tr><tr><td>Chapter 3, Processing Raw Text</td><td>2–4</td><td>2</td></tr><tr><td>Chapter 4, Writing Structured Programs</td><td>2–4</td><td>1–2</td></tr></table>",
              "bbox": [
                100,
                698,
                530,
                797
              ],
              "page": 18,
              "reading_order": 5
            },
            {
              "label": "cap",
              "text": "Table P-3. Suggested course plans; approximate number of lectures per chapter",
              "bbox": [
                99,
                677,
                485,
                690
              ],
              "page": 18,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_18_order_7",
          "label": "foot",
          "text": "xvi | Preface",
          "level": -1,
          "page": 18,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            157,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_19_order_0",
          "label": "tab",
          "text": "<table><tr><td>Chapter</td><td>Arts and Humanities</td><td>Science and Engineering</td></tr><tr><td>Chapter 5, Categorizing and Tagging Words</td><td>2–4</td><td>2–4</td></tr><tr><td>Chapter 6, Learning to Classify Text</td><td>0–2</td><td>2–4</td></tr><tr><td>Chapter 7, Extracting Information from Text</td><td>2</td><td>2–4</td></tr><tr><td>Chapter 8, Analyzing Sentence Structure</td><td>2–4</td><td>2–4</td></tr><tr><td>Chapter 9, Building Feature-Based Grammars</td><td>2–4</td><td>1–4</td></tr><tr><td>Chapter 10, Analyzing the Meaning of Sentences</td><td>1–2</td><td>1–4</td></tr><tr><td>Chapter 11, Managing Linguistic Data</td><td>1–2</td><td>1–4</td></tr><tr><td>Total</td><td>18–36</td><td>18–36</td></tr></table>",
          "level": -1,
          "page": 19,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            530,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_19_order_1",
      "label": "sec",
      "text": "Conventions Used in This Book",
      "level": 1,
      "page": 19,
      "reading_order": 1,
      "bbox": [
        97,
        277,
        342,
        297
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_19_order_3",
          "label": "sub_sec",
          "text": "Bold",
          "level": 2,
          "page": 19,
          "reading_order": 3,
          "bbox": [
            97,
            331,
            126,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_19_order_4",
              "label": "para",
              "text": "indicates new terms.",
              "level": -1,
              "page": 19,
              "reading_order": 4,
              "bbox": [
                126,
                349,
                243,
                367
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_19_order_5",
          "label": "sub_sec",
          "text": "Italic",
          "level": 2,
          "page": 19,
          "reading_order": 5,
          "bbox": [
            100,
            373,
            126,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_19_order_6",
              "label": "para",
              "text": "Jsed within paragraphs to refer to linguistic examples, the names of texts, and\nJRLs; also used for filenames and file extensions.",
              "level": -1,
              "page": 19,
              "reading_order": 6,
              "bbox": [
                126,
                385,
                584,
                421
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_19_order_7",
          "label": "sub_sec",
          "text": "Constant width",
          "level": 2,
          "page": 19,
          "reading_order": 7,
          "bbox": [
            97,
            428,
            184,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_19_order_8",
              "label": "para",
              "text": "Used for program listings, as well as within paragraphs to refer to program elements\nsuch as variable or function names, statements, and keywords; also used for pro-\ngram names.",
              "level": -1,
              "page": 19,
              "reading_order": 8,
              "bbox": [
                121,
                439,
                585,
                492
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_19_order_9",
          "label": "sub_sec",
          "text": "Constant width italic",
          "level": 2,
          "page": 19,
          "reading_order": 9,
          "bbox": [
            97,
            498,
            227,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_19_order_10",
              "label": "para",
              "text": "Shows text that should be replaced with user-supplied values or by values deter-\nmined by context; also used for metavariables within program code examples.",
              "level": -1,
              "page": 19,
              "reading_order": 10,
              "bbox": [
                122,
                510,
                584,
                546
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_19_order_11",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_019_figure_011.png)",
              "level": -1,
              "page": 19,
              "reading_order": 11,
              "bbox": [
                118,
                555,
                171,
                618
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_19_order_12",
              "label": "para",
              "text": "This icon signifies a tip, suggestion, or general note.",
              "level": -1,
              "page": 19,
              "reading_order": 12,
              "bbox": [
                171,
                573,
                440,
                586
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_19_order_13",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_019_figure_013.png)",
              "level": -1,
              "page": 19,
              "reading_order": 13,
              "bbox": [
                100,
                636,
                162,
                689
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_19_order_14",
              "label": "para",
              "text": "This icon indicates a warning or caution",
              "level": -1,
              "page": 19,
              "reading_order": 14,
              "bbox": [
                171,
                645,
                377,
                659
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_19_order_2",
          "label": "para",
          "text": "The following typographical conventions are used in this book:",
          "level": -1,
          "page": 19,
          "reading_order": 2,
          "bbox": [
            100,
            311,
            458,
            325
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_19_order_15",
      "label": "sec",
      "text": "Using Code Examples",
      "level": 1,
      "page": 19,
      "reading_order": 15,
      "bbox": [
        98,
        716,
        270,
        739
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_19_order_16",
          "label": "para",
          "text": "This book is here to help you get your job done. In general, you may use the code in\nthis book in your programs and documentation. You do not need to contact us for\npermission unless you’re reproducing a significant portion of the code. For example,",
          "level": -1,
          "page": 19,
          "reading_order": 16,
          "bbox": [
            97,
            743,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_19_order_17",
          "label": "foot",
          "text": "Preface | xvii",
          "level": -1,
          "page": 19,
          "reading_order": 17,
          "bbox": [
            521,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_0",
          "label": "para",
          "text": "writing a program that uses several chunks of code from this book does not require\npermission. Selling or distributing a CD-ROM of examples from O’Reilly books does\nrequire permission. Answering a question by citing this book and quoting example\ncode does not require permission. Incorporating a significant amount of example code\nfrom this book into your product’s documentation does require permission.",
          "level": -1,
          "page": 20,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_1",
          "label": "para",
          "text": "We appreciate, but do not require, attribution. An attribution usually includes the title,\nauthor, publisher, and ISBN. For example: “ Natural Language Processing with Py-\nthon, by Steven Bird, Ewan Klein, and Edward Loper. Copyright 2009 Steven Bird,\nEwan Klein, and Edward Loper, 978-0-596-51649-9.”",
          "level": -1,
          "page": 20,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            584,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_2",
          "label": "para",
          "text": "If you feel your use of code examples falls outside fair use or the permission given above,\nfeel free to contact us at permissions@oreilly.com.",
          "level": -1,
          "page": 20,
          "reading_order": 2,
          "bbox": [
            98,
            232,
            584,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_20_order_3",
      "label": "sec",
      "text": "Safari® Books Online",
      "level": 1,
      "page": 20,
      "reading_order": 3,
      "bbox": [
        97,
        295,
        261,
        313
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_20_order_4",
          "label": "sub_sec",
          "text": "Safari",
          "level": 2,
          "page": 20,
          "reading_order": 4,
          "bbox": [
            97,
            331,
            153,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_20_order_5",
              "label": "para",
              "text": "When you see a Safari® Books Online icon on the cover of your favorite\ntechnology book, that means the book is available online through the\nO’Reilly Network Safari Bookshelf.",
              "level": -1,
              "page": 20,
              "reading_order": 5,
              "bbox": [
                171,
                322,
                585,
                376
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_20_order_6",
              "label": "para",
              "text": "Safari offers a solution that’s better than e-books. It’s a virtual library that lets you easily\nsearch thousands of top tech books, cut and paste code samples, download chapters,\nand find quick answers when you need the most accurate, current information. Try it\nfor free at http://my.safaribooksonline.com.",
              "level": -1,
              "page": 20,
              "reading_order": 6,
              "bbox": [
                97,
                376,
                585,
                448
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": []
    },
    {
      "id": "page_20_order_7",
      "label": "sec",
      "text": "How to Contact Us",
      "level": 1,
      "page": 20,
      "reading_order": 7,
      "bbox": [
        98,
        473,
        243,
        492
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_20_order_8",
          "label": "para",
          "text": "Please address comments and questions concerning this book to the publisher:",
          "level": -1,
          "page": 20,
          "reading_order": 8,
          "bbox": [
            98,
            501,
            548,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_9",
          "label": "para",
          "text": "O'Reilly Media, Inc.",
          "level": -1,
          "page": 20,
          "reading_order": 9,
          "bbox": [
            118,
            519,
            243,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_10",
          "label": "para",
          "text": "1005 Gravenstein Highway North\nSebastopol, CA 95472",
          "level": -1,
          "page": 20,
          "reading_order": 10,
          "bbox": [
            118,
            546,
            315,
            575
          ],
          "section_number": "1005",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_12",
          "label": "para",
          "text": "800-998-9938 (in the United States or Canada)",
          "level": -1,
          "page": 20,
          "reading_order": 12,
          "bbox": [
            118,
            576,
            386,
            591
          ],
          "section_number": "800",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_13",
          "label": "para",
          "text": "707-829-0515 (international or local)",
          "level": -1,
          "page": 20,
          "reading_order": 13,
          "bbox": [
            122,
            591,
            333,
            609
          ],
          "section_number": "707",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_14",
          "label": "para",
          "text": "707-829-0104 (fax)",
          "level": -1,
          "page": 20,
          "reading_order": 14,
          "bbox": [
            118,
            609,
            234,
            627
          ],
          "section_number": "707",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_15",
          "label": "para",
          "text": "We have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at:",
          "level": -1,
          "page": 20,
          "reading_order": 15,
          "bbox": [
            97,
            634,
            583,
            665
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_16",
          "label": "para",
          "text": "http://www.oreilly.com/catalog/9780596516499",
          "level": -1,
          "page": 20,
          "reading_order": 16,
          "bbox": [
            125,
            672,
            388,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_20_order_17",
          "label": "foot",
          "text": "xviii | Preface",
          "level": -1,
          "page": 20,
          "reading_order": 17,
          "bbox": [
            97,
            824,
            162,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_0",
          "label": "para",
          "text": "The authors provide additional materials for each chapter via the NLTK website at:",
          "level": -1,
          "page": 21,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            574,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_1",
          "label": "para",
          "text": "http://www.nltk.org/",
          "level": -1,
          "page": 21,
          "reading_order": 1,
          "bbox": [
            118,
            98,
            243,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_2",
          "label": "para",
          "text": "To comment or ask technical questions about this book, send email to:",
          "level": -1,
          "page": 21,
          "reading_order": 2,
          "bbox": [
            100,
            122,
            503,
            136
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_3",
          "label": "para",
          "text": "bookquestions@oreilly.com",
          "level": -1,
          "page": 21,
          "reading_order": 3,
          "bbox": [
            118,
            143,
            272,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_4",
          "label": "para",
          "text": "For more information about our books, conferences, Resource Centers, and the\nO'Reilly Network, see our website at:",
          "level": -1,
          "page": 21,
          "reading_order": 4,
          "bbox": [
            97,
            168,
            585,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_5",
          "label": "para",
          "text": "http://www.oreilly.com",
          "level": -1,
          "page": 21,
          "reading_order": 5,
          "bbox": [
            118,
            206,
            252,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_21_order_6",
      "label": "sec",
      "text": "Acknowledgments",
      "level": 1,
      "page": 21,
      "reading_order": 6,
      "bbox": [
        97,
        249,
        246,
        272
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_21_order_7",
          "label": "para",
          "text": "The authors are indebted to the following people for feedback on earlier drafts of this\nbook: Doug Arnold, Michaela Atterer, Greg Aumann, Kenneth Beesley, Steven Bethard,\nOndrej Bojar, Chris Cieri, Robin Cooper, Grev Corbett, James Curran, Dan Garrette,\nJean Mark Gawron, Doug Hellmann, Nitin Indurkhya, Mark Liberman, Peter Ljunglöf,\nStefan Müller, Robin Munn, Joel Nothman, Adam Przepiorkowski, Brandon Rhodes,\nStuart Robinson, Jussi Salmela, Kyle Schlansker, Rob Speer, and Richard Sproat. We\nare thankful to many students and colleagues for their comments on the class materials\nthat evolved into these chapters, including participants at NLP and linguistics summer\nschools in Brazil, India, and the USA. This book would not exist without the members\nof the nltk-dev developer community, named on the NLTK website, who have given\nso freely of their time and expertise in building and extending NLTK.",
          "level": -1,
          "page": 21,
          "reading_order": 7,
          "bbox": [
            96,
            277,
            585,
            460
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_8",
          "label": "para",
          "text": "We are grateful to the U.S. National Science Foundation, the Linguistic Data Consor-\ntium, an Edward Clarence Dyason Fellowship, and the Universities of Pennsylvania,\nEdinburgh, and Melbourne for supporting our work on this book.",
          "level": -1,
          "page": 21,
          "reading_order": 8,
          "bbox": [
            97,
            465,
            585,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_9",
          "label": "para",
          "text": "We thank Julie Steele, Abby Fox, Loranah Dimant, and the rest of the O’Reilly team,\nfor organizing comprehensive reviews of our drafts from people across the NLP and\nPython communities, for cheerfully customizing O’Reilly’s production tools to accom-\nmodate our needs, and for meticulous copyediting work.",
          "level": -1,
          "page": 21,
          "reading_order": 9,
          "bbox": [
            97,
            519,
            585,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_10",
          "label": "para",
          "text": "Finally, we owe a huge debt of gratitude to our partners, Kay, Mimo, and Jee, for their\nlove, patience, and support over the many years that we worked on this book. We hope\nthat our children—Andrew, Alison, Kirsten, Leonie, and Maaike—catch our enthusi-\nasm for language and computation from these pages.",
          "level": -1,
          "page": 21,
          "reading_order": 10,
          "bbox": [
            97,
            599,
            585,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_21_order_11",
      "label": "sec",
      "text": "Royalties",
      "level": 1,
      "page": 21,
      "reading_order": 11,
      "bbox": [
        98,
        689,
        171,
        712
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_21_order_12",
          "label": "para",
          "text": "Royalties from the sale of this book are being used to support the development of the\nNatural Language Toolkit.",
          "level": -1,
          "page": 21,
          "reading_order": 12,
          "bbox": [
            98,
            716,
            585,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_21_order_13",
          "label": "foot",
          "text": "Preface | xix",
          "level": -1,
          "page": 21,
          "reading_order": 13,
          "bbox": [
            521,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_22_order_0",
          "label": "figure",
          "text": "Figure P-1. Edward Loper, Ewan Klein, and Steven Bird, Stanford, July 2005 [IMAGE: ![Figure](figures/NLTK_page_022_figure_000.png)]",
          "level": -1,
          "page": 22,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_022_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                367
              ],
              "page": 22,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure P-1. Edward Loper, Ewan Klein, and Steven Bird, Stanford, July 2005",
              "bbox": [
                97,
                376,
                469,
                394
              ],
              "page": 22,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_22_order_2",
          "label": "foot",
          "text": "xx | Preface",
          "level": -1,
          "page": 22,
          "reading_order": 2,
          "bbox": [
            97,
            824,
            155,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_23_order_0",
      "label": "sec",
      "text": "CHAPTER 1\n\nLanguage Processing and Python",
      "level": 1,
      "page": 23,
      "reading_order": 0,
      "bbox": [
        162,
        71,
        583,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_23_order_7",
          "label": "sub_sec",
          "text": "1.1 Computing with Language: Texts and Words",
          "level": 2,
          "page": 23,
          "reading_order": 7,
          "bbox": [
            98,
            698,
            485,
            726
          ],
          "section_number": "1.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_23_order_8",
              "label": "para",
              "text": "We’re all very familiar with text, since we read and write it every day. Here we will treat\ntext as raw data for the programs we write, programs that manipulate and analyze it in\na variety of interesting ways. But before we can do this, we have to get started with the\nPython interpreter.",
              "level": -1,
              "page": 23,
              "reading_order": 8,
              "bbox": [
                97,
                734,
                585,
                798
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_23_order_9",
              "label": "foot",
              "text": "1",
              "level": -1,
              "page": 23,
              "reading_order": 9,
              "bbox": [
                580,
                824,
                584,
                837
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_23_order_1",
          "label": "para",
          "text": "It is easy to get our hands on millions of words of text. What can we do with it, assuming\nwe can write some simple programs? In this chapter, we’ll address the following\nquestions:",
          "level": -1,
          "page": 23,
          "reading_order": 1,
          "bbox": [
            97,
            286,
            585,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_23_order_2",
          "label": "list_group",
          "text": "1. What can we achieve by combining simple programming techniques with large\nquantities of text?\n2. How can we automatically extract key words and phrases that sum up the style\nand content of a text?",
          "level": -1,
          "page": 23,
          "reading_order": 2,
          "bbox": [
            100,
            340,
            585,
            376
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "1. What can we achieve by combining simple programming techniques with large\nquantities of text?",
              "bbox": [
                100,
                340,
                585,
                376
              ],
              "page": 23,
              "reading_order": 2
            },
            {
              "label": "list",
              "text": "2. How can we automatically extract key words and phrases that sum up the style\nand content of a text?",
              "bbox": [
                100,
                376,
                585,
                412
              ],
              "page": 23,
              "reading_order": 3
            },
            {
              "label": "list",
              "text": "3. What tools and techniques does the Python programming language provide for\nsuch work?",
              "bbox": [
                100,
                412,
                585,
                448
              ],
              "page": 23,
              "reading_order": 4
            },
            {
              "label": "list",
              "text": "4. What are some of the interesting challenges of natural language processing?",
              "bbox": [
                100,
                456,
                557,
                474
              ],
              "page": 23,
              "reading_order": 5
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_23_order_6",
          "label": "para",
          "text": "This chapter is divided into sections that skip between two quite different styles. In the\n“ computing with language ” sections, we will take on some linguistically motivated\nprogramming tasks without necessarily explaining how they work. In the “ closer look\nat Python ” sections we will systematically review key programming concepts. We'll\nflag the two styles in the section titles, but later chapters will mix both styles without\nbeing so up-front about it. We hope this style of introduction gives you an authentic\ntaste of what will come later, while covering a range of elementary concepts in linguis-\ntics and computer science. If you have basic familiarity with both areas, you can skip\nto Section 1.5 ; we will repeat any important points in later chapters, and if you miss\nanything you can easily consult the online reference material at http://www.nltk.org/ . If\nthe material is completely new to you, this chapter will raise more questions than it\nanswers, questions that are addressed in the rest of this book.",
          "level": -1,
          "page": 23,
          "reading_order": 6,
          "bbox": [
            97,
            474,
            586,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_24_order_0",
      "label": "sec",
      "text": "Getting Started with Python",
      "level": 1,
      "page": 24,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        288,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_24_order_1",
          "label": "para",
          "text": "One of the friendly things about Python is that it allows you to type directly into the\ninteractive interpreter—the program that will be running your Python programs. You\ncan access the Python interpreter using a simple graphical interface called the In-\nteractive DeveLopment Environment (IDLE). On a Mac you can find this under Ap-\nplications→MacPython, and on Windows under All Programs→Python. Under Unix\nyou can run Python from the shell by typing idle (if this is not installed, try typing\npython). The interpreter will print a blurb about your Python version; simply check that\nyou are running Python 2.4 or 2.5 (here it is 2.5.1):",
          "level": -1,
          "page": 24,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            233
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_24_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_024_figure_003.png)",
          "level": -1,
          "page": 24,
          "reading_order": 3,
          "bbox": [
            109,
            304,
            171,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_24_order_4",
          "label": "para",
          "text": "If you are unable to run the Python interpreter, you probably don’t have\nPython installed correctly. Please visit http://python.org/ for detailed in-\nstructions.",
          "level": -1,
          "page": 24,
          "reading_order": 4,
          "bbox": [
            171,
            321,
            530,
            360
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_24_order_5",
          "label": "para",
          "text": "The >>> prompt indicates that the Python interpreter is now waiting for input. When\ncopying examples from this book, don’t type the “>>>” yourself. Now, let’s begin by\nusing Python as a calculator:",
          "level": -1,
          "page": 24,
          "reading_order": 5,
          "bbox": [
            97,
            385,
            585,
            441
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_24_order_7",
          "label": "para",
          "text": "Once the interpreter has finished calculating the answer and displaying it, the prompt\nreappears. This means the Python interpreter is waiting for another instruction.",
          "level": -1,
          "page": 24,
          "reading_order": 7,
          "bbox": [
            97,
            492,
            585,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_24_order_8",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_024_figure_008.png)",
          "level": -1,
          "page": 24,
          "reading_order": 8,
          "bbox": [
            109,
            537,
            171,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_24_order_9",
          "label": "para",
          "text": "Your Turn: Enter a few more expressions of your own. You can use\nasterisk (*) for multiplication and slash (/) for division, and parentheses\nfor bracketing expressions. Note that division doesn't always behave as\nyou might expect—it does integer division (with rounding of fractions\ndownwards) when you type 1/3 and “floating-point” (or decimal) divi-\nsion when you type 1.0/3.0. In order to get the expected behavior of\ndivision (standard in Python 3.0), you need to type: from __future__\nimport division.",
          "level": -1,
          "page": 24,
          "reading_order": 9,
          "bbox": [
            171,
            555,
            530,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_24_order_10",
          "label": "para",
          "text": "The preceding examples demonstrate how you can work interactively with the Python\ninterpreter, experimenting with various expressions in the language to see what they\ndo. Now let’s try a non-sensical expression to see how the interpreter handles it:",
          "level": -1,
          "page": 24,
          "reading_order": 10,
          "bbox": [
            97,
            689,
            585,
            744
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_24_order_11",
          "label": "foot",
          "text": "2 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 24,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            300,
            842
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_25_order_1",
          "label": "para",
          "text": "This produced a syntax error. In Python, it doesn’t make sense to end an instruction\nwith a plus sign. The Python interpreter indicates the line where the problem occurred\n(line 1 of <stdin>, which stands for “standard input”).",
          "level": -1,
          "page": 25,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            585,
            208
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_25_order_2",
          "label": "para",
          "text": "Now that we can use the Python interpreter, we're ready to start working with language\ndata.",
          "level": -1,
          "page": 25,
          "reading_order": 2,
          "bbox": [
            97,
            215,
            585,
            245
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_25_order_3",
      "label": "sec",
      "text": "Getting Started with NLTK",
      "level": 1,
      "page": 25,
      "reading_order": 3,
      "bbox": [
        97,
        259,
        272,
        286
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_25_order_4",
          "label": "para",
          "text": "Before going further you should install NLTK, downloadable for free from http://www\n.nltk.org/. Follow the instructions there to download the version required for your\nplatform.",
          "level": -1,
          "page": 25,
          "reading_order": 4,
          "bbox": [
            97,
            286,
            585,
            340
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_25_order_5",
          "label": "para",
          "text": "Once you’ve installed NLTK, start up the Python interpreter as before, and install the\ndata required for the book by typing the following two commands at the Python\nprompt, then selecting the book collection as shown in Figure 1-1 .",
          "level": -1,
          "page": 25,
          "reading_order": 5,
          "bbox": [
            97,
            348,
            585,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_25_order_7",
          "label": "figure",
          "text": "Figure 1-1. Downloading the NLTK Book Collection: Browse the available packages using\nnltk.download() . The Collections tab on the downloader shows how the packages are grouped into\nsets, and you should select the line labeled book to obtain all data required for the examples and\nexercises in this book. It consists of about 30 compressed files requiring about 100Mb disk space. The\nfull collection of data (i.e., all in the downloader) is about five times this size (at the time of writing)\nand continues to expand. [IMAGE: ![Figure](figures/NLTK_page_025_figure_007.png)]",
          "level": -1,
          "page": 25,
          "reading_order": 7,
          "bbox": [
            91,
            430,
            583,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_025_figure_007.png)",
              "bbox": [
                91,
                430,
                583,
                654
              ],
              "page": 25,
              "reading_order": 7
            },
            {
              "label": "cap",
              "text": "Figure 1-1. Downloading the NLTK Book Collection: Browse the available packages using\nnltk.download() . The Collections tab on the downloader shows how the packages are grouped into\nsets, and you should select the line labeled book to obtain all data required for the examples and\nexercises in this book. It consists of about 30 compressed files requiring about 100Mb disk space. The\nfull collection of data (i.e., all in the downloader) is about five times this size (at the time of writing)\nand continues to expand.",
              "bbox": [
                96,
                660,
                585,
                746
              ],
              "page": 25,
              "reading_order": 8
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_25_order_9",
          "label": "para",
          "text": "Once the data is downloaded to your machine, you can load some of it using the Python\ninterpreter. The first step is to type a special command at the Python prompt, which",
          "level": -1,
          "page": 25,
          "reading_order": 9,
          "bbox": [
            97,
            768,
            584,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_25_order_10",
          "label": "foot",
          "text": "1.1 Computing with Language: Texts and Words | 3",
          "level": -1,
          "page": 25,
          "reading_order": 10,
          "bbox": [
            368,
            824,
            584,
            842
          ],
          "section_number": "1.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_26_order_0",
          "label": "para",
          "text": "tells the interpreter to load some texts for us to explore: from nltk.book import *. This\nsays “from NLTK’s book module, load all items.” The book module contains all the data\nyou will need as you read this chapter. After printing a welcome message, it loads the\ntext of several books (this will take a few seconds). Here’s the command again, together\nwith the output that you will see. Take care to get spelling and punctuation right, and\nremember that you don’t type the >>>.",
          "level": -1,
          "page": 26,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_26_order_2",
          "label": "para",
          "text": "Any time we want to find out about these texts, we just have to enter their names at\nthe Python prompt:",
          "level": -1,
          "page": 26,
          "reading_order": 2,
          "bbox": [
            97,
            384,
            585,
            415
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_26_order_4",
          "label": "para",
          "text": "Now that we can use the Python interpreter, and have some data to work with, we're\nready to get started.",
          "level": -1,
          "page": 26,
          "reading_order": 4,
          "bbox": [
            98,
            492,
            585,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_26_order_5",
      "label": "sec",
      "text": "Searching Text",
      "level": 1,
      "page": 26,
      "reading_order": 5,
      "bbox": [
        97,
        544,
        198,
        564
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_26_order_6",
          "label": "para",
          "text": "There are many ways to examine the context of a text apart from simply reading it. A\nconcordance view shows us every occurrence of a given word, together with some\ncontext. Here we look up the word monstrous in Moby Dick by entering text1 followed\nby a period, then the term concordance, and then placing \"monstrous\" in parentheses:",
          "level": -1,
          "page": 26,
          "reading_order": 6,
          "bbox": [
            97,
            571,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_26_order_9",
          "label": "foot",
          "text": "4 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 26,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            300,
            842
          ],
          "section_number": "4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_27_order_0",
          "label": "para",
          "text": "ght have been rummaged out of this monstrous cabinet there is no telling . But\nof Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n>>>",
          "level": -1,
          "page": 27,
          "reading_order": 0,
          "bbox": [
            121,
            71,
            549,
            109
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_27_order_1",
          "label": "para",
          "text": "Your Turn: Try searching for other words; to save re-typing, you might\nbe able to use up-arrow, Ctrl-up-arrow, or Alt-p to access the previous\ncommand and modify the word being searched. You can also try search-\nes on some of the other texts we have included. For example, search\nSense and Sensibility for the word affection , using text2.concord\nance(\"affection\") . Search the book of Genesis to find out how long\nsome people lived, using: text3.concordance(\"lived\") . You could look\nat text4 , the Inaugural Address Corpus , to see examples of English going\nback to 1789, and search for words like nation , terror , god to see how\nthese words have been used differently over time. We've also included\ntext5 , the NPS Chat Corpus : search this for unconventional words like\nim , ur , lol . (Note that this corpus is uncensored!)",
          "level": -1,
          "page": 27,
          "reading_order": 1,
          "bbox": [
            171,
            140,
            530,
            314
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_27_order_2",
          "label": "para",
          "text": "Once you’ve spent a little while examining these texts, we hope you have a new sense\nof the richness and diversity of language. In the next chapter you will learn how to\naccess a broader range of text, including text in languages other than English.",
          "level": -1,
          "page": 27,
          "reading_order": 2,
          "bbox": [
            97,
            338,
            585,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_27_order_3",
          "label": "para",
          "text": "A concordance permits us to see words in context. For example, we saw that mon-\nstrous occurred in contexts such as the ___ pictures and the ___ size . What other words\nappear in a similar range of contexts? We can find out by appending the term\nsimilar to the name of the text in question, then inserting the relevant word in\nparentheses:",
          "level": -1,
          "page": 27,
          "reading_order": 3,
          "bbox": [
            97,
            394,
            585,
            475
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_27_order_5",
          "label": "para",
          "text": "Observe that we get different results for different texts. Austen uses this word quite\ndifferently from Melville; for her, monstrous has positive connotations, and sometimes\nfunctions as an intensifier like the word very.",
          "level": -1,
          "page": 27,
          "reading_order": 5,
          "bbox": [
            97,
            618,
            585,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_27_order_6",
          "label": "para",
          "text": "The term common_contexts allows us to examine just the contexts that are shared by\ntwo or more words, such as monstrous and very. We have to enclose these words by\nsquare brackets as well as parentheses, and separate them with a comma:",
          "level": -1,
          "page": 27,
          "reading_order": 6,
          "bbox": [
            97,
            679,
            585,
            726
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_27_order_8",
          "label": "foot",
          "text": "1.1 Computing with Language: Texts and Words | 5",
          "level": -1,
          "page": 27,
          "reading_order": 8,
          "bbox": [
            368,
            824,
            585,
            842
          ],
          "section_number": "1.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_28_order_0",
          "label": "figure",
          "text": "Figure 1-2. Lexical dispersion plot for words in U.S. Presidential Inaugural Addresses: This can be\nused to investigate changes in language use over time. [IMAGE: ![Figure](figures/NLTK_page_028_figure_000.png)]",
          "level": -1,
          "page": 28,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_028_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                295
              ],
              "page": 28,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 1-2. Lexical dispersion plot for words in U.S. Presidential Inaugural Addresses: This can be\nused to investigate changes in language use over time.",
              "bbox": [
                97,
                304,
                585,
                332
              ],
              "page": 28,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_28_order_2",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_028_figure_002.png)",
          "level": -1,
          "page": 28,
          "reading_order": 2,
          "bbox": [
            118,
            340,
            171,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_28_order_3",
          "label": "para",
          "text": "Your Turn: Pick another pair of words and compare their usage in two\ndifferent texts, using the similar() and common_contexts() functions.",
          "level": -1,
          "page": 28,
          "reading_order": 3,
          "bbox": [
            171,
            349,
            530,
            379
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_28_order_4",
          "label": "para",
          "text": "It is one thing to automatically detect that a particular word occurs in a text, and to\ndisplay some words that appear in the same context. However, we can also determine\nthe location of a word in the text: how many words from the beginning it appears. This\npositional information can be displayed using a dispersion plot. Each stripe represents\nan instance of a word, and each row represents the entire text. In Figure 1-2 we see\nsome striking patterns of word usage over the last 220 years (in an artificial text con-\nstructed by joining the texts of the Inaugural Address Corpus end-to-end). You can\nproduce this plot as shown below. You might like to try more words (e.g., liberty,\nconstitution) and different texts. Can you predict the dispersion of a word before you\nview it? As before, take care to get the quotes, commas, brackets, and parentheses\nexactly right.",
          "level": -1,
          "page": 28,
          "reading_order": 4,
          "bbox": [
            97,
            421,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_28_order_6",
          "label": "para",
          "text": "Important: You need to have Python’s NumPy and Matplotlib pack-\nages installed in order to produce the graphical plots used in this book.\nPlease see http://www.nltk.org/ for installation instructions.",
          "level": -1,
          "page": 28,
          "reading_order": 6,
          "bbox": [
            171,
            672,
            530,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_28_order_7",
          "label": "para",
          "text": "Now, just for fun, let’s try generating some random text in the various styles we have\njust seen. To do this, we type the name of the text followed by the term generate. (We\nneed to include the parentheses, but there’s nothing that goes between them.)",
          "level": -1,
          "page": 28,
          "reading_order": 7,
          "bbox": [
            97,
            743,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_28_order_8",
          "label": "foot",
          "text": "6 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 28,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            300,
            842
          ],
          "section_number": "6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_29_order_0",
          "label": "para",
          "text": ">> text3.generate(\nin the beginning of",
          "level": -1,
          "page": 29,
          "reading_order": 0,
          "bbox": [
            126,
            71,
            225,
            99
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_29_order_1",
          "label": "para",
          "text": "unto heaven ; and ye shall sow the land of Egypt there was no bread i\nall that he was taken out of the month , upon the earth . So shall th\nwages be ? And they made their father ; and Isaac was old , and kisse\nhim : and Laban with his cattle in the midst of the hands of Esau thy\nfirst born , and Phichol the chief butler unto his son Isaac , she\n>>>",
          "level": -1,
          "page": 29,
          "reading_order": 1,
          "bbox": [
            121,
            99,
            495,
            174
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_29_order_2",
          "label": "para",
          "text": "Note that the first time you run this command, it is slow because it gathers statistics\nabout word sequences. Each time you run it, you will get different output text. Now\ntry generating random text in the style of an inaugural address or an Internet chat room.\nAlthough the text is random, it reuses common words and phrases from the source text\nand gives us a sense of its style and content. (What is lacking in this randomly generated\ntext?)",
          "level": -1,
          "page": 29,
          "reading_order": 2,
          "bbox": [
            97,
            187,
            585,
            282
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_29_order_3",
          "label": "para",
          "text": "When generate produces its output, punctuation is split off from the\npreceding word. While this is not correct formatting for English text,\nwe do it to make clear that words and punctuation are independent of\none another. You will learn more about this in Chapter 3 .",
          "level": -1,
          "page": 29,
          "reading_order": 3,
          "bbox": [
            171,
            312,
            530,
            369
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_29_order_4",
      "label": "sec",
      "text": "Counting Vocabulary",
      "level": 1,
      "page": 29,
      "reading_order": 4,
      "bbox": [
        97,
        394,
        236,
        413
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_29_order_5",
          "label": "para",
          "text": "The most obvious fact about texts that emerges from the preceding examples is that\nthey differ in the vocabulary they use. In this section, we will see how to use the com-\nputer to count the words in a text in a variety of useful ways. As before, you will jump\nright in and experiment with the Python interpreter, even though you may not have\nstudied Python systematically yet. Test your understanding by modifying the examples,\nand trying the exercises at the end of the chapter.",
          "level": -1,
          "page": 29,
          "reading_order": 5,
          "bbox": [
            97,
            421,
            585,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_29_order_6",
          "label": "para",
          "text": "Let’\ns begin by finding out the length of a text from start to finish, in terms of the words\nand punctuation symbols that appear. We use the term len to get the length of some-\nthing, which we\n’ll apply here to the book of Genesis:",
          "level": -1,
          "page": 29,
          "reading_order": 6,
          "bbox": [
            97,
            528,
            585,
            575
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_29_order_8",
          "label": "para",
          "text": "So Genesis has 44,764 words and punctuation symbols, or “tokens.” A token is the\ntechnical name for a sequence of characters—such as hairy, his, or :)—that we want\nto treat as a group. When we count the number of tokens in a text, say, the phrase to\nbe or not to be, we are counting occurrences of these sequences. Thus, in our example\nphrase there are two occurrences of to, two of be, and one each of or and not. But there\nare only four distinct vocabulary items in this phrase. How many distinct words does\nthe book of Genesis contain? To work this out in Python, we have to pose the question\nslightly differently. The vocabulary of a text is just the set of tokens that it uses, since\nin a set, all duplicates are collapsed together. In Python we can obtain the vocabulary",
          "level": -1,
          "page": 29,
          "reading_order": 8,
          "bbox": [
            97,
            627,
            585,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_29_order_9",
          "label": "foot",
          "text": "1.1 Computing with Language: Texts and Words —7",
          "level": -1,
          "page": 29,
          "reading_order": 9,
          "bbox": [
            368,
            824,
            585,
            842
          ],
          "section_number": "1.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_30_order_0",
          "label": "para",
          "text": "items of text3 with the command: set(text3). When you do this, many screens of\nwords will fly past. Now try the following:",
          "level": -1,
          "page": 30,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_30_order_2",
          "label": "para",
          "text": "By wrapping sorted() around the Python expression set(text3) ❶ , we obtain a sorted\nlist of vocabulary items, beginning with various punctuation symbols and continuing\nwith words starting with A . All capitalized words precede lowercase words. We dis-\ncover the size of the vocabulary indirectly, by asking for the number of items in the set,\nand again we can use len to obtain this number ❷ . Although it has 44,764 tokens, this\nbook has only 2,789 distinct words, or “ word types. ” A word type is the form or\nspelling of the word independently of its specific occurrences in a text—that is, the\nword considered as a unique item of vocabulary. Our count of 2,789 items will include\npunctuation symbols, so we will generally call these unique items types instead of word\ntypes.",
          "level": -1,
          "page": 30,
          "reading_order": 2,
          "bbox": [
            97,
            214,
            585,
            377
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_30_order_3",
          "label": "para",
          "text": "Now, let’s calculate a measure of the lexical richness of the text. The next example\nshows us that each word is used 16 times on average (we need to make sure Python\nuses floating-point division):",
          "level": -1,
          "page": 30,
          "reading_order": 3,
          "bbox": [
            97,
            385,
            585,
            434
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_30_order_5",
          "label": "para",
          "text": "Next, let’s focus on particular words. We can count how often a word occurs in a text,\nand compute what percentage of the text is taken up by a specific word:",
          "level": -1,
          "page": 30,
          "reading_order": 5,
          "bbox": [
            97,
            501,
            584,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_30_order_7",
          "label": "para",
          "text": "Your Turn: How many times does the word lol appear in text5? How\nmuch is this as a percentage of the total number of words in this text?",
          "level": -1,
          "page": 30,
          "reading_order": 7,
          "bbox": [
            180,
            634,
            530,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_30_order_8",
          "label": "para",
          "text": "You may want to repeat such calculations on several texts, but it is tedious to keep\nretyping the formula. Instead, you can come up with your own name for a task, like\n“lexical _ diversity” or “percentage”, and associate it with a block of code. Now you\nonly have to type a short name instead of one or more complete lines of Python code,\nand you can reuse it as often as you like. The block of code that does a task for us is",
          "level": -1,
          "page": 30,
          "reading_order": 8,
          "bbox": [
            97,
            707,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_30_order_9",
          "label": "foot",
          "text": "8 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 30,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            300,
            842
          ],
          "section_number": "8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_31_order_0",
          "label": "para",
          "text": "called a function, and we define a short name for our function with the keyword def.\nThe next example shows how to define two new functions, lexical_diversity() and\npercentage():",
          "level": -1,
          "page": 31,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_31_order_2",
      "label": "sec",
      "text": "Caution!",
      "level": 1,
      "page": 31,
      "reading_order": 2,
      "bbox": [
        171,
        224,
        209,
        236
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_32_order_4",
          "label": "sub_sec",
          "text": "1.2 A Closer Look at Python: Texts as Lists of Words",
          "level": 2,
          "page": 32,
          "reading_order": 4,
          "bbox": [
            98,
            358,
            504,
            385
          ],
          "section_number": "1.2",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_32_order_6",
              "label": "sub_sub_sec",
              "text": "Lists",
              "level": 3,
              "page": 32,
              "reading_order": 6,
              "bbox": [
                98,
                430,
                127,
                456
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_32_order_7",
                  "label": "para",
                  "text": "What is a text? At one level, it is a sequence of symbols on a page such as this one. At\nanother level, it is a sequence of chapters, made up of a sequence of sections, where\neach section is a sequence of paragraphs, and so on. However, for our purposes, we\nwill think of a text as nothing more than a sequence of words and punctuation. Here's\nhow we represent text in Python, in this case the opening sentence of Moby Dick:",
                  "level": -1,
                  "page": 32,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    465,
                    585,
                    546
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_32_order_9",
                  "label": "para",
                  "text": "After the prompt we've given a name we made up, sent1, followed by the equals sign,\nand then some quoted words, separated with commas, and surrounded with brackets.\nThis bracketed material is known as a list in Python: it is how we store a text. We can\ninspect it by typing the name ❶ . We can ask for its length ❷ . We can even apply our\nown lexical_diversity() function to it ❸ .",
                  "level": -1,
                  "page": 32,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    582,
                    585,
                    672
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_32_order_11",
                  "label": "foot",
                  "text": "10 | Chapter 1: Language Processing and Python",
                  "level": -1,
                  "page": 32,
                  "reading_order": 11,
                  "bbox": [
                    97,
                    824,
                    306,
                    842
                  ],
                  "section_number": "10",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_0",
                  "label": "para",
                  "text": "Some more lists have been defined for you, one for the opening sentence of each of our\ntexts, sent2 ... sent9. We inspect two of them here; you can see the rest for yourself\nusing the Python interpreter (if you get an error saying that sent2 is not defined, you\nneed to first type from nltk.book import *).",
                  "level": -1,
                  "page": 33,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    586,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_2",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_033_figure_002.png)",
                  "level": -1,
                  "page": 33,
                  "reading_order": 2,
                  "bbox": [
                    118,
                    250,
                    164,
                    313
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_3",
                  "label": "para",
                  "text": "Your Turn: Make up a few sentences of your own, by typing a name,\nequals sign, and a list of words, like this: ex1 = ['Monty', 'Python',\n'and', 'the', 'Holy', 'Grail']. Repeat some of the other Python op-\nerations we saw earlier in Section 1.1, e.g., sorted(ex1), len(set(ex1)),\nex1.count('the').",
                  "level": -1,
                  "page": 33,
                  "reading_order": 3,
                  "bbox": [
                    171,
                    259,
                    530,
                    340
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_4",
                  "label": "para",
                  "text": "A pleasant surprise is that we can use Python's addition operator on lists. Adding two\nlists ❶ creates a new list with everything from the first list, followed by everything from\nthe second list:",
                  "level": -1,
                  "page": 33,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    358,
                    585,
                    405
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_6",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_033_figure_006.png)",
                  "level": -1,
                  "page": 33,
                  "reading_order": 6,
                  "bbox": [
                    118,
                    456,
                    171,
                    519
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_7",
                  "label": "para",
                  "text": "This special use of the addition operation is called concatenation; it\ncombines the lists together into a single list. We can concatenate sen-\ntences to build up a text.",
                  "level": -1,
                  "page": 33,
                  "reading_order": 7,
                  "bbox": [
                    171,
                    465,
                    530,
                    512
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_8",
                  "label": "para",
                  "text": "We don’t have to literally type the lists either; we can use short names that refer to pre-\ndefined lists.",
                  "level": -1,
                  "page": 33,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    537,
                    584,
                    573
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_10",
                  "label": "para",
                  "text": "What if we want to add a single item to a list? This is known as appending. When we\nappend() to a list, the list itself is updated as a result of the operation.",
                  "level": -1,
                  "page": 33,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    642,
                    585,
                    673
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_33_order_12",
                  "label": "foot",
                  "text": "1.2 A Closer Look at Python: Texts as Lists of Words | 11",
                  "level": -1,
                  "page": 33,
                  "reading_order": 12,
                  "bbox": [
                    350,
                    824,
                    584,
                    842
                  ],
                  "section_number": "1.2",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_32_order_5",
              "label": "para",
              "text": "You've seen some important elements of the Python programming language. Let’s take\na few moments to review them systematically.",
              "level": -1,
              "page": 32,
              "reading_order": 5,
              "bbox": [
                97,
                385,
                585,
                422
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_31_order_3",
          "label": "para",
          "text": "The Python interpreter changes the prompt from >>> to ... after en-\ncountering the colon at the end of the first line. The ... prompt indicates\nthat Python expects an indented code block to appear next. It is up to\nyou to do the indentation, by typing four spaces or hitting the Tab key.\nTo finish the indented block, just enter a blank line.",
          "level": -1,
          "page": 31,
          "reading_order": 3,
          "bbox": [
            171,
            241,
            530,
            316
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_31_order_4",
          "label": "para",
          "text": "In the definition of lexical diversity() ❶ , we specify a parameter labeled text . This\nparameter is a “ placeholder ” for the actual text whose lexical diversity we want to\ncompute, and reoccurs in the block of code that will run when the function is used, in\nline ❷ . Similarly, percentage() is defined to take two parameters, labeled count and\ntotal ❸ .",
          "level": -1,
          "page": 31,
          "reading_order": 4,
          "bbox": [
            97,
            340,
            585,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_31_order_5",
          "label": "para",
          "text": "Once Python knows that lexical_diversity() and percentage() are the names for spe-\ncific blocks of code, we can go ahead and use these functions:",
          "level": -1,
          "page": 31,
          "reading_order": 5,
          "bbox": [
            97,
            430,
            584,
            461
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_31_order_7",
          "label": "para",
          "text": "To recap, we use or call a function such as lexical_diversity() by typing its name,\nfollowed by an open parenthesis, the name of the text, and then a close parenthesis.\nThese parentheses will show up often; their role is to separate the name of a task—such\nas lexical_diversity()—from the data that the task is to be performed on—such as\ntext3. The data value that we place in the parentheses when we call a function is an\nargument to the function.",
          "level": -1,
          "page": 31,
          "reading_order": 7,
          "bbox": [
            97,
            591,
            585,
            692
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_31_order_8",
          "label": "para",
          "text": "You have already encountered several functions in this chapter, such as len(), set(),\nand sorted(). By convention, we will always add an empty pair of parentheses after a\nfunction name, as in len(), just to make clear that what we are talking about is a func-\ntion rather than some other kind of Python expression. Functions are an important\nconcept in programming, and we only mention them at the outset to give newcomers",
          "level": -1,
          "page": 31,
          "reading_order": 8,
          "bbox": [
            97,
            698,
            585,
            782
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_31_order_9",
          "label": "foot",
          "text": "1.1 Computing with Language: Texts and Words | 9",
          "level": -1,
          "page": 31,
          "reading_order": 9,
          "bbox": [
            368,
            824,
            585,
            842
          ],
          "section_number": "1.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_32_order_0",
          "label": "para",
          "text": "a sense of the power and creativity of programming. Don’t worry if you find it a bit\nconfusing right now.",
          "level": -1,
          "page": 32,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_32_order_1",
          "label": "para",
          "text": "Later we’ll see how to use functions when tabulating data, as in Table 1-1. Each row\nof the table will involve the same computation but with different data, and we’ll do this\nrepetitive work using a function.",
          "level": -1,
          "page": 32,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            162
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_32_order_2",
          "label": "table",
          "text": "Table 1-1. Lexical diversity of various genres in the Brown Corpus [TABLE: <table><tr><td>Genre</td><td>Tokens</td><td>Types</td><td>Lexical diversity</td></tr><tr><td>skill and hobbies</td><td>82345</td><td>11935</td><td>6.9</td></tr><tr><td>humor</td><td>21695</td><td>5017</td><td>4.3</td></tr><tr><td>fiction: science</td><td>14470</td><td>3233</td><td>4.5</td></tr><tr><td>press: reportage</td><td>100554</td><td>14394</td><td>7.0</td></tr><tr><td>fiction: romance</td><td>70022</td><td>8452</td><td>8.3</td></tr><tr><td>religion</td><td>39399</td><td>6373</td><td>6.2</td></tr></table>]",
          "level": -1,
          "page": 32,
          "reading_order": 2,
          "bbox": [
            100,
            197,
            333,
            332
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Genre</td><td>Tokens</td><td>Types</td><td>Lexical diversity</td></tr><tr><td>skill and hobbies</td><td>82345</td><td>11935</td><td>6.9</td></tr><tr><td>humor</td><td>21695</td><td>5017</td><td>4.3</td></tr><tr><td>fiction: science</td><td>14470</td><td>3233</td><td>4.5</td></tr><tr><td>press: reportage</td><td>100554</td><td>14394</td><td>7.0</td></tr><tr><td>fiction: romance</td><td>70022</td><td>8452</td><td>8.3</td></tr><tr><td>religion</td><td>39399</td><td>6373</td><td>6.2</td></tr></table>",
              "bbox": [
                100,
                197,
                333,
                332
              ],
              "page": 32,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Table 1-1. Lexical diversity of various genres in the Brown Corpus",
              "bbox": [
                99,
                170,
                422,
                189
              ],
              "page": 32,
              "reading_order": 3
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_34_order_0",
      "label": "sec",
      "text": "Indexing Lists",
      "level": 1,
      "page": 34,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        189,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_34_order_1",
          "label": "para",
          "text": "As we have seen, a text in Python is a list of words, represented using a combination\nof brackets and quotes. Just as with an ordinary page of text, we can count up the total\nnumber of words in text1 with len(text1), and count the occurrences in a text of a\nparticular word—say, heaven—using text1.count('heaven').",
          "level": -1,
          "page": 34,
          "reading_order": 1,
          "bbox": [
            97,
            103,
            584,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_34_order_2",
          "label": "para",
          "text": "With some patience, we can pick out the 1st, 173rd, or even 14,278th word in a printed\ntext. Analogously, we can identify the elements of a Python list by their order of oc-\ncurrence in the list. The number that represents this position is the item's index. We\ninstruct Python to show us the item that occurs at an index such as 173 in a text by\nwriting the name of the text followed by the index inside square brackets:",
          "level": -1,
          "page": 34,
          "reading_order": 2,
          "bbox": [
            97,
            177,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_34_order_4",
          "label": "para",
          "text": "We can do the converse; given a word, find the index of when it first occurs",
          "level": -1,
          "page": 34,
          "reading_order": 4,
          "bbox": [
            98,
            313,
            530,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_34_order_6",
          "label": "para",
          "text": "Indexes are a common way to access the words of a text, or, more generally, the ele-\nments of any list. Python permits us to access sublists as well, extracting manageable\npieces of language from large texts, a technique known as slicing.",
          "level": -1,
          "page": 34,
          "reading_order": 6,
          "bbox": [
            97,
            384,
            585,
            431
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_34_order_8",
          "label": "para",
          "text": "Indexes have some subtleties, and we’ll explore these with the help of an artificial\nsentence:",
          "level": -1,
          "page": 34,
          "reading_order": 8,
          "bbox": [
            97,
            564,
            584,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_34_order_10",
          "label": "para",
          "text": "Notice that our indexes start from zero: sent element zero, written sent[0], is the first\nword, 'word1', whereas sent element 9 is 'word10'. The reason is simple: the moment\nPython accesses the content of a list from the computer’s memory, it is already at the\nfirst element; we have to tell it how many elements forward to go. Thus, zero steps\nforward leaves it at the first element.",
          "level": -1,
          "page": 34,
          "reading_order": 10,
          "bbox": [
            97,
            704,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_34_order_11",
          "label": "foot",
          "text": "12 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 34,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            306,
            842
          ],
          "section_number": "12",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_35_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_035_figure_000.png)",
          "level": -1,
          "page": 35,
          "reading_order": 0,
          "bbox": [
            118,
            71,
            171,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_35_order_1",
          "label": "para",
          "text": "This practice of counting from zero is initially confusing, but typical of\nmodern programming languages. You'll quickly get the hang of it if\nyou've mastered the system of counting centuries where 19XY is a year\nin the 20th century, or if you live in a country where the floors of a\nbuilding are numbered from 1, and so walking up n-1 flights of stairs\ntakes you to level n.",
          "level": -1,
          "page": 35,
          "reading_order": 1,
          "bbox": [
            171,
            80,
            530,
            171
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_35_order_2",
          "label": "para",
          "text": "Now, if we accidentally use an index that is too large, we get an error:",
          "level": -1,
          "page": 35,
          "reading_order": 2,
          "bbox": [
            98,
            195,
            495,
            209
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_35_order_4",
          "label": "para",
          "text": "This time it is not a syntax error, because the program fragment is syntactically correct.\nInstead, it is a runtime error, and it produces a Traceback message that shows the\ncontext of the error, followed by the name of the error, IndexError, and a brief\nexplanation.",
          "level": -1,
          "page": 35,
          "reading_order": 4,
          "bbox": [
            97,
            286,
            586,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_35_order_5",
          "label": "para",
          "text": "Let’s take a closer look at slicing, using our artificial sentence again. Here we verify that\nthe slice 5:8 includes sent elements at indexes 5, 6, and 7:",
          "level": -1,
          "page": 35,
          "reading_order": 5,
          "bbox": [
            97,
            364,
            585,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_35_order_7",
          "label": "para",
          "text": "By convention, m:n means elements m...n-1. As the next example shows, we can omit\nthe first number if the slice begins at the start of the list ❶ , and we can omit the second\nnumber if the slice goes to the end ❷ :",
          "level": -1,
          "page": 35,
          "reading_order": 7,
          "bbox": [
            97,
            528,
            584,
            576
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_35_order_9",
          "label": "para",
          "text": "We can modify an element of a list by assigning to one of its index values. In the next\nexample, we put sent[0] on the left of the equals sign ❶ . We can also replace an entire\nslice with new material ❷ . A consequence of this last change is that the list only has\nfour elements, and accessing a later value generates an error ❸ .",
          "level": -1,
          "page": 35,
          "reading_order": 9,
          "bbox": [
            97,
            724,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_35_order_10",
          "label": "foot",
          "text": "1.2 A Closer Look at Python: Texts as Lists of Words | 13",
          "level": -1,
          "page": 35,
          "reading_order": 10,
          "bbox": [
            350,
            824,
            584,
            842
          ],
          "section_number": "1.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_36_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_036_figure_001.png)",
          "level": -1,
          "page": 36,
          "reading_order": 1,
          "bbox": [
            118,
            241,
            171,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_36_order_2",
          "label": "para",
          "text": "Your Turn: Take a few minutes to define a sentence of your own and\nmodify individual words and groups of words (slices) using the same\nmethods used earlier. Check your understanding by trying the exercises\non lists at the end of this chapter.",
          "level": -1,
          "page": 36,
          "reading_order": 2,
          "bbox": [
            171,
            257,
            530,
            314
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_36_order_3",
      "label": "sec",
      "text": "Variable",
      "level": 1,
      "page": 36,
      "reading_order": 3,
      "bbox": [
        99,
        339,
        153,
        358
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_37_order_6",
          "label": "sub_sec",
          "text": "Caution!",
          "level": 2,
          "page": 37,
          "reading_order": 6,
          "bbox": [
            171,
            509,
            209,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_37_order_7",
              "label": "para",
              "text": "Take care with your choice of names (or identifiers) for Python varia-\nbles. First, you should start the name with a letter, optionally followed\nby digits (0 to 9) or letters. Thus, abc23 is fine, but 23abc will cause a\nsyntax error. Names are case-sensitive, which means that myVar and\nmyvar are distinct variables. Variable names cannot contain whitespace,\nbut you can separate words using an underscore, e.g., my_var. Be careful\nnot to insert a hyphen instead of an underscore: my-var is wrong, since\nPython interprets the - as a minus sign.",
              "level": -1,
              "page": 37,
              "reading_order": 7,
              "bbox": [
                171,
                527,
                530,
                645
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_36_order_4",
          "label": "para",
          "text": "From the start of Section 1.1 , you have had access to texts called text1 , text2 , and so\non. It saved a lot of typing to be able to refer to a 250,000-word book with a short name\nlike this! In general, we can make up names for anything we care to calculate. We did\nthis ourselves in the previous sections, e.g., defining a variable sent1 , as follows:",
          "level": -1,
          "page": 36,
          "reading_order": 4,
          "bbox": [
            97,
            366,
            585,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_36_order_6",
          "label": "para",
          "text": "Such lines have the form: variable = expression . Python will evaluate the expression,\nand save its result to the variable. This process is called assignment . It does not gen-\nerate any output; you have to type the variable on a line of its own to inspect its contents.\nThe equals sign is slightly misleading, since information is moving from the right side\nto the left. It might help to think of it as a left-arrow. The name of the variable can be\nanything you like, e.g., my_sent , sentence , xyzzy . It must start with a letter, and can\ninclude numbers and underscores. Here are some examples of variables and\nassignments:",
          "level": -1,
          "page": 36,
          "reading_order": 6,
          "bbox": [
            97,
            473,
            585,
            603
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_36_order_8",
          "label": "para",
          "text": "Remember that capitalized words appear before lowercase words in sorted lists.",
          "level": -1,
          "page": 36,
          "reading_order": 8,
          "bbox": [
            98,
            734,
            557,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_36_order_9",
          "label": "foot",
          "text": "14 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 36,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            306,
            842
          ],
          "section_number": "14",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_37_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_037_figure_000.png)",
          "level": -1,
          "page": 37,
          "reading_order": 0,
          "bbox": [
            118,
            71,
            171,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_37_order_1",
          "label": "para",
          "text": "Notice in the previous example that we split the definition of my_sent\nover two lines. Python expressions can be split across multiple lines, so\nlong as this happens within any kind of brackets. Python uses the ...\nprompt to indicate that more input is expected. It doesn't matter how\nmuch indentation is used in these continuation lines, but some inden-\ntation usually makes them easier to read.",
          "level": -1,
          "page": 37,
          "reading_order": 1,
          "bbox": [
            171,
            80,
            530,
            171
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_37_order_2",
          "label": "para",
          "text": "It is good to choose meaningful variable names to remind you—and to help anyone\nelse who reads your Python code—what your code is meant to do. Python does not try\nto make sense of the names; it blindly follows your instructions, and does not object if\nyou do something confusing, such as one = 'two' or two = 3. The only restriction is\nthat a variable name cannot be any of Python’s reserved words, such as def, if, not,\nand import. If you use a reserved word, Python will produce a syntax error:",
          "level": -1,
          "page": 37,
          "reading_order": 2,
          "bbox": [
            97,
            195,
            586,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_37_order_4",
          "label": "para",
          "text": "We will often use variables to hold intermediate steps of a computation, especially\nwhen this makes the code easier to follow. Thus len(set(text1)) could also be written:",
          "level": -1,
          "page": 37,
          "reading_order": 4,
          "bbox": [
            97,
            385,
            585,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_37_order_8",
      "label": "sec",
      "text": "Strings",
      "level": 1,
      "page": 37,
      "reading_order": 8,
      "bbox": [
        97,
        663,
        144,
        689
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_38_order_6",
          "label": "sub_sec",
          "text": "1.3 Computing with Language: Simple Statistics",
          "level": 2,
          "page": 38,
          "reading_order": 6,
          "bbox": [
            98,
            439,
            485,
            467
          ],
          "section_number": "1.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_38_order_7",
              "label": "para",
              "text": "Let's return to our exploration of the ways we can bring our computational resources\nto bear on large quantities of text. We began this discussion in Section 1.1 , and saw\nhow to search for words in context, how to compile the vocabulary of a text, how to\ngenerate random text in the same style, and so on.",
              "level": -1,
              "page": 38,
              "reading_order": 7,
              "bbox": [
                97,
                474,
                585,
                540
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_38_order_8",
              "label": "para",
              "text": "In this section, we pick up the question of what makes a text distinct, and use automatic\nmethods to find characteristic words and expressions of a text. As in Section 1.1, you\ncan try new features of the Python language by copying them into the interpreter, and\nyou’ll learn about these features systematically in the following section.",
              "level": -1,
              "page": 38,
              "reading_order": 8,
              "bbox": [
                97,
                546,
                585,
                613
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_38_order_9",
              "label": "para",
              "text": "Before continuing further, you might like to check your understanding of the last sec-\ntion by predicting the output of the following code. You can use the interpreter to check\nwhether you got it right. If you’re not sure how to do this task, it would be a good idea\nto review the previous section before continuing further.",
              "level": -1,
              "page": 38,
              "reading_order": 9,
              "bbox": [
                97,
                618,
                585,
                686
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_38_order_11",
              "label": "foot",
              "text": "16 | Chapter 1: Language Processing and Python",
              "level": -1,
              "page": 38,
              "reading_order": 11,
              "bbox": [
                97,
                824,
                306,
                842
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_37_order_9",
          "label": "para",
          "text": "Some of the methods we used to access the elements of a list also work with individual\nwords, or strings. For example, we can assign a string to a variable ❶, index a string\n❷, and slice a string ❸.",
          "level": -1,
          "page": 37,
          "reading_order": 9,
          "bbox": [
            97,
            696,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_37_order_10",
          "label": "foot",
          "text": "1.2 A Closer Look at Python: Texts as Lists of Words | 15",
          "level": -1,
          "page": 37,
          "reading_order": 10,
          "bbox": [
            350,
            824,
            585,
            842
          ],
          "section_number": "1.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_38_order_1",
          "label": "para",
          "text": "We can also perform multiplication and addition with strings",
          "level": -1,
          "page": 38,
          "reading_order": 1,
          "bbox": [
            98,
            161,
            449,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_38_order_3",
          "label": "para",
          "text": "We can join the words of a list to make a single string, or split a string into a list, as\nfollows:",
          "level": -1,
          "page": 38,
          "reading_order": 3,
          "bbox": [
            98,
            250,
            585,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_38_order_5",
          "label": "para",
          "text": "We will come back to the topic of strings in Chapter 3. For the time being, we have\ntwo important building blocks—lists and strings—and are ready to get back to some\nlanguage analysis.",
          "level": -1,
          "page": 38,
          "reading_order": 5,
          "bbox": [
            97,
            367,
            585,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_39_order_0",
      "label": "sec",
      "text": "Frequency Distributions",
      "level": 1,
      "page": 39,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        255,
        95
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_39_order_1",
          "label": "para",
          "text": "How can we automatically identify the words of a text that are most informative about\nthe topic and genre of the text? Imagine how you might go about finding the 50 most\nfrequent words of a book. One method would be to keep a tally for each vocabulary\nitem, like that shown in Figure 1 - 3 . The tally would need thousands of rows, and it\nwould be an exceedingly laborious process—so laborious that we would rather assign\nthe task to a machine.",
          "level": -1,
          "page": 39,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_39_order_2",
          "label": "figure",
          "text": "Figure 1-3. Counting words appearing in a text (a frequency distribution). [IMAGE: ![Figure](figures/NLTK_page_039_figure_002.png)]",
          "level": -1,
          "page": 39,
          "reading_order": 2,
          "bbox": [
            100,
            206,
            583,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_039_figure_002.png)",
              "bbox": [
                100,
                206,
                583,
                394
              ],
              "page": 39,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Figure 1-3. Counting words appearing in a text (a frequency distribution).",
              "bbox": [
                97,
                402,
                458,
                415
              ],
              "page": 39,
              "reading_order": 3
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_39_order_4",
          "label": "para",
          "text": "The table in Figure 1 - 3 is known as a frequency distribution , and it tells us the\nfrequency of each vocabulary item in the text. (In general, it could count any kind of\nobservable event.) It is a “ distribution ” since it tells us how the total number of word\ntokens in the text are distributed across the vocabulary items. Since we often need\nfrequency distributions in language processing, NLTK provides built-in support for\nthem. Let's use a FreqDist to find the 50 most frequent words of Moby Dick . Try to\nwork out what is going on here, then read the explanation that follows.",
          "level": -1,
          "page": 39,
          "reading_order": 4,
          "bbox": [
            97,
            430,
            586,
            548
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_39_order_6",
          "label": "para",
          "text": "When we first invoke FreqDist , we pass the name of the text as an argument\n❶ . We\ncan inspect the total number of words (“outcomes”) that have been counted up\n❷ —\n260,819 in the case of Moby Dick . The expression keys() gives us a list of all the distinct\ntypes in the text\n❸ , and we can look at the first 50 of these by slicing the list\n❹ .",
          "level": -1,
          "page": 39,
          "reading_order": 6,
          "bbox": [
            97,
            734,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_39_order_7",
          "label": "foot",
          "text": "1.3 Computing with Language: Simple Statistics | 17",
          "level": -1,
          "page": 39,
          "reading_order": 7,
          "bbox": [
            359,
            824,
            585,
            842
          ],
          "section_number": "1.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_40_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_040_figure_000.png)",
          "level": -1,
          "page": 40,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            164,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_40_order_1",
          "label": "para",
          "text": "Your Turn: Try the preceding frequency distribution example for your-\nself, for text2. Be careful to use the correct parentheses and uppercase\nletters. If you get an error message NameError: name 'FreqDist' is not\ndefined, you need to start your work with from nltk.book import *.",
          "level": -1,
          "page": 40,
          "reading_order": 1,
          "bbox": [
            171,
            80,
            530,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_40_order_2",
          "label": "para",
          "text": "Do any words produced in the last example help us grasp the topic or genre of this text?\nOnly one word, whale, is slightly informative! It occurs over 900 times. The rest of the\nwords tell us nothing about the text; they’re just English “plumbing.” What proportion\nof the text is taken up with such words? We can generate a cumulative frequency plot\nfor these words, using fdist1.plot(50, cumulative=True), to produce the graph in\nFigure 1-4. These 50 words account for nearly half the book!",
          "level": -1,
          "page": 40,
          "reading_order": 2,
          "bbox": [
            97,
            161,
            585,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_40_order_3",
          "label": "figure",
          "text": "Figure 1-4. Cumulative frequency plot for the 50 most frequently used words in Moby Dick, which\naccount for nearly half of the tokens. [IMAGE: ![Figure](figures/NLTK_page_040_figure_003.png)]",
          "level": -1,
          "page": 40,
          "reading_order": 3,
          "bbox": [
            100,
            277,
            583,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_040_figure_003.png)",
              "bbox": [
                100,
                277,
                583,
                591
              ],
              "page": 40,
              "reading_order": 3
            },
            {
              "label": "cap",
              "text": "Figure 1-4. Cumulative frequency plot for the 50 most frequently used words in Moby Dick, which\naccount for nearly half of the tokens.",
              "bbox": [
                97,
                597,
                584,
                627
              ],
              "page": 40,
              "reading_order": 4
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_40_order_5",
          "label": "foot",
          "text": "18 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 40,
          "reading_order": 5,
          "bbox": [
            97,
            824,
            306,
            842
          ],
          "section_number": "18",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_41_order_0",
          "label": "para",
          "text": "If the frequent words don’t help us, how about the words that occur once only,\nthe so-called\nhapaxes?\nView them by typing\nfdist1.hapaxes(). This list contains\nlexicographer, cetological, contraband, expostulations, and about 9,000 others. It seems\nthat there are too many rare words, and without seeing the context we probably can’t\nguess what half of the hapaxes mean in any case! Since neither frequent nor infrequent\nwords help, we need to try something else.",
          "level": -1,
          "page": 41,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_41_order_1",
      "label": "sec",
      "text": "Fine-Grained Selection of Words",
      "level": 1,
      "page": 41,
      "reading_order": 1,
      "bbox": [
        98,
        188,
        310,
        206
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_41_order_2",
          "label": "para",
          "text": "Next, let's look at the long words of a text; perhaps these will be more characteristic\nand informative. For this we adapt some notation from set theory. We would like to\nfind the words from the vocabulary of the text that are more than 15 characters long.\nLet's call this property $P$ , so that $P(w)$ is true if and only if $w$ is more than 15 characters\nlong. Now we can express the words of interest using mathematical set notation as\nshown in (1a) . This means “ the set of all $w$ such that $w$ is an element of $V$ (the vocabu-\nlary) and $w$ has property $P$ . ”",
          "level": -1,
          "page": 41,
          "reading_order": 2,
          "bbox": [
            97,
            215,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_41_order_3",
          "label": "para",
          "text": "(1) a. $\\{w \\mid w \\in V \\& P(w)\\}$\nb. [w for w in V if p(w)",
          "level": -1,
          "page": 41,
          "reading_order": 3,
          "bbox": [
            118,
            340,
            297,
            378
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_41_order_4",
          "label": "para",
          "text": "The corresponding Python expression is given in (1b). (Note that it produces a list, not\na set, which means that duplicates are possible.) Observe how similar the two notations\nare. Let's go one more step and write executable Python code:",
          "level": -1,
          "page": 41,
          "reading_order": 4,
          "bbox": [
            100,
            393,
            585,
            440
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_41_order_6",
          "label": "para",
          "text": "For each word w in the vocabulary V, we check whether len(w) is greater than 15; all\nother words will be ignored. We will discuss this syntax more carefully later.",
          "level": -1,
          "page": 41,
          "reading_order": 6,
          "bbox": [
            97,
            582,
            584,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_41_order_7",
          "label": "para",
          "text": "Your Turn: Try out the previous statements in the Python interpreter,\nand experiment with changing the text and changing the length condi-\ntion. Does it make an difference to your results if you change the variable\nnames, e.g., using [word for word in vocab if ...]?",
          "level": -1,
          "page": 41,
          "reading_order": 7,
          "bbox": [
            171,
            645,
            530,
            703
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_41_order_8",
          "label": "foot",
          "text": "1.3 Computing with Language: Simple Statistics | 19",
          "level": -1,
          "page": 41,
          "reading_order": 8,
          "bbox": [
            359,
            824,
            585,
            842
          ],
          "section_number": "1.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_42_order_0",
          "label": "para",
          "text": "Let's return to our task of finding words that characterize a text. Notice that the long\nwords in text4 reflect its national focus— constitutionally, transcontinental — whereas\nthose\nin\ntext5\nreflect\nits\ninformal\ncontent:\nboooooooooooglyyyyyy\nand\nyuuuuuuuuuuummmmmmmmmmmmm . Have we succeeded in automatically extract-\ning words that typify a text? Well, these very long words are often hapaxes (i.e., unique)\nand perhaps it would be better to find frequently occurring long words. This seems\npromising since it eliminates frequent short words (e.g., the ) and infrequent long words\n(e.g., antiphilosophists ). Here are all words from the chat corpus that are longer than\nseven characters, that occur more than seven times:",
          "level": -1,
          "page": 42,
          "reading_order": 0,
          "bbox": [
            96,
            71,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_42_order_2",
          "label": "para",
          "text": "Notice how we have used two conditions: len(w) > 7 ensures that the words are longer\nthan seven letters, and fdist5[w] > 7 ensures that these words occur more than seven\ntimes. At last we have managed to automatically identify the frequently occurring con-\ntent-bearing words of the text. It is a modest but important milestone: a tiny piece of\ncode, processing tens of thousands of words, produces some informative output.",
          "level": -1,
          "page": 42,
          "reading_order": 2,
          "bbox": [
            97,
            330,
            586,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_42_order_3",
      "label": "sec",
      "text": "Collocations and Bigrams",
      "level": 1,
      "page": 42,
      "reading_order": 3,
      "bbox": [
        99,
        426,
        270,
        448
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_42_order_4",
          "label": "para",
          "text": "A collocation is a sequence of words that occur together unusually often. Thus red\nwine is a collocation, whereas the wine is not. A characteristic of collocations is that\nthey are resistant to substitution with words that have similar senses; for example,\nmaroon wine sounds very odd.",
          "level": -1,
          "page": 42,
          "reading_order": 4,
          "bbox": [
            97,
            454,
            584,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_42_order_5",
          "label": "para",
          "text": "To get a handle on collocations, we start off by extracting from a text a list of word\npairs, also known as bigrams. This is easily accomplished with the function bigrams():",
          "level": -1,
          "page": 42,
          "reading_order": 5,
          "bbox": [
            100,
            527,
            584,
            557
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_42_order_7",
          "label": "para",
          "text": "Here we see that the pair of words than-done is a bigram, and we write it in Python as\n('than', 'done') . Now, collocations are essentially just frequent bigrams, except that\nwe want to pay more attention to the cases that involve rare words. In particular, we\nwant to find bigrams that occur more often than we would expect based on the fre-\nquency of individual words. The collocations() function does this for us (we will see\nhow it works later):",
          "level": -1,
          "page": 42,
          "reading_order": 7,
          "bbox": [
            97,
            609,
            585,
            708
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_42_order_9",
          "label": "foot",
          "text": "20 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 42,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            306,
            842
          ],
          "section_number": "20",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_43_order_1",
          "label": "para",
          "text": "The collocations that emerge are very specific to the genre of the texts. In order to find\nred wine as a collocation, we would need to process a much larger body of text.",
          "level": -1,
          "page": 43,
          "reading_order": 1,
          "bbox": [
            100,
            197,
            585,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_43_order_2",
      "label": "sec",
      "text": "Counting Other Things",
      "level": 1,
      "page": 43,
      "reading_order": 2,
      "bbox": [
        99,
        247,
        246,
        268
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_44_order_4",
          "label": "sub_sec",
          "text": "1.4 Back to Python: Making Decisions and Taking Control",
          "level": 2,
          "page": 44,
          "reading_order": 4,
          "bbox": [
            98,
            474,
            557,
            501
          ],
          "section_number": "1.4",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_44_order_6",
              "label": "sub_sub_sec",
              "text": "Conditionals",
              "level": 3,
              "page": 44,
              "reading_order": 6,
              "bbox": [
                97,
                618,
                180,
                636
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_44_order_7",
                  "label": "para",
                  "text": "Python supports a wide range of operators, such as < and >=, for testing the relationship\nbetween values. The full set of these relational operators are shown in Table 1-3.",
                  "level": -1,
                  "page": 44,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    645,
                    585,
                    680
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_44_order_8",
                  "label": "table",
                  "text": "Table 1-3. Numerical comparison operators [TABLE: <table><tr><td>Operator</td><td>Relationship</td></tr><tr><td></td><td>Less than</td></tr><tr><td>=</td><td>Less than or equal to</td></tr><tr><td>Equal to (note this is two “=”signs, not one)</td></tr></table>]",
                  "level": -1,
                  "page": 44,
                  "reading_order": 8,
                  "bbox": [
                    100,
                    707,
                    324,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "tab",
                      "text": "<table><tr><td>Operator</td><td>Relationship</td></tr><tr><td></td><td>Less than</td></tr><tr><td>=</td><td>Less than or equal to</td></tr><tr><td>Equal to (note this is two “=”signs, not one)</td></tr></table>",
                      "bbox": [
                        100,
                        707,
                        324,
                        788
                      ],
                      "page": 44,
                      "reading_order": 8
                    },
                    {
                      "label": "cap",
                      "text": "Table 1-3. Numerical comparison operators",
                      "bbox": [
                        99,
                        689,
                        315,
                        707
                      ],
                      "page": 44,
                      "reading_order": 9
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_44_order_10",
                  "label": "foot",
                  "text": "22 | Chapter 1: Language Processing and Python",
                  "level": -1,
                  "page": 44,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    306,
                    842
                  ],
                  "section_number": "22",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_45_order_0",
                  "label": "tab",
                  "text": "<table><tr><td>Operator</td><td>Relationship</td></tr><tr><td>!=</td><td>Not equal to</td></tr><tr><td></td><td>Greater than</td></tr><tr><td>=</td><td>Greater than or equal to</td></tr></table>",
                  "level": -1,
                  "page": 45,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    324,
                    161
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_45_order_1",
                  "label": "para",
                  "text": "We can use these to select different words from a sentence of news text. Here are some\nexamples—notice only the operator is changed from one line to the next. They all use\nsent7, the first sentence from text7 (Wall Street Journal). As before, if you get an error\nsaying that sent7 is undefined, you need to first type: from nltk.book import *.",
                  "level": -1,
                  "page": 45,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    170,
                    585,
                    241
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_45_order_3",
                  "label": "para",
                  "text": "There is a common pattern to all of these examples: [w for w in text if condition],\nwhere condition is a Python “test” that yields either true or false. In the cases shown\nin the previous code example, the condition is always a numerical comparison. How-\never, we can also test various properties of words, using the functions listed in Table 1-4 .",
                  "level": -1,
                  "page": 45,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    421,
                    584,
                    488
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_45_order_4",
                  "label": "table",
                  "text": "Table 1-4. Some word comparison operators [TABLE: <table><tr><td>Function</td><td>Meaning</td></tr><tr><td>s.startswith(t)</td><td>Test if s starts with t</td></tr><tr><td>s.endswith(t)</td><td>Test if s ends with t</td></tr><tr><td>t in s</td><td>Test if t is contained inside s</td></tr><tr><td>s.islower()</td><td>Test if all cased characters in s are lowercase</td></tr><tr><td>s.isupper()</td><td>Test if all cased characters in s are uppercase</td></tr><tr><td>s.isalpha()</td><td>Test if all characters in s are alphabetic</td></tr><tr><td>s.isalnum()</td><td>Test if all characters in s are alphanumeric</td></tr><tr><td>s.isdigit()</td><td>Test if all characters in s are digits</td></tr><tr><td>s.istitle()</td><td>Test if s is titlecased (all words in s have initial capitals)</td></tr></table>]",
                  "level": -1,
                  "page": 45,
                  "reading_order": 4,
                  "bbox": [
                    100,
                    519,
                    422,
                    725
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "tab",
                      "text": "<table><tr><td>Function</td><td>Meaning</td></tr><tr><td>s.startswith(t)</td><td>Test if s starts with t</td></tr><tr><td>s.endswith(t)</td><td>Test if s ends with t</td></tr><tr><td>t in s</td><td>Test if t is contained inside s</td></tr><tr><td>s.islower()</td><td>Test if all cased characters in s are lowercase</td></tr><tr><td>s.isupper()</td><td>Test if all cased characters in s are uppercase</td></tr><tr><td>s.isalpha()</td><td>Test if all characters in s are alphabetic</td></tr><tr><td>s.isalnum()</td><td>Test if all characters in s are alphanumeric</td></tr><tr><td>s.isdigit()</td><td>Test if all characters in s are digits</td></tr><tr><td>s.istitle()</td><td>Test if s is titlecased (all words in s have initial capitals)</td></tr></table>",
                      "bbox": [
                        100,
                        519,
                        422,
                        725
                      ],
                      "page": 45,
                      "reading_order": 4
                    },
                    {
                      "label": "cap",
                      "text": "Table 1-4. Some word comparison operators",
                      "bbox": [
                        100,
                        501,
                        315,
                        515
                      ],
                      "page": 45,
                      "reading_order": 5
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_45_order_6",
                  "label": "para",
                  "text": "Here are some examples of these operators being used to select words from our texts:\nwords ending with -ableness; words containing gnt; words having an initial capital; and\nwords consisting entirely of digits.",
                  "level": -1,
                  "page": 45,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    741,
                    584,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_45_order_7",
                  "label": "foot",
                  "text": "1.4 Back to Python: Making Decisions and Taking Control | 23",
                  "level": -1,
                  "page": 45,
                  "reading_order": 7,
                  "bbox": [
                    324,
                    824,
                    584,
                    842
                  ],
                  "section_number": "1.4",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_46_order_1",
                  "label": "para",
                  "text": "We can also create more complex conditions. If $c$ is a condition, then not c is also a\ncondition. If we have two conditions $c_1$ and $c_2$ , then we can combine them to form a\nnew condition using conjunction and disjunction: $c_1$ and $c_2$ , $c_1$ or $c_2$ .",
                  "level": -1,
                  "page": 46,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    197,
                    584,
                    250
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_46_order_2",
                  "label": "para",
                  "text": "Your Turn: Run the following examples and try to explain what is going\non in each one. Next, try to make up some conditions of your own.",
                  "level": -1,
                  "page": 46,
                  "reading_order": 2,
                  "bbox": [
                    171,
                    276,
                    530,
                    304
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_44_order_5",
              "label": "para",
              "text": "So far, our little programs have had some interesting qualities: the ability to work with\nlanguage, and the potential to save human effort through automation. A key feature of\nprogramming is the ability of machines to make decisions on our behalf, executing\ninstructions when certain conditions are met, or repeatedly looping through text data\nuntil some condition is satisfied. This feature is known as control , and is the focus of\nthis section.",
              "level": -1,
              "page": 44,
              "reading_order": 5,
              "bbox": [
                97,
                501,
                586,
                601
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_43_order_3",
          "label": "para",
          "text": "Counting words is useful, but we can count other things too. For example, we can look\nat the distribution of word lengths in a text, by creating a FreqDist out of a long list of\nnumbers, where each number is the length of the corresponding word in the text:",
          "level": -1,
          "page": 43,
          "reading_order": 3,
          "bbox": [
            97,
            268,
            586,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_43_order_5",
          "label": "para",
          "text": "We start by deriving a list of the lengths of words in text1 ❶ , and the FreqDist then\ncounts the number of times each of these occurs ❷ . The result ❸ is a distribution\ncontaining a quarter of a million items, each of which is a number corresponding to a\nword token in the text. But there are only 20 distinct items being counted, the numbers\n1 through 20, because there are only 20 different word lengths. I.e., there are words\nconsisting of just 1 character, 2 characters, ..., 20 characters, but none with 21 or more\ncharacters. One might wonder how frequent the different lengths of words are (e.g.,\nhow many words of length 4 appear in the text, are there more words of length 5 than\nlength 4, etc.). We can do this as follows:",
          "level": -1,
          "page": 43,
          "reading_order": 5,
          "bbox": [
            97,
            439,
            585,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_43_order_7",
          "label": "para",
          "text": "From this we see that the most frequent word length is 3, and that words of length 3\naccount for roughly 50,000 (or 20 % ) of the words making up the book. Although we\nwill not pursue it here, further analysis of word length might help us understand",
          "level": -1,
          "page": 43,
          "reading_order": 7,
          "bbox": [
            97,
            750,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_43_order_8",
          "label": "foot",
          "text": "1.3 Computing with Language: Simple Statistics | 21",
          "level": -1,
          "page": 43,
          "reading_order": 8,
          "bbox": [
            359,
            824,
            584,
            842
          ],
          "section_number": "1.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_44_order_0",
          "label": "para",
          "text": "differences between authors, genres, or languages. Table 1-2 summarizes the functions\ndefined in frequency distributions.",
          "level": -1,
          "page": 44,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_44_order_1",
          "label": "table",
          "text": "Table 1-2. Functions defined for NLTK’s frequency distributions [TABLE: <table><tr><td>Example</td><td>Description</td></tr><tr><td>fdist = FreqDist(samples)</td><td>Create a frequency distribution containing the given samples</td></tr><tr><td>fdist.inc(sample)</td><td>Increment the count for this sample</td></tr><tr><td>fdist['monstrous']</td><td>Count of the number of times a given sample occurred</td></tr><tr><td>fdist.freq('monstrous')</td><td>Frequency of a given sample</td></tr><tr><td>fdist.N()</td><td>Total number of samples</td></tr><tr><td>fdist.keys()</td><td>The samples sorted in order of decreasing frequency</td></tr><tr><td>for sample in fdist:</td><td>Iterate over the samples, in order of decreasing frequency</td></tr><tr><td>fdist.max()</td><td>Sample with the greatest count</td></tr><tr><td>fdist.tabulate()</td><td>Tabulate the frequency distribution</td></tr><tr><td>fdist.plot()</td><td>Graphical plot of the frequency distribution</td></tr><tr><td>fdist.plot(cumulative=True)</td><td>Cumulative plot of the frequency distribution</td></tr><tr><td>fdist1 &lt; fdist2</td><td>Test if samples in fdist1 occur less frequently than in fdist2</td></tr></table>]",
          "level": -1,
          "page": 44,
          "reading_order": 1,
          "bbox": [
            100,
            134,
            521,
            396
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Example</td><td>Description</td></tr><tr><td>fdist = FreqDist(samples)</td><td>Create a frequency distribution containing the given samples</td></tr><tr><td>fdist.inc(sample)</td><td>Increment the count for this sample</td></tr><tr><td>fdist['monstrous']</td><td>Count of the number of times a given sample occurred</td></tr><tr><td>fdist.freq('monstrous')</td><td>Frequency of a given sample</td></tr><tr><td>fdist.N()</td><td>Total number of samples</td></tr><tr><td>fdist.keys()</td><td>The samples sorted in order of decreasing frequency</td></tr><tr><td>for sample in fdist:</td><td>Iterate over the samples, in order of decreasing frequency</td></tr><tr><td>fdist.max()</td><td>Sample with the greatest count</td></tr><tr><td>fdist.tabulate()</td><td>Tabulate the frequency distribution</td></tr><tr><td>fdist.plot()</td><td>Graphical plot of the frequency distribution</td></tr><tr><td>fdist.plot(cumulative=True)</td><td>Cumulative plot of the frequency distribution</td></tr><tr><td>fdist1 &lt; fdist2</td><td>Test if samples in fdist1 occur less frequently than in fdist2</td></tr></table>",
              "bbox": [
                100,
                134,
                521,
                396
              ],
              "page": 44,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Table 1-2. Functions defined for NLTK’s frequency distributions",
              "bbox": [
                99,
                116,
                413,
                134
              ],
              "page": 44,
              "reading_order": 2
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_44_order_3",
          "label": "para",
          "text": "Our discussion of frequency distributions has introduced some important Python con-\ncepts, and we will look at them systematically in Section 1.4 .",
          "level": -1,
          "page": 44,
          "reading_order": 3,
          "bbox": [
            97,
            419,
            585,
            449
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_46_order_4",
      "label": "sec",
      "text": "Operating on Every Element",
      "level": 1,
      "page": 46,
      "reading_order": 4,
      "bbox": [
        99,
        382,
        288,
        403
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_46_order_5",
          "label": "para",
          "text": "In Section 1.3 , we saw some examples of counting items other than words. Let's take\na closer look at the notation we used:",
          "level": -1,
          "page": 46,
          "reading_order": 5,
          "bbox": [
            97,
            410,
            585,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_46_order_7",
          "label": "para",
          "text": "These expressions have the form [f(w) for ...] or [w.f() for ...], where f is a\nfunction that operates on a word to compute its length, or to convert it to uppercase.\nFor now, you don’t need to understand the difference between the notations f(w) and\nw.f(). Instead, simply learn this Python idiom which performs the same operation on\nevery element of a list. In the preceding examples, it goes through each word in\ntext1, assigning each one in turn to the variable w and performing the specified oper-\nation on the variable.",
          "level": -1,
          "page": 46,
          "reading_order": 7,
          "bbox": [
            97,
            519,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_46_order_8",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_046_figure_008.png)",
          "level": -1,
          "page": 46,
          "reading_order": 8,
          "bbox": [
            118,
            654,
            171,
            708
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_46_order_9",
          "label": "para",
          "text": "The notation just described is called a “list comprehension.” This is our\nfirst example of a Python idiom, a fixed notation that we use habitually\nwithout bothering to analyze each time. Mastering such idioms is an\nimportant part of becoming a fluent Python programmer.",
          "level": -1,
          "page": 46,
          "reading_order": 9,
          "bbox": [
            171,
            663,
            530,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_46_order_10",
          "label": "para",
          "text": "Let's return to the question of vocabulary size, and apply the same idiom here:",
          "level": -1,
          "page": 46,
          "reading_order": 10,
          "bbox": [
            98,
            743,
            548,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_46_order_12",
          "label": "foot",
          "text": "24 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 46,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            306,
            842
          ],
          "section_number": "24",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_47_order_1",
          "label": "para",
          "text": "Now that we are not double-counting words like This and this, which differ only in\ncapitalization, we’ve wiped 2,000 off the vocabulary count! We can go a step further\nand eliminate numbers and punctuation from the vocabulary count by filtering out any\nnon-alphabetic items:",
          "level": -1,
          "page": 47,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_47_order_3",
          "label": "para",
          "text": "This example is slightly complicated: it lowercases all the purely alphabetic items. Per-\nhaps it would have been simpler just to count the lowercase-only items, but this gives\nthe wrong answer (why?).",
          "level": -1,
          "page": 47,
          "reading_order": 3,
          "bbox": [
            97,
            268,
            585,
            315
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_47_order_4",
          "label": "para",
          "text": "Don't worry if you don't feel confident with list comprehensions yet, since you’ll see\nmany more examples along with explanations in the following chapters.",
          "level": -1,
          "page": 47,
          "reading_order": 4,
          "bbox": [
            97,
            322,
            585,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_47_order_5",
      "label": "sec",
      "text": "Nested Code Blocks",
      "level": 1,
      "page": 47,
      "reading_order": 5,
      "bbox": [
        98,
        367,
        225,
        386
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_47_order_6",
          "label": "para",
          "text": "Most programming languages permit us to execute a block of code when a conditional\nexpression, or if statement, is satisfied. We already saw examples of conditional tests\nin code like [w for w in sent7 if len(w) < 4]. In the following program, we have\ncreated a variable called word containing the string value 'cat'. The if statement checks\nwhether the test len(word) < 5 is true. It is, so the body of the if statement is invoked\nand the print statement is executed, displaying a message to the user. Remember to\nindent the print statement by typing four spaces.",
          "level": -1,
          "page": 47,
          "reading_order": 6,
          "bbox": [
            97,
            394,
            585,
            512
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_47_order_8",
          "label": "para",
          "text": "When we use the Python interpreter we have to add an extra blank line 1 in order for\nit to detect that the nested block is complete.",
          "level": -1,
          "page": 47,
          "reading_order": 8,
          "bbox": [
            97,
            608,
            583,
            638
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_47_order_9",
          "label": "para",
          "text": "If we change the conditional test to len(word) >= 5, to check that the length of word is\ngreater than or equal to 5, then the test will no longer be true. This time, the body of\nthe if statement will not be executed, and no message is shown to the user:",
          "level": -1,
          "page": 47,
          "reading_order": 9,
          "bbox": [
            97,
            645,
            586,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_47_order_11",
          "label": "foot",
          "text": "1.4 Back to Python: Making Decisions and Taking Control | 25",
          "level": -1,
          "page": 47,
          "reading_order": 11,
          "bbox": [
            324,
            824,
            585,
            842
          ],
          "section_number": "1.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_48_order_0",
          "label": "para",
          "text": "An if statement is known as a control structure because it controls whether the code\nin the indented block will be run. Another control structure is the for loop. Try the\nfollowing, and remember to include the colon and the four spaces:",
          "level": -1,
          "page": 48,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_48_order_2",
          "label": "para",
          "text": "This is called a loop because Python executes the code in circular fashion. It starts by\nperforming the assignment word = 'Call', effectively using the word variable to name\nthe first item of the list. Then, it displays the value of word to the user. Next, it goes\nback to the for statement, and performs the assignment word = 'me' before displaying\nthis new value to the user, and so on. It continues in this fashion until every item of the\nlist has been processed.",
          "level": -1,
          "page": 48,
          "reading_order": 2,
          "bbox": [
            97,
            241,
            585,
            341
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_48_order_3",
      "label": "sec",
      "text": "Looping with Conditions",
      "level": 1,
      "page": 48,
      "reading_order": 3,
      "bbox": [
        100,
        356,
        261,
        376
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_49_order_4",
          "label": "sub_sec",
          "text": "1.5 Automatic Natural Language Understanding",
          "level": 2,
          "page": 49,
          "reading_order": 4,
          "bbox": [
            98,
            367,
            485,
            394
          ],
          "section_number": "1.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_49_order_5",
              "label": "para",
              "text": "We have been exploring language bottom-up, with the help of texts and the Python\nprogramming language. However, we're also interested in exploiting our knowledge of\nlanguage and computation by building useful language technologies. We’ll take the\nopportunity now to step back from the nitty-gritty of code in order to paint a bigger\npicture of natural language processing.",
              "level": -1,
              "page": 49,
              "reading_order": 5,
              "bbox": [
                97,
                402,
                586,
                483
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_49_order_6",
              "label": "para",
              "text": "At a purely practical level, we all need help to navigate the universe of information\nlocked up in text on the Web. Search engines have been crucial to the growth and\npopularity of the Web, but have some shortcomings. It takes skill, knowledge, and\nsome luck, to extract answers to such questions as: What tourist sites can I visit between\nPhiladelphia and Pittsburgh on a limited budget? What do experts say about digital SLR\ncameras? What predictions about the steel market were made by credible commentators\nin the past week? Getting a computer to answer them automatically involves a range of\nlanguage processing tasks, including information extraction, inference, and summari-\nzation, and would need to be carried out on a scale and with a level of robustness that\nis still beyond our current capabilities.",
              "level": -1,
              "page": 49,
              "reading_order": 6,
              "bbox": [
                97,
                491,
                586,
                655
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_49_order_7",
              "label": "para",
              "text": "On a more philosophical level, a long-standing challenge within artificial intelligence\nhas been to build intelligent machines, and a major part of intelligent behavior is un-\nderstanding language. For many years this goal has been seen as too difficult. However,\nas NLP technologies become more mature, and robust methods for analyzing unre-\nstricted text become more widespread, the prospect of natural language understanding\nhas re-emerged as a plausible goal.",
              "level": -1,
              "page": 49,
              "reading_order": 7,
              "bbox": [
                97,
                663,
                585,
                761
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_49_order_8",
              "label": "foot",
              "text": "1.5 Automatic Natural Language Understanding | 27",
              "level": -1,
              "page": 49,
              "reading_order": 8,
              "bbox": [
                359,
                824,
                585,
                842
              ],
              "section_number": "1.5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_50_order_0",
              "label": "para",
              "text": "In this section we describe some language understanding technologies, to give you a\nsense of the interesting challenges that are waiting for you.",
              "level": -1,
              "page": 50,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_48_order_4",
          "label": "para",
          "text": "Now we can combine the if and for statements. We will loop over every item of the\nlist, and print the item only if it ends with the letter l. We’ll pick another name for the\nvariable to demonstrate that Python doesn’t try to make sense of variable names.",
          "level": -1,
          "page": 48,
          "reading_order": 4,
          "bbox": [
            97,
            384,
            585,
            431
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_48_order_6",
          "label": "para",
          "text": "You will notice that if and for statements have a colon at the end of the line, before\nthe indentation begins. In fact, all Python control structures end with a colon. The\ncolon indicates that the current statement relates to the indented block that follows.",
          "level": -1,
          "page": 48,
          "reading_order": 6,
          "bbox": [
            97,
            546,
            585,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_48_order_7",
          "label": "para",
          "text": "We can also specify an action to be taken if the condition of the if statement is not\nmet. Here we see the elif (else if) statement, and the else statement. Notice that these\nalso have colons before the indented code.",
          "level": -1,
          "page": 48,
          "reading_order": 7,
          "bbox": [
            97,
            609,
            585,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_48_order_9",
          "label": "foot",
          "text": "26 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 48,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            306,
            842
          ],
          "section_number": "26",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_49_order_1",
          "label": "para",
          "text": "As you can see, even with this small amount of Python knowledge, you can start to\nbuild multiline Python programs. It’s important to develop such programs in pieces,\ntesting that each piece does what you expect before combining them into a program.\nThis is why the Python interactive interpreter is so invaluable, and why you should get\ncomfortable using it.",
          "level": -1,
          "page": 49,
          "reading_order": 1,
          "bbox": [
            97,
            116,
            585,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_49_order_2",
          "label": "para",
          "text": "Finally, let’s combine the idioms we’ve been exploring. First, we create a list of cie and\ncei words, then we loop over each item and print it. Notice the comma at the end of\nthe print statement, which tells Python to produce its output on a single line.",
          "level": -1,
          "page": 49,
          "reading_order": 2,
          "bbox": [
            97,
            206,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_50_order_1",
      "label": "sec",
      "text": "Word Sense Disambiguation",
      "level": 1,
      "page": 50,
      "reading_order": 1,
      "bbox": [
        97,
        116,
        288,
        140
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_50_order_2",
          "label": "para",
          "text": "In word sense disambiguation we want to work out which sense of a word was in-\ntended in a given context. Consider the ambiguous words serve and dish:",
          "level": -1,
          "page": 50,
          "reading_order": 2,
          "bbox": [
            97,
            143,
            584,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_3",
          "label": "para",
          "text": "(2) a. serve: help with food or drink; hold an office; put ball into play\nb. dish: plate; course of a meal; communications device",
          "level": -1,
          "page": 50,
          "reading_order": 3,
          "bbox": [
            118,
            188,
            530,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_4",
          "label": "para",
          "text": "In a sentence containing the phrase: he served the dish , you can detect that both serve\nand dish are being used with their food meanings. It's unlikely that the topic of discus-\nsion shifted from sports to crockery in the space of three words. This would force you\nto invent bizarre images, like a tennis pro taking out his frustrations on a china tea-set\nlaid out beside the court. In other words, we automatically disambiguate words using\ncontext, exploiting the simple fact that nearby words have closely related meanings. As\nanother example of this contextual effect, consider the word by , which has several\nmeanings, for example, the book by Chesterton (agentive—Chesterton was the author\nof the book); the cup by the stove (locative—the stove is where the cup is); and submit\nby Friday (temporal—Friday is the time of the submitting). Observe in (3) that the\nmeaning of the italicized word helps us interpret the meaning of by .",
          "level": -1,
          "page": 50,
          "reading_order": 4,
          "bbox": [
            97,
            241,
            585,
            423
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_5",
          "label": "para",
          "text": "(3) a. The lost children were found by the searchers (agentive)",
          "level": -1,
          "page": 50,
          "reading_order": 5,
          "bbox": [
            118,
            430,
            485,
            456
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_6",
          "label": "para",
          "text": "b. The lost children were found by the mountain (locative)",
          "level": -1,
          "page": 50,
          "reading_order": 6,
          "bbox": [
            144,
            456,
            485,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_7",
          "label": "para",
          "text": "c. The lost children were found by the afternoon (temporal)",
          "level": -1,
          "page": 50,
          "reading_order": 7,
          "bbox": [
            144,
            474,
            494,
            493
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_50_order_8",
      "label": "sec",
      "text": "Pronoun Resolution",
      "level": 1,
      "page": 50,
      "reading_order": 8,
      "bbox": [
        100,
        509,
        234,
        528
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_50_order_9",
          "label": "para",
          "text": "A deeper kind of language understanding is to work out “ who did what to whom,\n” i.e.,\nto detect the subjects and objects of verbs. You learned to do this in elementary school,\nbut it’s harder than you might think. In the sentence the thieves stole the paintings, it is\neasy to tell who performed the stealing action. Consider three possible following sen-\ntences in (4), and try to determine what was sold, caught, and found (one case is\nambiguous).",
          "level": -1,
          "page": 50,
          "reading_order": 9,
          "bbox": [
            97,
            537,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_10",
          "label": "para",
          "text": "(4) a. The thieves stole the paintings. They were subsequently sold",
          "level": -1,
          "page": 50,
          "reading_order": 10,
          "bbox": [
            118,
            645,
            512,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_11",
          "label": "para",
          "text": "b. The thieves stole the paintings. They were subsequently caught.",
          "level": -1,
          "page": 50,
          "reading_order": 11,
          "bbox": [
            144,
            663,
            530,
            683
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_12",
          "label": "para",
          "text": "c. The thieves stole the paintings. They were subsequently found.",
          "level": -1,
          "page": 50,
          "reading_order": 12,
          "bbox": [
            144,
            689,
            530,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_13",
          "label": "para",
          "text": "Answering this question involves finding the antecedent of the pronoun they , either\nthieves or paintings. Computational techniques for tackling this problem include ana-\nphora resolution—identifying what a pronoun or noun phrase refers to—and",
          "level": -1,
          "page": 50,
          "reading_order": 13,
          "bbox": [
            97,
            716,
            585,
            765
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_50_order_14",
          "label": "foot",
          "text": "28 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 50,
          "reading_order": 14,
          "bbox": [
            97,
            824,
            306,
            842
          ],
          "section_number": "28",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_0",
          "label": "para",
          "text": "semantic role labeling—identifying how a noun phrase relates to the verb (as agent,\npatient, instrument, and so on).",
          "level": -1,
          "page": 51,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_51_order_1",
      "label": "sec",
      "text": "Generating Language Output",
      "level": 1,
      "page": 51,
      "reading_order": 1,
      "bbox": [
        97,
        116,
        297,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_51_order_2",
          "label": "para",
          "text": "If we can automatically solve such problems of language understanding, we will be able\nto move on to tasks that involve generating language output, such as question\nanswering and machine translation. In the first case, a machine should be able to\nanswer a user's questions relating to collection of texts:",
          "level": -1,
          "page": 51,
          "reading_order": 2,
          "bbox": [
            97,
            143,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_3",
          "label": "para",
          "text": "(5) a. Text: ... The thieves stole the paintings. They were subsequently sold. ...",
          "level": -1,
          "page": 51,
          "reading_order": 3,
          "bbox": [
            118,
            224,
            583,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_4",
          "label": "para",
          "text": "b. Human: Who or what was sold?",
          "level": -1,
          "page": 51,
          "reading_order": 4,
          "bbox": [
            144,
            241,
            350,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_5",
          "label": "para",
          "text": "c. Machine: The paintings",
          "level": -1,
          "page": 51,
          "reading_order": 5,
          "bbox": [
            144,
            268,
            300,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_6",
          "label": "para",
          "text": "The machine’s answer demonstrates that it has correctly worked out that they refers to\npaintings and not to thieves. In the second case, the machine should be able to translate\nthe text into another language, accurately conveying the meaning of the original text.\nIn translating the example text into French, we are forced to choose the gender of the\npronoun in the second sentence: ils (masculine) if the thieves are sold, and elles (fem-\ninine) if the paintings are sold. Correct translation actually depends on correct under-\nstanding of the pronoun.",
          "level": -1,
          "page": 51,
          "reading_order": 6,
          "bbox": [
            97,
            295,
            585,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_7",
          "label": "para",
          "text": "(6) a. The thieves stole the paintings. They were subsequently found.",
          "level": -1,
          "page": 51,
          "reading_order": 7,
          "bbox": [
            118,
            421,
            530,
            440
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_8",
          "label": "para",
          "text": "b. Les voleurs ont volé les peintures. Ils ont été trouvés plus tard. (the thieves)",
          "level": -1,
          "page": 51,
          "reading_order": 8,
          "bbox": [
            144,
            440,
            584,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_9",
          "label": "para",
          "text": "c. Les voleurs ont volé les peintures. Elles ont été trouvées plus tard. (the\npaintings)",
          "level": -1,
          "page": 51,
          "reading_order": 9,
          "bbox": [
            144,
            465,
            567,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_10",
          "label": "para",
          "text": "In all of these examples, working out the sense of a word, the subject of a verb, and the\nantecedent of a pronoun are steps in establishing the meaning of a sentence, things we\nwould expect a language understanding system to be able to do.",
          "level": -1,
          "page": 51,
          "reading_order": 10,
          "bbox": [
            97,
            510,
            585,
            559
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_51_order_11",
      "label": "sec",
      "text": "Machine Translation",
      "level": 1,
      "page": 51,
      "reading_order": 11,
      "bbox": [
        97,
        573,
        234,
        591
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_51_order_12",
          "label": "para",
          "text": "For a long time now, machine translation (MT) has been the holy grail of language\nunderstanding, ultimately seeking to provide high-quality, idiomatic translation be-\ntween any pair of languages. Its roots go back to the early days of the Cold War, when\nthe promise of automatic translation led to substantial government sponsorship, and\nwith it, the genesis of NLP itself.",
          "level": -1,
          "page": 51,
          "reading_order": 12,
          "bbox": [
            97,
            600,
            585,
            683
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_13",
          "label": "para",
          "text": "Today, practical translation systems exist for particular pairs of languages, and some\nare integrated into web search engines. However, these systems have some serious\nshortcomings. We can explore them with the help of NLTK's “babelizer” (which is\nautomatically loaded when you import this chapter’s materials using from nltk.book\nimport *). This program submits a sentence for translation into a specified language,",
          "level": -1,
          "page": 51,
          "reading_order": 13,
          "bbox": [
            97,
            689,
            585,
            773
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_51_order_14",
          "label": "foot",
          "text": "1.5 Automatic Natural Language Understanding | 29",
          "level": -1,
          "page": 51,
          "reading_order": 14,
          "bbox": [
            359,
            824,
            585,
            842
          ],
          "section_number": "1.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_52_order_0",
          "label": "para",
          "text": "then submits the resulting sentence for translation back into English. It stops after 12\niterations, or if it receives a translation that was produced already (indicating a loop):",
          "level": -1,
          "page": 52,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_52_order_2",
          "label": "para",
          "text": "Observe that the system correctly translates Alice Springs from English to German (in\nthe line starting 1>), but on the way back to English, this ends up as Alice jump\n(line 2). The preposition before is initially translated into the corresponding German\npreposition vor , but later into the conjunction bevor (line 5). After line 5 the sentences\nbecome non-sensical (but notice the various phrasings indicated by the commas, and\nthe change from jump to leap ). The translation system did not recognize when a word\nwas part of a proper name, and it misinterpreted the grammatical structure. The gram-\nmatical problems are more obvious in the following example. Did John find the pig, or\ndid the pig find John?",
          "level": -1,
          "page": 52,
          "reading_order": 2,
          "bbox": [
            97,
            357,
            585,
            504
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_52_order_4",
          "label": "para",
          "text": "Machine translation is difficult because a given word could have several possible trans-\nlations (depending on its meaning), and because word order must be changed in keep-\ning with the grammatical structure of the target language. Today these difficulties are\nbeing faced by collecting massive quantities of parallel texts from news and government\nwebsites that publish documents in two or more languages. Given a document in Ger-\nman and English, and possibly a bilingual dictionary, we can automatically pair up the\nsentences, a process called text alignment. Once we have a million or more sentence\npairs, we can detect corresponding words and phrases, and build a model that can be\nused for translating new text.",
          "level": -1,
          "page": 52,
          "reading_order": 4,
          "bbox": [
            97,
            609,
            585,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_52_order_5",
          "label": "foot",
          "text": "30 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 52,
          "reading_order": 5,
          "bbox": [
            97,
            824,
            300,
            842
          ],
          "section_number": "30",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_53_order_0",
      "label": "sec",
      "text": "Spoken Dialogue Systems",
      "level": 1,
      "page": 53,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        270,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_53_order_1",
          "label": "para",
          "text": "In the history of artificial intelligence, the chief measure of intelligence has been a lin-\nguistic one, namely the Turing Test: can a dialogue system, responding to a user’s text\ninput, perform so naturally that we cannot distinguish it from a human-generated re-\nsponse? In contrast, today’s commercial dialogue systems are very limited, but still\nperform useful functions in narrowly defined domains, as we see here:",
          "level": -1,
          "page": 53,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_2",
          "label": "para",
          "text": "S: How may I help you?",
          "level": -1,
          "page": 53,
          "reading_order": 2,
          "bbox": [
            122,
            193,
            261,
            207
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_3",
          "label": "para",
          "text": "U: When is Saving Private Ryan playing?",
          "level": -1,
          "page": 53,
          "reading_order": 3,
          "bbox": [
            118,
            207,
            353,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_4",
          "label": "para",
          "text": "S: For what theater?",
          "level": -1,
          "page": 53,
          "reading_order": 4,
          "bbox": [
            122,
            224,
            236,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_5",
          "label": "para",
          "text": "U: The Paramount theater",
          "level": -1,
          "page": 53,
          "reading_order": 5,
          "bbox": [
            118,
            241,
            272,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_6",
          "label": "para",
          "text": "S: Saving Private Ryan is not playing at the Paramount theater, but\nit’s playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30.",
          "level": -1,
          "page": 53,
          "reading_order": 6,
          "bbox": [
            122,
            259,
            503,
            290
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_7",
          "label": "para",
          "text": "1980年\n,\n他在1982年\n,\n他在1983年\n,\n他在1984年\n,\n他在1985年\n,\n他在1986年\n,\n他在1987年\n,\n他在1988年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989年\n,\n他在1989\n,\n他在1989年\n,\n他在1989\n,\n他在1989\n,\n他在1989\n,\n他在1989\n,\n他在1989\n,\n他在19898年\n,\n他在1989\n,\n他在19898年\n,\n他在19898\n,\n他在19898\n,\n他在19898年\n,\n他在19898\n,\n他在19898\n,\n他在19898\n,\n他在19898\n,\n他在19898\n,\n他在1989898\n,\n他在1989898",
          "level": -1,
          "page": 53,
          "reading_order": 7,
          "bbox": [
            125,
            290,
            494,
            295
          ],
          "section_number": "1980",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_8",
          "label": "para",
          "text": "You could not ask this system to provide driving instructions or details of nearby res-\ntaurants unless the required information had already been stored and suitable question-\nanswer pairs had been incorporated into the language processing system.",
          "level": -1,
          "page": 53,
          "reading_order": 8,
          "bbox": [
            97,
            295,
            584,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_9",
          "label": "para",
          "text": "Observe that this system seems to understand the user's goals: the user asks when a\nmovie is showing and the system correctly determines from this that the user wants to\nsee the movie. This inference seems so obvious that you probably didn't notice it was\nmade, yet a natural language system needs to be endowed with this capability in order\nto interact naturally. Without it, when asked, Do you know when Saving Private Ryan\nis playing? , a system might unhelpfully respond with a cold Yes . However, the devel-\nopers of commercial dialogue systems use contextual assumptions and business logic\nto ensure that the different ways in which a user might express requests or provide\ninformation are handled in a way that makes sense for the particular application. So,\nif you type When is ..., or I want to know when ..., or Can you tell me when ..., simple\nrules will always yield screening times. This is enough for the system to provide a useful\nservice.",
          "level": -1,
          "page": 53,
          "reading_order": 9,
          "bbox": [
            97,
            356,
            585,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_10",
          "label": "para",
          "text": "Dialogue systems give us an opportunity to mention the commonly assumed pipeline\nfor NLP. Figure 1 - 5 shows the architecture of a simple dialogue system. Along the top\nof the diagram, moving from left to right, is a “ pipeline ” of some language understand-\ning components . These map from speech input via syntactic parsing to some kind of\nmeaning representation. Along the middle, moving from right to left, is the reverse\npipeline of components for converting concepts to speech. These components make\nup the dynamic aspects of the system. At the bottom of the diagram are some repre-\nsentative bodies of static information: the repositories of language-related data that the\nprocessing components draw on to do their work.",
          "level": -1,
          "page": 53,
          "reading_order": 10,
          "bbox": [
            97,
            555,
            586,
            708
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_11",
          "label": "para",
          "text": "Your Turn: For an example of a primitive dialogue system, try having\na conversation with an NLTK chatbot. To see the available chatbots,\nrun nltk.chat.chatbots(). (Remember to import nltk first.)",
          "level": -1,
          "page": 53,
          "reading_order": 11,
          "bbox": [
            173,
            743,
            530,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_53_order_12",
          "label": "foot",
          "text": "1.5 Automatic Natural Language Understanding | 31",
          "level": -1,
          "page": 53,
          "reading_order": 12,
          "bbox": [
            359,
            824,
            584,
            842
          ],
          "section_number": "1.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_54_order_0",
          "label": "figure",
          "text": "Figure 1-5. Simple pipeline architecture for a spoken dialogue system: Spoken input (top left) is\nanalyzed, words are recognized, sentences are parsed and interpreted in context, application-specific\nactions take place (top right); a response is planned, realized as a syntactic structure, then to suitably\ninflected words, and finally to spoken output; different types of linguistic knowledge inform each stage\nof the process. [IMAGE: ![Figure](figures/NLTK_page_054_figure_000.png)]",
          "level": -1,
          "page": 54,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_054_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                331
              ],
              "page": 54,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 1-5. Simple pipeline architecture for a spoken dialogue system: Spoken input (top left) is\nanalyzed, words are recognized, sentences are parsed and interpreted in context, application-specific\nactions take place (top right); a response is planned, realized as a syntactic structure, then to suitably\ninflected words, and finally to spoken output; different types of linguistic knowledge inform each stage\nof the process.",
              "bbox": [
                97,
                340,
                585,
                415
              ],
              "page": 54,
              "reading_order": 1
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_54_order_2",
      "label": "sec",
      "text": "Textual Entailment",
      "level": 1,
      "page": 54,
      "reading_order": 2,
      "bbox": [
        97,
        421,
        225,
        448
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_55_order_1",
          "label": "sub_sub_sec",
          "text": "Limitations of NLP",
          "level": 3,
          "page": 55,
          "reading_order": 1,
          "bbox": [
            98,
            170,
            219,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_55_order_2",
              "label": "para",
              "text": "Despite the research-led advances in tasks such as RTE, natural language systems that\nhave been deployed for real-world applications still cannot perform common-sense\nreasoning or draw on world knowledge in a general and robust manner. We can wait\nfor these difficult artificial intelligence problems to be solved, but in the meantime it is\nnecessary to live with some severe limitations on the reasoning and knowledge capa-\nbilities of natural language systems. Accordingly, right from the beginning, an impor-\ntant goal of NLP research has been to make progress on the difficult task of building\ntechnologies that “understand language,” using superficial yet powerful techniques\ninstead of unrestricted knowledge and reasoning capabilities. Indeed, this is one of the\ngoals of this book, and we hope to equip you with the knowledge and skills to build\nuseful NLP systems, and to contribute to the long-term aspiration of building intelligent\nmachines.",
              "level": -1,
              "page": 55,
              "reading_order": 2,
              "bbox": [
                97,
                197,
                585,
                394
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_55_order_3",
          "label": "sub_sec",
          "text": "1.6 Summary",
          "level": 2,
          "page": 55,
          "reading_order": 3,
          "bbox": [
            98,
            421,
            207,
            444
          ],
          "section_number": "1.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_55_order_4",
              "label": "list_group",
              "text": "• Texts are represented in Python using lists: ['Monty', 'Python']. We can use in-\ndexing, slicing, and the len() function on lists.\n• A word “ token ” is a particular appearance of a given word in a text; a word “ type ”\nis the unique form of the word as a particular sequence of letters. We count word\ntokens using len(text) and word types using len(set(text)) .",
              "level": -1,
              "page": 55,
              "reading_order": 4,
              "bbox": [
                106,
                448,
                584,
                485
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "• Texts are represented in Python using lists: ['Monty', 'Python']. We can use in-\ndexing, slicing, and the len() function on lists.",
                  "bbox": [
                    106,
                    448,
                    584,
                    485
                  ],
                  "page": 55,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "• A word “ token ” is a particular appearance of a given word in a text; a word “ type ”\nis the unique form of the word as a particular sequence of letters. We count word\ntokens using len(text) and word types using len(set(text)) .",
                  "bbox": [
                    106,
                    492,
                    584,
                    539
                  ],
                  "page": 55,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "• We obtain the vocabulary of a text t using sorted(set(t)).",
                  "bbox": [
                    106,
                    545,
                    458,
                    564
                  ],
                  "page": 55,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "• We operate on each item of a text using  [f(x) for x in text]",
                  "bbox": [
                    106,
                    564,
                    476,
                    582
                  ],
                  "page": 55,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "• To derive the vocabulary, collapsing case distinctions and ignoring punctuation,\nwe can write set([w.lower() for w in text if w.isalpha()]).",
                  "bbox": [
                    106,
                    582,
                    584,
                    618
                  ],
                  "page": 55,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "• We process each word in a text using a for statement, such as for w in t: or for\nword in text:. This must be followed by the colon character and an indented block\nof code, to be executed each time through the loop.",
                  "bbox": [
                    106,
                    618,
                    585,
                    672
                  ],
                  "page": 55,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "• We test a condition using an if statement: if len(word) < 5:. This must be fol-\nlowed by the colon character and an indented block of code, to be executed only\nif the condition is true.",
                  "bbox": [
                    106,
                    678,
                    585,
                    725
                  ],
                  "page": 55,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "• A frequency distribution is a collection of items along with their frequency counts\n(e.g., the words of a text and their frequency of appearance).",
                  "bbox": [
                    106,
                    725,
                    585,
                    762
                  ],
                  "page": 55,
                  "reading_order": 11
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_55_order_12",
              "label": "foot",
              "text": "1.6 Summary | 33",
              "level": -1,
              "page": 55,
              "reading_order": 12,
              "bbox": [
                494,
                824,
                584,
                842
              ],
              "section_number": "1.6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_56_order_0",
              "label": "list_group",
              "text": "• A function is a block of code that has been assigned a name and can be reused.\nFunctions are defined using the def keyword, as in def mult(x, y); x and y are\nparameters of the function, and act as placeholders for actual data values.\n• A function is called by specifying its name followed by one or more arguments\ninside parentheses, like this: mult(3, 4), e.g., len(text1).",
              "level": -1,
              "page": 56,
              "reading_order": 0,
              "bbox": [
                106,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "• A function is a block of code that has been assigned a name and can be reused.\nFunctions are defined using the def keyword, as in def mult(x, y); x and y are\nparameters of the function, and act as placeholders for actual data values.",
                  "bbox": [
                    106,
                    71,
                    585,
                    125
                  ],
                  "page": 56,
                  "reading_order": 0
                },
                {
                  "label": "list",
                  "text": "• A function is called by specifying its name followed by one or more arguments\ninside parentheses, like this: mult(3, 4), e.g., len(text1).",
                  "bbox": [
                    106,
                    125,
                    585,
                    161
                  ],
                  "page": 56,
                  "reading_order": 1
                }
              ],
              "is_merged": true
            }
          ]
        },
        {
          "id": "page_56_order_2",
          "label": "sub_sec",
          "text": "1.7 Further Reading",
          "level": 2,
          "page": 56,
          "reading_order": 2,
          "bbox": [
            98,
            170,
            261,
            199
          ],
          "section_number": "1.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_56_order_3",
              "label": "para",
              "text": "This chapter has introduced new concepts in programming, natural language process-\ning, and linguistics, all mixed in together. Many of them are consolidated in the fol-\nlowing chapters. However, you may also want to consult the online materials provided\nwith this chapter (at http://www.nltk.org/ ), including links to additional background\nmaterials, and links to online NLP systems. You may also like to read up on some\nlinguistics and NLP-related concepts in Wikipedia (e.g., collocations, the Turing Test,\nthe type-token distinction).",
              "level": -1,
              "page": 56,
              "reading_order": 3,
              "bbox": [
                97,
                206,
                585,
                322
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_56_order_4",
              "label": "para",
              "text": "You should acquaint yourself with the Python documentation available at http://docs\n.python.org/, including the many tutorials and comprehensive reference materials\nlinked there. A Beginner's Guide to Python is available at http://wiki.python.org/moin/\nBeginnersGuide. Miscellaneous questions about Python might be answered in the FAQ\nat http://www.python.org/doc/faq/general/.",
              "level": -1,
              "page": 56,
              "reading_order": 4,
              "bbox": [
                97,
                331,
                585,
                412
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_56_order_5",
              "label": "para",
              "text": "As you delve into NLTK, you might want to subscribe to the mailing list where new\nreleases of the toolkit are announced. There is also an NLTK-Users mailing list, where\nusers help each other as they learn how to use Python and NLTK for language analysis\nwork. Details of these lists are available at http://www.nltk.org/ .",
              "level": -1,
              "page": 56,
              "reading_order": 5,
              "bbox": [
                97,
                420,
                585,
                484
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_56_order_6",
              "label": "para",
              "text": "For more information on the topics covered in Section 1.5, and on NLP more generally,\nyou might like to consult one of the following excellent books:",
              "level": -1,
              "page": 56,
              "reading_order": 6,
              "bbox": [
                97,
                492,
                584,
                528
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_56_order_7",
              "label": "list_group",
              "text": "• Indurkhya, Nitin and Fred Damerau (eds., 2010) Handbook of Natural Language\nProcessing (second edition), Chapman & Hall/CRC.\n• Jurafsky, Daniel and James Martin (2008) Speech and Language Processing (second\nedition), Prentice Hall.",
              "level": -1,
              "page": 56,
              "reading_order": 7,
              "bbox": [
                106,
                528,
                585,
                565
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "• Indurkhya, Nitin and Fred Damerau (eds., 2010) Handbook of Natural Language\nProcessing (second edition), Chapman & Hall/CRC.",
                  "bbox": [
                    106,
                    528,
                    585,
                    565
                  ],
                  "page": 56,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "• Jurafsky, Daniel and James Martin (2008) Speech and Language Processing (second\nedition), Prentice Hall.",
                  "bbox": [
                    106,
                    572,
                    583,
                    601
                  ],
                  "page": 56,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "• Mitkov, Ruslan (ed., 2002) The Oxford Handbook of Computational Linguistics .\nOxford University Press. (second edition expected in 2010).",
                  "bbox": [
                    106,
                    609,
                    584,
                    645
                  ],
                  "page": 56,
                  "reading_order": 9
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_56_order_10",
              "label": "para",
              "text": "The Association for Computational Linguistics is the international organization that\nrepresents the field of NLP. The ACL website hosts many useful resources, including:\ninformation about international and regional conferences and workshops; the ACL\nWiki with links to hundreds of useful resources; and the ACL Anthology , which contains\nmost of the NLP research literature from the past 50 years, fully indexed and freely\ndownloadable.",
              "level": -1,
              "page": 56,
              "reading_order": 10,
              "bbox": [
                97,
                645,
                585,
                744
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_56_order_11",
              "label": "foot",
              "text": "34 | Chapter 1: Language Processing and Python",
              "level": -1,
              "page": 56,
              "reading_order": 11,
              "bbox": [
                97,
                824,
                306,
                842
              ],
              "section_number": "34",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_0",
              "label": "para",
              "text": "Some excellent introductory linguistics textbooks are: (Finegan, 2007), (O’Grady et\nal., 2004), (OSU, 2007). You might like to consult LanguageLog, a popular linguistics\nblog with occasional posts that use the techniques described in this book.",
              "level": -1,
              "page": 57,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_57_order_1",
          "label": "sub_sec",
          "text": "1.8 Exercises",
          "level": 2,
          "page": 57,
          "reading_order": 1,
          "bbox": [
            98,
            143,
            207,
            170
          ],
          "section_number": "1.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_57_order_2",
              "label": "para",
              "text": "1. ◦ Try using the Python interpreter as a calculator, and typing expressions like 12 /\n(4 + 1).",
              "level": -1,
              "page": 57,
              "reading_order": 2,
              "bbox": [
                100,
                179,
                584,
                215
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_3",
              "label": "para",
              "text": "2. ◦ Given an alphabet of 26 letters, there are 26 to the power 10, or 26 ** 10, 10-\nletter strings we can form. That works out to 141167095653376L (the L at the end\njust indicates that this is Python’s long-number format). How many hundred-letter\nstrings are possible?",
              "level": -1,
              "page": 57,
              "reading_order": 3,
              "bbox": [
                100,
                215,
                585,
                286
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_4",
              "label": "para",
              "text": "3. ◦ The Python multiplication operation can be applied to lists. What happens when\nyou type ['Monty', 'Python'] * 20, or 3 * sent1?",
              "level": -1,
              "page": 57,
              "reading_order": 4,
              "bbox": [
                100,
                286,
                584,
                322
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_5",
              "label": "para",
              "text": "4. ◦ Review Section 1.1 on computing with language. How many words are there in\ntext2? How many distinct words are there?",
              "level": -1,
              "page": 57,
              "reading_order": 5,
              "bbox": [
                100,
                322,
                584,
                358
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_6",
              "label": "para",
              "text": "5. ◦ Compare the lexical diversity scores for humor and romance fiction in Ta-\nble 1-1. Which genre is more lexically diverse?",
              "level": -1,
              "page": 57,
              "reading_order": 6,
              "bbox": [
                100,
                358,
                584,
                394
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_7",
              "label": "para",
              "text": "6. ◦ Produce a dispersion plot of the four main protagonists in Sense and Sensibility:\nElinor, Marianne, Edward, and Willoughby. What can you observe about the\ndifferent roles played by the males and females in this novel? Can you identify the\ncouples?",
              "level": -1,
              "page": 57,
              "reading_order": 7,
              "bbox": [
                100,
                394,
                585,
                465
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_8",
              "label": "para",
              "text": "7. ◦ Find the collocations in text5.",
              "level": -1,
              "page": 57,
              "reading_order": 8,
              "bbox": [
                100,
                465,
                306,
                483
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_9",
              "label": "para",
              "text": "8. ◦ Consider the following Python expression: len(set(text4)). State the purpose\nof this expression. Describe the two steps involved in performing this computation.",
              "level": -1,
              "page": 57,
              "reading_order": 9,
              "bbox": [
                100,
                492,
                585,
                522
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_10",
              "label": "para",
              "text": "9. ◦ Review Section 1.2 on lists and strings",
              "level": -1,
              "page": 57,
              "reading_order": 10,
              "bbox": [
                100,
                528,
                352,
                546
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_11",
              "label": "para",
              "text": "a. Define a string and assign it to a variable, e.g., my_string = 'My String' (but\nput something more interesting in the string). Print the contents of this variable\nin two ways, first by simply typing the variable name and pressing Enter, then\nby using the print statement.",
              "level": -1,
              "page": 57,
              "reading_order": 11,
              "bbox": [
                126,
                546,
                585,
                618
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_12",
              "label": "para",
              "text": "b. Try adding the string to itself using my_string + my_string, or multiplying it\nby a number, e.g., my_string * 3. Notice that the strings are joined together\nwithout any spaces. How could you fix this?",
              "level": -1,
              "page": 57,
              "reading_order": 12,
              "bbox": [
                126,
                618,
                585,
                672
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_13",
              "label": "para",
              "text": "10. ◦ Define a variable my_sent to be a list of words, using the syntax my_sent = [\"My\",\n\"sent\"] (but with your own words, or a favorite saying).",
              "level": -1,
              "page": 57,
              "reading_order": 13,
              "bbox": [
                100,
                672,
                583,
                707
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_14",
              "label": "para",
              "text": "a. Use' '.join(my_sent) to convert this into a string.",
              "level": -1,
              "page": 57,
              "reading_order": 14,
              "bbox": [
                126,
                707,
                440,
                725
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_15",
              "label": "para",
              "text": "b. Use split() to split the string back into the list form you had to start with.",
              "level": -1,
              "page": 57,
              "reading_order": 15,
              "bbox": [
                126,
                725,
                574,
                746
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_16",
              "label": "para",
              "text": "11. ◦ Define several variables containing lists of words, e.g., phrase1, phrase2, and so\non. Join them together in various combinations (using the plus operator) to form",
              "level": -1,
              "page": 57,
              "reading_order": 16,
              "bbox": [
                100,
                752,
                584,
                788
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_57_order_17",
              "label": "foot",
              "text": "1.8 Exercises | 35",
              "level": -1,
              "page": 57,
              "reading_order": 17,
              "bbox": [
                503,
                824,
                585,
                842
              ],
              "section_number": "1.8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_0",
              "label": "para",
              "text": "whole sentences. What is the relationship between len(phrase1 + phrase2) and\nlen(phrase1) + len(phrase2)?",
              "level": -1,
              "page": 58,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_1",
              "label": "para",
              "text": "12. ◦ Consider the following two expressions, which have the same value. Which one\nwill typically be more relevant in NLP? Why?",
              "level": -1,
              "page": 58,
              "reading_order": 1,
              "bbox": [
                100,
                107,
                585,
                143
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_2",
              "label": "para",
              "text": "a. \"Monty Python\"[6:12]",
              "level": -1,
              "page": 58,
              "reading_order": 2,
              "bbox": [
                126,
                149,
                270,
                163
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_3",
              "label": "para",
              "text": "b. [\"Monty\", \"Python\"][1]",
              "level": -1,
              "page": 58,
              "reading_order": 3,
              "bbox": [
                126,
                170,
                281,
                184
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_4",
              "label": "para",
              "text": "13. ◦ We have seen how to represent a sentence as a list of words, where each word is\na sequence of characters. What does sent1[2][2] do? Why? Experiment with other\nindex values.",
              "level": -1,
              "page": 58,
              "reading_order": 4,
              "bbox": [
                100,
                188,
                585,
                235
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_5",
              "label": "para",
              "text": "14. ◦ The first sentence of text3 is provided to you in the variable sent3. The index of\nthe in sent3 is 1, because sent3[1] gives us 'the'. What are the indexes of the two\nother occurrences of this word in sent3?",
              "level": -1,
              "page": 58,
              "reading_order": 5,
              "bbox": [
                100,
                241,
                586,
                290
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_6",
              "label": "para",
              "text": "15. ◦ Review the discussion of conditionals in Section 1.4. Find all words in the Chat\nCorpus ( text5) starting with the letter b. Show them in alphabetical order.",
              "level": -1,
              "page": 58,
              "reading_order": 6,
              "bbox": [
                100,
                295,
                585,
                331
              ],
              "section_number": "15",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_7",
              "label": "para",
              "text": "16. ◦ Type the expression range(10) at the interpreter prompt. Now try range(10,\n20),range(10, 20, 2), and range(20, 10, -2). We will see a variety of uses for this\nbuilt-in function in later chapters.",
              "level": -1,
              "page": 58,
              "reading_order": 7,
              "bbox": [
                100,
                331,
                585,
                385
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_8",
              "label": "para",
              "text": "17. o Use text9.index() to find the index of the word sunset. You’ll need to insert this\nword as an argument between the parentheses. By a process of trial and error, find\nthe slice for the complete sentence that contains this word.",
              "level": -1,
              "page": 58,
              "reading_order": 8,
              "bbox": [
                100,
                385,
                585,
                439
              ],
              "section_number": "17",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_9",
              "label": "para",
              "text": "18. o Using list addition, and the set and sorted operations, compute the vocabulary\nof the sentences sent1 ... sent8.",
              "level": -1,
              "page": 58,
              "reading_order": 9,
              "bbox": [
                100,
                439,
                585,
                474
              ],
              "section_number": "18",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_10",
              "label": "para",
              "text": "19. o What is the difference between the following two lines? Which one will give a\nlarger value? Will this be the case for other texts?",
              "level": -1,
              "page": 58,
              "reading_order": 10,
              "bbox": [
                100,
                474,
                584,
                511
              ],
              "section_number": "19",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_12",
              "label": "para",
              "text": "20.\nWhat is the difference between the following two tests: w.isupper() and not\nw.islower()?",
              "level": -1,
              "page": 58,
              "reading_order": 12,
              "bbox": [
                98,
                546,
                584,
                582
              ],
              "section_number": "20",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_13",
              "label": "para",
              "text": "21. o Write the slice expression that extracts the last two words of text2.",
              "level": -1,
              "page": 58,
              "reading_order": 13,
              "bbox": [
                98,
                582,
                521,
                603
              ],
              "section_number": "21",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_14",
              "label": "para",
              "text": "22. o Find all the four-letter words in the Chat Corpus ( text5 ). With the help of a\nfrequency distribution ( FreqDist ), show these words in decreasing order of fre-\nquency.",
              "level": -1,
              "page": 58,
              "reading_order": 14,
              "bbox": [
                98,
                609,
                584,
                657
              ],
              "section_number": "22",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_15",
              "label": "para",
              "text": "23. o Review the discussion of looping with conditions in Section 1.4 . Use a combi-\nnation of for and if statements to loop over the words of the movie script for\nMonty Python and the Holy Grail ( text6 ) and print all the uppercase words, one\nper line.",
              "level": -1,
              "page": 58,
              "reading_order": 15,
              "bbox": [
                98,
                663,
                585,
                727
              ],
              "section_number": "23",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_16",
              "label": "para",
              "text": "24. o Write expressions for finding all words in text6 that meet the following condi-\ntions. The result should be in the form of a list of words: ['word1', 'word2', ...].",
              "level": -1,
              "page": 58,
              "reading_order": 16,
              "bbox": [
                98,
                734,
                584,
                763
              ],
              "section_number": "24",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_58_order_17",
              "label": "foot",
              "text": "36 | Chapter 1: Language Processing and Python",
              "level": -1,
              "page": 58,
              "reading_order": 17,
              "bbox": [
                97,
                824,
                306,
                842
              ],
              "section_number": "36",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_0",
              "label": "para",
              "text": "a. Ending in iza",
              "level": -1,
              "page": 59,
              "reading_order": 0,
              "bbox": [
                126,
                71,
                217,
                89
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_1",
              "label": "para",
              "text": "b. Containing the letter z",
              "level": -1,
              "page": 59,
              "reading_order": 1,
              "bbox": [
                126,
                89,
                274,
                110
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_2",
              "label": "para",
              "text": "c. Containing the sequence of letters p",
              "level": -1,
              "page": 59,
              "reading_order": 2,
              "bbox": [
                126,
                116,
                351,
                134
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_3",
              "label": "para",
              "text": "d. All lowercase letters except for an initial capital (i.e., titlecase",
              "level": -1,
              "page": 59,
              "reading_order": 3,
              "bbox": [
                126,
                134,
                504,
                152
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_4",
              "label": "para",
              "text": "25. o Define sent to be the list of words ['she', 'sells', 'sea', 'shells', 'by',\n'the', 'sea', 'shore']. Now write code to perform the following tasks:",
              "level": -1,
              "page": 59,
              "reading_order": 4,
              "bbox": [
                98,
                152,
                583,
                188
              ],
              "section_number": "25",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_5",
              "label": "para",
              "text": "a. Print all words beginning with sh.",
              "level": -1,
              "page": 59,
              "reading_order": 5,
              "bbox": [
                126,
                188,
                342,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_6",
              "label": "para",
              "text": "b. Print all words longer than four characters",
              "level": -1,
              "page": 59,
              "reading_order": 6,
              "bbox": [
                126,
                215,
                388,
                232
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_7",
              "label": "para",
              "text": "26. o What does the following Python code do? sum([len(w) for w in text1]) Can\nyou use it to work out the average word length of a text?",
              "level": -1,
              "page": 59,
              "reading_order": 7,
              "bbox": [
                98,
                232,
                584,
                268
              ],
              "section_number": "26",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_8",
              "label": "para",
              "text": "27. o Define a function called vocab_size(text) that has a single parameter for the\ntext, and which returns the vocabulary size of the text.",
              "level": -1,
              "page": 59,
              "reading_order": 8,
              "bbox": [
                98,
                268,
                585,
                304
              ],
              "section_number": "27",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_9",
              "label": "para",
              "text": "28. o Define a function percent(word, text) that calculates how often a given word\noccurs in a text and expresses the result as a percentage.",
              "level": -1,
              "page": 59,
              "reading_order": 9,
              "bbox": [
                98,
                304,
                584,
                341
              ],
              "section_number": "28",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_10",
              "label": "para",
              "text": "29. o We have been using sets to store vocabularies. Try the following Python expres-\nsion: set(sent3) < set(text1). Experiment with this using different arguments to\nset(). What does it do? Can you think of a practical application for this?",
              "level": -1,
              "page": 59,
              "reading_order": 10,
              "bbox": [
                98,
                348,
                585,
                395
              ],
              "section_number": "29",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_59_order_11",
              "label": "foot",
              "text": "1.8 Exercises | 37",
              "level": -1,
              "page": 59,
              "reading_order": 11,
              "bbox": [
                503,
                824,
                585,
                842
              ],
              "section_number": "1.8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_60_order_0",
              "label": "para",
              "text": "_",
              "level": -1,
              "page": 60,
              "reading_order": 0,
              "bbox": [
                153,
                161,
                494,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_54_order_3",
          "label": "para",
          "text": "The challenge of language understanding has been brought into focus in recent years\nby a public “shared task” called Recognizing Textual Entailment (RTE). The basic\nscenario is simple. Suppose you want to find evidence to support the hypothesis: Sandra\nGoudie was defeated by Max Purnell, and that you have another short text that seems\nto be relevant, for example, Sandra Goudie was first elected to Parliament in the 2002\nelections, narrowly winning the seat of Coromandel by defeating Labour candidate Max\nPurnell and pushing incumbent Green MP Jeanette Fitzsimons into third place. Does the\ntext provide enough evidence for you to accept the hypothesis? In this particular case,\nthe answer will be “No.” You can draw this conclusion easily, but it is very hard to\ncome up with automated methods for making the right decision. The RTE Challenges\nprovide data that allow competitors to develop their systems, but not enough data for\n“brute force” machine learning techniques (a topic we will cover in Chapter 6 ). Con-\nsequently, some linguistic analysis is crucial. In the previous example, it is important\nfor the system to note that Sandra Goudie names the person being defeated in the\nhypothesis, not the person doing the defeating in the text. As another illustration of\nthe difficulty of the task, consider the following text-hypothesis pair:",
          "level": -1,
          "page": 54,
          "reading_order": 3,
          "bbox": [
            97,
            448,
            586,
            718
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_54_order_4",
          "label": "list_group",
          "text": "(7) a. Text: David Golinkin is the editor or author of 18 books, and over 150\nresponsa, articles, sermons and books\nb. Hypothesis: Golinkin has written 18 books",
          "level": -1,
          "page": 54,
          "reading_order": 4,
          "bbox": [
            118,
            725,
            574,
            763
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "(7) a. Text: David Golinkin is the editor or author of 18 books, and over 150\nresponsa, articles, sermons and books",
              "bbox": [
                118,
                725,
                574,
                763
              ],
              "page": 54,
              "reading_order": 4
            },
            {
              "label": "list",
              "text": "b. Hypothesis: Golinkin has written 18 books",
              "bbox": [
                144,
                770,
                413,
                788
              ],
              "page": 54,
              "reading_order": 5
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_54_order_6",
          "label": "foot",
          "text": "32 | Chapter 1: Language Processing and Python",
          "level": -1,
          "page": 54,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            306,
            842
          ],
          "section_number": "32",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_55_order_0",
          "label": "para",
          "text": "In order to determine whether the hypothesis is supported by the text, the system needs\nthe following background knowledge: (i) if someone is an author of a book, then he/she has written that book; (ii) if someone is an editor of a book, then he/she has not\nwritten (all of) that book; (iii) if someone is editor or author of 18 books, then one\ncannot conclude that he/she is author of 18 books.",
          "level": -1,
          "page": 55,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_61_order_0",
      "label": "sec",
      "text": "CHAPTER 2\nAccessing Text Corpora\nand Lexical Resources",
      "level": 1,
      "page": 61,
      "reading_order": 0,
      "bbox": [
        294,
        78,
        584,
        172
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_61_order_6",
          "label": "sub_sec",
          "text": "2.1 Accessing Text Corpora",
          "level": 2,
          "page": 61,
          "reading_order": 6,
          "bbox": [
            97,
            573,
            315,
            600
          ],
          "section_number": "2.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_61_order_7",
              "label": "para",
              "text": "As just mentioned, a text corpus is a large body of text. Many corpora are designed to\ncontain a careful balance of material in one or more genres. We examined some small\ntext collections in Chapter 1 , such as the speeches known as the US Presidential Inau-\ngural Addresses. This particular corpus actually contains dozens of individual texts—\none per address—but for convenience we glued them end-to-end and treated them as\na single text. Chapter 1 also used various predefined texts that we accessed by typing\nfrom book import * . However, since we want to be able to work with other texts, this\nsection examines a variety of text corpora. We'll see how to select individual texts, and\nhow to work with them.",
              "level": -1,
              "page": 61,
              "reading_order": 7,
              "bbox": [
                97,
                607,
                585,
                752
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_61_order_8",
              "label": "foot",
              "text": "39",
              "level": -1,
              "page": 61,
              "reading_order": 8,
              "bbox": [
                574,
                824,
                585,
                837
              ],
              "section_number": "39",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_61_order_1",
          "label": "para",
          "text": "Practical work in Natural Language Processing typically uses large bodies of linguistic\ndata, or corpora . The goal of this chapter is to answer the following questions:",
          "level": -1,
          "page": 61,
          "reading_order": 1,
          "bbox": [
            97,
            328,
            585,
            359
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_61_order_2",
          "label": "para",
          "text": "1. What are some useful text corpora and lexical resources, and how can we access\nthem with Python?",
          "level": -1,
          "page": 61,
          "reading_order": 2,
          "bbox": [
            100,
            367,
            585,
            403
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_61_order_3",
          "label": "para",
          "text": "2. Which Python constructs are most helpful for this work",
          "level": -1,
          "page": 61,
          "reading_order": 3,
          "bbox": [
            100,
            403,
            441,
            421
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_61_order_4",
          "label": "para",
          "text": "3. How do we avoid repeating ourselves when writing Python code?",
          "level": -1,
          "page": 61,
          "reading_order": 4,
          "bbox": [
            100,
            421,
            494,
            441
          ],
          "section_number": "3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_61_order_5",
          "label": "para",
          "text": "This chapter continues to present programming concepts by example, in the context\nof a linguistic processing task. We will wait until later before exploring each Python\nconstruct systematically. Don’t worry if you see an example that contains something\nunfamiliar; simply try it out and see what it does, and—if you’re game—modify it by\nsubstituting some part of the code with a different text or word. This way you will\nassociate a task with a programming idiom, and learn the hows and whys later.",
          "level": -1,
          "page": 61,
          "reading_order": 5,
          "bbox": [
            97,
            448,
            585,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_62_order_0",
      "label": "sec",
      "text": "Gutenberg Corpus",
      "level": 1,
      "page": 62,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        217,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_62_order_1",
          "label": "para",
          "text": "NLTK includes a small selection of texts from the Project Gutenberg electronic text\narchive, which contains some 25,000 free electronic books, hosted at http://www.gu\ntenberg.org/. We begin by getting the Python interpreter to load the NLTK package,\nthen ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:",
          "level": -1,
          "page": 62,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            584,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_62_order_3",
          "label": "para",
          "text": "Let’s pick out the first of these texts—Emma by Jane Austen—and give it a short name,\nemma, then find out how many words it contains:",
          "level": -1,
          "page": 62,
          "reading_order": 3,
          "bbox": [
            97,
            286,
            584,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_62_order_5",
          "label": "para",
          "text": "In Section 1.1 , we showed how you could carry out concordance of a\ntext such as text1 with the command text1.concordance() . However,\nthis assumes that you are using one of the nine texts obtained as a result\nof doing from nltk.book import * . Now that you have started examining\ndata from nltk.corpus , as in the previous example, you have to employ\nthe following pair of statements to perform concordancing and other\ntasks from Section 1.1 :",
          "level": -1,
          "page": 62,
          "reading_order": 5,
          "bbox": [
            171,
            385,
            530,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_62_order_7",
          "label": "para",
          "text": "When we defined emma, we invoked the words() function of the gutenberg object in\nNLTK's corpus package. But since it is cumbersome to type such long names all the\ntime, Python provides another version of the import statement, as follows:",
          "level": -1,
          "page": 62,
          "reading_order": 7,
          "bbox": [
            97,
            546,
            585,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_62_order_9",
          "label": "para",
          "text": "Let’s write a short program to display other information about each text, by looping\nover all the values of fileid corresponding to the gutenberg file identifiers listed earlier\nand then computing statistics for each text. For a compact output display, we will make\nsure that the numbers are all integers, using int().",
          "level": -1,
          "page": 62,
          "reading_order": 9,
          "bbox": [
            97,
            663,
            585,
            730
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_62_order_11",
          "label": "foot",
          "text": "40 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 62,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "40",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_63_order_1",
          "label": "para",
          "text": "This program displays three statistics for each text: average word length, average sen-\ntence length, and the number of times each vocabulary item appears in the text on\naverage (our lexical diversity score). Observe that average word length appears to be a\ngeneral property of English, since it has a recurrent value of 4. (In fact, the average word\nlength is really 3, not 4, since the num_chars variable counts space characters.) By con-\ntrast average sentence length and lexical diversity appear to be characteristics of par-\nticular authors.",
          "level": -1,
          "page": 63,
          "reading_order": 1,
          "bbox": [
            97,
            367,
            585,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_63_order_2",
          "label": "para",
          "text": "The previous example also showed how we can access the “ raw ” text of the book\n❶ , not split up into tokens. The raw() function gives us the contents of the file without\nany linguistic processing. So, for example, len(gutenberg.raw('blake-poems.txt') tells\nus how many letters occur in the text, including the spaces between words. The\nsents() function divides the text up into its sentences, where each sentence is a list of\nwords:",
          "level": -1,
          "page": 63,
          "reading_order": 2,
          "bbox": [
            97,
            492,
            585,
            586
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_63_order_4",
          "label": "foot",
          "text": "2.1 Accessing Text Corpora | 41",
          "level": -1,
          "page": 63,
          "reading_order": 4,
          "bbox": [
            448,
            824,
            584,
            842
          ],
          "section_number": "2.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_64_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_064_figure_000.png)",
          "level": -1,
          "page": 64,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            171,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_64_order_1",
          "label": "para",
          "text": "Most NLTK corpus readers include a variety of access methods apart\nfrom words(), raw(), and sents(). Richer linguistic content is available\nfrom some corpora, such as part-of-speech tags, dialogue tags, syntactic\ntrees, and so forth; we will see these in later chapters.",
          "level": -1,
          "page": 64,
          "reading_order": 1,
          "bbox": [
            171,
            80,
            530,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_64_order_2",
      "label": "sec",
      "text": "Web and Chat Text",
      "level": 1,
      "page": 64,
      "reading_order": 2,
      "bbox": [
        97,
        161,
        225,
        182
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_64_order_3",
          "label": "para",
          "text": "Although Project Gutenberg contains thousands of books, it represents established\nliterature. It is important to consider less formal language as well. NLTK's small col-\nlection of web text includes content from a Firefox discussion forum, conversations\noverheard in New York, the movie script of Pirates of the Carribean , personal adver-\ntisements, and wine reviews:",
          "level": -1,
          "page": 64,
          "reading_order": 3,
          "bbox": [
            97,
            194,
            585,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_64_order_5",
          "label": "para",
          "text": "There is also a corpus of instant messaging chat sessions, originally collected by the\nNaval Postgraduate School for research on automatic detection of Internet predators.\nThe corpus contains over 10,000 posts, anonymized by replacing usernames with\ngeneric names of the form “UserNNN”, and manually edited to remove any other\nidentifying information. The corpus is organized into 15 files, where each file contains\nseveral hundred posts collected on a given date, for an age-specific chatroom (teens,\n20s, 30s, 40s, plus a generic adults chatroom). The filename contains the date, chat-\nroom, and number of posts; e.g., 10-19-20s_706posts.xml contains 706 posts gathered\nfrom the 20s chat room on 10/19/2006.",
          "level": -1,
          "page": 64,
          "reading_order": 5,
          "bbox": [
            97,
            421,
            585,
            565
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_64_order_7",
      "label": "sec",
      "text": "Brown Corpus",
      "level": 1,
      "page": 64,
      "reading_order": 7,
      "bbox": [
        98,
        654,
        189,
        675
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_64_order_8",
          "label": "para",
          "text": "The Brown Corpus was the first million-word electronic corpus of English, created in\n1961 at Brown University. This corpus contains text from 500 sources, and the sources\nhave been categorized by genre, such as news, editorial, and so on. Table 2­1 gives an\nexample of each genre (for a complete list, see http://icame.uib.no/brown/bcm­los.html).",
          "level": -1,
          "page": 64,
          "reading_order": 8,
          "bbox": [
            97,
            680,
            585,
            747
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_64_order_9",
          "label": "foot",
          "text": "42 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 64,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "42",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_65_order_0",
          "label": "table",
          "text": "Table 2-1. Example document for each section of the Brown Corpus [TABLE: <table><tr><td>ID</td><td>File</td><td>Genre</td><td>Description</td></tr><tr><td>A16</td><td>ca16</td><td>news</td><td>Chicago Tribune: Society Reportage</td></tr><tr><td>B02</td><td>cb02</td><td>editorial</td><td>Christian Science Monitor: Editorials</td></tr><tr><td>C17</td><td>cc17</td><td>reviews</td><td>Time Magazine: Reviews</td></tr><tr><td>D12</td><td>cd12</td><td>religion</td><td>Underwood: Probing the Ethics of Realtors</td></tr><tr><td>E36</td><td>ce36</td><td>hobbies</td><td>Norling: Renting a Car in Europe</td></tr><tr><td>F25</td><td>cf25</td><td>lore</td><td>Boroff: Jewish Teenage Culture</td></tr><tr><td>G22</td><td>cg22</td><td>elles_lettres</td><td>Reiner: Coping with Runaway Technology</td></tr><tr><td>H15</td><td>ch15</td><td>government</td><td>US Office of Civil and Defence Mobilization: The Family Fallout Shelter</td></tr><tr><td>J17</td><td>cj19</td><td>learned</td><td>Mosteller: Probability with Statistical Applications</td></tr><tr><td>K04</td><td>ck04</td><td>fiction</td><td>W.E.B. Du Bois: Worlds of Color</td></tr><tr><td>L13</td><td>cl13</td><td>mystery</td><td>Hitchens: Footsteps in the Night</td></tr><tr><td>M01</td><td>cm01</td><td>science_fiction</td><td>Heinlein: Stranger in a Strange Land</td></tr><tr><td>N14</td><td>cn15</td><td>adventure</td><td>Field: Rattlesnake Ridge</td></tr><tr><td>P12</td><td>cp12</td><td>romance</td><td>Callaghan: A Passion in Rome</td></tr><tr><td>R06</td><td>cr06</td><td>humor</td><td>Thurber: The Future, If Any, of Comedy</td></tr></table>]",
          "level": -1,
          "page": 65,
          "reading_order": 0,
          "bbox": [
            100,
            89,
            512,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>ID</td><td>File</td><td>Genre</td><td>Description</td></tr><tr><td>A16</td><td>ca16</td><td>news</td><td>Chicago Tribune: Society Reportage</td></tr><tr><td>B02</td><td>cb02</td><td>editorial</td><td>Christian Science Monitor: Editorials</td></tr><tr><td>C17</td><td>cc17</td><td>reviews</td><td>Time Magazine: Reviews</td></tr><tr><td>D12</td><td>cd12</td><td>religion</td><td>Underwood: Probing the Ethics of Realtors</td></tr><tr><td>E36</td><td>ce36</td><td>hobbies</td><td>Norling: Renting a Car in Europe</td></tr><tr><td>F25</td><td>cf25</td><td>lore</td><td>Boroff: Jewish Teenage Culture</td></tr><tr><td>G22</td><td>cg22</td><td>elles_lettres</td><td>Reiner: Coping with Runaway Technology</td></tr><tr><td>H15</td><td>ch15</td><td>government</td><td>US Office of Civil and Defence Mobilization: The Family Fallout Shelter</td></tr><tr><td>J17</td><td>cj19</td><td>learned</td><td>Mosteller: Probability with Statistical Applications</td></tr><tr><td>K04</td><td>ck04</td><td>fiction</td><td>W.E.B. Du Bois: Worlds of Color</td></tr><tr><td>L13</td><td>cl13</td><td>mystery</td><td>Hitchens: Footsteps in the Night</td></tr><tr><td>M01</td><td>cm01</td><td>science_fiction</td><td>Heinlein: Stranger in a Strange Land</td></tr><tr><td>N14</td><td>cn15</td><td>adventure</td><td>Field: Rattlesnake Ridge</td></tr><tr><td>P12</td><td>cp12</td><td>romance</td><td>Callaghan: A Passion in Rome</td></tr><tr><td>R06</td><td>cr06</td><td>humor</td><td>Thurber: The Future, If Any, of Comedy</td></tr></table>",
              "bbox": [
                100,
                89,
                512,
                412
              ],
              "page": 65,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Table 2-1. Example document for each section of the Brown Corpus",
              "bbox": [
                99,
                71,
                431,
                89
              ],
              "page": 65,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_65_order_2",
          "label": "para",
          "text": "We can access the corpus as a list of words or a list of sentences (where each sentence\nis itself just a list of words). We can optionally specify particular categories or files to\nread:",
          "level": -1,
          "page": 65,
          "reading_order": 2,
          "bbox": [
            97,
            430,
            585,
            478
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_65_order_4",
          "label": "para",
          "text": "The Brown Corpus is a convenient resource for studying systematic differences between\ngenres, a kind of linguistic inquiry known as stylistics. Let’s compare genres in their\nusage of modal verbs. The first step is to produce the counts for a particular genre.\nRemember to import nltk before doing the following:",
          "level": -1,
          "page": 65,
          "reading_order": 4,
          "bbox": [
            97,
            636,
            585,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_65_order_6",
          "label": "foot",
          "text": "2.1 Accessing Text Corpora | 43",
          "level": -1,
          "page": 65,
          "reading_order": 6,
          "bbox": [
            448,
            824,
            584,
            842
          ],
          "section_number": "2.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_66_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_066_figure_001.png)",
          "level": -1,
          "page": 66,
          "reading_order": 1,
          "bbox": [
            118,
            116,
            162,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_66_order_2",
          "label": "para",
          "text": "Your Turn: Choose a different section of the Brown Corpus, and adapt\nthe preceding example to count a selection of wh words, such as what,\nwhen, where, who and why.",
          "level": -1,
          "page": 66,
          "reading_order": 2,
          "bbox": [
            171,
            125,
            530,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_66_order_3",
          "label": "para",
          "text": "Next, we need to obtain counts for each genre of interest. We'll use NLTK's support\nfor conditional frequency distributions. These are presented systematically in Sec-\ntion 2.2 , where we also unpick the following code line by line. For the moment, you\ncan ignore the details and just concentrate on the output.",
          "level": -1,
          "page": 66,
          "reading_order": 3,
          "bbox": [
            97,
            197,
            584,
            264
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_66_order_5",
          "label": "para",
          "text": "Observe that the most frequent modal in the news genre is will, while the most frequent\nmodal in the romance genre is could. Would you have predicted this? The idea that\nword counts might distinguish genres will be taken up again in Chapter 6.",
          "level": -1,
          "page": 66,
          "reading_order": 5,
          "bbox": [
            97,
            463,
            585,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_66_order_6",
      "label": "sec",
      "text": "Reuters Corpus",
      "level": 1,
      "page": 66,
      "reading_order": 6,
      "bbox": [
        100,
        526,
        198,
        546
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_66_order_7",
          "label": "para",
          "text": "The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The\ndocuments have been classified into 90 topics, and grouped into two sets, called “ train-\ning” and “ test”; thus, the text with fileid 'test/14826' is a document drawn from the\ntest set. This split is for training and testing algorithms that automatically detect the\ntopic of a document, as we will see in Chapter 6 .",
          "level": -1,
          "page": 66,
          "reading_order": 7,
          "bbox": [
            97,
            554,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_66_order_9",
          "label": "para",
          "text": "Unlike the Brown Corpus, categories in the Reuters Corpus overlap with each other,\nsimply because a news story often covers multiple topics. We can ask for the topics",
          "level": -1,
          "page": 66,
          "reading_order": 9,
          "bbox": [
            97,
            742,
            584,
            773
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_66_order_10",
          "label": "foot",
          "text": "44 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 66,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "44",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_67_order_0",
          "label": "para",
          "text": "covered by one or more documents, or for the documents included in one or more\ncategories. For convenience, the corpus methods accept a single fileid or a list of fileids.",
          "level": -1,
          "page": 67,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_67_order_2",
          "label": "para",
          "text": "Similarly, we can specify the words or sentences we want in terms of files or categories.\nThe first handful of words in each of these texts are the titles, which by convention are\nstored as uppercase.",
          "level": -1,
          "page": 67,
          "reading_order": 2,
          "bbox": [
            97,
            240,
            585,
            287
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_67_order_4",
      "label": "sec",
      "text": "Inaugural Address Corpus",
      "level": 1,
      "page": 67,
      "reading_order": 4,
      "bbox": [
        100,
        427,
        270,
        448
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_67_order_5",
          "label": "para",
          "text": "In Section 1.1 , we looked at the Inaugural Address Corpus, but treated it as a single\ntext. The graph in Figure 1 - 2 used “ word offset ” as one of the axes; this is the numerical\nindex of the word in the corpus, counting from the first word of the first address.\nHowever, the corpus is actually a collection of 55 texts, one for each presidential ad-\ndress. An interesting property of this collection is its time dimension:",
          "level": -1,
          "page": 67,
          "reading_order": 5,
          "bbox": [
            97,
            454,
            585,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_67_order_7",
          "label": "para",
          "text": "Notice that the year of each text appears in its filename. To get the year out of the\nfilename, we extracted the first four characters, using fileid[:4].",
          "level": -1,
          "page": 67,
          "reading_order": 7,
          "bbox": [
            98,
            617,
            585,
            648
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_67_order_8",
          "label": "para",
          "text": "Let's look at how the words America and citizen are used over time. The following code\nconverts the words in the Inaugural corpus to lowercase using w.lower() ❶ , then checks\nwhether they start with either of the “ targets ” america or citizen using startswith()\n❶ . Thus it will count words such as American's and Citizens . We'll learn about condi-\ntional frequency distributions in Section 2.2 ; for now, just consider the output, shown\nin Figure 2 - 1 .",
          "level": -1,
          "page": 67,
          "reading_order": 8,
          "bbox": [
            97,
            654,
            585,
            754
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_67_order_9",
          "label": "foot",
          "text": "2.1 Accessing Text Corpora | 45",
          "level": -1,
          "page": 67,
          "reading_order": 9,
          "bbox": [
            448,
            824,
            585,
            842
          ],
          "section_number": "2.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_68_order_1",
          "label": "figure",
          "text": "Figure 2-1. Plot of a conditional frequency distribution: All words in the Inaugural Address Corpus\nthat begin with america or citizen are counted; separate counts are kept for each address; these are\nplotted so that trends in usage over time can be observed; counts are not normalized for document\nlength. [IMAGE: ![Figure](figures/NLTK_page_068_figure_001.png)]",
          "level": -1,
          "page": 68,
          "reading_order": 1,
          "bbox": [
            100,
            179,
            583,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_068_figure_001.png)",
              "bbox": [
                100,
                179,
                583,
                412
              ],
              "page": 68,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Figure 2-1. Plot of a conditional frequency distribution: All words in the Inaugural Address Corpus\nthat begin with america or citizen are counted; separate counts are kept for each address; these are\nplotted so that trends in usage over time can be observed; counts are not normalized for document\nlength.",
              "bbox": [
                97,
                421,
                585,
                476
              ],
              "page": 68,
              "reading_order": 2
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_68_order_3",
      "label": "sec",
      "text": "Annotated Text Corpora",
      "level": 1,
      "page": 68,
      "reading_order": 3,
      "bbox": [
        97,
        501,
        261,
        528
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_68_order_4",
          "label": "para",
          "text": "Many text corpora contain linguistic annotations, representing part-of-speech tags,\nnamed entities, syntactic structures, semantic roles, and so forth. NLTK provides\nconvenient ways to access several of these corpora, and has data packages containing\ncorpora and corpus samples, freely downloadable for use in teaching and research.\nTable 2 - 2 lists some of the corpora. For information about downloading them, see\nhttp://www.nltk.org/data . For more examples of how to access NLTK corpora, please\nconsult the Corpus HOWTO at http://www.nltk.org/howto .",
          "level": -1,
          "page": 68,
          "reading_order": 4,
          "bbox": [
            97,
            528,
            585,
            647
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_68_order_5",
          "label": "table",
          "text": "Table 2-2. Some of the corpora and corpus samples distributed with NLTK [TABLE: <table><tr><td>Corpus</td><td>Compiler</td><td>Contents</td></tr><tr><td>Brown Corpus</td><td>Francis, Kucera</td><td>15 genres, 1.15M words, tagged, categorized</td></tr><tr><td>CESS Treebanks</td><td>CLiC-UB</td><td>1M words, tagged and parsed (Catalan, Spanish)</td></tr><tr><td>Chat-80 Data Files</td><td>Pereira &amp; Warren</td><td>World Geographic Database</td></tr><tr><td>CMU Pronouncing Dictionary</td><td>CMU</td><td>127k entries</td></tr><tr><td>CoNLL 2000 Chunking Data</td><td>CoNLL</td><td>270k words, tagged and chunked</td></tr></table>]",
          "level": -1,
          "page": 68,
          "reading_order": 5,
          "bbox": [
            100,
            680,
            583,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Corpus</td><td>Compiler</td><td>Contents</td></tr><tr><td>Brown Corpus</td><td>Francis, Kucera</td><td>15 genres, 1.15M words, tagged, categorized</td></tr><tr><td>CESS Treebanks</td><td>CLiC-UB</td><td>1M words, tagged and parsed (Catalan, Spanish)</td></tr><tr><td>Chat-80 Data Files</td><td>Pereira &amp; Warren</td><td>World Geographic Database</td></tr><tr><td>CMU Pronouncing Dictionary</td><td>CMU</td><td>127k entries</td></tr><tr><td>CoNLL 2000 Chunking Data</td><td>CoNLL</td><td>270k words, tagged and chunked</td></tr></table>",
              "bbox": [
                100,
                680,
                583,
                797
              ],
              "page": 68,
              "reading_order": 5
            },
            {
              "label": "cap",
              "text": "Table 2-2. Some of the corpora and corpus samples distributed with NLTK",
              "bbox": [
                99,
                661,
                467,
                674
              ],
              "page": 68,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_68_order_7",
          "label": "foot",
          "text": "46 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 68,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "46",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_69_order_0",
          "label": "tab",
          "text": "<table><tr><td>Corpus</td><td>Compiler</td><td>Contents</td></tr><tr><td>CoNLL 2002 Named Entity</td><td>CoNLL</td><td>700k words, POS and named entity tagged (Dutch, Spanish)</td></tr><tr><td>CoNLL 2007 Dependency Parsed Tree-banks (selections)</td><td>CoNLL</td><td>150k words, dependency parsed (Basque, Catalan)</td></tr><tr><td>Dependency Treebank</td><td>Narad</td><td>Dependency parsed version of Penn Treebank sample</td></tr><tr><td>Floresta Treebank</td><td>Diana Santos et al.</td><td>9k sentences, tagged and parsed (Portuguese)</td></tr><tr><td>Gazetteer Lists</td><td>Various</td><td>Lists of cities and countries</td></tr><tr><td>Genesis Corpus</td><td>Misc web sources</td><td>6 texts, 200k words, 6 languages</td></tr><tr><td>Gutenberg (selections)</td><td>Hart, Newby, et al.</td><td>18 texts, 2M words</td></tr><tr><td>Inaugural Address Corpus</td><td>CSpan</td><td>U.S. Presidential Inaugural Addresses (1789–present)</td></tr><tr><td>Indian POS Tagged Corpus</td><td>Kumaran et al.</td><td>60k words, tagged (Bangla, Hindi, Marathi, Telugu)</td></tr><tr><td>MacMorpho Corpus</td><td>NILC, USP, Brazil</td><td>1M words, tagged (Brazilian Portuguese)</td></tr><tr><td>Movie Reviews</td><td>Pang, Lee</td><td>2k movie reviews with sentiment polarity classification</td></tr><tr><td>Names Corpus</td><td>Kantrowitz, Ross</td><td>8k male and female names</td></tr><tr><td>NIST 1999 Info Extr (selections)</td><td>Garofolo</td><td>63k words, newswire and named entity SGML markup</td></tr><tr><td>NPS Chat Corpus</td><td>Forsyth, Martell</td><td>10k IM chat posts, POS and dialogue-act tagged</td></tr><tr><td>Penn Treebank (selections)</td><td>LDC</td><td>40k words, tagged and parsed</td></tr><tr><td>PP Attachment Corpus</td><td>Ratnaparkh</td><td>28k prepositional phrases, tagged as noun or verb modifiers</td></tr><tr><td>Proposition Bank</td><td>Palmer</td><td>113k propositions, 3,300 verb frames</td></tr><tr><td>Question Classification</td><td>Li, Roth</td><td>6k questions, categorized</td></tr><tr><td>Reuters Corpus</td><td>Reuters</td><td>1.3M words, 10k news documents, categorized</td></tr><tr><td>Roget’s Thesaurus</td><td>Project Gutenberg</td><td>200k words, formatted text</td></tr><tr><td>RTE Textual Entailment</td><td>Dagan et al.</td><td>8k sentence pairs, categorized</td></tr><tr><td>SEMCOR</td><td>Rus, Mihalcea</td><td>880k words, POS and sense tagged</td></tr><tr><td>Senseval 2 Corpus</td><td>Pedersen</td><td>600k words, POS and sense tagged</td></tr><tr><td>Shakespeare texts (selections)</td><td>Bosak</td><td>8 books in XML format</td></tr><tr><td>State of the Union Corpus</td><td>CSpan</td><td>485k words, formatted text</td></tr><tr><td>Stopwords Corpus</td><td>Porter et al.</td><td>2,400 stopwords for 11 languages</td></tr><tr><td>Swadesh Corpus</td><td>Wiktionary</td><td>Comparative wordlists in 24 languages</td></tr><tr><td>Switchboard Corpus (selections)</td><td>LDC</td><td>36 phone calls, transcribed, parsed</td></tr><tr><td>TIMIT Corpus (selections)</td><td>NIST/LDC</td><td>Audio files and transcripts for 16 speakers</td></tr><tr><td>Univ Decl of Human Rights</td><td>United Nations</td><td>480k words, 300+ languages</td></tr><tr><td>VerbNet 2.1</td><td>Palmer et al.</td><td>5k verbs, hierarchically organized, linked to WordNet</td></tr><tr><td>Wordlist Corpus</td><td>OpenOffice.org et al.</td><td>960k words and 20k affixes for 8 languages</td></tr><tr><td>WordNet 3.0 (English)</td><td>Miller, Fellbaum</td><td>145k synonym sets</td></tr></table>",
          "level": -1,
          "page": 69,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_69_order_1",
          "label": "foot",
          "text": "2.1 Accessing Text Corpora | 47",
          "level": -1,
          "page": 69,
          "reading_order": 1,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "2.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_70_order_0",
      "label": "sec",
      "text": "Corpora in Other Languages",
      "level": 1,
      "page": 70,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        282,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_70_order_1",
          "label": "para",
          "text": "NLTK comes with corpora for many languages, though in some cases you will need to\nlearn how to manipulate character encodings in Python before using these corpora (see\nSection 3.3).",
          "level": -1,
          "page": 70,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_70_order_3",
          "label": "para",
          "text": "The last of these corpora, udhr , contains the Universal Declaration of Human Rights\nin over 300 languages. The fileids for this corpus include information about the char-\nacter encoding used in the file, such as UTF8 or Latin1 . Let's use a conditional frequency\ndistribution to examine the differences in word lengths for a selection of languages\nincluded in the udhr corpus. The output is shown in Figure 2 - 2 (run the program your-\nself to see a color plot). Note that True and False are Python's built-in Boolean values.",
          "level": -1,
          "page": 70,
          "reading_order": 3,
          "bbox": [
            97,
            349,
            585,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_70_order_5",
          "label": "para",
          "text": "Your Turn: Pick a language of interest in udhr.fileids(), and define a\nvariable raw_text = udhr.raw(Language-Latin1). Now plot a frequency\ndistribution of the letters of the text using",
          "level": -1,
          "page": 70,
          "reading_order": 5,
          "bbox": [
            171,
            591,
            530,
            637
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_70_order_6",
          "label": "para",
          "text": "nltk.FreqDist(raw_text).plot()",
          "level": -1,
          "page": 70,
          "reading_order": 6,
          "bbox": [
            197,
            645,
            342,
            656
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_70_order_7",
          "label": "para",
          "text": "Unfortunately, for many languages, substantial corpora are not yet available. Often\nthere is insufficient government or industrial support for developing language resour-\nces, and individual efforts are piecemeal and hard to discover or reuse. Some languages\nhave no established writing system, or are endangered. (See Section 2.7 for suggestions\non how to locate language resources.)",
          "level": -1,
          "page": 70,
          "reading_order": 7,
          "bbox": [
            97,
            680,
            585,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_70_order_8",
          "label": "foot",
          "text": "48 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 70,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "48",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_71_order_0",
          "label": "figure",
          "text": "Figure 2-2. Cumulative word length distributions: Six translations of the Universal Declaration of\nHuman Rights are processed; this graph shows that words having five or fewer letters account for\nabout 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text. [IMAGE: ![Figure](figures/NLTK_page_071_figure_000.png)]",
          "level": -1,
          "page": 71,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_071_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                465
              ],
              "page": 71,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 2-2. Cumulative word length distributions: Six translations of the Universal Declaration of\nHuman Rights are processed; this graph shows that words having five or fewer letters account for\nabout 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.",
              "bbox": [
                97,
                465,
                586,
                512
              ],
              "page": 71,
              "reading_order": 1
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_71_order_2",
      "label": "sec",
      "text": "Text Corpus Structure",
      "level": 1,
      "page": 71,
      "reading_order": 2,
      "bbox": [
        97,
        519,
        243,
        546
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_71_order_3",
          "label": "para",
          "text": "We have seen a variety of corpus structures so far; these are summarized in Fig-\nure 2 - 3 . The simplest kind lacks any structure: it is just a collection of texts. Often,\ntexts are grouped into categories that might correspond to genre, source, author, lan-\nguage, etc. Sometimes these categories overlap, notably in the case of topical categories,\nas a text can be relevant to more than one topic. Occasionally, text collections have\ntemporal structure, news collections being the most common example.",
          "level": -1,
          "page": 71,
          "reading_order": 3,
          "bbox": [
            97,
            546,
            584,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_71_order_4",
          "label": "para",
          "text": "NLTK’s corpus readers support efficient access to a variety of corpora, and can be used\nto work with new corpora. Table 2-3 lists functionality provided by the corpus readers.",
          "level": -1,
          "page": 71,
          "reading_order": 4,
          "bbox": [
            97,
            654,
            585,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_71_order_5",
          "label": "foot",
          "text": "2.1 Accessing Text Corpora | 49",
          "level": -1,
          "page": 71,
          "reading_order": 5,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "2.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_72_order_0",
          "label": "figure",
          "text": "Figure 2-3. Common structures for text corpora: The simplest kind of corpus is a collection of isolated\ntexts with no particular organization; some corpora are structured into categories, such as genre\n(Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other\ncorpora represent language use over time (Inaugural Address Corpus). [IMAGE: ![Figure](figures/NLTK_page_072_figure_000.png)]",
          "level": -1,
          "page": 72,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_072_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                197
              ],
              "page": 72,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 2-3. Common structures for text corpora: The simplest kind of corpus is a collection of isolated\ntexts with no particular organization; some corpora are structured into categories, such as genre\n(Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other\ncorpora represent language use over time (Inaugural Address Corpus).",
              "bbox": [
                97,
                197,
                585,
                259
              ],
              "page": 72,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_72_order_2",
          "label": "table",
          "text": "Table 2-3. Basic corpus functionality defined in NLTK: More documentation can be found using\nhelp(nltk.corpus.reader) and by reading the online Corpus HOWTO at http://www.nltk.org/howto. [TABLE: <table><tr><td>Example</td><td>Description</td></tr><tr><td>fileids()</td><td>The files of the corpus</td></tr><tr><td>fileids([categories])</td><td>The files of the corpus corresponding to these categories</td></tr><tr><td>categories()</td><td>The categories of the corpus</td></tr><tr><td>categories([fileids])</td><td>The categories of the corpus corresponding to these files</td></tr><tr><td>raw()</td><td>The raw content of the corpus</td></tr><tr><td>raw(fileids=[f1,f2,f3])</td><td>The raw content of the specified files</td></tr><tr><td>raw(categories=[c1,c2])</td><td>The raw content of the specified categories</td></tr><tr><td>words()</td><td>The words of the whole corpus</td></tr><tr><td>words(fileids=[f1,f2,f3])</td><td>The words of the specified fileids</td></tr><tr><td>words(categories=[c1,c2])</td><td>The words of the specified categories</td></tr><tr><td>sents()</td><td>The sentences of the specified categories</td></tr><tr><td>sents(fileids=[f1,f2,f3])</td><td>The sentences of the specified fileids</td></tr><tr><td>sents(categories=[c1,c2])</td><td>The sentences of the specified categories</td></tr><tr><td>abspath(fileid)</td><td>The location of the given file on disk</td></tr><tr><td>encoding(fileid)</td><td>The encoding of the file (if known)</td></tr><tr><td>open(fileid)</td><td>Open a stream for reading the given corpus file</td></tr><tr><td>root()</td><td>The path to the root of locally installed corpus</td></tr><tr><td>readme()</td><td>The contents of the README file of the corpus</td></tr></table>]",
          "level": -1,
          "page": 72,
          "reading_order": 2,
          "bbox": [
            100,
            313,
            468,
            690
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Example</td><td>Description</td></tr><tr><td>fileids()</td><td>The files of the corpus</td></tr><tr><td>fileids([categories])</td><td>The files of the corpus corresponding to these categories</td></tr><tr><td>categories()</td><td>The categories of the corpus</td></tr><tr><td>categories([fileids])</td><td>The categories of the corpus corresponding to these files</td></tr><tr><td>raw()</td><td>The raw content of the corpus</td></tr><tr><td>raw(fileids=[f1,f2,f3])</td><td>The raw content of the specified files</td></tr><tr><td>raw(categories=[c1,c2])</td><td>The raw content of the specified categories</td></tr><tr><td>words()</td><td>The words of the whole corpus</td></tr><tr><td>words(fileids=[f1,f2,f3])</td><td>The words of the specified fileids</td></tr><tr><td>words(categories=[c1,c2])</td><td>The words of the specified categories</td></tr><tr><td>sents()</td><td>The sentences of the specified categories</td></tr><tr><td>sents(fileids=[f1,f2,f3])</td><td>The sentences of the specified fileids</td></tr><tr><td>sents(categories=[c1,c2])</td><td>The sentences of the specified categories</td></tr><tr><td>abspath(fileid)</td><td>The location of the given file on disk</td></tr><tr><td>encoding(fileid)</td><td>The encoding of the file (if known)</td></tr><tr><td>open(fileid)</td><td>Open a stream for reading the given corpus file</td></tr><tr><td>root()</td><td>The path to the root of locally installed corpus</td></tr><tr><td>readme()</td><td>The contents of the README file of the corpus</td></tr></table>",
              "bbox": [
                100,
                313,
                468,
                690
              ],
              "page": 72,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Table 2-3. Basic corpus functionality defined in NLTK: More documentation can be found using\nhelp(nltk.corpus.reader) and by reading the online Corpus HOWTO at http://www.nltk.org/howto.",
              "bbox": [
                97,
                277,
                583,
                307
              ],
              "page": 72,
              "reading_order": 3
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_72_order_4",
          "label": "para",
          "text": "We illustrate the difference between some of the corpus access methods here:",
          "level": -1,
          "page": 72,
          "reading_order": 4,
          "bbox": [
            98,
            707,
            539,
            727
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_72_order_6",
          "label": "foot",
          "text": "50 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 72,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "50",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_73_order_1",
      "label": "sec",
      "text": "Loading Your Own Corpus",
      "level": 1,
      "page": 73,
      "reading_order": 1,
      "bbox": [
        98,
        188,
        270,
        215
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_74_order_0",
          "label": "sub_sec",
          "text": "2.2 Conditional Frequency Distributions",
          "level": 2,
          "page": 74,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            416,
            100
          ],
          "section_number": "2.2",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_74_order_5",
              "label": "sub_sub_sec",
              "text": "Conditions and Events",
              "level": 3,
              "page": 74,
              "reading_order": 5,
              "bbox": [
                97,
                519,
                244,
                546
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_74_order_6",
                  "label": "para",
                  "text": "A frequency distribution counts observable events, such as the appearance of words in\na text. A conditional frequency distribution needs to pair each event with a condition.\nSo instead of processing a sequence of words 1 , we have to process a sequence of\npairs 2 :",
                  "level": -1,
                  "page": 74,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    554,
                    584,
                    618
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_74_order_8",
                  "label": "para",
                  "text": "Each pair has the form (condition, event). If we were processing the entire Brown\nCorpus by genre, there would be 15 conditions (one per genre) and 1,161,192 events\n(one per word).",
                  "level": -1,
                  "page": 74,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    661,
                    585,
                    708
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            },
            {
              "id": "page_74_order_9",
              "label": "sub_sub_sec",
              "text": "Counting Words by Genre",
              "level": 3,
              "page": 74,
              "reading_order": 9,
              "bbox": [
                97,
                724,
                270,
                743
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_74_order_10",
                  "label": "para",
                  "text": "In Section 2.1 , we saw a conditional frequency distribution where the condition was\nthe section of the Brown Corpus, and for each condition we counted words. Whereas\nFreqDist() takes a simple list as input, ConditionalFreqDist() takes a list of pairs.",
                  "level": -1,
                  "page": 74,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    751,
                    585,
                    798
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_74_order_11",
                  "label": "foot",
                  "text": "52 | Chapter 2: Accessing Text Corpora and Lexical Resources",
                  "level": -1,
                  "page": 74,
                  "reading_order": 11,
                  "bbox": [
                    97,
                    824,
                    353,
                    842
                  ],
                  "section_number": "52",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_75_order_1",
                  "label": "para",
                  "text": "Let’s break this down, and look at just two genres, news and romance. For each genre ❷\n,\nwe loop over every word in the genre ❸, producing pairs consisting of the genre and\nthe word ❶:",
                  "level": -1,
                  "page": 75,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    148,
                    584,
                    197
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_75_order_3",
                  "label": "para",
                  "text": "So, as we can see in the following code, pairs at the beginning of the list genre_word will\nbe of the form ('news', word) ❶, whereas those at the end will be of the form ('roman\nce', word) ❷.",
                  "level": -1,
                  "page": 75,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    277,
                    584,
                    323
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_75_order_5",
                  "label": "para",
                  "text": "We can now use this list of pairs to create a ConditionalFreqDist, and save it in a variable\ncfd. As usual, we can type the name of the variable to inspect it ❶, and verify it has two\nconditions ❷:",
                  "level": -1,
                  "page": 75,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    394,
                    585,
                    439
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_75_order_7",
                  "label": "para",
                  "text": "Let’s access the two conditions, and satisfy ourselves that each is just a frequency\ndistribution:",
                  "level": -1,
                  "page": 75,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    519,
                    585,
                    555
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_74_order_1",
              "label": "para",
              "text": "We introduced frequency distributions in Section 1.3 . We saw that given some list\nmylist of words or other items, FreqDist(mylist) would compute the number of\noccurrences of each item in the list. Here we will generalize this idea.",
              "level": -1,
              "page": 74,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                586,
                156
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_74_order_2",
              "label": "para",
              "text": "When the texts of a corpus are divided into several categories (by genre, topic, author,\netc.), we can maintain separate frequency distributions for each category. This will\nallow us to study systematic differences between the categories. In the previous section,\nwe achieved this using NLTK's ConditionalFreqDist data type. A conditional fre-\nquency distribution is a collection of frequency distributions, each one for a different\n“ condition. ” The condition will often be the category of the text. Figure 2-4 depicts a\nfragment of a conditional frequency distribution having just two conditions, one for\nnews text and one for romance text.",
              "level": -1,
              "page": 74,
              "reading_order": 2,
              "bbox": [
                97,
                161,
                585,
                295
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_74_order_3",
              "label": "figure",
              "text": "Figure 2-4. Counting words appearing in a text collection (a conditional frequency distribution). [IMAGE: ![Figure](figures/NLTK_page_074_figure_003.png)]",
              "level": -1,
              "page": 74,
              "reading_order": 3,
              "bbox": [
                100,
                304,
                583,
                474
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_074_figure_003.png)",
                  "bbox": [
                    100,
                    304,
                    583,
                    474
                  ],
                  "page": 74,
                  "reading_order": 3
                },
                {
                  "label": "cap",
                  "text": "Figure 2-4. Counting words appearing in a text collection (a conditional frequency distribution).",
                  "bbox": [
                    97,
                    490,
                    566,
                    503
                  ],
                  "page": 74,
                  "reading_order": 4
                }
              ],
              "is_merged": true
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_73_order_2",
          "label": "para",
          "text": "If you have a your own collection of text files that you would like to access using the\nmethods discussed earlier, you can easily load them with the help of NLTK's Plain\ntextCorpusReader . Check the location of your files on your file system; in the following\nexample, we have taken this to be the directory /usr/share/dict . Whatever the location,\nset this to be the value of corpus_root O . The second parameter of the PlaintextCor\npusReader initializer O can be a list of fileids, like ['a.txt', 'test/b.txt'] , or a pattern\nthat matches all fileids, like '[abc]/.*\\.txt' (see Section 3.4 for information about\nregular expressions).",
          "level": -1,
          "page": 73,
          "reading_order": 2,
          "bbox": [
            97,
            220,
            585,
            350
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_73_order_4",
          "label": "para",
          "text": "As another example, suppose you have your own local copy of Penn Treebank (release\n3), in C:\\corpora. We can use the BracketParseCorpusReader to access this corpus. We\nspecify the corpus_root to be the location of the parsed Wall Street Journal component\nof the corpus ❶ , and give a file_pattern that matches the files contained within its\nsubfolders ❷ (using forward slashes).",
          "level": -1,
          "page": 73,
          "reading_order": 4,
          "bbox": [
            97,
            456,
            585,
            538
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_73_order_6",
          "label": "foot",
          "text": "2.1 Accessing Text Corpora | 51",
          "level": -1,
          "page": 73,
          "reading_order": 6,
          "bbox": [
            448,
            824,
            584,
            842
          ],
          "section_number": "2.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_75_order_9",
      "label": "sec",
      "text": "Plotting and Tabulating Distributions",
      "level": 1,
      "page": 75,
      "reading_order": 9,
      "bbox": [
        98,
        707,
        350,
        726
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_75_order_10",
          "label": "para",
          "text": "Apart from combining two or more frequency distributions, and being easy to initialize,\na ConditionalFreqDist provides some useful methods for tabulation and plotting.",
          "level": -1,
          "page": 75,
          "reading_order": 10,
          "bbox": [
            97,
            734,
            584,
            765
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_75_order_11",
          "label": "foot",
          "text": "2.2 Conditional Frequency Distributions | 53",
          "level": -1,
          "page": 75,
          "reading_order": 11,
          "bbox": [
            395,
            824,
            584,
            842
          ],
          "section_number": "2.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_76_order_0",
          "label": "para",
          "text": "The plot in Figure 2-1 was based on a conditional frequency distribution reproduced\nin the following code. The condition is either of the words america or citizen ☺ , and\nthe counts being plotted are the number of times the word occurred in a particular\nspeech. It exploits the fact that the filename for each speech—for example,\n1865-Lincoln.txt —contains the year as the first four characters ☺ . This code generates\nthe pair ('america', '1865') for every instance of a word whose lowercased form starts\nwith america —such as Americans —in the file 1865-Lincoln.txt .",
          "level": -1,
          "page": 76,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_76_order_2",
          "label": "para",
          "text": "The plot in Figure 2-2 was also based on a conditional frequency distribution, repro­\nduced in the following code. This time, the condition is the name of the language, and\nthe counts being plotted are derived from word lengths ❶ . It exploits the fact that the\nfilename for each language is the language name followed by  '-Latin1'  (the character\nencoding).",
          "level": -1,
          "page": 76,
          "reading_order": 2,
          "bbox": [
            97,
            295,
            585,
            377
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_76_order_4",
          "label": "para",
          "text": "In the plot() and tabulate() methods, we can optionally specify which conditions to\ndisplay with a conditions= parameter. When we omit it, we get all the conditions.\nSimilarly, we can limit the samples to display with a samples= parameter. This makes\nit possible to load a large quantity of data into a conditional frequency distribution,\nand then to explore it by plotting or tabulating selected conditions and samples. It also\ngives us full control over the order of conditions and samples in any displays. For ex-\nample, we can tabulate the cumulative frequency data just for two languages, and for\nwords less than 10 characters long, as shown next. We interpret the last cell on the top\nrow to mean that 1,638 words of the English text have nine or fewer letters.",
          "level": -1,
          "page": 76,
          "reading_order": 4,
          "bbox": [
            97,
            483,
            585,
            632
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_76_order_6",
          "label": "foot",
          "text": "54 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 76,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "54",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_77_order_0",
          "label": "para",
          "text": "Your Turn: Working with the news and romance genres from the\nBrown Corpus, find out which days of the week are most newsworthy,\nand which are most romantic. Define a variable called days containing\na list of days of the week, i.e., ['Monday', ...]. Now tabulate the counts\nfor these words using cfd.tabulate(samples=days). Now try the same\nthing using plot in place of tabulate. You may control the output order\nof\ndays\nwith\nthe\nhelp\nof\nan\nextra\nparameter:\ncondi\ntions=['Monday', ...].",
          "level": -1,
          "page": 77,
          "reading_order": 0,
          "bbox": [
            171,
            80,
            530,
            201
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_77_order_1",
          "label": "para",
          "text": "You may have noticed that the multiline expressions we have been using with condi-\ntional frequency distributions look like list comprehensions, but without the brackets.\nIn general, when we use a list comprehension as a parameter to a function, like\nset([w.lower for w in t]) , we are permitted to omit the square brackets and just write\nset(w.lower() for w in t) . (See the discussion of “ generator expressions ” in Sec-\ntion 4.2 for more about this.)",
          "level": -1,
          "page": 77,
          "reading_order": 1,
          "bbox": [
            97,
            224,
            585,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_77_order_2",
      "label": "sec",
      "text": "Generating Random Text with Bigram",
      "level": 1,
      "page": 77,
      "reading_order": 2,
      "bbox": [
        99,
        337,
        350,
        358
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_78_order_5",
          "label": "sub_sec",
          "text": "2.3 More Python: Reusing Code",
          "level": 2,
          "page": 78,
          "reading_order": 5,
          "bbox": [
            97,
            510,
            350,
            537
          ],
          "section_number": "2.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_78_order_7",
              "label": "sub_sub_sec",
              "text": "Creating Programs with a Text Editor",
              "level": 3,
              "page": 78,
              "reading_order": 7,
              "bbox": [
                99,
                642,
                343,
                663
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_78_order_8",
                  "label": "para",
                  "text": "The Python interactive interpreter performs your instructions as soon as you type them.\nOften, it is better to compose a multiline program using a text editor, then ask Python\nto run the whole program at once. Using IDLE, you can do this by going to the File\nmenu and opening a new window. Try this now, and enter the following one-line\nprogram:",
                  "level": -1,
                  "page": 78,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    669,
                    585,
                    752
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_78_order_9",
                  "label": "para",
                  "text": "print 'Monty Python",
                  "level": -1,
                  "page": 78,
                  "reading_order": 9,
                  "bbox": [
                    122,
                    758,
                    225,
                    770
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_78_order_10",
                  "label": "foot",
                  "text": "56 | Chapter 2: Accessing Text Corpora and Lexical Resources",
                  "level": -1,
                  "page": 78,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    353,
                    842
                  ],
                  "section_number": "56",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_79_order_0",
                  "label": "para",
                  "text": "Save this program in a file called monty.py, then go to the Run menu and select the\ncommand Run Module. (We’ll learn what modules are shortly.) The result in the main\nIDLE window should look like this:",
                  "level": -1,
                  "page": 79,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_79_order_2",
                  "label": "para",
                  "text": "You can also type from monty import * and it will do the same thing.",
                  "level": -1,
                  "page": 79,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    188,
                    494,
                    206
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_79_order_3",
                  "label": "para",
                  "text": "From now on, you have a choice of using the interactive interpreter or a text editor to\ncreate your programs. It is often convenient to test your ideas using the interpreter,\nrevising a line of code until it does what you expect. Once you’re ready, you can paste\nthe code (minus any >>> or ... prompts) into the text editor, continue to expand it,\nand finally save the program in a file so that you don’t have to type it in again later.\nGive the file a short but descriptive name, using all lowercase letters and separating\nwords with underscore, and using the .py filename extension, e.g., monty_python.py.",
                  "level": -1,
                  "page": 79,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    215,
                    585,
                    331
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_79_order_4",
                  "label": "para",
                  "text": "Important: Our inline code examples include the >>> and ... prompts\nas if we are interacting directly with the interpreter. As they get more\ncomplicated, you should instead type them into the editor, without the\nprompts, and run them from the editor as shown earlier. When we pro-\nvide longer programs in this book, we will leave out the prompts to\nremind you to type them into a file rather than using the interpreter.\nYou can see this already in Example 2-1 . Note that the example still\nincludes a couple of lines with the Python prompt; this is the interactive\npart of the task where you inspect some data and invoke a function.\nRemember that all code samples like Example 2-1 are downloadable\nfrom http://www.nltk.org/ .",
                  "level": -1,
                  "page": 79,
                  "reading_order": 4,
                  "bbox": [
                    171,
                    357,
                    530,
                    519
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_79_order_5",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_079_figure_005.png)",
                  "level": -1,
                  "page": 79,
                  "reading_order": 5,
                  "bbox": [
                    118,
                    347,
                    171,
                    403
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_78_order_6",
              "label": "para",
              "text": "By this time you’ve probably typed and retyped a lot of code in the Python interactive\ninterpreter. If you mess up when retyping a complex example, you have to enter it again.\nUsing the arrow keys to access and modify previous commands is helpful but only goes\nso far. In this section, we see two important ways to reuse code: text editors and Python\nfunctions.",
              "level": -1,
              "page": 78,
              "reading_order": 6,
              "bbox": [
                97,
                546,
                585,
                627
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_77_order_3",
          "label": "para",
          "text": "We can use a conditional frequency distribution to create a table of bigrams (word\npairs, introduced in Section 1.3 ). The bigrams() function takes a list of words and builds\na list of consecutive word pairs:",
          "level": -1,
          "page": 77,
          "reading_order": 3,
          "bbox": [
            97,
            365,
            585,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_77_order_5",
          "label": "para",
          "text": "In Example 2-1, we treat each word as a condition, and for each one we effectively\ncreate a frequency distribution over the following words. The function gener\nate_model() contains a simple loop to generate text. When we call the function, we\nchoose a word (such as 'living' ) as our initial context. Then, once inside the loop, we\nprint the current value of the variable word , and reset word to be the most likely token\nin that context (using max() ); next time through the loop, we use that word as our new\ncontext. As you can see by inspecting the output, this simple approach to text gener-\nation tends to get stuck in loops. Another method would be to randomly choose the\nnext word from among the available words.",
          "level": -1,
          "page": 77,
          "reading_order": 5,
          "bbox": [
            97,
            501,
            585,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_77_order_6",
          "label": "para",
          "text": "Example 2-1. Generating random text: This program obtains all bigrams from the text of the book\nof Genesis, then constructs a conditional frequency distribution to record which words are most likely\nto follow a given word; e.g., after the word living, the most likely word is creature; the\ngenerate_model() function uses this data, and a seed word, to generate random text.",
          "level": -1,
          "page": 77,
          "reading_order": 6,
          "bbox": [
            97,
            663,
            585,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_77_order_8",
          "label": "foot",
          "text": "2.2 Conditional Frequency Distributions | 55",
          "level": -1,
          "page": 77,
          "reading_order": 8,
          "bbox": [
            395,
            824,
            585,
            842
          ],
          "section_number": "2.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_78_order_2",
          "label": "para",
          "text": "Conditional frequency distributions are a useful data structure for many NLP tasks.\nTheir commonly used methods are summarized in Table 2-4.",
          "level": -1,
          "page": 78,
          "reading_order": 2,
          "bbox": [
            97,
            206,
            584,
            242
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_78_order_3",
          "label": "table",
          "text": "Table 2-4. NLTK’s conditional frequency distributions: Commonly used methods and idioms for\ndefining, accessing, and visualizing a conditional frequency distribution of counters [TABLE: <table><tr><td>Example</td><td>Description</td></tr><tr><td>cfdist = ConditionalFreqDist(pairs)</td><td>Create a conditional frequency distribution from a list of pairs</td></tr><tr><td>cfdist.conditions()</td><td>Alphabetically sorted list of conditions</td></tr><tr><td>cfdist[condition]</td><td>The frequency distribution for this condition</td></tr><tr><td>cfdist[condition][sample]</td><td>Frequency for the given sample for this condition</td></tr><tr><td>cfdist.tabulate()</td><td>Tabulate the conditional frequency distribution</td></tr><tr><td>cfdist.tabulate(samples, conditions)</td><td>Tabulation limited to the specified samples and conditions</td></tr><tr><td>cfdist.plot()</td><td>Graphical plot of the conditional frequency distribution</td></tr><tr><td>cfdist.plot(samples, conditions)</td><td>Graphical plot limited to the specified samples and conditions</td></tr><tr><td>cfdist1 &lt; cfdist2</td><td>Test if samples in c fdist1 occur less frequently than in c fdist2</td></tr></table>]",
          "level": -1,
          "page": 78,
          "reading_order": 3,
          "bbox": [
            100,
            286,
            583,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Example</td><td>Description</td></tr><tr><td>cfdist = ConditionalFreqDist(pairs)</td><td>Create a conditional frequency distribution from a list of pairs</td></tr><tr><td>cfdist.conditions()</td><td>Alphabetically sorted list of conditions</td></tr><tr><td>cfdist[condition]</td><td>The frequency distribution for this condition</td></tr><tr><td>cfdist[condition][sample]</td><td>Frequency for the given sample for this condition</td></tr><tr><td>cfdist.tabulate()</td><td>Tabulate the conditional frequency distribution</td></tr><tr><td>cfdist.tabulate(samples, conditions)</td><td>Tabulation limited to the specified samples and conditions</td></tr><tr><td>cfdist.plot()</td><td>Graphical plot of the conditional frequency distribution</td></tr><tr><td>cfdist.plot(samples, conditions)</td><td>Graphical plot limited to the specified samples and conditions</td></tr><tr><td>cfdist1 &lt; cfdist2</td><td>Test if samples in c fdist1 occur less frequently than in c fdist2</td></tr></table>",
              "bbox": [
                100,
                286,
                583,
                492
              ],
              "page": 78,
              "reading_order": 3
            },
            {
              "label": "cap",
              "text": "Table 2-4. NLTK’s conditional frequency distributions: Commonly used methods and idioms for\ndefining, accessing, and visualizing a conditional frequency distribution of counters",
              "bbox": [
                97,
                250,
                566,
                286
              ],
              "page": 78,
              "reading_order": 4
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_79_order_6",
      "label": "sec",
      "text": "Functions",
      "level": 1,
      "page": 79,
      "reading_order": 6,
      "bbox": [
        98,
        537,
        162,
        556
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_81_order_0",
          "label": "sub_sub_sec",
          "text": "Modules",
          "level": 3,
          "page": 81,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            154,
            91
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_81_order_1",
              "label": "para",
              "text": "Over time you will find that you create a variety of useful little text-processing functions,\nand you end up copying them from old programs to new ones. Which file contains the\nlatest version of the function you want to use? It makes life a lot easier if you can collect\nyour work into a single place, and access previously defined functions without making\ncopies.",
              "level": -1,
              "page": 81,
              "reading_order": 1,
              "bbox": [
                97,
                98,
                585,
                188
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_81_order_2",
              "label": "para",
              "text": "To do this, save your function(s) in a file called (say) textproc.py. Now, you can access\nyour work simply by importing it from the file:",
              "level": -1,
              "page": 81,
              "reading_order": 2,
              "bbox": [
                97,
                188,
                585,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_81_order_4",
              "label": "para",
              "text": "Our plural function obviously has an error, since the plural of fan is fans. Instead of\ntyping in a new version of the function, we can simply edit the existing one. Thus, at\nevery stage, there is only one version of our plural function, and no confusion about\nwhich one is being used.",
              "level": -1,
              "page": 81,
              "reading_order": 4,
              "bbox": [
                97,
                304,
                584,
                370
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_81_order_5",
              "label": "para",
              "text": "A collection of variable and function definitions in a file is called a Python module. A\ncollection of related modules is called a package. NLTK’s code for processing the\nBrown Corpus is an example of a module, and its collection of code for processing all\nthe different corpora is an example of a package. NLTK itself is a set of packages,\nsometimes called a library.",
              "level": -1,
              "page": 81,
              "reading_order": 5,
              "bbox": [
                97,
                376,
                585,
                460
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_81_order_6",
              "label": "para",
              "text": "Caution!",
              "level": -1,
              "page": 81,
              "reading_order": 6,
              "bbox": [
                171,
                474,
                209,
                492
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_81_order_7",
              "label": "para",
              "text": "If you are creating a file to contain some of your Python code, do not\nname your file nltk.py: it may get imported in place of the “real” NLTK\npackage. When it imports modules, Python first looks in the current\ndirectory (folder).",
              "level": -1,
              "page": 81,
              "reading_order": 7,
              "bbox": [
                171,
                492,
                530,
                555
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_81_order_8",
          "label": "sub_sec",
          "text": "2.4 Lexical Resources",
          "level": 2,
          "page": 81,
          "reading_order": 8,
          "bbox": [
            97,
            580,
            270,
            600
          ],
          "section_number": "2.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_81_order_9",
              "label": "para",
              "text": "A lexicon, or lexical resource, is a collection of words and/or phrases along with asso-\nciated information, such as part-of-speech and sense definitions. Lexical resources are\nsecondary to texts, and are usually created and enriched with the help of texts. For\nexample, if we have defined a text my_text , then vocab = sorted(set(my_text)) builds\nthe vocabulary of my_text , whereas word_freq = FreqDist(my_text) counts the fre-\nquency of each word in the text. Both vocab and word_freq are simple lexical resources.\nSimilarly, a concordance like the one we saw in Section 1.1 gives us information about\nword usage that might help in the preparation of a dictionary. Standard terminology\nfor lexicons is illustrated in Figure 2 - 5 . A lexical entry consists of a headword (also\nknown as a lemma ) along with additional information, such as the part-of-speech and",
              "level": -1,
              "page": 81,
              "reading_order": 9,
              "bbox": [
                97,
                609,
                585,
                779
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_81_order_10",
              "label": "foot",
              "text": "2.4 Lexical Resources | 59",
              "level": -1,
              "page": 81,
              "reading_order": 10,
              "bbox": [
                467,
                824,
                585,
                842
              ],
              "section_number": "2.4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_82_order_0",
              "label": "figure",
              "text": "Figure 2-5. Lexicon terminology: Lexical entries for two lemmas having the same spelling\n(homonyms), providing part-of-speech and gloss information. [IMAGE: ![Figure](figures/NLTK_page_082_figure_000.png)]",
              "level": -1,
              "page": 82,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                583,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_082_figure_000.png)",
                  "bbox": [
                    100,
                    71,
                    583,
                    215
                  ],
                  "page": 82,
                  "reading_order": 0
                },
                {
                  "label": "cap",
                  "text": "Figure 2-5. Lexicon terminology: Lexical entries for two lemmas having the same spelling\n(homonyms), providing part-of-speech and gloss information.",
                  "bbox": [
                    97,
                    224,
                    584,
                    254
                  ],
                  "page": 82,
                  "reading_order": 1
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_82_order_2",
              "label": "para",
              "text": "the sense definition. Two distinct words having the same spelling are called\nhomonyms .",
              "level": -1,
              "page": 82,
              "reading_order": 2,
              "bbox": [
                97,
                265,
                583,
                295
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_82_order_3",
              "label": "para",
              "text": "The simplest kind of lexicon is nothing more than a sorted list of words. Sophisticated\nlexicons include complex structure within and across the individual entries. In this\nsection, we’ll look at some lexical resources included with NLTK.",
              "level": -1,
              "page": 82,
              "reading_order": 3,
              "bbox": [
                97,
                304,
                585,
                350
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_79_order_7",
          "label": "para",
          "text": "Suppose that you work on analyzing text that involves different forms of the same word,\nand that part of your program needs to work out the plural form of a given singular\nnoun. Suppose it needs to do this work in two places, once when it is processing some\ntexts and again when it is processing user input.",
          "level": -1,
          "page": 79,
          "reading_order": 7,
          "bbox": [
            97,
            564,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_79_order_8",
          "label": "para",
          "text": "Rather than repeating the same code several times over, it is more efficient and reliable\nto localize this work inside a function. A function is just a named block of code that\nperforms some well-defined task, as we saw in Section 1.1 . A function is usually defined\nto take some inputs, using special variables known as parameters, and it may produce\na result, also known as a return value. We define a function using the keyword def\nfollowed by the function name and any input parameters, followed by the body of the\nfunction. Here's the function we saw in Section 1.1 (including the import statement\nthat makes division behave as expected):",
          "level": -1,
          "page": 79,
          "reading_order": 8,
          "bbox": [
            97,
            636,
            585,
            772
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_79_order_9",
          "label": "foot",
          "text": "2.3 More Python: Reusing Code | 57",
          "level": -1,
          "page": 79,
          "reading_order": 9,
          "bbox": [
            430,
            824,
            585,
            842
          ],
          "section_number": "2.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_80_order_1",
          "label": "para",
          "text": "We use the keyword return to indicate the value that is produced as output by the\nfunction. In this example, all the work of the function is done in the return statement.\nHere's an equivalent definition that does the same work using multiple lines of code.\nWe'll change the parameter name from text to my_text_data to remind you that this is\nan arbitrary choice:",
          "level": -1,
          "page": 80,
          "reading_order": 1,
          "bbox": [
            97,
            122,
            585,
            199
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_80_order_3",
          "label": "para",
          "text": "Notice that we’ve created some new variables inside the body of the function. These\nare local variables and are not accessible outside the function. So now we have defined\na function with the name lexical_diversity. But just defining it won’t produce any\noutput! Functions do nothing until they are “called” (or “invoked”).",
          "level": -1,
          "page": 80,
          "reading_order": 3,
          "bbox": [
            97,
            285,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_80_order_4",
          "label": "para",
          "text": "Let’s return to our earlier scenario, and actually define a simple function to work out\nEnglish plurals. The function plural() in Example 2-2 takes a singular noun and gen-\nerates a plural form, though it is not always correct. (We’ll discuss functions at greater\nlength in Section 4.4.)",
          "level": -1,
          "page": 80,
          "reading_order": 4,
          "bbox": [
            97,
            358,
            585,
            422
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_80_order_5",
          "label": "para",
          "text": "Example 2-2. A Python function: This function tries to work out the plural form of any English noun;\nthe keyword def (define) is followed by the function name, then a parameter inside parentheses, and\na colon; the body of the function is the indented block of code; it tries to recognize patterns within the\nword and process the word accordingly; e.g., if the word ends with y, delete the y and add ies.",
          "level": -1,
          "page": 80,
          "reading_order": 5,
          "bbox": [
            97,
            436,
            585,
            493
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_80_order_8",
          "label": "para",
          "text": "The endswith() function is always associated with a string object (e.g., word in Exam-\nple 2-2). To call such functions, we give the name of the object, a period, and then the\nname of the function. These functions are usually known as methods.",
          "level": -1,
          "page": 80,
          "reading_order": 8,
          "bbox": [
            100,
            689,
            585,
            737
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_80_order_9",
          "label": "foot",
          "text": "58 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 80,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "58",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_82_order_4",
      "label": "sec",
      "text": "Wordlist Corpora",
      "level": 1,
      "page": 82,
      "reading_order": 4,
      "bbox": [
        97,
        367,
        209,
        387
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_82_order_5",
          "label": "para",
          "text": "NLTK includes some corpora that are nothing more than wordlists. The Words Corpus\nis the /usr/dict/words file from Unix, used by some spellcheckers. We can use it to find\nunusual or misspelled words in a text corpus, as shown in Example 2-3.",
          "level": -1,
          "page": 82,
          "reading_order": 5,
          "bbox": [
            97,
            394,
            585,
            442
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_82_order_6",
          "label": "para",
          "text": "Example 2-3. Filtering a text: This program computes the vocabulary of a text, then removes all items\nthat occur in an existing wordlist, leaving just the uncommon or misspelled words.",
          "level": -1,
          "page": 82,
          "reading_order": 6,
          "bbox": [
            97,
            456,
            584,
            484
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_82_order_9",
          "label": "para",
          "text": "There is also a corpus of stopwords, that is, high-frequency words such as the, to, and\nalso that we sometimes want to filter out of a document before further processing.\nStopwords usually have little lexical content, and their presence in a text fails to dis-\ntinguish it from other texts.",
          "level": -1,
          "page": 82,
          "reading_order": 9,
          "bbox": [
            97,
            680,
            584,
            747
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_82_order_11",
          "label": "foot",
          "text": "60 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 82,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "60",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_83_order_1",
          "label": "para",
          "text": "Let’s define a function to compute what fraction of words in a text are not in the stop­\nwords list:",
          "level": -1,
          "page": 83,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            584,
            137
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_83_order_3",
          "label": "para",
          "text": "Thus, with the help of stopwords, we filter out a third of the words of the text. Notice\nthat we’ve combined two different kinds of corpus here, using a lexical resource to filter\nthe content of a text corpus.",
          "level": -1,
          "page": 83,
          "reading_order": 3,
          "bbox": [
            97,
            248,
            585,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_83_order_4",
          "label": "figure",
          "text": "Figure 2-6. A word puzzle: A grid of randomly chosen letters with rules for creating words out of the\nletters; this puzzle is known as\n“Target.\n” [IMAGE: ![Figure](figures/NLTK_page_083_figure_004.png)]",
          "level": -1,
          "page": 83,
          "reading_order": 4,
          "bbox": [
            100,
            313,
            583,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_083_figure_004.png)",
              "bbox": [
                100,
                313,
                583,
                421
              ],
              "page": 83,
              "reading_order": 4
            },
            {
              "label": "cap",
              "text": "Figure 2-6. A word puzzle: A grid of randomly chosen letters with rules for creating words out of the\nletters; this puzzle is known as\n“Target.\n”",
              "bbox": [
                97,
                430,
                585,
                458
              ],
              "page": 83,
              "reading_order": 5
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_83_order_6",
          "label": "para",
          "text": "A wordlist is useful for solving word puzzles, such as the one in Figure 2-6. Our program\niterates through every word and, for each one, checks whether it meets the conditions.\nIt is easy to check obligatory letter ❷ and length ❸ constraints (and we’ll only look for\nwords with six or more letters here). It is trickier to check that candidate solutions only\nuse combinations of the supplied letters, especially since some of the supplied letters\nappear twice (here, the letter v). The FreqDist comparison method ❸ permits us to\ncheck that the frequency of each letter in the candidate word is less than or equal to the\nfrequency of the corresponding letter in the puzzle.",
          "level": -1,
          "page": 83,
          "reading_order": 6,
          "bbox": [
            97,
            483,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_83_order_8",
          "label": "para",
          "text": "One more wordlist corpus is the Names Corpus, containing 8,000 first names catego-\nrized by gender. The male and female names are stored in separate files. Let’s find names\nthat appear in both files, i.e., names that are ambiguous for gender:",
          "level": -1,
          "page": 83,
          "reading_order": 8,
          "bbox": [
            97,
            751,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_83_order_9",
          "label": "foot",
          "text": "2.4 Lexical Resources|61",
          "level": -1,
          "page": 83,
          "reading_order": 9,
          "bbox": [
            467,
            824,
            584,
            842
          ],
          "section_number": "2.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_84_order_1",
          "label": "para",
          "text": "It is well known that names ending in the letter a are almost always female. We can see\nthis and some other patterns in the graph in Figure 2-7, produced by the following code.\nRemember that name[-1] is the last letter of name.",
          "level": -1,
          "page": 84,
          "reading_order": 1,
          "bbox": [
            97,
            197,
            585,
            246
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_84_order_3",
          "label": "figure",
          "text": "Figure 2-7. Conditional frequency distribution: This plot shows the number of female and male names\nending with each letter of the alphabet; most names ending with a, e, or i are female; names ending\nin h and l are equally likely to be male or female; names ending in k, o, r, s, and t are likely to be male. [IMAGE: ![Figure](figures/NLTK_page_084_figure_003.png)]",
          "level": -1,
          "page": 84,
          "reading_order": 3,
          "bbox": [
            100,
            349,
            583,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_084_figure_003.png)",
              "bbox": [
                100,
                349,
                583,
                734
              ],
              "page": 84,
              "reading_order": 3
            },
            {
              "label": "cap",
              "text": "Figure 2-7. Conditional frequency distribution: This plot shows the number of female and male names\nending with each letter of the alphabet; most names ending with a, e, or i are female; names ending\nin h and l are equally likely to be male or female; names ending in k, o, r, s, and t are likely to be male.",
              "bbox": [
                97,
                741,
                584,
                784
              ],
              "page": 84,
              "reading_order": 4
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_84_order_5",
          "label": "foot",
          "text": "62 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 84,
          "reading_order": 5,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "62",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_85_order_0",
      "label": "sec",
      "text": "A Pronouncing Dictionary",
      "level": 1,
      "page": 85,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        270,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_85_order_1",
          "label": "para",
          "text": "A slightly richer kind of lexical resource is a table (or spreadsheet), containing a word\nplus some properties in each row. NLTK includes the CMU Pronouncing Dictionary\nfor U.S. English, which was designed for use by speech synthesizers.",
          "level": -1,
          "page": 85,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_85_order_3",
          "label": "para",
          "text": "For each word, this lexicon provides a list of phonetic codes—distinct labels for each\ncontrastive sound—known as phones . Observe that fire has two pronunciations (in\nU.S. English): the one-syllable F AY1 R , and the two-syllable F AY1 ERO . The symbols\nin the CMU Pronouncing Dictionary are from the Arpabet , described in more detail at\nhttp://en.wikipedia.org/wiki/Arpabet .",
          "level": -1,
          "page": 85,
          "reading_order": 3,
          "bbox": [
            97,
            349,
            585,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_85_order_4",
          "label": "para",
          "text": "Each entry consists of two parts, and we can process these individually using a more\ncomplex version of the for statement. Instead of writing for entry in entries:, we\nreplace entry with two variable names, word, pron ①. Now, each time through the loop,\nword is assigned the first part of the entry, and pron is assigned the second part of the\nentry:",
          "level": -1,
          "page": 85,
          "reading_order": 4,
          "bbox": [
            97,
            439,
            585,
            520
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_85_order_6",
          "label": "para",
          "text": "The program just shown scans the lexicon looking for entries whose pronunciation\nconsists of three phones ❷ . If the condition is true, it assigns the contents of pron to\nthree new variables: ph1 , ph2 , and ph3 . Notice the unusual form of the statement that\ndoes that work ❸ .",
          "level": -1,
          "page": 85,
          "reading_order": 6,
          "bbox": [
            97,
            645,
            585,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_85_order_7",
          "label": "para",
          "text": "Here’s another example of the same for statement, this time used inside a list compre-\nhension. This program finds all words whose pronunciation ends with a syllable\nsounding like nicks. You could use this method to find rhyming words.",
          "level": -1,
          "page": 85,
          "reading_order": 7,
          "bbox": [
            97,
            725,
            585,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_85_order_8",
          "label": "foot",
          "text": "2.4 Lexical Resources | 63",
          "level": -1,
          "page": 85,
          "reading_order": 8,
          "bbox": [
            467,
            824,
            584,
            842
          ],
          "section_number": "2.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_86_order_1",
          "label": "para",
          "text": "Notice that the one pronunciation is spelled in several ways: nics, niks, nix, and even\nntic's with a silent t, for the word atlantic's. Let’s look for some other mismatches\nbetween pronunciation and writing. Can you summarize the purpose of the following\nexamples and explain how they work?",
          "level": -1,
          "page": 86,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_86_order_3",
          "label": "para",
          "text": "The phones contain digits to represent primary stress (1), secondary stress (2), and no\nstress (0). As our final example, we define a function to extract the stress digits and then\nscan our lexicon to find words having a particular stress pattern.",
          "level": -1,
          "page": 86,
          "reading_order": 3,
          "bbox": [
            97,
            277,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_86_order_5",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_086_figure_005.png)",
          "level": -1,
          "page": 86,
          "reading_order": 5,
          "bbox": [
            121,
            490,
            171,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_86_order_6",
          "label": "para",
          "text": "A subtlety of this program is that our user-defined function stress() is\ninvoked inside the condition of a list comprehension. There is also a\ndoubly nested for loop. There's a lot going on here, and you might want\nto return to this once you've had more experience using list compre-\nhensions.",
          "level": -1,
          "page": 86,
          "reading_order": 6,
          "bbox": [
            171,
            492,
            530,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_86_order_7",
          "label": "para",
          "text": "We can use a conditional frequency distribution to help us find minimally contrasting\nsets of words. Here we find all the p words consisting of three sounds ❷ , and group\nthem according to their first and last sounds ❸ .",
          "level": -1,
          "page": 86,
          "reading_order": 7,
          "bbox": [
            97,
            589,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_86_order_9",
          "label": "foot",
          "text": "64 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 86,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "64",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_87_order_1",
          "label": "para",
          "text": "Rather than iterating over the whole dictionary, we can also access it by looking up\nparticular words. We will use Python's dictionary data structure, which we will study\nsystematically in Section 5.3 . We look up a dictionary by specifying its name, followed\nby a key (such as the word 'fire') inside square brackets ☎ .",
          "level": -1,
          "page": 87,
          "reading_order": 1,
          "bbox": [
            97,
            187,
            585,
            251
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_87_order_3",
          "label": "para",
          "text": "If we try to look up a non-existent key ❷, we get a KeyError. This is similar to what\nhappens when we index a list with an integer that is too large, producing an IndexEr\nror. The word blog is missing from the pronouncing dictionary, so we tweak our version\nby assigning a value for this key ❸ (this has no effect on the NLTK corpus; next time\nwe access it, blog will still be absent).",
          "level": -1,
          "page": 87,
          "reading_order": 3,
          "bbox": [
            97,
            394,
            585,
            479
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_87_order_4",
          "label": "para",
          "text": "We can use any lexical resource to process a text, e.g., to filter out words having some\nlexical property (like nouns), or mapping every word of the text. For example, the\nfollowing text-to-speech function looks up each word of the text in the pronunciation\ndictionary:",
          "level": -1,
          "page": 87,
          "reading_order": 4,
          "bbox": [
            97,
            483,
            585,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_87_order_6",
      "label": "sec",
      "text": "Comparative Wordlists",
      "level": 1,
      "page": 87,
      "reading_order": 6,
      "bbox": [
        97,
        626,
        248,
        645
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_87_order_7",
          "label": "para",
          "text": "Another example of a tabular lexicon is the comparative wordlist . NLTK includes\nso-called Swadesh wordlists, lists of about 200 common words in several languages.\nThe languages are identified using an ISO 639 two-letter code.",
          "level": -1,
          "page": 87,
          "reading_order": 7,
          "bbox": [
            97,
            654,
            585,
            701
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_87_order_9",
          "label": "foot",
          "text": "2.4 Lexical Resources | 65",
          "level": -1,
          "page": 87,
          "reading_order": 9,
          "bbox": [
            467,
            824,
            585,
            842
          ],
          "section_number": "2.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_88_order_1",
          "label": "para",
          "text": "We can access cognate words from multiple languages using the entries() method,\nspecifying a list of languages. With one further step we can convert this into a simple\ndictionary (we’ll learn about dict() in Section 5.3 ).",
          "level": -1,
          "page": 88,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            156
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_88_order_3",
          "label": "para",
          "text": "We can make our simple translator more useful by adding other source languages. Let's\nget the German-English and Spanish-English pairs, convert each to a dictionary using\ndict(), then update our original translate dictionary with these additional mappings:",
          "level": -1,
          "page": 88,
          "reading_order": 3,
          "bbox": [
            97,
            277,
            585,
            325
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_88_order_5",
          "label": "para",
          "text": "We can compare words in various Germanic and Romance languages",
          "level": -1,
          "page": 88,
          "reading_order": 5,
          "bbox": [
            99,
            446,
            494,
            460
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_88_order_7",
      "label": "sec",
      "text": "Shoebox and Toolbox Lexicons",
      "level": 1,
      "page": 88,
      "reading_order": 7,
      "bbox": [
        97,
        582,
        299,
        602
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_89_order_4",
          "label": "sub_sec",
          "text": "2.5 WordNet",
          "level": 2,
          "page": 89,
          "reading_order": 4,
          "bbox": [
            97,
            403,
            207,
            430
          ],
          "section_number": "2.5",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_89_order_6",
              "label": "sub_sub_sec",
              "text": "Senses and Synonyms",
              "level": 3,
              "page": 89,
              "reading_order": 6,
              "bbox": [
                97,
                517,
                243,
                537
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_89_order_7",
                  "label": "para",
                  "text": "Consider the sentence in (1a) . If we replace the word motorcar in (1a) with automo-\nbile , to get (1b) , the meaning of the sentence stays pretty much the same:",
                  "level": -1,
                  "page": 89,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    544,
                    584,
                    575
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_89_order_8",
                  "label": "list_group",
                  "text": "(1) a. Benz is credited with the invention of the motorcar\nb. Benz is credited with the invention of the automobile",
                  "level": -1,
                  "page": 89,
                  "reading_order": 8,
                  "bbox": [
                    118,
                    590,
                    458,
                    602
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "list",
                      "text": "(1) a. Benz is credited with the invention of the motorcar",
                      "bbox": [
                        118,
                        590,
                        458,
                        602
                      ],
                      "page": 89,
                      "reading_order": 8
                    },
                    {
                      "label": "list",
                      "text": "b. Benz is credited with the invention of the automobile",
                      "bbox": [
                        150,
                        609,
                        469,
                        627
                      ],
                      "page": 89,
                      "reading_order": 9
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_89_order_10",
                  "label": "para",
                  "text": "Since everything else in the sentence has remained unchanged, we can conclude that\nthe words motorcar and automobile have the same meaning, i.e., they are synonyms.\nWe can explore these words with the help of WordNet:",
                  "level": -1,
                  "page": 89,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    636,
                    584,
                    689
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_89_order_12",
                  "label": "para",
                  "text": "Thus, motorcar has just one possible meaning and it is identified as car.n.01 , the first\nnoun sense of car . The entity car.n.01 is called a synset , or “ synonym set, ” a collection\nof synonymous words (or “ lemmas ” ):",
                  "level": -1,
                  "page": 89,
                  "reading_order": 12,
                  "bbox": [
                    100,
                    743,
                    585,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_89_order_13",
                  "label": "foot",
                  "text": "2.5 WordNet | 67",
                  "level": -1,
                  "page": 89,
                  "reading_order": 13,
                  "bbox": [
                    503,
                    824,
                    585,
                    842
                  ],
                  "section_number": "2.5",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_90_order_1",
                  "label": "para",
                  "text": "Each word of a synset can have several meanings, e.g., car can also signify a train car-\nriage, a gondola, or an elevator car. However, we are only interested in the single\nmeaning that is common to all words of this synset. Synsets also come with a prose\ndefinition and some example sentences:",
                  "level": -1,
                  "page": 90,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    107,
                    585,
                    173
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_90_order_3",
                  "label": "para",
                  "text": "Although definitions help humans to understand the intended meaning of a synset, the\nwords of the synset are often more useful for our programs. To eliminate ambiguity,\nwe will identify these words as car.n.01.automobile , car.n.01.motorcar , and so on.\nThis pairing of a synset with a word is called a lemma. We can get all the lemmas for\na given synset ❶ , look up a particular lemma ❷ , get the synset corresponding to a lemma\n❸ , and get the “ name ” of a lemma ❸ :",
                  "level": -1,
                  "page": 90,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    241,
                    585,
                    340
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_90_order_5",
                  "label": "para",
                  "text": "Unlike the words automobile and motorcar, which are unambiguous and have one syn-\nset, the word car is ambiguous, having five synsets:",
                  "level": -1,
                  "page": 90,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    474,
                    584,
                    504
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_90_order_7",
                  "label": "para",
                  "text": "For convenience, we can access all the lemmas involving the word car as follows:",
                  "level": -1,
                  "page": 90,
                  "reading_order": 7,
                  "bbox": [
                    98,
                    663,
                    558,
                    680
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_90_order_9",
                  "label": "foot",
                  "text": "68 | Chapter 2: Accessing Text Corpora and Lexical Resources",
                  "level": -1,
                  "page": 90,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    824,
                    353,
                    842
                  ],
                  "section_number": "68",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_91_order_0",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_091_figure_000.png)",
                  "level": -1,
                  "page": 91,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    171,
                    134
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_91_order_1",
                  "label": "para",
                  "text": "Your Turn: Write down all the senses of the word dish that you can\nthink of. Now, explore this word with the help of WordNet, using the\nsame operations shown earlier.",
                  "level": -1,
                  "page": 91,
                  "reading_order": 1,
                  "bbox": [
                    171,
                    85,
                    530,
                    127
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_89_order_5",
              "label": "para",
              "text": "WordNet is a semantically oriented dictionary of English, similar to a traditional the-\nsaurus but with a richer structure. NLTK includes the English WordNet, with 155,287\nwords and 117,659 synonym sets. We'll begin by looking at synonyms and how they\nare accessed in WordNet.",
              "level": -1,
              "page": 89,
              "reading_order": 5,
              "bbox": [
                97,
                437,
                585,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_88_order_8",
          "label": "para",
          "text": "Perhaps the single most popular tool used by linguists for managing data is Toolbox ,\npreviously known as Shoebox since it replaces the field linguist's traditional shoebox\nfull of file cards. Toolbox is freely downloadable from http://www.sil.org/computing/\ntoolbox/ .",
          "level": -1,
          "page": 88,
          "reading_order": 8,
          "bbox": [
            97,
            609,
            585,
            675
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_88_order_9",
          "label": "para",
          "text": "A Toolbox file consists of a collection of entries, where each entry is made up of one\nor more fields. Most fields are optional or repeatable, which means that this kind of\nlexical resource cannot be treated as a table or spreadsheet.",
          "level": -1,
          "page": 88,
          "reading_order": 9,
          "bbox": [
            97,
            687,
            586,
            735
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_88_order_10",
          "label": "para",
          "text": "Here is a dictionary for the Rotokas language. We see just the first entry, for the word\nkaa, meaning “to gag”:",
          "level": -1,
          "page": 88,
          "reading_order": 10,
          "bbox": [
            97,
            743,
            584,
            774
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_88_order_11",
          "label": "foot",
          "text": "66 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 88,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "66",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_89_order_1",
          "label": "para",
          "text": "Entries consist of a series of attribute-value pairs, such as ('ps', 'V') to indicate that\nthe part-of-speech is 'V' (verb), and ('ge', 'gag') to indicate that the gloss-into-\nEnglish is 'gag'. The last three pairs contain an example sentence in Rotokas and its\ntranslations into Tok Pisin and English.",
          "level": -1,
          "page": 89,
          "reading_order": 1,
          "bbox": [
            97,
            170,
            585,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_89_order_2",
          "label": "para",
          "text": "The loose structure of Toolbox files makes it hard for us to do much more with them\nat this stage. XML provides a powerful way to process this kind of corpus, and we will\nreturn to this topic in Chapter 11.",
          "level": -1,
          "page": 89,
          "reading_order": 2,
          "bbox": [
            100,
            241,
            585,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_89_order_3",
          "label": "para",
          "text": "The Rotokas language is spoken on the island of Bougainville, Papua\nNew Guinea. This lexicon was contributed to NLTK by Stuart Robin-\nson. Rotokas is notable for having an inventory of just 12 phonemes\n(contrastive sounds); see http://en.wikipedia.org/wiki/Rotokas_language",
          "level": -1,
          "page": 89,
          "reading_order": 3,
          "bbox": [
            171,
            322,
            530,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_91_order_2",
      "label": "sec",
      "text": "The WordNet Hierarchy",
      "level": 1,
      "page": 91,
      "reading_order": 2,
      "bbox": [
        100,
        159,
        252,
        179
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_91_order_3",
          "label": "para",
          "text": "WordNet synsets correspond to abstract concepts, and they don’t always have corre-\nsponding words in English. These concepts are linked together in a hierarchy. Some\nconcepts are very general, such as Entity, State, Event; these are called unique begin-\nners or root synsets. Others, such as gas guzzler and hatchback, are much more specific.\nA small portion of a concept hierarchy is illustrated in Figure 2-8.",
          "level": -1,
          "page": 91,
          "reading_order": 3,
          "bbox": [
            97,
            186,
            585,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_91_order_4",
          "label": "figure",
          "text": "Figure 2-8. Fragment of WordNet concept hierarchy: Nodes correspond to synsets; edges indicate the\nhypernym/hyponym relation, i.e., the relation between superordinate and subordinate concepts. [IMAGE: ![Figure](figures/NLTK_page_091_figure_004.png)]",
          "level": -1,
          "page": 91,
          "reading_order": 4,
          "bbox": [
            100,
            277,
            583,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_091_figure_004.png)",
              "bbox": [
                100,
                277,
                583,
                537
              ],
              "page": 91,
              "reading_order": 4
            },
            {
              "label": "cap",
              "text": "Figure 2-8. Fragment of WordNet concept hierarchy: Nodes correspond to synsets; edges indicate the\nhypernym/hyponym relation, i.e., the relation between superordinate and subordinate concepts.",
              "bbox": [
                97,
                546,
                585,
                574
              ],
              "page": 91,
              "reading_order": 5
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_91_order_6",
          "label": "para",
          "text": "WordNet makes it easy to navigate between concepts. For example, given a concept\nlike motorcar, we can look at the concepts that are more specific—the (immediate)\nhyponyms.",
          "level": -1,
          "page": 91,
          "reading_order": 6,
          "bbox": [
            97,
            600,
            585,
            649
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_91_order_8",
          "label": "foot",
          "text": "2.5 WordNet | 69",
          "level": -1,
          "page": 91,
          "reading_order": 8,
          "bbox": [
            503,
            824,
            585,
            842
          ],
          "section_number": "2.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_92_order_1",
          "label": "para",
          "text": "We can also navigate up the hierarchy by visiting hypernyms. Some words have multiple\npaths, because they can be classified in more than one way. There are two paths between\ncar.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a\nvehicle and a container.",
          "level": -1,
          "page": 92,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_92_order_3",
          "label": "para",
          "text": "We can get the most general hypernyms (or root hypernyms) of a synset as follows:",
          "level": -1,
          "page": 92,
          "reading_order": 3,
          "bbox": [
            98,
            411,
            574,
            425
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_92_order_5",
          "label": "para",
          "text": "Your Turn: Try out NLTK’s convenient graphical WordNet browser:\n\nnltk.app.wordnet(). Explore the WordNet hierarchy by following the\nhypernym and hyponym links.",
          "level": -1,
          "page": 92,
          "reading_order": 5,
          "bbox": [
            171,
            483,
            530,
            529
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_92_order_6",
      "label": "sec",
      "text": "More Lexical Relations",
      "level": 1,
      "page": 92,
      "reading_order": 6,
      "bbox": [
        100,
        561,
        246,
        576
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_92_order_7",
          "label": "para",
          "text": "Hypernyms and hyponyms are called lexical relations because they relate one synset\nto another. These two relations navigate up and down the “ is-a ” hierarchy. Another\nimportant way to navigate the WordNet network is from items to their components\n( meronyms ) or to the things they are contained in ( holonyms ). For example, the parts\nof a tree are its trunk , crown , and so on; these are the part_meronyms() . The substance\na tree is made of includes heartwood and sapwood , i.e., the substance_meronyms() . A\ncollection of trees forms a forest , i.e., the member_holonyms() :",
          "level": -1,
          "page": 92,
          "reading_order": 7,
          "bbox": [
            97,
            588,
            585,
            702
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_92_order_9",
          "label": "foot",
          "text": "70 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 92,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "70",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_93_order_1",
          "label": "para",
          "text": "To see just how intricate things can get, consider the word mint, which has several\nclosely related senses. We can see that mint.n.04 is part of mint.n.02 and the substance\nfrom which mint.n.05 is made.",
          "level": -1,
          "page": 93,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            154
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_93_order_3",
          "label": "para",
          "text": "There are also relationships between verbs. For example, the act of walking involves\nthe act of stepping , so walking entails stepping. Some verbs have multiple entailments:",
          "level": -1,
          "page": 93,
          "reading_order": 3,
          "bbox": [
            97,
            356,
            585,
            386
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_93_order_5",
          "label": "para",
          "text": "Some lexical relationships hold between lemmas, e.g., antonymy",
          "level": -1,
          "page": 93,
          "reading_order": 5,
          "bbox": [
            97,
            482,
            471,
            496
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_93_order_7",
          "label": "para",
          "text": "You can see the lexical relations, and the other methods defined on a synset, using\ndir() . For example, try dir(wn.synset('harmony.n.02')) .",
          "level": -1,
          "page": 93,
          "reading_order": 7,
          "bbox": [
            97,
            617,
            585,
            648
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_93_order_8",
      "label": "sec",
      "text": "Semantic Similarity",
      "level": 1,
      "page": 93,
      "reading_order": 8,
      "bbox": [
        97,
        663,
        227,
        683
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_95_order_1",
          "label": "sub_sec",
          "text": "2.6 Summary",
          "level": 2,
          "page": 95,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            207,
            191
          ],
          "section_number": "2.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_95_order_2",
              "label": "list_group",
              "text": "• A text corpus is a large, structured collection of texts. NLTK comes with many\ncorpora, e.g., the Brown Corpus, nltk.corpus.brown.\n• Some text corpora are categorized, e.g., by genre or topic; sometimes the categories\nof a corpus overlap each other.",
              "level": -1,
              "page": 95,
              "reading_order": 2,
              "bbox": [
                106,
                197,
                585,
                232
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "• A text corpus is a large, structured collection of texts. NLTK comes with many\ncorpora, e.g., the Brown Corpus, nltk.corpus.brown.",
                  "bbox": [
                    106,
                    197,
                    585,
                    232
                  ],
                  "page": 95,
                  "reading_order": 2
                },
                {
                  "label": "list",
                  "text": "• Some text corpora are categorized, e.g., by genre or topic; sometimes the categories\nof a corpus overlap each other.",
                  "bbox": [
                    106,
                    232,
                    585,
                    269
                  ],
                  "page": 95,
                  "reading_order": 3
                },
                {
                  "label": "list",
                  "text": "• A conditional frequency distribution is a collection of frequency distributions, each\none for a different condition. They can be used for counting word frequencies,\ngiven a context or a genre.",
                  "bbox": [
                    106,
                    269,
                    584,
                    323
                  ],
                  "page": 95,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "• Python programs more than a few lines long should be entered using a text editor,\nsaved to a file with a .py extension, and accessed using an import statement.",
                  "bbox": [
                    106,
                    323,
                    584,
                    360
                  ],
                  "page": 95,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "• Python functions permit you to associate a name with a particular block of code,\nand reuse that code as often as necessary.",
                  "bbox": [
                    106,
                    366,
                    584,
                    394
                  ],
                  "page": 95,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "• Some functions, known as “ methods, ” are associated with an object, and we give\nthe object name followed by a period followed by the method name, like this:\nx.funct(y) , e.g., word.isalpha() .",
                  "bbox": [
                    106,
                    403,
                    584,
                    451
                  ],
                  "page": 95,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "• To find out about some variable v, type help(v) in the Python interactive interpreter\nto read the help entry for this kind of object.",
                  "bbox": [
                    106,
                    456,
                    585,
                    492
                  ],
                  "page": 95,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "• WordNet is a semantically oriented dictionary of English, consisting of synonym\nsets—or synsets—and organized into a network.",
                  "bbox": [
                    106,
                    492,
                    584,
                    528
                  ],
                  "page": 95,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "• Some functions are not available by default, but must be accessed using Python’s\nimport statement.",
                  "bbox": [
                    106,
                    528,
                    585,
                    564
                  ],
                  "page": 95,
                  "reading_order": 10
                }
              ],
              "is_merged": true
            }
          ]
        },
        {
          "id": "page_95_order_11",
          "label": "sub_sec",
          "text": "2.7 Further Reading",
          "level": 2,
          "page": 95,
          "reading_order": 11,
          "bbox": [
            97,
            582,
            261,
            612
          ],
          "section_number": "2.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_95_order_12",
              "label": "para",
              "text": "Extra materials for this chapter are posted at http://www.nltk.org/ , including links to\nfreely available resources on the Web. The corpus methods are summarized in the\nCorpus HOWTO, at http://www.nltk.org/howto , and documented extensively in the\nonline API documentation.",
              "level": -1,
              "page": 95,
              "reading_order": 12,
              "bbox": [
                97,
                618,
                585,
                681
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_95_order_13",
              "label": "para",
              "text": "Significant sources of published corpora are the Linguistic Data Consortium (LDC) and\nthe European Language Resources Agency (ELRA). Hundreds of annotated text and\nspeech corpora are available in dozens of languages. Non-commercial licenses permit\nthe data to be used in teaching and research. For some corpora, commercial licenses\nare also available (but for a higher fee).",
              "level": -1,
              "page": 95,
              "reading_order": 13,
              "bbox": [
                97,
                689,
                585,
                774
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_95_order_14",
              "label": "foot",
              "text": "2.7 Further Reading | 73",
              "level": -1,
              "page": 95,
              "reading_order": 14,
              "bbox": [
                467,
                824,
                584,
                842
              ],
              "section_number": "2.7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_0",
              "label": "para",
              "text": "These and many other language resources have been documented using OLAC Meta-\ndata, and can be searched via the OLAC home page at http://www.language-archives\n.org/ . Corpora List (see http://gandalf.aksis.uib.no/corpora/sub.html ) is a mailing list for\ndiscussions about corpora, and you can find resources by searching the list archives or\nposting to the list. The most complete inventory of the world’s languages is Ethno-\nlogue , http://www.ethnologue.com/ . Of 7,000 languages, only a few dozen have sub-\nstantial digital resources suitable for use in NLP.",
              "level": -1,
              "page": 96,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                188
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_1",
              "label": "para",
              "text": "This chapter has touched on the field of Corpus Linguistics . Other useful books in\nthis area include (Biber, Conrad, & Reppen, 1998) , (McEnery, 2006) , (Meyer, 2002) ,\n(Sampson & McCarthy, 2005) , and (Scott & Tribble, 2006) . Further readings in quan-\ntitative data analysis in linguistics are: (Baayen, 2008) , (Gries, 2009) , and (Woods,\nFletcher, & Hughes, 1986) .",
              "level": -1,
              "page": 96,
              "reading_order": 1,
              "bbox": [
                97,
                197,
                585,
                278
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_2",
              "label": "para",
              "text": "The original description of WordNet is (Fellbaum, 1998) . Although WordNet was\noriginally developed for research in psycholinguistics, it is now widely used in NLP and\nInformation Retrieval. WordNets are being developed for many other languages, as\ndocumented at http://www.globalwordnet.org/ . For a study of WordNet similarity\nmeasures, see (Budanitsky & Hirst, 2006) .",
              "level": -1,
              "page": 96,
              "reading_order": 2,
              "bbox": [
                97,
                286,
                585,
                368
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_3",
              "label": "para",
              "text": "Other topics touched on in this chapter were phonetics and lexical semantics, and we\nrefer readers to Chapters 7 and 20 of (Jurafsky & Martin, 2008).",
              "level": -1,
              "page": 96,
              "reading_order": 3,
              "bbox": [
                97,
                376,
                585,
                412
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_96_order_4",
          "label": "sub_sec",
          "text": "2.8 Exercises",
          "level": 2,
          "page": 96,
          "reading_order": 4,
          "bbox": [
            97,
            430,
            207,
            456
          ],
          "section_number": "2.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_96_order_5",
              "label": "para",
              "text": "1. ◦ Create a variable phrase containing a list of words. Experiment with the opera-\ntions described in this chapter, including addition, multiplication, indexing, slic-\ning, and sorting.",
              "level": -1,
              "page": 96,
              "reading_order": 5,
              "bbox": [
                100,
                465,
                584,
                514
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_6",
              "label": "para",
              "text": "2. ◦ Use the corpus module to explore austen-persuasion.txt. How many word\ntokens does this book have? How many word types?",
              "level": -1,
              "page": 96,
              "reading_order": 6,
              "bbox": [
                100,
                519,
                584,
                555
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_7",
              "label": "para",
              "text": "3. ◦ Use the Brown Corpus reader nltk.corpus.brown.words() or the Web Text Cor-\npus reader nltk.corpus.webtext.words() to access some sample text in two differ-\nent genres.",
              "level": -1,
              "page": 96,
              "reading_order": 7,
              "bbox": [
                100,
                555,
                584,
                609
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_8",
              "label": "para",
              "text": "4. ◦ Read in the texts of the State of the Union addresses, using the state_union corpus\nreader. Count occurrences of men, women, and people in each document. What has\nhappened to the usage of these words over time?",
              "level": -1,
              "page": 96,
              "reading_order": 8,
              "bbox": [
                100,
                609,
                585,
                663
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_9",
              "label": "para",
              "text": "5. ◦ Investigate the holonym-meronym relations for some nouns. Remember that\nthere are three kinds of holonomym-meronym relation, so you need to use member_mer\nonyms(),\npart_meronyms(),\nsubstance_meronyms(),\nmember_holonyms(),\npart_holonyms(), and substance_holonyms().",
              "level": -1,
              "page": 96,
              "reading_order": 9,
              "bbox": [
                105,
                663,
                584,
                730
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_10",
              "label": "para",
              "text": "6. ◦ In the discussion of comparative wordlists, we created an object called trans\nlate, which you could look up using words in both German and Italian in order",
              "level": -1,
              "page": 96,
              "reading_order": 10,
              "bbox": [
                100,
                734,
                585,
                770
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_96_order_11",
              "label": "foot",
              "text": "74 | Chapter 2: Accessing Text Corpora and Lexical Resources",
              "level": -1,
              "page": 96,
              "reading_order": 11,
              "bbox": [
                97,
                824,
                353,
                842
              ],
              "section_number": "74",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_0",
              "label": "para",
              "text": "to get corresponding words in English. What problem might arise with this ap­\nproach? Can you suggest a way to avoid this problem?",
              "level": -1,
              "page": 97,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_1",
              "label": "para",
              "text": "7. ◦ According to Strunk and White's Elements of Style , the word however , used at\nthe start of a sentence, means “in whatever way” or “to whatever extent,” and not\n“nevertheless.” They give this example of correct usage: However you advise him,\nhe will probably do as he thinks best. (http://www.bartleby.com/141/strunk3.html)\nUse the concordance tool to study actual usage of this word in the various texts we\nhave been considering. See also the LanguageLog posting “Fossilized prejudices\nabout ‘however’” at http://itre.cis.upenn.edu/~myl/languagelog/archives/001913\n.html .",
              "level": -1,
              "page": 97,
              "reading_order": 1,
              "bbox": [
                100,
                107,
                585,
                241
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_2",
              "label": "para",
              "text": "8. o Define a conditional frequency distribution over the Names Corpus that allows\nyou to see which initial letters are more frequent for males versus females (see\nFigure 2-7).",
              "level": -1,
              "page": 97,
              "reading_order": 2,
              "bbox": [
                100,
                241,
                585,
                296
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_3",
              "label": "para",
              "text": "9. o Pick a pair of texts and study the differences between them, in terms of vocabu-\nlary, vocabulary richness, genre, etc. Can you find pairs of words that have quite\ndifferent meanings across the two texts, such as monstrous in Moby Dick and in\nSense and Sensibility?",
              "level": -1,
              "page": 97,
              "reading_order": 3,
              "bbox": [
                100,
                296,
                585,
                367
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_4",
              "label": "para",
              "text": "10. • Read the BBC News article: “UK’s Vicky Pollards ‘left behind’” at http://news\n.bbc.co.uk/1/hi/education/6173441.stm . The article gives the following statistic\nabout teen language: “the top 20 words used, including yeah, no, but and like,\naccount for around a third of all words.” How many word types account for a third\nof all word tokens, for a variety of text sources? What do you conclude about this\nstatistic? Read more about this on LanguageLog , at http://itre.cis.upenn.edu/~myl/\nlanguagelog/archives/003993.html .",
              "level": -1,
              "page": 97,
              "reading_order": 4,
              "bbox": [
                100,
                367,
                585,
                487
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_5",
              "label": "para",
              "text": "11. o Investigate the table of modal distributions and look for other patterns. Try to\nexplain them in terms of your own impressionistic understanding of the different\ngenres. Can you find other closed classes of words that exhibit significant differ-\nences across different genres?",
              "level": -1,
              "page": 97,
              "reading_order": 5,
              "bbox": [
                100,
                492,
                585,
                557
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_6",
              "label": "para",
              "text": "12. • The CMU Pronouncing Dictionary contains multiple pronunciations for certain\nwords. How many distinct words does it contain? What fraction of words in this\ndictionary have more than one possible pronunciation?",
              "level": -1,
              "page": 97,
              "reading_order": 6,
              "bbox": [
                100,
                564,
                585,
                611
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_7",
              "label": "para",
              "text": "13. o What percentage of noun synsets have no hyponyms? You can get all noun syn-\nsets using wn.all_synsets('n').",
              "level": -1,
              "page": 97,
              "reading_order": 7,
              "bbox": [
                100,
                611,
                584,
                654
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_8",
              "label": "para",
              "text": "14. o Define a function supergloss(s) that takes a synset s as its argument and returns\na string consisting of the concatenation of the definition of s, and the definitions\nof all the hypernyms and hyponyms of s.",
              "level": -1,
              "page": 97,
              "reading_order": 8,
              "bbox": [
                100,
                654,
                585,
                707
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_9",
              "label": "para",
              "text": "15. o Write a program to find all words that occur at least three times in the Brown\nCorpus.",
              "level": -1,
              "page": 97,
              "reading_order": 9,
              "bbox": [
                100,
                707,
                584,
                743
              ],
              "section_number": "15",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_10",
              "label": "para",
              "text": "16. o Write a program to generate a table of lexical diversity scores (i.e., token/type\nratios), as we saw in Table 1-1 . Include the full set of Brown Corpus genres",
              "level": -1,
              "page": 97,
              "reading_order": 10,
              "bbox": [
                100,
                743,
                585,
                779
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_97_order_11",
              "label": "foot",
              "text": "2.8 Exercises | 75",
              "level": -1,
              "page": 97,
              "reading_order": 11,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "2.8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_0",
              "label": "para",
              "text": "(nltk.corpus.brown.categories()). Which genre has the lowest diversity (greatest\nnumber of tokens per type)? Is this what you would have expected?",
              "level": -1,
              "page": 98,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                585,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_1",
              "label": "para",
              "text": "17. o Write a function that finds the 50 most frequently occurring words of a text that\nare not stopwords.",
              "level": -1,
              "page": 98,
              "reading_order": 1,
              "bbox": [
                100,
                107,
                585,
                143
              ],
              "section_number": "17",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_2",
              "label": "para",
              "text": "18. o Write a program to print the 50 most frequent bigrams (pairs of adjacent words)\nof a text, omitting bigrams that contain stopwords.",
              "level": -1,
              "page": 98,
              "reading_order": 2,
              "bbox": [
                100,
                143,
                584,
                180
              ],
              "section_number": "18",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_3",
              "label": "para",
              "text": "19. o Write a program to create a table of word frequencies by genre, like the one given\nin Section 2.1 for modals. Choose your own words and try to find words whose\npresence (or absence) is typical of a genre. Discuss your findings.",
              "level": -1,
              "page": 98,
              "reading_order": 3,
              "bbox": [
                100,
                187,
                585,
                234
              ],
              "section_number": "19",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_4",
              "label": "para",
              "text": "20. Write a function word_freq() that takes a word and the name of a section of the\nBrown Corpus as arguments, and computes the frequency of the word in that sec-\ntion of the corpus.",
              "level": -1,
              "page": 98,
              "reading_order": 4,
              "bbox": [
                98,
                234,
                585,
                288
              ],
              "section_number": "20",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_5",
              "label": "para",
              "text": "21. o Write a program to guess the number of syllables contained in a text, making\nuse of the CMU Pronouncing Dictionary.",
              "level": -1,
              "page": 98,
              "reading_order": 5,
              "bbox": [
                98,
                294,
                585,
                325
              ],
              "section_number": "21",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_6",
              "label": "para",
              "text": "22. o Define a function hedge(text) that processes a text and produces a new version\nwith the word 'like' between every third word.",
              "level": -1,
              "page": 98,
              "reading_order": 6,
              "bbox": [
                98,
                331,
                584,
                359
              ],
              "section_number": "22",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_7",
              "label": "para",
              "text": "23. • Zipf's Law: Let f(w) be the frequency of a word w in free text. Suppose that all\nthe words of a text are ranked according to their frequency, with the most frequent\nword first. Zipf's Law states that the frequency of a word type is inversely\nproportional to its rank (i.e., f × r = k, for some constant k). For example, the 50th\nmost common word type should occur three times as frequently as the 150th most\ncommon word type.",
              "level": -1,
              "page": 98,
              "reading_order": 7,
              "bbox": [
                98,
                367,
                585,
                466
              ],
              "section_number": "23",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_8",
              "label": "para",
              "text": "a. Write a function to process a large text and plot word frequency against word\nrank using pylab.plot. Do you confirm Zipf's law? (Hint: it helps to use a\nlogarithmic scale.) What is going on at the extreme ends of the plotted line?",
              "level": -1,
              "page": 98,
              "reading_order": 8,
              "bbox": [
                126,
                466,
                585,
                519
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_9",
              "label": "para",
              "text": "b. Generate random text, e.g., using random.choice(\"abcdefg \"), taking care to\ninclude the space character. You will need to import random first. Use the string\nconcatenation operator to accumulate characters into a (very) long string.\nThen tokenize this string, generate the Zipf plot as before, and compare the\ntwo plots. What do you make of Zipf's Law in the light of this?",
              "level": -1,
              "page": 98,
              "reading_order": 9,
              "bbox": [
                126,
                519,
                585,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_10",
              "label": "para",
              "text": "24. • Modify the text generation program in Example 2-1 further, to do the following\ntasks:",
              "level": -1,
              "page": 98,
              "reading_order": 10,
              "bbox": [
                98,
                609,
                585,
                645
              ],
              "section_number": "24",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_98_order_11",
              "label": "foot",
              "text": "76 | Chapter 2: Accessing Text Corpora and Lexical Resources",
              "level": -1,
              "page": 98,
              "reading_order": 11,
              "bbox": [
                97,
                824,
                353,
                842
              ],
              "section_number": "76",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_99_order_0",
              "label": "para",
              "text": "a. Store the n most likely words in a list words, then randomly choose a word\nfrom the list using random.choice(). (You will need to import random first.)",
              "level": -1,
              "page": 99,
              "reading_order": 0,
              "bbox": [
                126,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_99_order_1",
              "label": "para",
              "text": "b. Select a particular genre, such as a section of the Brown Corpus or a Genesis\ntranslation, one of the Gutenberg texts, or one of the Web texts. Train the\nmodel on this corpus and get it to generate random text. You may have to\nexperiment with different start words. How intelligible is the text? Discuss the\nstrengths and weaknesses of this method of generating random text.",
              "level": -1,
              "page": 99,
              "reading_order": 1,
              "bbox": [
                126,
                107,
                585,
                197
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_99_order_2",
              "label": "para",
              "text": "c. Now train your system using two distinct genres and experiment with gener-\nating text in the hybrid genre. Discuss your observations.",
              "level": -1,
              "page": 99,
              "reading_order": 2,
              "bbox": [
                126,
                197,
                584,
                232
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_99_order_3",
              "label": "para",
              "text": "25. • Define a function find_language() that takes a string as its argument and returns\na list of languages that have that string as a word. Use the udhr corpus and limit\nyour searches to files in the Latin-1 encoding.",
              "level": -1,
              "page": 99,
              "reading_order": 3,
              "bbox": [
                98,
                232,
                585,
                286
              ],
              "section_number": "25",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_99_order_4",
              "label": "para",
              "text": "26. • What is the branching factor of the noun hypernym hierarchy? I.e., for every\nnoun synset that has hyponyms—or children in the hypernym hierarchy—how\nmany do they have on average? You can get all noun synsets using wn.all_syn\nsets('n').",
              "level": -1,
              "page": 99,
              "reading_order": 4,
              "bbox": [
                98,
                286,
                585,
                354
              ],
              "section_number": "26",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_99_order_5",
              "label": "para",
              "text": "27. • The polysemy of a word is the number of senses it has. Using WordNet, we can\ndetermine that the noun dog has seven senses with len(wn.synsets('dog', 'n')) .\nCompute the average polysemy of nouns, verbs, adjectives, and adverbs according\nto WordNet.",
              "level": -1,
              "page": 99,
              "reading_order": 5,
              "bbox": [
                98,
                358,
                585,
                421
              ],
              "section_number": "27",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_99_order_6",
              "label": "para",
              "text": "28. • Use one of the predefined similarity measures to score the similarity of each of\nthe following pairs of words. Rank the pairs in order of decreasing similarity. How\nclose is your ranking to the order given here, an order that was established exper-\nimentally by (Miller & Charles, 1998): car-automobile, gem-jewel, journey-voyage,\nboy-lad, coast-shore, asylum-madhouse, magician-wizard, midday-noon, furnace-\nstove, food-fruit, bird-cock, bird-crane, tool-implement, brother-monk, lad-\nbrother, crane-implement, journey-car, monk-oracle, cemetery-woodland, food-\nrooster, coast-hill, forest-graveyard, shore-woodland, monk-slave, coast-forest,\nlad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string.",
              "level": -1,
              "page": 99,
              "reading_order": 6,
              "bbox": [
                98,
                430,
                586,
                582
              ],
              "section_number": "28",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_99_order_7",
              "label": "foot",
              "text": "2.8 Exercises | 77",
              "level": -1,
              "page": 99,
              "reading_order": 7,
              "bbox": [
                503,
                824,
                585,
                842
              ],
              "section_number": "2.8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_100_order_0",
              "label": "para",
              "text": "_",
              "level": -1,
              "page": 100,
              "reading_order": 0,
              "bbox": [
                153,
                161,
                494,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_93_order_9",
          "label": "para",
          "text": "We have seen that synsets are linked by a complex network of lexical relations. Given\na particular synset, we can traverse the WordNet network to find synsets with related\nmeanings. Knowing which words are semantically related is useful for indexing a col-\nlection of texts, so that a search for a general term such as vehicle will match documents\ncontaining specific terms such as limousine .",
          "level": -1,
          "page": 93,
          "reading_order": 9,
          "bbox": [
            97,
            689,
            585,
            771
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_93_order_10",
          "label": "foot",
          "text": "2.5 WordNet | 71",
          "level": -1,
          "page": 93,
          "reading_order": 10,
          "bbox": [
            503,
            824,
            584,
            842
          ],
          "section_number": "2.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_94_order_0",
          "label": "para",
          "text": "Recall that each synset has one or more hypernym paths that link it to a root hypernym\nsuch as entity.n.01. Two synsets linked to the same root may have several hypernyms\nin common (see Figure 2-8). If two synsets share a very specific hypernym—one that\nis low down in the hypernym hierarchy—they must be closely related.",
          "level": -1,
          "page": 94,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_94_order_2",
          "label": "para",
          "text": "Of course we know that whale is very specific (and baleen whale even more so), whereas\nvertebrate is more general and entity is completely general. We can quantify this concept\nof generality by looking up the depth of each synset:",
          "level": -1,
          "page": 94,
          "reading_order": 2,
          "bbox": [
            97,
            322,
            585,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_94_order_4",
          "label": "para",
          "text": "Similarity measures have been defined over the collection of WordNet synsets that\nincorporate this insight. For example, path_similarity assigns a score in the range\n0–1 based on the shortest path that connects the concepts in the hypernym hierarchy\n(-1 is returned in those cases where a path cannot be found). Comparing a synset with\nitself will return 1. Consider the following similarity scores, relating right whale to minke\nwhale, orca, tortoise, and novel. Although the numbers won't mean much, they decrease\nas we move away from the semantic space of sea creatures to inanimate objects.",
          "level": -1,
          "page": 94,
          "reading_order": 4,
          "bbox": [
            97,
            492,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_94_order_6",
          "label": "foot",
          "text": "72 | Chapter 2: Accessing Text Corpora and Lexical Resources",
          "level": -1,
          "page": 94,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            353,
            842
          ],
          "section_number": "72",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_95_order_0",
          "label": "para",
          "text": "Several other similarity measures are available; you can type help(wn)\nfor more information. NLTK also includes VerbNet, a hierarchical verb\nlexicon linked to WordNet. It can be accessed with nltk.corpus.verb\nnet.",
          "level": -1,
          "page": 95,
          "reading_order": 0,
          "bbox": [
            171,
            80,
            530,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_101_order_0",
      "label": "sec",
      "text": "CHAPTER 3\nProcessing Raw Text",
      "level": 1,
      "page": 101,
      "reading_order": 0,
      "bbox": [
        324,
        78,
        585,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_102_order_0",
          "label": "sub_sec",
          "text": "3.1 Accessing Text from the Web and from Disk",
          "level": 2,
          "page": 102,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            474,
            100
          ],
          "section_number": "3.1",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_102_order_1",
              "label": "sub_sub_sec",
              "text": "Electronic Books",
              "level": 3,
              "page": 102,
              "reading_order": 1,
              "bbox": [
                100,
                115,
                207,
                134
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_102_order_2",
                  "label": "para",
                  "text": "A small sample of texts from Project Gutenberg appears in the NLTK corpus collection.\nHowever, you may be interested in analyzing other texts from Project Gutenberg. You\ncan browse the catalog of 25,000 free online books at http://www.gutenberg.org/cata\nlog/ , and obtain a URL to an ASCII text file. Although 90% of the texts in Project\nGutenberg are in English, it includes material in over 50 other languages, including\nCatalan, Chinese, Dutch, Finnish, French, German, Italian, Portuguese, and Spanish\n(with more than 100 texts each).",
                  "level": -1,
                  "page": 102,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    143,
                    585,
                    259
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_102_order_3",
                  "label": "para",
                  "text": "Text number 2554 is an English translation of Crime and Punishment, and we can access\nt as follows.",
                  "level": -1,
                  "page": 102,
                  "reading_order": 3,
                  "bbox": [
                    100,
                    259,
                    585,
                    295
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_102_order_5",
                  "label": "para",
                  "text": "The read() process will take a few seconds as it downloads this large\nbook. If you’re using an Internet proxy that is not correctly detected by\nPython, you may need to specify the proxy manually as follows:",
                  "level": -1,
                  "page": 102,
                  "reading_order": 5,
                  "bbox": [
                    171,
                    448,
                    530,
                    492
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_102_order_7",
                  "label": "para",
                  "text": "The variable raw contains a string with 1,176,831 characters. (We can see that it is a\nstring, using type(raw) .) This is the raw content of the book, including many details\nwe are not interested in, such as whitespace, line breaks, and blank lines. Notice the\n\\r and \\n in the opening line of the file, which is how Python displays the special carriage\nreturn and line-feed characters (the file must have been created on a Windows ma-\nchine). For our language processing, we want to break up the string into words and\npunctuation, as we saw in Chapter 1. This step is called tokenization , and it produces\nour familiar structure, a list of words and punctuation.",
                  "level": -1,
                  "page": 102,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    546,
                    585,
                    676
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_102_order_9",
                  "label": "foot",
                  "text": "80 | Chapter3: Processing Raw Text",
                  "level": -1,
                  "page": 102,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    824,
                    255,
                    842
                  ],
                  "section_number": "80",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_103_order_0",
                  "label": "para",
                  "text": "Notice that NLTK was needed for tokenization, but not for any of the earlier tasks of\nopening a URL and reading it into a string. If we now take the further step of creating\nan NLTK text from this list, we can carry out all of the other linguistic processing we\nsaw in Chapter 1, along with the regular list operations, such as slicing:",
                  "level": -1,
                  "page": 103,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    586,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_103_order_2",
                  "label": "para",
                  "text": "Notice that Project Gutenberg appears as a collocation. This is because each text down-\nloaded from Project Gutenberg contains a header with the name of the text, the author,\nthe names of people who scanned and corrected the text, a license, and so on. Some-\ntimes this information appears in a footer at the end of the file. We cannot reliably\ndetect where the content begins and ends, and so have to resort to manual inspection\nof the file, to discover unique strings that mark the beginning and the end, before\ntrimming raw to be just the content and nothing else:",
                  "level": -1,
                  "page": 103,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    338,
                    585,
                    452
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_103_order_4",
                  "label": "para",
                  "text": "The find() and rfind() (“reverse find”) methods help us get the right index values to\nuse for slicing the string 0 . We overwrite raw with this slice, so now it begins with\n“PART I” and goes up to (but not including) the phrase that marks the end of the\ncontent.",
                  "level": -1,
                  "page": 103,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    555,
                    585,
                    621
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_103_order_5",
                  "label": "para",
                  "text": "This was our first brush with the reality of the Web: texts found on the Web may contain\nunwanted material, and there may not be an automatic way to remove it. But with a\nsmall amount of extra work we can extract the material we need.",
                  "level": -1,
                  "page": 103,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    633,
                    584,
                    680
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_101_order_1",
          "label": "para",
          "text": "The most important source of texts is undoubtedly the Web. It's convenient to have\nexisting text collections to explore, such as the corpora we saw in the previous chapters.\nHowever, you probably have your own text sources in mind, and need to learn how to\naccess them.",
          "level": -1,
          "page": 101,
          "reading_order": 1,
          "bbox": [
            97,
            286,
            585,
            350
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_101_order_2",
          "label": "para",
          "text": "The goal of this chapter is to answer the following questions",
          "level": -1,
          "page": 101,
          "reading_order": 2,
          "bbox": [
            100,
            358,
            441,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_101_order_3",
          "label": "para",
          "text": "1. How can we write programs to access text from local files and from the Web, in\norder to get hold of an unlimited range of language material?",
          "level": -1,
          "page": 101,
          "reading_order": 3,
          "bbox": [
            100,
            385,
            584,
            421
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_101_order_4",
          "label": "para",
          "text": "2. How can we split documents up into individual words and punctuation symbols,\nso we can carry out the same kinds of analysis we did with text corpora in earlier\nchapters?",
          "level": -1,
          "page": 101,
          "reading_order": 4,
          "bbox": [
            100,
            421,
            585,
            474
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_101_order_5",
          "label": "para",
          "text": "3. How can we write programs to produce formatted output and save it in a file?",
          "level": -1,
          "page": 101,
          "reading_order": 5,
          "bbox": [
            100,
            474,
            566,
            492
          ],
          "section_number": "3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_101_order_6",
          "label": "para",
          "text": "In order to address these questions, we will be covering key concepts in NLP, including\ntokenization and stemming. Along the way you will consolidate your Python knowl-\nedge and learn about strings, files, and regular expressions. Since so much text on the\nWeb is in HTML format, we will also see how to dispense with markup.",
          "level": -1,
          "page": 101,
          "reading_order": 6,
          "bbox": [
            97,
            501,
            585,
            567
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_101_order_7",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_101_figure_007.png)",
          "level": -1,
          "page": 101,
          "reading_order": 7,
          "bbox": [
            118,
            582,
            162,
            638
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_101_order_8",
          "label": "para",
          "text": "Important: From this chapter onwards, our program samples will as-\nsume you begin your interactive session or your program with the fol-\nlowing import statements:",
          "level": -1,
          "page": 101,
          "reading_order": 8,
          "bbox": [
            171,
            591,
            530,
            637
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_101_order_10",
          "label": "foot",
          "text": "79",
          "level": -1,
          "page": 101,
          "reading_order": 10,
          "bbox": [
            574,
            824,
            585,
            842
          ],
          "section_number": "79",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_103_order_6",
      "label": "sec",
      "text": "Dealing with HTML",
      "level": 1,
      "page": 103,
      "reading_order": 6,
      "bbox": [
        100,
        696,
        225,
        716
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_103_order_7",
          "label": "para",
          "text": "Much of the text on the Web is in the form of HTML documents. You can use a web\nbrowser to save a page as text to a local file, then access this as described in the later\nsection on files. However, if you’re going to do this often, it’s easiest to get Python to\ndo the work directly. The first step is the same as before, using urlopen . For fun we’ll",
          "level": -1,
          "page": 103,
          "reading_order": 7,
          "bbox": [
            97,
            724,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_103_order_8",
          "label": "foot",
          "text": "3.1 Accessing Text from the Web and from Disk | 81",
          "level": -1,
          "page": 103,
          "reading_order": 8,
          "bbox": [
            367,
            824,
            584,
            842
          ],
          "section_number": "3.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_104_order_0",
          "label": "para",
          "text": "pick a BBC News story called “Blondes to die out in 200 years,\n” an urban legend passed\nalong by the BBC as established scientific fact:",
          "level": -1,
          "page": 104,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_104_order_2",
          "label": "para",
          "text": "You can type print html to see the HTML content in all its glory, including meta tags,\nan image map, JavaScript, forms, and tables.",
          "level": -1,
          "page": 104,
          "reading_order": 2,
          "bbox": [
            97,
            170,
            584,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_104_order_3",
          "label": "para",
          "text": "Getting text out of HTML is a sufficiently common task that NLTK provides a helper\nfunction nltk.clean_html(), which takes an HTML string and returns raw text. We\ncan then tokenize this to get our familiar text structure:",
          "level": -1,
          "page": 104,
          "reading_order": 3,
          "bbox": [
            97,
            215,
            585,
            262
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_104_order_5",
          "label": "para",
          "text": "This still contains unwanted material concerning site navigation and related stories.\nWith some trial and error you can find the start and end indexes of the content and\nselect the tokens of interest, and initialize a text as before.",
          "level": -1,
          "page": 104,
          "reading_order": 5,
          "bbox": [
            97,
            331,
            584,
            377
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_104_order_7",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_104_figure_007.png)",
          "level": -1,
          "page": 104,
          "reading_order": 7,
          "bbox": [
            118,
            508,
            171,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_104_order_8",
          "label": "para",
          "text": "For more sophisticated processing of HTML, use the Beautiful Soup\npackage, available at http://www.crummy.com/software/BeautifulSoup/ .",
          "level": -1,
          "page": 104,
          "reading_order": 8,
          "bbox": [
            171,
            518,
            530,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_104_order_9",
      "label": "sec",
      "text": "Processing Search Engine Results",
      "level": 1,
      "page": 104,
      "reading_order": 9,
      "bbox": [
        98,
        591,
        317,
        611
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_104_order_10",
          "label": "para",
          "text": "The Web can be thought of as a huge corpus of unannotated text. Web search engines\nprovide an efficient means of searching this large quantity of text for relevant linguistic\nexamples. The main advantage of search engines is size: since you are searching such\na large set of documents, you are more likely to find any linguistic pattern you are\ninterested in. Furthermore, you can make use of very specific patterns, which would\nmatch only one or two examples on a smaller example, but which might match tens of\nthousands of examples when run on the Web. A second advantage of web search en-\ngines is that they are very easy to use. Thus, they provide a very convenient tool for\nquickly checking a theory, to see if it is reasonable. See Table 3-1 for an example.",
          "level": -1,
          "page": 104,
          "reading_order": 10,
          "bbox": [
            97,
            618,
            586,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_104_order_11",
          "label": "foot",
          "text": "82 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 104,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            255,
            842
          ],
          "section_number": "82",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_105_order_0",
          "label": "table",
          "text": "Table 3-1. Google hits for collocations: The number of hits for collocations involving the words\nabsolutely or definitely, followed by one of adore, love, like, or prefer. (Liberman, in LanguageLog,\n2005) [TABLE: <table><tr><td>Google hits</td><td>adore</td><td>love</td><td>like</td><td>prefer</td></tr><tr><td>absolutely</td><td>289,000</td><td>905,000</td><td>16,200</td><td>644</td></tr><tr><td>definitely</td><td>1,460</td><td>51,000</td><td>158,000</td><td>62,600</td></tr><tr><td>ratio</td><td>198:1</td><td>18:1</td><td>1:10</td><td>1:97</td></tr></table>]",
          "level": -1,
          "page": 105,
          "reading_order": 0,
          "bbox": [
            91,
            125,
            342,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Google hits</td><td>adore</td><td>love</td><td>like</td><td>prefer</td></tr><tr><td>absolutely</td><td>289,000</td><td>905,000</td><td>16,200</td><td>644</td></tr><tr><td>definitely</td><td>1,460</td><td>51,000</td><td>158,000</td><td>62,600</td></tr><tr><td>ratio</td><td>198:1</td><td>18:1</td><td>1:10</td><td>1:97</td></tr></table>",
              "bbox": [
                91,
                125,
                342,
                206
              ],
              "page": 105,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Table 3-1. Google hits for collocations: The number of hits for collocations involving the words\nabsolutely or definitely, followed by one of adore, love, like, or prefer. (Liberman, in LanguageLog,\n2005)",
              "bbox": [
                97,
                71,
                584,
                116
              ],
              "page": 105,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_105_order_2",
          "label": "para",
          "text": "Unfortunately, search engines have some significant shortcomings. First, the allowable\nrange of search patterns is severely restricted. Unlike local corpora, where you write\nprograms to search for arbitrarily complex patterns, search engines generally only allow\nyou to search for individual words or strings of words, sometimes with wildcards. Sec-\nond, search engines give inconsistent results, and can give widely different figures when\nused at different times or in different geographical regions. When content has been\nduplicated across multiple sites, search results may be boosted. Finally, the markup in\nthe result returned by a search engine may change unpredictably, breaking any pattern-\nbased method of locating particular content (a problem which is ameliorated by the\nuse of search engine APIs).",
          "level": -1,
          "page": 105,
          "reading_order": 2,
          "bbox": [
            97,
            215,
            585,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_105_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_105_figure_003.png)",
          "level": -1,
          "page": 105,
          "reading_order": 3,
          "bbox": [
            109,
            403,
            171,
            457
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_105_order_4",
          "label": "para",
          "text": "Your Turn: Search the Web for \"the of\" (inside quotes). Based on the\nlarge count, can we conclude that the of is a frequent collocation in\nEnglish?",
          "level": -1,
          "page": 105,
          "reading_order": 4,
          "bbox": [
            171,
            412,
            530,
            456
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_105_order_5",
      "label": "sec",
      "text": "Processing RSS Feeds",
      "level": 1,
      "page": 105,
      "reading_order": 5,
      "bbox": [
        98,
        483,
        243,
        507
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_105_order_6",
          "label": "para",
          "text": "The blogosphere is an important source of text, in both formal and informal registers.\nWith the help of a third-party Python library called the Universal Feed Parser , freely\ndownloadable from http://feedparser.org/ , we can access the content of a blog, as shown\nhere:",
          "level": -1,
          "page": 105,
          "reading_order": 6,
          "bbox": [
            97,
            510,
            585,
            576
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_105_order_8",
          "label": "foot",
          "text": "3.1 Accessing Text from the Web and from Disk | 83",
          "level": -1,
          "page": 105,
          "reading_order": 8,
          "bbox": [
            367,
            824,
            584,
            842
          ],
          "section_number": "3.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_106_order_1",
          "label": "para",
          "text": "Note that the resulting strings have a u prefix to indicate that they are Unicode strings\n(see Section 3.3 ). With some further work, we can write programs to create a small\ncorpus of blog posts, and use this as the basis for our NLP work.",
          "level": -1,
          "page": 106,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            156
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_106_order_2",
      "label": "sec",
      "text": "Reading Local Files",
      "level": 1,
      "page": 106,
      "reading_order": 2,
      "bbox": [
        98,
        170,
        225,
        191
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_106_order_3",
          "label": "para",
          "text": "In order to read a local file, we need to use Python's built-in open() function, followed\nby the read() method. Supposing you have a file document.txt , you can load its contents\nlike this:",
          "level": -1,
          "page": 106,
          "reading_order": 3,
          "bbox": [
            97,
            197,
            585,
            244
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_106_order_5",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_106_figure_005.png)",
          "level": -1,
          "page": 106,
          "reading_order": 5,
          "bbox": [
            109,
            286,
            171,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_106_order_6",
          "label": "para",
          "text": "Your Turn: Create a file called document.txt using a text editor, and\ntype in a few lines of text, and save it as plain text. If you are using IDLE,\nselect the New Window command in the File menu, typing the required\ntext into this window, and then saving the file as document.txt inside\nthe directory that IDLE offers in the pop-up dialogue box. Next, in the\nPython interpreter, open the file using f = open('document.txt'), then\ninspect its contents using print f.read().",
          "level": -1,
          "page": 106,
          "reading_order": 6,
          "bbox": [
            171,
            304,
            530,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_106_order_7",
          "label": "para",
          "text": "Various things might have gone wrong when you tried this. If the interpreter couldn’t\nfind your file, you would have seen an error like this:",
          "level": -1,
          "page": 106,
          "reading_order": 7,
          "bbox": [
            98,
            430,
            585,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_106_order_9",
          "label": "para",
          "text": "To check that the file that you are trying to open is really in the right directory, use\nIDLE's Open command in the File menu; this will display a list of all the files in the\ndirectory where IDLE is running. An alternative is to examine the current directory\nfrom within Python:",
          "level": -1,
          "page": 106,
          "reading_order": 9,
          "bbox": [
            97,
            546,
            585,
            610
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_106_order_11",
          "label": "para",
          "text": "Another possible problem you might have encountered when accessing a text file is the\nnewline conventions, which are different for different operating systems. The built-in\nopen() function has a second parameter for controlling how the file is opened: open('do\ncument.txt', 'rU'). 'r' means to open the file for reading (the default), and 'U' stands\nfor “Universal”, which lets us ignore the different conventions used for marking new-\nlines.",
          "level": -1,
          "page": 106,
          "reading_order": 11,
          "bbox": [
            97,
            653,
            585,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_106_order_12",
          "label": "para",
          "text": "Assuming that you can open the file, there are several methods for reading it. The\nread() method creates a string with the contents of the entire file:",
          "level": -1,
          "page": 106,
          "reading_order": 12,
          "bbox": [
            97,
            759,
            585,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_106_order_13",
          "label": "foot",
          "text": "84 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 106,
          "reading_order": 13,
          "bbox": [
            97,
            824,
            255,
            842
          ],
          "section_number": "84",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_107_order_1",
          "label": "para",
          "text": "Recall that the '\\n' characters are newlines; this is equivalent to pressing Enter on a\nkeyboard and starting a new line.",
          "level": -1,
          "page": 107,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            584,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_107_order_2",
          "label": "para",
          "text": "We can also read a file one line at a time using a for loop",
          "level": -1,
          "page": 107,
          "reading_order": 2,
          "bbox": [
            99,
            149,
            423,
            163
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_107_order_4",
          "label": "para",
          "text": "Here we use the strip() method to remove the newline character at the end of the input\nline.",
          "level": -1,
          "page": 107,
          "reading_order": 4,
          "bbox": [
            97,
            241,
            585,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_107_order_5",
          "label": "para",
          "text": "NLTK's corpus files can also be accessed using these methods. We simply have to use\nnltk.data.find() to get the filename for any corpus item. Then we can open and read\nit in the way we just demonstrated:",
          "level": -1,
          "page": 107,
          "reading_order": 5,
          "bbox": [
            97,
            285,
            585,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_107_order_7",
      "label": "sec",
      "text": "Extracting Text from PDF, MSWord, and Other Binary Formats",
      "level": 1,
      "page": 107,
      "reading_order": 7,
      "bbox": [
        98,
        403,
        505,
        425
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_107_order_8",
          "label": "para",
          "text": "ASCII text and HTML text are human-readable formats. Text often comes in binary\nformats—such as PDF and MSWord—that can only be opened using specialized soft-\nware. Third-party libraries such as pypdf and pywin32 provide access to these formats.\nExtracting text from multicolumn documents is particularly challenging. For one-off\nconversion of a few documents, it is simpler to open the document with a suitable\napplication, then save it as text to your local drive, and access it as described below. If\nthe document is already on the Web, you can enter its URL in Google’s search box.\nThe search result often includes a link to an HTML version of the document, which\nyou can save as text.",
          "level": -1,
          "page": 107,
          "reading_order": 8,
          "bbox": [
            97,
            430,
            586,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_107_order_9",
      "label": "sec",
      "text": "Capturing User Input",
      "level": 1,
      "page": 107,
      "reading_order": 9,
      "bbox": [
        97,
        591,
        236,
        618
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_107_order_10",
          "label": "para",
          "text": "Sometimes we want to capture the text that a user inputs when she is interacting with\nour program. To prompt the user to type a line of input, call the Python function\nraw_input(). After saving the input to a variable, we can manipulate it just as we have\ndone for other strings.",
          "level": -1,
          "page": 107,
          "reading_order": 10,
          "bbox": [
            97,
            618,
            585,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_107_order_12",
          "label": "foot",
          "text": "3.1 Accessing Text from the Web and from Disk | 85",
          "level": -1,
          "page": 107,
          "reading_order": 12,
          "bbox": [
            367,
            824,
            585,
            842
          ],
          "section_number": "3.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_108_order_0",
      "label": "sec",
      "text": "The NLP Pipeline",
      "level": 1,
      "page": 108,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        209,
        95
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_109_order_4",
          "label": "sub_sec",
          "text": "3.2 Strings: Text Processing at the Lowest Level",
          "level": 2,
          "page": 109,
          "reading_order": 4,
          "bbox": [
            97,
            331,
            477,
            354
          ],
          "section_number": "3.2",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_109_order_6",
              "label": "sub_sub_sec",
              "text": "Basic Operations with Strings",
              "level": 3,
              "page": 109,
              "reading_order": 6,
              "bbox": [
                98,
                492,
                297,
                511
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_109_order_7",
                  "label": "para",
                  "text": "Strings are specified using single quotes ❶ or double quotes ❷ , as shown in the fol-\nlowing code example. If a string contains a single quote, we must backslash-escape the\nquote ❸ so Python knows a literal quote character is intended, or else put the string in\ndouble quotes ❷ . Otherwise, the quote inside the string ❹ will be interpreted as a close\nquote, and the Python interpreter will report a syntax error:",
                  "level": -1,
                  "page": 109,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    519,
                    585,
                    600
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_109_order_9",
                  "label": "foot",
                  "text": "3.2 Strings: Text Processing at the Lowest Level | 87",
                  "level": -1,
                  "page": 109,
                  "reading_order": 9,
                  "bbox": [
                    359,
                    824,
                    585,
                    842
                  ],
                  "section_number": "3.2",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_110_order_0",
                  "label": "para",
                  "text": "Sometimes strings go over several lines. Python provides us with various ways of en-\ntering them. In the next example, a sequence of two strings is joined into a single string.\nWe need to use backslash ❶ or parentheses ❷ so that the interpreter knows that the\nstatement is not complete after the first line.",
                  "level": -1,
                  "page": 110,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_110_order_2",
                  "label": "para",
                  "text": "Unfortunately these methods do not give us a newline between the two lines of the\nsonnet. Instead, we can use a triple-quoted string as follows:",
                  "level": -1,
                  "page": 110,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    259,
                    584,
                    291
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_110_order_4",
                  "label": "para",
                  "text": "Now that we can define strings, we can try some simple operations on them. First let's\nlook at the + operation, known as concatenation ❶. It produces a new string that is a\ncopy of the two original strings pasted together end-to-end. Notice that concatenation\ndoesn’t do anything clever like insert a space between the words. We can even multiply\nstrings ❷:",
                  "level": -1,
                  "page": 110,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    438,
                    585,
                    519
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_110_order_6",
                  "label": "para",
                  "text": "Your Turn: Try running the following code, then try to use your un-\nderstanding of the string + and * operations to figure out how it works.\nBe careful to distinguish between the string ' ', which is a single white-\nspace character, and '', which is the empty string.",
                  "level": -1,
                  "page": 110,
                  "reading_order": 6,
                  "bbox": [
                    171,
                    606,
                    530,
                    663
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_110_order_8",
                  "label": "para",
                  "text": "We’ve seen that the addition and multiplication operations apply to strings, not just\nnumbers. However, note that we cannot use subtraction or division with strings:",
                  "level": -1,
                  "page": 110,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    741,
                    584,
                    771
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_110_order_9",
                  "label": "foot",
                  "text": "88 | Chapter3: Processing Raw Text",
                  "level": -1,
                  "page": 110,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    824,
                    255,
                    842
                  ],
                  "section_number": "88",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_111_order_1",
                  "label": "para",
                  "text": "These error messages are another example of Python telling us that we have got our\ndata types in a muddle. In the first case, we are told that the operation of subtraction\n(i.e., -) cannot apply to objects of type str (strings), while in the second, we are told\nthat division cannot take str and int as its two operands.",
                  "level": -1,
                  "page": 111,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    187,
                    585,
                    251
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_109_order_5",
              "label": "para",
              "text": "It’s time to study a fundamental data type that we’ve been studiously avoiding so far.\nIn earlier chapters we focused on a text as a list of words. We didn’t look too closely\nat words and how they are handled in the programming language. By using NLTK's\ncorpus interface we were able to ignore the files that these texts had come from. The\ncontents of a word, and of a file, are represented by programming languages as a fun-\ndamental data type known as a string. In this section, we explore strings in detail, and\nshow the connection between strings, words, texts, and files.",
              "level": -1,
              "page": 109,
              "reading_order": 5,
              "bbox": [
                97,
                358,
                585,
                476
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_108_order_1",
          "label": "para",
          "text": "Figure 3-1 summarizes what we have covered in this section, including the process of\nbuilding a vocabulary that we saw in Chapter 1 . (One step, normalization, will be\ndiscussed in Section 3.6 .)",
          "level": -1,
          "page": 108,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            586,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_108_order_2",
          "label": "figure",
          "text": "Figure 3-1. The processing pipeline: We open a URL and read its HTML content, remove the markup\nand select a slice of characters; this is then tokenized and optionally converted into an nltk.Text\nobject; we can also lowercase all the words and extract the vocabulary. [IMAGE: ![Figure](figures/NLTK_page_108_figure_002.png)]",
          "level": -1,
          "page": 108,
          "reading_order": 2,
          "bbox": [
            100,
            161,
            583,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_108_figure_002.png)",
              "bbox": [
                100,
                161,
                583,
                376
              ],
              "page": 108,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Figure 3-1. The processing pipeline: We open a URL and read its HTML content, remove the markup\nand select a slice of characters; this is then tokenized and optionally converted into an nltk.Text\nobject; we can also lowercase all the words and extract the vocabulary.",
              "bbox": [
                97,
                376,
                585,
                423
              ],
              "page": 108,
              "reading_order": 3
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_108_order_4",
          "label": "para",
          "text": "There’s a lot going on in this pipeline. To understand it properly, it helps to be clear\nabout the type of each variable that it mentions. We find out the type of any Python\nobject x using type(x); e.g., type(1) is <int> since 1 is an integer.",
          "level": -1,
          "page": 108,
          "reading_order": 4,
          "bbox": [
            100,
            439,
            585,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_108_order_5",
          "label": "para",
          "text": "When we load the contents of a URL or file, and when we strip out HTML markup,\nwe are dealing with strings, Python’s <str> data type (we will learn more about strings\nin Section 3.2):",
          "level": -1,
          "page": 108,
          "reading_order": 5,
          "bbox": [
            97,
            492,
            585,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_108_order_7",
          "label": "para",
          "text": "When we tokenize a string we produce a list (of words), and this is Python’s <list>\ntype. Normalizing and sorting lists produces other lists:",
          "level": -1,
          "page": 108,
          "reading_order": 7,
          "bbox": [
            97,
            600,
            584,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_108_order_9",
          "label": "para",
          "text": "The type of an object determines what operations you can perform on it. So, for ex-\nample, we can append to a list but not to a string:",
          "level": -1,
          "page": 108,
          "reading_order": 9,
          "bbox": [
            100,
            761,
            584,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_108_order_10",
          "label": "foot",
          "text": "86 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 108,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            255,
            842
          ],
          "section_number": "86",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_109_order_1",
          "label": "para",
          "text": "Similarly, we can concatenate strings with strings, and lists with lists, but we cannot\nconcatenate strings with lists:",
          "level": -1,
          "page": 109,
          "reading_order": 1,
          "bbox": [
            97,
            148,
            585,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_109_order_3",
          "label": "para",
          "text": "In the next section, we examine strings more closely and further explore the relationship\nbetween strings and lists.",
          "level": -1,
          "page": 109,
          "reading_order": 3,
          "bbox": [
            97,
            268,
            585,
            305
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_111_order_2",
      "label": "sec",
      "text": "Printing Strings",
      "level": 1,
      "page": 111,
      "reading_order": 2,
      "bbox": [
        100,
        267,
        207,
        286
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_111_order_3",
          "label": "para",
          "text": "So far, when we have wanted to look at the contents of a variable or see the result of a\ncalculation, we have just typed the variable name into the interpreter. We can also see\nthe contents of a variable using the print statement:",
          "level": -1,
          "page": 111,
          "reading_order": 3,
          "bbox": [
            97,
            294,
            585,
            341
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_111_order_5",
          "label": "para",
          "text": "Notice that there are no quotation marks this time. When we inspect a variable by\ntyping its name in the interpreter, the interpreter prints the Python representation of\nits value. Since it's a string, the result is quoted. However, when we tell the interpreter\nto print the contents of the variable, we don't see quotation characters, since there are\nnone inside the string.",
          "level": -1,
          "page": 111,
          "reading_order": 5,
          "bbox": [
            97,
            385,
            586,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_111_order_6",
          "label": "para",
          "text": "The print statement allows us to display more than one item on a line in various ways,\nas shown here:",
          "level": -1,
          "page": 111,
          "reading_order": 6,
          "bbox": [
            100,
            474,
            584,
            502
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_111_order_8",
      "label": "sec",
      "text": "Accessing Individual Characters",
      "level": 1,
      "page": 111,
      "reading_order": 8,
      "bbox": [
        97,
        618,
        306,
        638
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_111_order_9",
          "label": "para",
          "text": "As we saw in Section 1.2 for lists, strings are indexed, starting from zero. When we\nindex a string, we get one of its characters (or letters). A single character is nothing\nspecial—it's just a string of length 1.",
          "level": -1,
          "page": 111,
          "reading_order": 9,
          "bbox": [
            97,
            645,
            585,
            693
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_111_order_11",
          "label": "foot",
          "text": "3.2 Strings: Text Processing at the Lowest Level | 89",
          "level": -1,
          "page": 111,
          "reading_order": 11,
          "bbox": [
            359,
            824,
            585,
            842
          ],
          "section_number": "3.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_112_order_0",
          "label": "para",
          "text": "As with lists, if we try to access an index that is outside of the string, we get an error:",
          "level": -1,
          "page": 112,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            583,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_112_order_2",
          "label": "para",
          "text": "Again as with lists, we can use negative indexes for strings, where -1 is the index of the\nlast character O. Positive and negative indexes give us two ways to refer to any position\nin a string. In this case, when the string had a length of 12, indexes 5 and -7 both refer\nto the same character (a space). (Notice that 5 = len(monty) - 7.)",
          "level": -1,
          "page": 112,
          "reading_order": 2,
          "bbox": [
            97,
            158,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_112_order_4",
          "label": "para",
          "text": "We can write for loops to iterate over the characters in strings. This print statement\nends with a trailing comma, which is how we tell Python not to print a newline at the\nend.",
          "level": -1,
          "page": 112,
          "reading_order": 4,
          "bbox": [
            97,
            313,
            585,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_112_order_6",
          "label": "para",
          "text": "We can count individual characters as well. We should ignore the case distinction by\nnormalizing everything to lowercase, and filter out non-alphabetic characters:",
          "level": -1,
          "page": 112,
          "reading_order": 6,
          "bbox": [
            97,
            447,
            585,
            478
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_112_order_8",
          "label": "para",
          "text": "This gives us the letters of the alphabet, with the most frequently occurring letters listed\nfirst (this is quite complicated and we’ll explain it more carefully later). You might like\nto visualize the distribution using fdist.plot(). The relative character frequencies of\na text can be used in automatically identifying the language of the text.",
          "level": -1,
          "page": 112,
          "reading_order": 8,
          "bbox": [
            97,
            573,
            586,
            637
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_112_order_9",
      "label": "sec",
      "text": "Accessing Substrings",
      "level": 1,
      "page": 112,
      "reading_order": 9,
      "bbox": [
        97,
        653,
        235,
        672
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_112_order_10",
          "label": "para",
          "text": "A substring is any continuous section of a string that we want to pull out for further\nprocessing. We can easily access substrings using the same slice notation we used for\nlists (see Figure 3-2 ). For example, the following code accesses the substring starting\nat index 6, up to (but not including) index 10:",
          "level": -1,
          "page": 112,
          "reading_order": 10,
          "bbox": [
            97,
            680,
            585,
            744
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_112_order_12",
          "label": "foot",
          "text": "90 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 112,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            255,
            842
          ],
          "section_number": "90",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_113_order_0",
          "label": "figure",
          "text": "Figure 3-2. String slicing: The string Monty Python is shown along with its positive and negative\nindexes; two substrings are selected using “ slice” notation. The slice [m,n] contains the characters\nfrom position m through n-1. [IMAGE: ![Figure](figures/NLTK_page_113_figure_000.png)]",
          "level": -1,
          "page": 113,
          "reading_order": 0,
          "bbox": [
            126,
            80,
            557,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_113_figure_000.png)",
              "bbox": [
                126,
                80,
                557,
                241
              ],
              "page": 113,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 3-2. String slicing: The string Monty Python is shown along with its positive and negative\nindexes; two substrings are selected using “ slice” notation. The slice [m,n] contains the characters\nfrom position m through n-1.",
              "bbox": [
                96,
                250,
                585,
                295
              ],
              "page": 113,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_113_order_2",
          "label": "para",
          "text": "Here we see the characters are 'P', 'y', 't', and 'h', which correspond to monty[6] ...\nmonty[9] but not monty[10]. This is because a slice starts at the first index but finishes\none before the end index.",
          "level": -1,
          "page": 113,
          "reading_order": 2,
          "bbox": [
            97,
            304,
            584,
            351
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_113_order_3",
          "label": "para",
          "text": "We can also slice with negative indexes—the same basic rule of starting from the start\nindex and stopping one before the end index applies; here we stop before the space\ncharacter.",
          "level": -1,
          "page": 113,
          "reading_order": 3,
          "bbox": [
            97,
            358,
            585,
            406
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_113_order_5",
          "label": "para",
          "text": "As with list slices, if we omit the first value, the substring begins at the start of the string.\nIf we omit the second value, the substring continues to the end of the string:",
          "level": -1,
          "page": 113,
          "reading_order": 5,
          "bbox": [
            97,
            448,
            584,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_113_order_7",
          "label": "para",
          "text": "We test if a string contains a particular substring using the in operator, as follows:",
          "level": -1,
          "page": 113,
          "reading_order": 7,
          "bbox": [
            99,
            553,
            566,
            567
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_113_order_9",
          "label": "para",
          "text": "We can also find the position of a substring within a string, using find()",
          "level": -1,
          "page": 113,
          "reading_order": 9,
          "bbox": [
            98,
            636,
            512,
            650
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_113_order_11",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_113_figure_011.png)",
          "level": -1,
          "page": 113,
          "reading_order": 11,
          "bbox": [
            118,
            698,
            162,
            754
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_113_order_12",
          "label": "para",
          "text": "Your Turn: Make up a sentence and assign it to a variable, e.g., sent =\n'my sentence...'. Now write slice expressions to pull out individual\nwords. (This is obviously not a convenient way to process the words of\na text!)",
          "level": -1,
          "page": 113,
          "reading_order": 12,
          "bbox": [
            171,
            707,
            530,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_113_order_13",
          "label": "foot",
          "text": "3.2 Strings: Text Processing at the Lowest Level | 91",
          "level": -1,
          "page": 113,
          "reading_order": 13,
          "bbox": [
            359,
            824,
            584,
            842
          ],
          "section_number": "3.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_114_order_0",
      "label": "sec",
      "text": "More Operations on Strings",
      "level": 1,
      "page": 114,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        279,
        95
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_114_order_1",
          "label": "para",
          "text": "Python has comprehensive support for processing strings. A summary, including some\noperations we haven’t seen yet, is shown in Table 3-2. For more information on strings,\ntype help(str) at the Python prompt.",
          "level": -1,
          "page": 114,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_114_order_2",
          "label": "table",
          "text": "Table 3-2. Useful string methods: Operations on strings in addition to the string tests shown in\nTable 1-4; all methods produce a new string or list [TABLE: <table><tr><td>Method</td><td>Functionality</td></tr><tr><td>s.find(t)</td><td>Index of first instance of string t inside s (-1 if not found)</td></tr><tr><td>s.rfind(t)</td><td>Index of last instance of string t inside s (-1 if not found)</td></tr><tr><td>s.index(t)</td><td>Like s.find(t), except it raises ValueError if not found</td></tr><tr><td>s.rindex(t)</td><td>Like s.rfind(t), except it raises ValueError if not found</td></tr><tr><td>s.join(text)</td><td>Combine the words of the text into a string using s as the glue</td></tr><tr><td>s.split(t)</td><td>Split s into a list wherever a t is found (whitespace by default)</td></tr><tr><td>s.splitlines()</td><td>Split s into a list of strings, one per line</td></tr><tr><td>s.lower()</td><td>A lowercased version of the string s</td></tr><tr><td>s.upper()</td><td>An uppercased version of the string s</td></tr><tr><td>s.titlecase()</td><td>A titlecased version of the string s</td></tr><tr><td>s.strip()</td><td>A copy of s without leading or trailing whitespace</td></tr><tr><td>s.replace(t, u)</td><td>Replace instances of t with u inside s</td></tr></table>]",
          "level": -1,
          "page": 114,
          "reading_order": 2,
          "bbox": [
            100,
            197,
            449,
            457
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Method</td><td>Functionality</td></tr><tr><td>s.find(t)</td><td>Index of first instance of string t inside s (-1 if not found)</td></tr><tr><td>s.rfind(t)</td><td>Index of last instance of string t inside s (-1 if not found)</td></tr><tr><td>s.index(t)</td><td>Like s.find(t), except it raises ValueError if not found</td></tr><tr><td>s.rindex(t)</td><td>Like s.rfind(t), except it raises ValueError if not found</td></tr><tr><td>s.join(text)</td><td>Combine the words of the text into a string using s as the glue</td></tr><tr><td>s.split(t)</td><td>Split s into a list wherever a t is found (whitespace by default)</td></tr><tr><td>s.splitlines()</td><td>Split s into a list of strings, one per line</td></tr><tr><td>s.lower()</td><td>A lowercased version of the string s</td></tr><tr><td>s.upper()</td><td>An uppercased version of the string s</td></tr><tr><td>s.titlecase()</td><td>A titlecased version of the string s</td></tr><tr><td>s.strip()</td><td>A copy of s without leading or trailing whitespace</td></tr><tr><td>s.replace(t, u)</td><td>Replace instances of t with u inside s</td></tr></table>",
              "bbox": [
                100,
                197,
                449,
                457
              ],
              "page": 114,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Table 3-2. Useful string methods: Operations on strings in addition to the string tests shown in\nTable 1-4; all methods produce a new string or list",
              "bbox": [
                99,
                161,
                558,
                192
              ],
              "page": 114,
              "reading_order": 3
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_114_order_4",
      "label": "sec",
      "text": "The Difference Between Lists and Strings",
      "level": 1,
      "page": 114,
      "reading_order": 4,
      "bbox": [
        97,
        474,
        369,
        498
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_115_order_7",
          "label": "sub_sec",
          "text": "3.3 Text Processing with Unicode",
          "level": 2,
          "page": 115,
          "reading_order": 7,
          "bbox": [
            97,
            609,
            363,
            638
          ],
          "section_number": "3.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_115_order_8",
              "label": "para",
              "text": "Our programs will often need to deal with different languages, and different character\nsets. The concept of “ plain text ” is a fiction. If you live in the English-speaking world\nyou probably use ASCII, possibly without realizing it. If you live in Europe you might\nuse one of the extended Latin character sets, containing such characters as “ ø ” for\nDanish and Norwegian, “ Õ ” for Hungarian, “ ñ ” for Spanish and Breton, and “ Ñ  ̃ for\nCzech and Slovak. In this section, we will give an overview of how to use Unicode for\nprocessing texts that use non-ASCII character sets.",
              "level": -1,
              "page": 115,
              "reading_order": 8,
              "bbox": [
                97,
                645,
                585,
                761
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_115_order_9",
              "label": "foot",
              "text": "3.3 Text Processing with Unicode | 93",
              "level": -1,
              "page": 115,
              "reading_order": 9,
              "bbox": [
                422,
                824,
                584,
                842
              ],
              "section_number": "3.3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_114_order_5",
          "label": "para",
          "text": "Strings and lists are both kinds of sequence. We can pull them apart by indexing and\nslicing them, and we can join them together by concatenating them. However, we can-\nnot join strings and lists:",
          "level": -1,
          "page": 114,
          "reading_order": 5,
          "bbox": [
            97,
            501,
            585,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_114_order_7",
          "label": "foot",
          "text": "92 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 114,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            255,
            842
          ],
          "section_number": "92",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_115_order_0",
          "label": "para",
          "text": "When we open a file for reading into a Python program, we get a string corresponding\nto the contents of the whole file. If we use a for loop to process the elements of this\nstring, all we can pick out are the individual characters—we don’t get to choose the\ngranularity. By contrast, the elements of a list can be as big or small as we like: for\nexample, they could be paragraphs, sentences, phrases, words, characters. So lists have\nthe advantage that we can be flexible about the elements they contain, and corre-\nspondingly flexible about any downstream processing. Consequently, one of the first\nthings we are likely to do in a piece of NLP code is tokenize a string into a list of strings\n( Section 3.7 ). Conversely, when we want to write our results to a file, or to a terminal,\nwe will usually format them as a string ( Section 3.9 ).",
          "level": -1,
          "page": 115,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            238
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_115_order_1",
          "label": "para",
          "text": "Lists and strings do not have exactly the same functionality. Lists have the added power\nthat you can change their elements:",
          "level": -1,
          "page": 115,
          "reading_order": 1,
          "bbox": [
            97,
            248,
            585,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_115_order_3",
          "label": "para",
          "text": "On the other hand, if we try to do that with a string—changing the 0th character in\nquery to 'F'—we get:",
          "level": -1,
          "page": 115,
          "reading_order": 3,
          "bbox": [
            97,
            348,
            584,
            378
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_115_order_5",
          "label": "para",
          "text": "This is because strings are immutable: you can’t change a string once you have created\nit. However, lists are mutable, and their contents can be modified at any time. As a\nresult, lists support operations that modify the original value rather than producing a\nnew value.",
          "level": -1,
          "page": 115,
          "reading_order": 5,
          "bbox": [
            100,
            448,
            585,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_115_order_6",
          "label": "para",
          "text": "Your Turn: Consolidate your knowledge of strings by trying some of\nthe exercises on strings at the end of this chapter.",
          "level": -1,
          "page": 115,
          "reading_order": 6,
          "bbox": [
            171,
            537,
            530,
            568
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_116_order_0",
      "label": "sec",
      "text": "What Is Unicode?",
      "level": 1,
      "page": 116,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        211,
        91
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_116_order_1",
          "label": "para",
          "text": "Unicode supports over a million characters. Each character is assigned a number, called\na code point. In Python, code points are written in the form \\uXXXXX, where XXXX\nis the number in four-digit hexadecimal form.",
          "level": -1,
          "page": 116,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_116_order_2",
          "label": "para",
          "text": "Within a program, we can manipulate Unicode strings just like normal strings. How-\never, when Unicode characters are stored in files or displayed on a terminal, they must\nbe encoded as a stream of bytes. Some encodings (such as ASCII and Latin-2) use a\nsingle byte per code point, so they can support only a small subset of Unicode, enough\nfor a single language. Other encodings (such as UTF-8) use multiple bytes and can\nrepresent the full range of Unicode characters.",
          "level": -1,
          "page": 116,
          "reading_order": 2,
          "bbox": [
            97,
            152,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_116_order_3",
          "label": "para",
          "text": "Text in files will be in a particular encoding, so we need some mechanism for translating\nit into Unicode—translation into Unicode is called decoding. Conversely, to write out\nUnicode to a file or a terminal, we first need to translate it into a suitable encoding—\nthis translation out of Unicode is called encoding, and is illustrated in Figure 3-3.",
          "level": -1,
          "page": 116,
          "reading_order": 3,
          "bbox": [
            97,
            259,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_116_order_4",
          "label": "figure",
          "text": "Figure 3-3. Unicode decoding and encoding. [IMAGE: ![Figure](figures/NLTK_page_116_figure_004.png)]",
          "level": -1,
          "page": 116,
          "reading_order": 4,
          "bbox": [
            100,
            340,
            583,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_116_figure_004.png)",
              "bbox": [
                100,
                340,
                583,
                591
              ],
              "page": 116,
              "reading_order": 4
            },
            {
              "label": "cap",
              "text": "Figure 3-3. Unicode decoding and encoding.",
              "bbox": [
                97,
                591,
                315,
                609
              ],
              "page": 116,
              "reading_order": 5
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_116_order_6",
          "label": "para",
          "text": "From a Unicode perspective, characters are abstract entities that can be realized as one\nor more glyphs. Only glyphs can appear on a screen or be printed on paper. A font is\na mapping from characters to glyphs.",
          "level": -1,
          "page": 116,
          "reading_order": 6,
          "bbox": [
            97,
            627,
            585,
            675
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_116_order_7",
      "label": "sec",
      "text": "Extracting Encoded Text from Files",
      "level": 1,
      "page": 116,
      "reading_order": 7,
      "bbox": [
        98,
        689,
        326,
        710
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_116_order_8",
          "label": "para",
          "text": "Let's assume that we have a small text file, and that we know how it is encoded. For\nexample, polish-lat2.txt , as the name suggests, is a snippet of Polish text (from the Polish\nWikipedia; see http://pl.wikipedia.org/wiki/Biblioteka_Pruska) . This file is encoded as\nLatin-2, also known as ISO-8859-2. The function nltk.data.find() locates the file for\nus.",
          "level": -1,
          "page": 116,
          "reading_order": 8,
          "bbox": [
            97,
            716,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_116_order_9",
          "label": "foot",
          "text": "94 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 116,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            255,
            842
          ],
          "section_number": "94",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_117_order_0",
          "label": "para",
          "text": ">>> path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')",
          "level": -1,
          "page": 117,
          "reading_order": 0,
          "bbox": [
            118,
            71,
            494,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_117_order_1",
          "label": "para",
          "text": "The Python codecs module provides functions to read encoded data into Unicode\nstrings, and to write out Unicode strings in encoded form. The codecs.open() function\ntakes an encoding parameter to specify the encoding of the file being read or written.\nSo let's import the codecs module, and call it with the encoding 'latin2' to open our\nPolish file as Unicode:",
          "level": -1,
          "page": 117,
          "reading_order": 1,
          "bbox": [
            97,
            89,
            585,
            173
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_117_order_3",
          "label": "para",
          "text": "For a list of encoding parameters allowed by codecs, see http://docs.python.org/lib/\nstandard-encodings.html. Note that we can write Unicode-encoded data to a file using\nf = codecs.open(path, 'w', encoding='utf-8').",
          "level": -1,
          "page": 117,
          "reading_order": 3,
          "bbox": [
            97,
            215,
            585,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_117_order_4",
          "label": "para",
          "text": "Text read from the file object f will be returned in Unicode. As we pointed out earlier,\nin order to view this text on a terminal, we need to encode it, using a suitable encoding.\nThe Python-specific encoding unicode_escape is a dummy encoding that converts all\nnon-ASCII characters into their \\uXXXX representations. Code points above the ASCII\n0–127 range but below 256 are represented in the two-digit form \\xXX.",
          "level": -1,
          "page": 117,
          "reading_order": 4,
          "bbox": [
            100,
            276,
            584,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_117_order_6",
          "label": "para",
          "text": "The first line in this output illustrates a Unicode escape string preceded by the \\u escape\nstring, namely \\u0144. The relevant Unicode character will be displayed on the screen\nas the glyph ñ. In the third line of the preceding example, we see \\xf3, which corre-\nsponds to the glyph ó, and is within the 128–255 range.",
          "level": -1,
          "page": 117,
          "reading_order": 6,
          "bbox": [
            97,
            491,
            585,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_117_order_7",
          "label": "para",
          "text": "In Python, a Unicode string literal can be specified by preceding an ordinary string\nliteral with a u, as in u'hello'. Arbitrary Unicode characters are defined using the\n\\uXXXX escape sequence inside a Unicode string literal. We find the integer ordinal\nof a character using ord(). For example:",
          "level": -1,
          "page": 117,
          "reading_order": 7,
          "bbox": [
            97,
            564,
            585,
            628
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_117_order_10",
          "label": "para",
          "text": "The hexadecimal four-digit notation for 97 is 0061, so we can define a Unicode string\niteral with the appropriate escape sequence:",
          "level": -1,
          "page": 117,
          "reading_order": 10,
          "bbox": [
            100,
            671,
            585,
            702
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_117_order_12",
          "label": "foot",
          "text": "3.3 Text Processing with Unicode | 95",
          "level": -1,
          "page": 117,
          "reading_order": 12,
          "bbox": [
            422,
            824,
            585,
            842
          ],
          "section_number": "3.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_118_order_0",
          "label": "para",
          "text": "Notice that the Python print statement is assuming a default encoding of the Unicode\ncharacter, namely ASCII. However, f1 is outside the ASCII range, so cannot be printed\nunless we specify an encoding. In the following example, we have specified that\nprint should use the repr() of the string, which outputs the UTF-8 escape sequences\n(of the form \\xXX) rather than trying to render the glyphs.",
          "level": -1,
          "page": 118,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_118_order_2",
          "label": "para",
          "text": "If your operating system and locale are set up to render UTF-8 encoded characters, you\nought to be able to give the Python command print_nacute_utf and see fn on your\nscreen.",
          "level": -1,
          "page": 118,
          "reading_order": 2,
          "bbox": [
            97,
            250,
            585,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_118_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_118_figure_003.png)",
          "level": -1,
          "page": 118,
          "reading_order": 3,
          "bbox": [
            109,
            313,
            171,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_118_order_4",
          "label": "para",
          "text": "There are many factors determining what glyphs are rendered on your\nscreen. If you are sure that you have the correct encoding, but your\nPython code is still failing to produce the glyphs you expected, you\nshould also check that you have the necessary fonts installed on your\nsystem.",
          "level": -1,
          "page": 118,
          "reading_order": 4,
          "bbox": [
            171,
            322,
            530,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_118_order_5",
          "label": "para",
          "text": "The module unicodedata lets us inspect the properties of Unicode characters. In the\nfollowing example, we select all characters in the third line of our Polish text outside\nthe ASCII range and print their UTF-8 escaped value, followed by their code point\ninteger using the standard Unicode convention (i.e., prefixing the hex digits with U+),\nfollowed by their Unicode name.",
          "level": -1,
          "page": 118,
          "reading_order": 5,
          "bbox": [
            97,
            421,
            585,
            502
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_118_order_7",
          "label": "para",
          "text": "If you replace the %r (which yields the repr() value) by %s in the format string of the\npreceding code sample, and if your system supports UTF-8, you should see an output\nlike the following:",
          "level": -1,
          "page": 118,
          "reading_order": 7,
          "bbox": [
            97,
            689,
            585,
            736
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_118_order_9",
          "label": "foot",
          "text": "96 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 118,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            255,
            842
          ],
          "section_number": "96",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_119_order_1",
          "label": "para",
          "text": "Alternatively, you may need to replace the encoding 'utf8' in the example by\n'latin2', again depending on the details of your system.",
          "level": -1,
          "page": 119,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_119_order_2",
          "label": "para",
          "text": "The next examples illustrate how Python string methods and the re module accept\nUnicode strings.",
          "level": -1,
          "page": 119,
          "reading_order": 2,
          "bbox": [
            98,
            149,
            585,
            180
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_119_order_4",
          "label": "para",
          "text": "NLTK tokenizers allow Unicode strings as input, and correspondingly yield Unicode\nstrings as output.",
          "level": -1,
          "page": 119,
          "reading_order": 4,
          "bbox": [
            97,
            313,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_119_order_6",
      "label": "sec",
      "text": "Using Your Local Encoding in Python",
      "level": 1,
      "page": 119,
      "reading_order": 6,
      "bbox": [
        98,
        403,
        338,
        425
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_119_order_9",
          "label": "sub_sec",
          "text": "3.4 Regular Expressions for Detecting Word Patterns",
          "level": 2,
          "page": 119,
          "reading_order": 9,
          "bbox": [
            97,
            564,
            521,
            591
          ],
          "section_number": "3.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_119_order_10",
              "label": "para",
              "text": "Many linguistic processing tasks involve pattern matching. For example, we can find\nwords ending with ed using endswith('ed'). We saw a variety of such “ word tests ” in\nTable 1-4. Regular expressions give us a more powerful and flexible method for de-\nscribing the character patterns we are interested in.",
              "level": -1,
              "page": 119,
              "reading_order": 10,
              "bbox": [
                97,
                591,
                584,
                663
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_119_order_11",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_119_figure_011.png)",
              "level": -1,
              "page": 119,
              "reading_order": 11,
              "bbox": [
                118,
                672,
                171,
                734
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_119_order_12",
              "label": "para",
              "text": "There are many other published introductions to regular expressions,\norganized around the syntax of regular expressions and applied to\nsearching text files. Instead of doing this again, we focus on the use of\nregular expressions at different stages of linguistic processing. As usual,\nwe'll adopt a problem-based approach and present new features only as\nthey are needed to solve practical problems. In our discussion we will\nmark regular expressions using chevrons like this: « patt » .",
              "level": -1,
              "page": 119,
              "reading_order": 12,
              "bbox": [
                171,
                688,
                530,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_119_order_13",
              "label": "foot",
              "text": "3.4 Regular Expressions for Detecting Word Patterns | 97",
              "level": -1,
              "page": 119,
              "reading_order": 13,
              "bbox": [
                342,
                824,
                585,
                842
              ],
              "section_number": "3.4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_120_order_0",
              "label": "figure",
              "text": "Figure 3-4. Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor; this requires that\nan appropriate font is set in IDLE's preferences; here we have chosen Courier CE. [IMAGE: ![Figure](figures/NLTK_page_120_figure_000.png)]",
              "level": -1,
              "page": 120,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                583,
                349
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_120_figure_000.png)",
                  "bbox": [
                    100,
                    71,
                    583,
                    349
                  ],
                  "page": 120,
                  "reading_order": 0
                },
                {
                  "label": "cap",
                  "text": "Figure 3-4. Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor; this requires that\nan appropriate font is set in IDLE's preferences; here we have chosen Courier CE.",
                  "bbox": [
                    97,
                    349,
                    584,
                    385
                  ],
                  "page": 120,
                  "reading_order": 1
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_120_order_2",
              "label": "para",
              "text": "To use regular expressions in Python, we need to import the re library using: import\nre . We also need a list of words to search; we'll use the Words Corpus again ( Sec-\ntion 2.4 ). We will preprocess it to remove any proper names.",
              "level": -1,
              "page": 120,
              "reading_order": 2,
              "bbox": [
                97,
                392,
                584,
                439
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_119_order_7",
          "label": "para",
          "text": "If you are used to working with characters in a particular local encoding, you probably\nwant to be able to use your standard methods for inputting and editing strings in a\nPython file. In order to do this, you need to include the string '# -*- coding: <coding>\n-*-' as the first or second line of your file. Note that <coding> has to be a string like\n'latin-1', 'big5', or 'utf-8' (see Figure 3-4).",
          "level": -1,
          "page": 119,
          "reading_order": 7,
          "bbox": [
            97,
            430,
            585,
            514
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_119_order_8",
          "label": "para",
          "text": "Figure 3-4 also illustrates how regular expressions can use encoded strings.",
          "level": -1,
          "page": 119,
          "reading_order": 8,
          "bbox": [
            98,
            519,
            530,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_120_order_4",
      "label": "sec",
      "text": "Using Basic Metacharacters",
      "level": 1,
      "page": 120,
      "reading_order": 4,
      "bbox": [
        98,
        510,
        279,
        532
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_120_order_5",
          "label": "para",
          "text": "Let's find words ending with ed using the regular expression «ed$». We will use the\nre.search(p, s) function to check whether the pattern p can be found somewhere\ninside the string s. We need to specify the characters of interest, and use the dollar sign,\nwhich has a special behavior in the context of regular expressions in that it matches the\nend of the word:",
          "level": -1,
          "page": 120,
          "reading_order": 5,
          "bbox": [
            97,
            537,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_120_order_7",
          "label": "para",
          "text": "The . wildcard symbol matches any single character. Suppose we have room in a\ncrossword puzzle for an eight-letter word, with j as its third letter and t as its sixth letter.\nIn place of each blank cell we use a period:",
          "level": -1,
          "page": 120,
          "reading_order": 7,
          "bbox": [
            97,
            663,
            584,
            711
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_120_order_9",
          "label": "foot",
          "text": "98 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 120,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            255,
            842
          ],
          "section_number": "98",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_121_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_121_figure_000.png)",
          "level": -1,
          "page": 121,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            171,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_121_order_1",
          "label": "para",
          "text": "Your Turn: The caret symbol ^ matches the start of a string, just like\nthe $ matches the end. What results do we get with the example just\nshown if we leave out both of these, and search for «..j..t..»?",
          "level": -1,
          "page": 121,
          "reading_order": 1,
          "bbox": [
            171,
            80,
            530,
            127
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_121_order_2",
          "label": "para",
          "text": "Finally, the ? symbol specifies that the previous character is optional. Thus «^e-?mail\n$» will match both email and e-mail. We could count the total number of occurrences\nof this word (in either spelling) in a text using sum(1 for w in text if re.search('^e-?\nmail$', w)).",
          "level": -1,
          "page": 121,
          "reading_order": 2,
          "bbox": [
            97,
            152,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_121_order_3",
      "label": "sec",
      "text": "Ranges and Closures",
      "level": 1,
      "page": 121,
      "reading_order": 3,
      "bbox": [
        98,
        232,
        234,
        256
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_124_order_0",
          "label": "sub_sec",
          "text": "3.5 Useful Applications of Regular Expressions",
          "level": 2,
          "page": 124,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            467,
            100
          ],
          "section_number": "3.5",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_124_order_2",
              "label": "sub_sub_sec",
              "text": "Extracting Word Pieces",
              "level": 3,
              "page": 124,
              "reading_order": 2,
              "bbox": [
                98,
                188,
                252,
                207
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_124_order_3",
                  "label": "para",
                  "text": "The re.findall() (“find all”) method finds all (non-overlapping) matches of the given\nregular expression. Let’s find all the vowels in a word, then count them:",
                  "level": -1,
                  "page": 124,
                  "reading_order": 3,
                  "bbox": [
                    100,
                    215,
                    584,
                    250
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_124_order_5",
                  "label": "para",
                  "text": "Let’s look for all sequences of two or more vowels in some text, and determine their\nrelative frequency:",
                  "level": -1,
                  "page": 124,
                  "reading_order": 5,
                  "bbox": [
                    98,
                    322,
                    585,
                    359
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_124_order_7",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_124_figure_007.png)",
                  "level": -1,
                  "page": 124,
                  "reading_order": 7,
                  "bbox": [
                    118,
                    474,
                    162,
                    529
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_124_order_8",
                  "label": "para",
                  "text": "Your Turn: In the W3C Date Time Format, dates are represented like\nthis: 2009-12-31. Replace the ? in the following Python code with a\nregular expression, in order to convert the string '2009-12-31' to a list\nof integers [2009, 12, 31]:",
                  "level": -1,
                  "page": 124,
                  "reading_order": 8,
                  "bbox": [
                    171,
                    483,
                    530,
                    546
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_124_order_9",
                  "label": "para",
                  "text": "[int(n) for n in re.findall(?,'2009-12-31')",
                  "level": -1,
                  "page": 124,
                  "reading_order": 9,
                  "bbox": [
                    171,
                    546,
                    413,
                    564
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            },
            {
              "id": "page_124_order_10",
              "label": "sub_sub_sec",
              "text": "Doing More with Word Pieces",
              "level": 3,
              "page": 124,
              "reading_order": 10,
              "bbox": [
                100,
                589,
                297,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_124_order_11",
                  "label": "para",
                  "text": "Once we can use re.findall() to extract material from words, there are interesting\nthings to do with the pieces, such as glue them back together or plot them.",
                  "level": -1,
                  "page": 124,
                  "reading_order": 11,
                  "bbox": [
                    97,
                    609,
                    585,
                    647
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_124_order_12",
                  "label": "para",
                  "text": "It is sometimes noted that English text is highly redundant, and it is still easy to read\nwhen word-internal vowels are left out. For example, declaration becomes dclrtn , and\ninalienable becomes inlnble , retaining any initial or final vowel sequences. The regular\nexpression in our next example matches initial vowel sequences, final vowel sequences,\nand all consonants; everything else is ignored. This three-way disjunction is processed\nleft-to-right, and if one of the three parts matches the word, any later parts of the regular\nexpression are ignored. We use re.findall() to extract all the matching pieces, and\n''.join() to join them together (see Section 3.9 for more about the join operation).",
                  "level": -1,
                  "page": 124,
                  "reading_order": 12,
                  "bbox": [
                    97,
                    654,
                    585,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_124_order_13",
                  "label": "foot",
                  "text": "102 | Chapter3: Processing Raw Text",
                  "level": -1,
                  "page": 124,
                  "reading_order": 13,
                  "bbox": [
                    97,
                    824,
                    261,
                    842
                  ],
                  "section_number": "102",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_125_order_1",
                  "label": "para",
                  "text": "Next, let's combine regular expressions with conditional frequency distributions. Here\nwe will extract all consonant-vowel sequences from the words of Rotokas, such as ka\nand si . Since each of these is a pair, it can be used to initialize a conditional frequency\ndistribution. We then tabulate the frequency of each pair:",
                  "level": -1,
                  "page": 125,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    239,
                    585,
                    304
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_125_order_3",
                  "label": "para",
                  "text": "Examining the rows for s and t, we see they are in partial “ complementary distribution,\n”\nwhich is evidence that they are not distinct phonemes in the language. Thus, we could\nconceivably drop s from the Rotokas alphabet and simply have a pronunciation rule\nthat the letter t is pronounced s when followed by i. (Note that the single entry having\nsu, namely kasuari,\n‘cassowary’ is borrowed from English).",
                  "level": -1,
                  "page": 125,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    463,
                    585,
                    546
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_125_order_4",
                  "label": "para",
                  "text": "If we want to be able to inspect the words behind the numbers in that table, it would\nbe helpful to have an index, allowing us to quickly find the list of words that contains\na given consonant-vowel pair. For example, cv_index['su'] should give us all words\ncontaining su. Here’s how we can do this:",
                  "level": -1,
                  "page": 125,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    553,
                    585,
                    618
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_125_order_6",
                  "label": "para",
                  "text": "This program processes each word w in turn, and for each one, finds every substring\nthat matches the regular expression «[ptksvr][aeiou]». In the case of the word ka-\nsuari, it finds ka, su, and ri. Therefore, the cv_word_pairs list will contain ('ka', 'ka",
                  "level": -1,
                  "page": 125,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    734,
                    585,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_125_order_7",
                  "label": "foot",
                  "text": "3.5 Useful Applications of Regular Expressions | 103",
                  "level": -1,
                  "page": 125,
                  "reading_order": 7,
                  "bbox": [
                    366,
                    824,
                    584,
                    842
                  ],
                  "section_number": "3.5",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_126_order_0",
                  "label": "para",
                  "text": "suari'), ('su',\n'kasuari'), and ('ri',\n'kasuari'). One further step, using\nnltk.Index(), converts this into a useful index.",
                  "level": -1,
                  "page": 126,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    107
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_124_order_1",
              "label": "para",
              "text": "The previous examples all involved searching for words w that match some regular\nexpression regexp using re.search(regexp, w). Apart from checking whether a regular\nexpression matches a word, we can use regular expressions to extract material from\nwords, or to modify words in specific ways.",
              "level": -1,
              "page": 124,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                173
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_121_order_4",
          "label": "para",
          "text": "The T9 system is used for entering text on mobile phones (see Figure 3-5). Two or more\nwords that are entered with the same sequence of keystrokes are known as\ntextonyms. For example, both hole and golf are entered by pressing the sequence 4653.\nWhat other words could be produced with the same sequence? Here we use the regular\nexpression «^[ghi][mno][jlk][def]$»:",
          "level": -1,
          "page": 121,
          "reading_order": 4,
          "bbox": [
            97,
            259,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_121_order_5",
          "label": "para",
          "text": ">>> [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]\n['gold', 'golf', 'hold', 'hole']",
          "level": -1,
          "page": 121,
          "reading_order": 5,
          "bbox": [
            118,
            349,
            485,
            378
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_121_order_6",
          "label": "para",
          "text": "The first part of the expression, «^[ghi]», matches the start of a word followed by g,\nh, or i. The next part of the expression, «[mmo]», constrains the second character to be m,\nn, or o. The third and fourth characters are also constrained. Only four words satisfy\nall these constraints. Note that the order of characters inside the square brackets is not\nsignificant, so we could have written «^[hig][nom][1jk][fed]$» and matched the same\nwords.",
          "level": -1,
          "page": 121,
          "reading_order": 6,
          "bbox": [
            97,
            385,
            585,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_121_order_7",
          "label": "figure",
          "text": "Figure 3-5. T9: Text on 9 keys [IMAGE: ![Figure](figures/NLTK_page_121_figure_007.png)]",
          "level": -1,
          "page": 121,
          "reading_order": 7,
          "bbox": [
            100,
            510,
            583,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_121_figure_007.png)",
              "bbox": [
                100,
                510,
                583,
                654
              ],
              "page": 121,
              "reading_order": 7
            },
            {
              "label": "cap",
              "text": "Figure 3-5. T9: Text on 9 keys",
              "bbox": [
                97,
                663,
                246,
                680
              ],
              "page": 121,
              "reading_order": 8
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_121_order_9",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_121_figure_009.png)",
          "level": -1,
          "page": 121,
          "reading_order": 9,
          "bbox": [
            109,
            698,
            171,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_121_order_10",
          "label": "para",
          "text": "Your Turn: Look for some “finger-twisters,\n” by searching for words\nthat use only part of the number-pad. For example «^[ghijklmno]+$»,\nor more concisely, «^[g­o]+$», will match words that only use keys 4,\n5,6 in the center row, and «^[a­fj­o]+$» will match words that use keys\n2, 3, 5, 6 in the top-right corner. What do ­ and + mean?",
          "level": -1,
          "page": 121,
          "reading_order": 10,
          "bbox": [
            171,
            707,
            530,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_121_order_11",
          "label": "foot",
          "text": "3.4 Regular Expressions for Detecting Word Patterns | 99",
          "level": -1,
          "page": 121,
          "reading_order": 11,
          "bbox": [
            342,
            824,
            585,
            842
          ],
          "section_number": "3.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_122_order_0",
          "label": "para",
          "text": "Let’s explore the + symbol a bit further. Notice that it can be applied to individual\nletters, or to bracketed sets of letters:",
          "level": -1,
          "page": 122,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_122_order_2",
          "label": "para",
          "text": "It should be clear that + simply means “one or more instances of the preceding item,”\nwhich could be an individual character like m, a set like [fed], or a range like [d-f].\nNow let’s replace + with *, which means “zero or more instances of the preceding item.”\nThe regular expression «^m*i*n*e*$» will match everything that we found using «^m+i\n+n+e+$», but also words where some of the letters don’t appear at all, e.g., me, min, and\nmmmmm. Note that the + and * symbols are sometimes referred to as Kleene clo-\nsures, or simply closures.",
          "level": -1,
          "page": 122,
          "reading_order": 2,
          "bbox": [
            97,
            224,
            584,
            341
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_122_order_3",
          "label": "para",
          "text": "The ^ operator has another function when it appears as the first character inside square\nbrackets. For example, «[^aeiouAEIOU]» matches any character other than a vowel. We\ncan search the NPS Chat Corpus for words that are made up entirely of non-vowel\ncharacters using «^[^aeiouAEIOU]+$» to find items like these: :):):), grrr, cyb3r, and\nzzzzzzz. Notice this includes non-alphabetic characters.",
          "level": -1,
          "page": 122,
          "reading_order": 3,
          "bbox": [
            97,
            349,
            585,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_122_order_4",
          "label": "para",
          "text": "Here are some more examples of regular expressions being used to find tokens that\nmatch a particular pattern, illustrating the use of some new symbols: \\, {}, (), and |.",
          "level": -1,
          "page": 122,
          "reading_order": 4,
          "bbox": [
            97,
            439,
            584,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_122_order_6",
          "label": "para",
          "text": "Your Turn: Study the previous examples and try to work out what the \\,\n{}, (), and | notations mean before you read on.",
          "level": -1,
          "page": 122,
          "reading_order": 6,
          "bbox": [
            171,
            713,
            530,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_122_order_7",
          "label": "foot",
          "text": "100 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 122,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            261,
            842
          ],
          "section_number": "100",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_123_order_0",
          "label": "para",
          "text": "You probably worked out that a backslash means that the following character is de-\nprived of its special powers and must literally match a specific character in the word.\nThus, while . is special, \\. only matches a period. The braced expressions, like {3,5},\nspecify the number of repeats of the previous item. The pipe character indicates a choice\nbetween the material on its left or its right. Parentheses indicate the scope of an oper-\nator, and they can be used together with the pipe (or disjunction) symbol like this:\n«w(i|e|ai|oo)t», matching wit, wet, wait, and woot. It is instructive to see what happens\nwhen you omit the parentheses from the last expression in the example, and search for\n«ed|ing$».",
          "level": -1,
          "page": 123,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_123_order_1",
          "label": "para",
          "text": "The metacharacters we have seen are summarized in Table 3-3 .",
          "level": -1,
          "page": 123,
          "reading_order": 1,
          "bbox": [
            100,
            231,
            458,
            242
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_123_order_2",
          "label": "table",
          "text": "Table 3-3. Basic regular expression metacharacters, including wildcards, ranges, and closures [TABLE: <table><tr><td>Operator</td><td>Behavior</td></tr><tr><td>-</td><td>Wildcard, matches any character</td></tr><tr><td>^abc</td><td>Matches some pattern abc at the start of a string</td></tr><tr><td>abc$</td><td>Matches some pattern abc at the end of a string</td></tr><tr><td>[abc]</td><td>Matches one of a set of characters</td></tr><tr><td>[A-Z0-9]</td><td>Matches one of a range of characters</td></tr><tr><td>ed|ing|s</td><td>Matches one of the specified strings (disjunction)</td></tr><tr><td>*</td><td>Zero or more of previous item, e.g., a*, [a-z]* (also known as Kleene Closure )</td></tr><tr><td>+</td><td>One or more of previous item, e.g., a+, [a-z]+</td></tr><tr><td>?</td><td>Zero or one of the previous item (i.e., optional), e.g., a?, [a-z]?</td></tr><tr><td>{n}</td><td>Exactly n repeats where n is a non-negative integer</td></tr><tr><td>{n,}</td><td>At least n repeats</td></tr><tr><td>{,n}</td><td>No more than n repeats</td></tr><tr><td>{m,n}</td><td>At least m and no more than n repeats</td></tr><tr><td>a(b|c)+</td><td>Parentheses that indicate the scope of the operators</td></tr></table>]",
          "level": -1,
          "page": 123,
          "reading_order": 2,
          "bbox": [
            100,
            277,
            467,
            575
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Operator</td><td>Behavior</td></tr><tr><td>-</td><td>Wildcard, matches any character</td></tr><tr><td>^abc</td><td>Matches some pattern abc at the start of a string</td></tr><tr><td>abc$</td><td>Matches some pattern abc at the end of a string</td></tr><tr><td>[abc]</td><td>Matches one of a set of characters</td></tr><tr><td>[A-Z0-9]</td><td>Matches one of a range of characters</td></tr><tr><td>ed|ing|s</td><td>Matches one of the specified strings (disjunction)</td></tr><tr><td>*</td><td>Zero or more of previous item, e.g., a*, [a-z]* (also known as Kleene Closure )</td></tr><tr><td>+</td><td>One or more of previous item, e.g., a+, [a-z]+</td></tr><tr><td>?</td><td>Zero or one of the previous item (i.e., optional), e.g., a?, [a-z]?</td></tr><tr><td>{n}</td><td>Exactly n repeats where n is a non-negative integer</td></tr><tr><td>{n,}</td><td>At least n repeats</td></tr><tr><td>{,n}</td><td>No more than n repeats</td></tr><tr><td>{m,n}</td><td>At least m and no more than n repeats</td></tr><tr><td>a(b|c)+</td><td>Parentheses that indicate the scope of the operators</td></tr></table>",
              "bbox": [
                100,
                277,
                467,
                575
              ],
              "page": 123,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Table 3-3. Basic regular expression metacharacters, including wildcards, ranges, and closures",
              "bbox": [
                99,
                259,
                557,
                272
              ],
              "page": 123,
              "reading_order": 3
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_123_order_4",
          "label": "para",
          "text": "To the Python interpreter, a regular expression is just like any other string. If the string\ncontains a backslash followed by particular characters, it will interpret these specially.\nFor example, \\b would be interpreted as the backspace character. In general, when\nusing regular expressions containing backslash, we should instruct the interpreter not\nto look inside the string at all, but simply to pass it directly to the re library for pro-\ncessing. We do this by prefixing the string with the letter r , to indicate that it is a raw\nstring . For example, the raw string r'\\band\\b' contains two \\b symbols that are\ninterpreted by the re library as matching word boundaries instead of backspace char-\nacters. If you get into the habit of using r'...' for regular expressions—as we will do\nfrom now on—you will avoid having to think about these complications.",
          "level": -1,
          "page": 123,
          "reading_order": 4,
          "bbox": [
            97,
            598,
            585,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_123_order_5",
          "label": "foot",
          "text": "3.4 Regular Expressions for Detecting Word Patterns | 101",
          "level": -1,
          "page": 123,
          "reading_order": 5,
          "bbox": [
            340,
            824,
            584,
            842
          ],
          "section_number": "3.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_126_order_1",
      "label": "sec",
      "text": "Finding Word Stems",
      "level": 1,
      "page": 126,
      "reading_order": 1,
      "bbox": [
        98,
        116,
        234,
        140
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_126_order_2",
          "label": "para",
          "text": "When we use a web search engine, we usually don't mind (or even notice) if the words\nin the document differ from our search terms in having different endings. A query for\nlaptops finds documents containing laptop and vice versa. Indeed, laptop and laptops\nare just two forms of the same dictionary word (or lemma). For some language pro-\ncessing tasks we want to ignore word endings, and just deal with word stems.",
          "level": -1,
          "page": 126,
          "reading_order": 2,
          "bbox": [
            97,
            149,
            585,
            229
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_126_order_3",
          "label": "para",
          "text": "There are various ways we can pull out the stem of a word. Here’s a simple-minded\napproach that just strips off anything that looks like a suffix:",
          "level": -1,
          "page": 126,
          "reading_order": 3,
          "bbox": [
            100,
            232,
            584,
            269
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_126_order_5",
          "label": "para",
          "text": "Although we will ultimately use NLTK’s built-in stemmers, it’s interesting to see how\nwe can use regular expressions for this task. Our first step is to build up a disjunction\nof all the suffixes. We need to enclose it in parentheses in order to limit the scope of\nthe disjunction.",
          "level": -1,
          "page": 126,
          "reading_order": 5,
          "bbox": [
            97,
            349,
            586,
            415
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_126_order_7",
          "label": "para",
          "text": "Here, re.findall() just gave us the suffix even though the regular expression matched\nthe entire word. This is because the parentheses have a second function, to select sub-\nstrings to be extracted. If we want to use the parentheses to specify the scope of the\ndisjunction, but not to select the material to be output, we have to add ?:, which is just\none of many arcane subtleties of regular expressions. Here's the revised version.",
          "level": -1,
          "page": 126,
          "reading_order": 7,
          "bbox": [
            97,
            456,
            585,
            539
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_126_order_9",
          "label": "para",
          "text": "However, we'd actually like to split the word into stem and suffix. So we should just\nparenthesize both parts of the regular expression:",
          "level": -1,
          "page": 126,
          "reading_order": 9,
          "bbox": [
            97,
            582,
            584,
            613
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_126_order_11",
          "label": "para",
          "text": "This looks promising, but still has a problem. Let's look at a different word, processes:",
          "level": -1,
          "page": 126,
          "reading_order": 11,
          "bbox": [
            100,
            654,
            584,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_126_order_13",
          "label": "para",
          "text": "The regular expression incorrectly found an -s suffix instead of an -es suffix. This dem-\nonstrates another subtlety: the star operator is “greedy” and so the .* part of the ex-\npression tries to consume as much of the input as possible. If we use the “non-greedy”\nversion of the star operator, written *?, we get what we want:",
          "level": -1,
          "page": 126,
          "reading_order": 13,
          "bbox": [
            97,
            714,
            584,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_126_order_14",
          "label": "foot",
          "text": "104 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 126,
          "reading_order": 14,
          "bbox": [
            97,
            824,
            261,
            842
          ],
          "section_number": "104",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_127_order_1",
          "label": "para",
          "text": "This works even when we allow an empty suffix, by making the content of the second\nparentheses optional:",
          "level": -1,
          "page": 127,
          "reading_order": 1,
          "bbox": [
            100,
            107,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_127_order_3",
          "label": "para",
          "text": "This approach still has many problems (can you spot them?), but we will move on to\ndefine a function to perform stemming, and apply it to a whole text:",
          "level": -1,
          "page": 127,
          "reading_order": 3,
          "bbox": [
            97,
            179,
            584,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_127_order_5",
          "label": "para",
          "text": "Notice that our regular expression removed the s from ponds but also from is and\nbasis. It produced some non-words, such as distribut and deriv, but these are acceptable\nstems in some applications.",
          "level": -1,
          "page": 127,
          "reading_order": 5,
          "bbox": [
            97,
            412,
            585,
            460
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_127_order_6",
      "label": "sec",
      "text": "Searching Tokenized Text",
      "level": 1,
      "page": 127,
      "reading_order": 6,
      "bbox": [
        97,
        474,
        270,
        495
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_129_order_0",
          "label": "sub_sec",
          "text": "3.6 Normalizing Text",
          "level": 2,
          "page": 129,
          "reading_order": 0,
          "bbox": [
            97,
            77,
            270,
            100
          ],
          "section_number": "3.6",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_129_order_3",
              "label": "sub_sub_sec",
              "text": "Stemmers",
              "level": 3,
              "page": 129,
              "reading_order": 3,
              "bbox": [
                97,
                295,
                165,
                313
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_129_order_4",
                  "label": "para",
                  "text": "NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer, you\nshould use one of these in preference to crafting your own using regular expressions,\nsince NLTK's stemmers handle a wide range of irregular cases. The Porter and Lan-\ncaster stemmers follow their own rules for stripping affixes. Observe that the Porter\nstemmer correctly handles the word lying (mapping it to lie ), whereas the Lancaster\nstemmer does not.",
                  "level": -1,
                  "page": 129,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    322,
                    585,
                    421
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_129_order_6",
                  "label": "para",
                  "text": "Stemming is not a well-defined process, and we typically pick the stemmer that best\nsuits the application we have in mind. The Porter Stemmer is a good choice if you are\nindexing some texts and want to support search using alternative forms of words (il-\nlustrated in Example 3 - 1 , which uses object-oriented programming techniques that are\noutside the scope of this book, string formatting techniques to be covered in Sec-\ntion 3.9 , and the enumerate() function to be explained in Section 4.2 ).",
                  "level": -1,
                  "page": 129,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    591,
                    585,
                    692
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_129_order_7",
                  "label": "para",
                  "text": "Example 3-1. Indexing a text using a stemmer.",
                  "level": -1,
                  "page": 129,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    706,
                    325,
                    719
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_129_order_10",
                  "label": "foot",
                  "text": "3.6 Normalizing Text | 107",
                  "level": -1,
                  "page": 129,
                  "reading_order": 10,
                  "bbox": [
                    466,
                    824,
                    585,
                    842
                  ],
                  "section_number": "3.6",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_129_order_1",
              "label": "para",
              "text": "In earlier program examples we have often converted text to lowercase before doing\nanything with its words, e.g., set(w.lower() for w in text). By using lower(), we have\nnormalized the text to lowercase so that the distinction between The and the is ignored.\nOften we want to go further than this and strip off any affixes, a task known as stem-\nming. A further step is to make sure that the resulting form is a known word in a\ndictionary, a task known as lemmatization. We discuss each of these in turn. First, we\nneed to define the data we will use in this section:",
              "level": -1,
              "page": 129,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_127_order_7",
          "label": "para",
          "text": "You can use a special kind of regular expression for searching across multiple words in\na text (where a text is a list of tokens). For example, \"<a> <man>\" finds all instances of\na man in the text. The angle brackets are used to mark token boundaries, and any\nwhitespace between the angle brackets is ignored (behaviors that are unique to NLTK's\nfindall() method for texts). In the following example, we include <.*> ❶ , which will\nmatch any single token, and enclose it in parentheses so only the matched word (e.g.,\nmonied ) and not the matched phrase (e.g., a monied man ) is produced. The second\nexample finds three-word phrases ending with the word bro ☺ . The last example finds\nsequences of three or more words starting with the letter l ☹ .",
          "level": -1,
          "page": 127,
          "reading_order": 7,
          "bbox": [
            97,
            501,
            585,
            650
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_127_order_9",
          "label": "foot",
          "text": "3.5 Useful Applications of Regular Expressions | 105",
          "level": -1,
          "page": 127,
          "reading_order": 9,
          "bbox": [
            366,
            824,
            585,
            842
          ],
          "section_number": "3.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_1",
          "label": "para",
          "text": "Your Turn: Consolidate your understanding of regular expression pat-\nterns and substitutions using nltk.re_show(p, s) , which annotates the\nstring s to show every place where pattern p was matched, and\nnltk.app.nemo() , which provides a graphical interface for exploring reg-\nular expressions. For more practice, try some of the exercises on regular\nexpressions at the end of this chapter.",
          "level": -1,
          "page": 128,
          "reading_order": 1,
          "bbox": [
            171,
            134,
            530,
            226
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_2",
          "label": "para",
          "text": "It is easy to build search patterns when the linguistic phenomenon we're studying is\ntied to particular words. In some cases, a little creativity will go a long way. For instance,\nsearching a large text corpus for expressions of the form x and other ys allows us to\ndiscover hypernyms (see Section 2.5 ):",
          "level": -1,
          "page": 128,
          "reading_order": 2,
          "bbox": [
            97,
            250,
            585,
            314
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_4",
          "label": "para",
          "text": "With enough text, this approach would give us a useful store of information about the\ntaxonomy of objects, without the need for any manual labor. However, our search\nresults will usually contain false positives, i.e., cases that we would want to exclude.\nFor example, the result demands and other factors suggests that demand is an instance\nof the type factor , but this sentence is actually about wage demands. Nevertheless, we\ncould construct our own ontology of English concepts by manually correcting the out-\nput of such searches.",
          "level": -1,
          "page": 128,
          "reading_order": 4,
          "bbox": [
            97,
            430,
            585,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_5",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_128_figure_005.png)",
          "level": -1,
          "page": 128,
          "reading_order": 5,
          "bbox": [
            118,
            564,
            171,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_6",
          "label": "para",
          "text": "This combination of automatic and manual processing is the most com-\nmon way for new corpora to be constructed. We will return to this in\nChapter 11.",
          "level": -1,
          "page": 128,
          "reading_order": 6,
          "bbox": [
            171,
            573,
            530,
            620
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_7",
          "label": "para",
          "text": "Searching corpora also suffers from the problem of false negatives, i.e., omitting cases\nthat we would want to include. It is risky to conclude that some linguistic phenomenon\ndoesn’t exist in a corpus just because we couldn’t find any instances of a search pattern.\nPerhaps we just didn’t think carefully enough about suitable patterns.",
          "level": -1,
          "page": 128,
          "reading_order": 7,
          "bbox": [
            97,
            645,
            585,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_8",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_128_figure_008.png)",
          "level": -1,
          "page": 128,
          "reading_order": 8,
          "bbox": [
            118,
            733,
            171,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_9",
          "label": "para",
          "text": "Your Turn: Look for instances of the pattern as x as y to discover in-\nformation about entities and their properties.",
          "level": -1,
          "page": 128,
          "reading_order": 9,
          "bbox": [
            171,
            743,
            530,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_128_order_10",
          "label": "foot",
          "text": "106 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 128,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            261,
            842
          ],
          "section_number": "106",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_130_order_4",
      "label": "sec",
      "text": "Lemmatization",
      "level": 1,
      "page": 130,
      "reading_order": 4,
      "bbox": [
        100,
        447,
        198,
        465
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_131_order_0",
          "label": "sub_sec",
          "text": "3.7 Regular Expressions for Tokenizing Text",
          "level": 2,
          "page": 131,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            447,
            100
          ],
          "section_number": "3.7",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_131_order_2",
              "label": "sub_sub_sec",
              "text": "Simple Approaches to Tokenization",
              "level": 3,
              "page": 131,
              "reading_order": 2,
              "bbox": [
                97,
                205,
                331,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_131_order_3",
                  "label": "para",
                  "text": "The very simplest method for tokenizing text is to split on whitespace. Consider the\nfollowing text from Alice's Adventures in Wonderland:",
                  "level": -1,
                  "page": 131,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    232,
                    585,
                    263
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_131_order_5",
                  "label": "para",
                  "text": "We could split this raw text on whitespace using raw.split(). To do the same using a\nregular expression, it is not enough to match any space characters in the string ❶, since\nthis results in tokens that contain a \\n newline character; instead, we need to match\nany number of spaces, tabs, or newlines ❷:",
                  "level": -1,
                  "page": 131,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    313,
                    585,
                    385
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_131_order_7",
                  "label": "para",
                  "text": "The regular expression «[ \\t\\n]+» matches one or more spaces, tabs (\\t), or newlines\n(\\n). Other whitespace characters, such as carriage return and form feed, should really\nbe included too. Instead, we will use a built-in re abbreviation, \\s, which means any\nwhitespace character. The second statement in the preceding example can be rewritten\nas re.split(r'\\s+', raw).",
                  "level": -1,
                  "page": 131,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    528,
                    585,
                    611
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_131_order_8",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_131_figure_008.png)",
                  "level": -1,
                  "page": 131,
                  "reading_order": 8,
                  "bbox": [
                    118,
                    627,
                    164,
                    689
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_131_order_9",
                  "label": "para",
                  "text": "Important: Remember to prefix regular expressions with the letter r\n(meaning “raw”), which instructs the Python interpreter to treat the\nstring literally, rather than processing any backslashed characters it\ncontains.",
                  "level": -1,
                  "page": 131,
                  "reading_order": 9,
                  "bbox": [
                    171,
                    636,
                    530,
                    693
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_131_order_10",
                  "label": "para",
                  "text": "Splitting on whitespace gives us tokens like '(not' and 'herself,'. An alternative is to\nuse the fact that Python provides us with a character class \\w for word characters,\nequivalent to [a-zA-Z0-9_]. It also defines the complement of this class, \\W, i.e., all",
                  "level": -1,
                  "page": 131,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    716,
                    584,
                    770
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_131_order_11",
                  "label": "foot",
                  "text": "3.7 Regular Expressions for Tokenizing Text | 109",
                  "level": -1,
                  "page": 131,
                  "reading_order": 11,
                  "bbox": [
                    376,
                    824,
                    585,
                    842
                  ],
                  "section_number": "3.7",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_132_order_0",
                  "label": "para",
                  "text": "characters other than letters, digits, or underscore. We can use \\W in a simple regular\nexpression to split the input on anything other than a word character:",
                  "level": -1,
                  "page": 132,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    107
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_132_order_2",
                  "label": "para",
                  "text": "Observe that this gives us empty strings at the start and the end (to understand why,\ntry doing 'xx'.split('x')). With re.findall(r'\\w+', raw), we get the same tokens,\nbut without the empty strings, using a pattern that matches the words instead of the\nspaces. Now that we’re matching the words, we’re in a position to extend the regular\nexpression to cover a wider range of cases. The regular expression «\\w+|\\$\\w*» will first\ntry to match any sequence of word characters. If no match is found, it will try to match\nany non-whitespace character (\\$ is the complement of \\s) followed by further word\ncharacters. This means that punctuation is grouped with any following letters\n(e.g., ’s) but that sequences of two or more punctuation characters are separated.",
                  "level": -1,
                  "page": 132,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    197,
                    585,
                    349
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_132_order_4",
                  "label": "para",
                  "text": "Let's generalize the \\w+ in the preceding expression to permit word-internal hyphens\nand apostrophes: «\\w+([-']\\w+)*». This expression means \\w+ followed by zero or more\ninstances of [-']\\w+; it would match hot-tempered and it’s. (We need to include ?: in\nthis expression for reasons discussed earlier.) We’ll also add a pattern to match quote\ncharacters so these are kept separate from the text they enclose.",
                  "level": -1,
                  "page": 132,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    439,
                    585,
                    523
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_132_order_6",
                  "label": "para",
                  "text": "The expression in this example also included «[- .(]+», which causes the double hy-\nphen, ellipsis, and open parenthesis to be tokenized separately.",
                  "level": -1,
                  "page": 132,
                  "reading_order": 6,
                  "bbox": [
                    100,
                    618,
                    584,
                    654
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_132_order_7",
                  "label": "para",
                  "text": "Table 3-4 lists the regular expression character class symbols we have seen in this sec-\ntion, in addition to some other useful symbols.",
                  "level": -1,
                  "page": 132,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    654,
                    584,
                    689
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_132_order_8",
                  "label": "table",
                  "text": "Table 3-4. Regular expression symbols [TABLE: <table><tr><td>Symbol</td><td>Function</td></tr><tr><td>\\</td><td>Word boundary (zero width)</td></tr><tr><td>\\d</td><td>Any decimal digit (equivalent to[0-9])</td></tr></table>]",
                  "level": -1,
                  "page": 132,
                  "reading_order": 8,
                  "bbox": [
                    100,
                    725,
                    413,
                    779
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "tab",
                      "text": "<table><tr><td>Symbol</td><td>Function</td></tr><tr><td>\\</td><td>Word boundary (zero width)</td></tr><tr><td>\\d</td><td>Any decimal digit (equivalent to[0-9])</td></tr></table>",
                      "bbox": [
                        100,
                        725,
                        413,
                        779
                      ],
                      "page": 132,
                      "reading_order": 8
                    },
                    {
                      "label": "cap",
                      "text": "Table 3-4. Regular expression symbols",
                      "bbox": [
                        99,
                        698,
                        288,
                        717
                      ],
                      "page": 132,
                      "reading_order": 9
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_132_order_10",
                  "label": "foot",
                  "text": "110 | Chapter3: Processing Raw Text",
                  "level": -1,
                  "page": 132,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    261,
                    842
                  ],
                  "section_number": "110",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_133_order_0",
                  "label": "tab",
                  "text": "<table><tr><td>Symbol</td><td>Function</td></tr><tr><td>\\D</td><td>Any non-digit character (equivalent to [^0-9])</td></tr><tr><td>\\s</td><td>Any whitespace character (equivalent to [ \\t\\n\\r\\f\\v]</td></tr><tr><td>\\S</td><td>Any non-whitespace character (equivalent to [^ \\t\\n\\r\\f\\v])</td></tr><tr><td>\\w</td><td>Any alphanumeric character (equivalent to [a-zA-Z0-9_])</td></tr><tr><td>\\W</td><td>Any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])</td></tr><tr><td>\\t</td><td>The tab character</td></tr><tr><td>\\n</td><td>The newline character</td></tr></table>",
                  "level": -1,
                  "page": 133,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    80,
                    413,
                    232
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_131_order_1",
              "label": "para",
              "text": "Tokenization is the task of cutting a string into identifiable linguistic units that consti-\ntute a piece of language data. Although it is a fundamental task, we have been able to\ndelay it until now because many corpora are already tokenized, and because NLTK\nincludes some tokenizers. Now that you are familiar with regular expressions, you can\nlearn how to use them to tokenize text, and to have much more control over the process.",
              "level": -1,
              "page": 131,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                189
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_130_order_5",
          "label": "para",
          "text": "The WordNet lemmatizer removes affixes only if the resulting word is in its dictionary.\nThis additional checking process makes the lemmatizer slower than the stemmers just\nmentioned. Notice that it doesn’t handle lying, but it converts women to woman.",
          "level": -1,
          "page": 130,
          "reading_order": 5,
          "bbox": [
            100,
            474,
            585,
            522
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_130_order_7",
          "label": "para",
          "text": "The WordNet lemmatizer is a good choice if you want to compile the vocabulary of\nsome texts and want a list of valid lemmas (or lexicon headwords).",
          "level": -1,
          "page": 130,
          "reading_order": 7,
          "bbox": [
            97,
            627,
            584,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_130_order_8",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_130_figure_008.png)",
          "level": -1,
          "page": 130,
          "reading_order": 8,
          "bbox": [
            118,
            679,
            164,
            735
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_130_order_9",
          "label": "para",
          "text": "Another normalization task involves identifying non-standard\nwords , including numbers, abbreviations, and dates, and mapping any\nsuch tokens to a special vocabulary. For example, every decimal number\ncould be mapped to a single token 0.0 , and every acronym could be\nmapped to AAA . This keeps the vocabulary small and improves the ac-\ncuracy of many language modeling tasks.",
          "level": -1,
          "page": 130,
          "reading_order": 9,
          "bbox": [
            171,
            689,
            530,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_130_order_10",
          "label": "foot",
          "text": "108 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 130,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            261,
            842
          ],
          "section_number": "108",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_133_order_1",
      "label": "sec",
      "text": "NLTK's Regular Expression Tokenizer",
      "level": 1,
      "page": 133,
      "reading_order": 1,
      "bbox": [
        98,
        250,
        342,
        277
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_133_order_2",
          "label": "para",
          "text": "The function nltk.regexp_tokenize() is similar to re.findall() (as we’ve been using\nit for tokenization). However, nltk.regexp_tokenize() is more efficient for this task,\nand avoids the need for special treatment of parentheses. For readability we break up\nthe regular expression over several lines and add a comment about each line. The special\n(?x) “verbose flag” tells Python to strip out the embedded whitespace and comments.",
          "level": -1,
          "page": 133,
          "reading_order": 2,
          "bbox": [
            97,
            277,
            585,
            363
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_133_order_4",
          "label": "para",
          "text": "When using the verbose flag, you can no longer use ' ' to match a space character; use\n\\s instead. The regexp_tokenize() function has an optional gaps parameter. When set\nto True, the regular expression specifies the gaps between tokens, as with re.split().",
          "level": -1,
          "page": 133,
          "reading_order": 4,
          "bbox": [
            97,
            510,
            585,
            558
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_133_order_5",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_133_figure_005.png)",
          "level": -1,
          "page": 133,
          "reading_order": 5,
          "bbox": [
            118,
            573,
            164,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_133_order_6",
          "label": "para",
          "text": "We can evaluate a tokenizer by comparing the resulting tokens with a\nwordlist, and then report any tokens that don’t appear in the wordlist,\nusing set(tokens).difference(wordlist). You’ll probably want to\nlowercase all the tokens first.",
          "level": -1,
          "page": 133,
          "reading_order": 6,
          "bbox": [
            171,
            582,
            530,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_133_order_7",
      "label": "sec",
      "text": "Further Issues with Tokenization",
      "level": 1,
      "page": 133,
      "reading_order": 7,
      "bbox": [
        98,
        663,
        315,
        683
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_134_order_2",
          "label": "sub_sec",
          "text": "3.8 Segmentation",
          "level": 2,
          "page": 134,
          "reading_order": 2,
          "bbox": [
            97,
            222,
            244,
            245
          ],
          "section_number": "3.8",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_134_order_5",
              "label": "sub_sub_sec",
              "text": "Sentence Segmentation",
              "level": 3,
              "page": 134,
              "reading_order": 5,
              "bbox": [
                97,
                356,
                261,
                376
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_134_order_6",
                  "label": "para",
                  "text": "Manipulating texts at the level of individual words often presupposes the ability to\ndivide a text into individual sentences. As we have seen, some corpora already provide\naccess at the sentence level. In the following example, we compute the average number\nof words per sentence in the Brown Corpus:",
                  "level": -1,
                  "page": 134,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    384,
                    585,
                    448
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_134_order_8",
                  "label": "para",
                  "text": "In other cases, the text is available only as a stream of characters. Before tokenizing the\ntext into words, we need to segment it into sentences. NLTK facilitates this by including\nthe Punkt sentence segmenter (Kiss & Strunk, 2006) . Here is an example of its use in\nsegmenting the text of a novel. (Note that if the segmenter's internal data has been\nupdated by the time you read this, you will see different output.)",
                  "level": -1,
                  "page": 134,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    491,
                    585,
                    573
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_134_order_10",
                  "label": "foot",
                  "text": "112 | Chapter3: Processing Raw Text",
                  "level": -1,
                  "page": 134,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    261,
                    842
                  ],
                  "section_number": "112",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_135_order_0",
                  "label": "para",
                  "text": "Notice that this example is really a single sentence, reporting the speech of Mr. Lucian\nGregory. However, the quoted speech contains several sentences, and these have been\nsplit into individual strings. This is reasonable behavior for most applications.",
                  "level": -1,
                  "page": 135,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    584,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_135_order_1",
                  "label": "para",
                  "text": "Sentence segmentation is difficult because a period is used to mark abbreviations, and\nsome periods simultaneously mark an abbreviation and terminate a sentence, as often\nhappens with acronyms like U.S.A.",
                  "level": -1,
                  "page": 135,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    132,
                    585,
                    179
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_135_order_2",
                  "label": "para",
                  "text": "For another approach to sentence segmentation, see Section 6.2.",
                  "level": -1,
                  "page": 135,
                  "reading_order": 2,
                  "bbox": [
                    98,
                    188,
                    467,
                    206
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_134_order_3",
              "label": "para",
              "text": "This section discusses more advanced concepts, which you may prefer to skip on the\nfirst time through this chapter.",
              "level": -1,
              "page": 134,
              "reading_order": 3,
              "bbox": [
                97,
                250,
                585,
                286
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_134_order_4",
              "label": "para",
              "text": "Tokenization is an instance of a more general problem of segmentation. In this section,\nwe will look at two other instances of this problem, which use radically different tech-\nniques to the ones we have seen so far in this chapter.",
              "level": -1,
              "page": 134,
              "reading_order": 4,
              "bbox": [
                97,
                293,
                585,
                340
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_133_order_8",
          "label": "para",
          "text": "Tokenization turns out to be a far more difficult task than you might have expected.\nNo single solution works well across the board, and we must decide what counts as a\ntoken depending on the application domain.",
          "level": -1,
          "page": 133,
          "reading_order": 8,
          "bbox": [
            97,
            689,
            584,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_133_order_9",
          "label": "para",
          "text": "When developing a tokenizer it helps to have access to raw text which has been man-\nually tokenized, in order to compare the output of your tokenizer with high-quality (or",
          "level": -1,
          "page": 133,
          "reading_order": 9,
          "bbox": [
            97,
            752,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_133_order_10",
          "label": "foot",
          "text": "3.7 Regular Expressions for Tokenizing Text | 111",
          "level": -1,
          "page": 133,
          "reading_order": 10,
          "bbox": [
            376,
            824,
            584,
            842
          ],
          "section_number": "3.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_134_order_0",
          "label": "para",
          "text": "“gold-standard”) tokens. The NLTK corpus collection includes a sample of Penn Tree-\nbank data, including the raw Wall Street Journal text (nltk.corpus.tree\nbank_raw.raw()) and the tokenized version (nltk.corpus.treebank.words()).",
          "level": -1,
          "page": 134,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_134_order_1",
          "label": "para",
          "text": "A final issue for tokenization is the presence of contractions, such as didn\n’t. If we are\nanalyzing the meaning of a sentence, it would probably be more useful to normalize\nthis form to two separate forms: did and n't (or not). We can do this work with the help\nof a lookup table.",
          "level": -1,
          "page": 134,
          "reading_order": 1,
          "bbox": [
            97,
            132,
            585,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_135_order_3",
      "label": "sec",
      "text": "Word Segmentation",
      "level": 1,
      "page": 135,
      "reading_order": 3,
      "bbox": [
        97,
        215,
        234,
        237
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_138_order_3",
          "label": "sub_sec",
          "text": "3.9 Formatting: From Lists to Strings",
          "level": 2,
          "page": 138,
          "reading_order": 3,
          "bbox": [
            97,
            338,
            392,
            361
          ],
          "section_number": "3.9",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_138_order_5",
              "label": "sub_sub_sec",
              "text": "From Lists to Strings",
              "level": 3,
              "page": 138,
              "reading_order": 5,
              "bbox": [
                98,
                510,
                234,
                537
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_138_order_6",
                  "label": "para",
                  "text": "The simplest kind of structured object we use for text processing is lists of words. When\nwe want to output these to a display or a file, we must convert these lists into strings.\nTo do this in Python we use the join() method, and specify the string to be used as the\n“glue”:",
                  "level": -1,
                  "page": 138,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    537,
                    585,
                    609
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_138_order_8",
                  "label": "para",
                  "text": "So ' '.join(silly) means: take all the items in silly and concatenate them as one big\nstring, using ' ' as a spacer between the items. I.e., join() is a method of the string\nthat you want to use as the glue. (Many people find this notation for join() counter-\nintuitive.) The join() method only works on a list of strings—what we have been calling\na text—a complex type that enjoys some privileges in Python.",
                  "level": -1,
                  "page": 138,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    715,
                    585,
                    797
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_138_order_9",
                  "label": "foot",
                  "text": "116 | Chapter3: Processing Raw Text",
                  "level": -1,
                  "page": 138,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    824,
                    261,
                    842
                  ],
                  "section_number": "116",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_138_order_4",
              "label": "para",
              "text": "Often we write a program to report a single data item, such as a particular element in\na corpus that meets some complicated criterion, or a single summary statistic such as\na word-count or the performance of a tagger. More often, we write a program to produce\na structured result; for example, a tabulation of numbers or linguistic forms, or a re-\nformatting of the original data. When the results to be presented are linguistic, textual\noutput is usually the most natural choice. However, when the results are numerical, it\nmay be preferable to produce graphical output. In this section, you will learn about a\nvariety of ways to present program output.",
              "level": -1,
              "page": 138,
              "reading_order": 4,
              "bbox": [
                97,
                367,
                585,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_135_order_4",
          "label": "para",
          "text": "For some writing systems, tokenizing text is made more difficult by the fact that there\nis no visual representation of word boundaries. For example, in Chinese, the three-\ncharacter string: 爱国人 (ai4 “love” [verb], guo3 “country”\n, ren2 “person”) could be\ntokenized as 爱国 / 人,\n“country-loving person,\n” or as 爱 / 国人,\n“love country-person.\n”",
          "level": -1,
          "page": 135,
          "reading_order": 4,
          "bbox": [
            97,
            241,
            585,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_135_order_5",
          "label": "para",
          "text": "A similar problem arises in the processing of spoken language, where the hearer must\nsegment a continuous speech stream into individual words. A particularly challenging\nversion of this problem arises when we don’t know the words in advance. This is the\nproblem faced by a language learner, such as a child hearing utterances from a parent.\nConsider the following artificial example, where word boundaries have been removed:",
          "level": -1,
          "page": 135,
          "reading_order": 5,
          "bbox": [
            97,
            313,
            585,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_135_order_6",
          "label": "para",
          "text": "1) a. doyouseethekitty\nb. seethedoggy\nc. doyoulikethekitty\nd. likethedoggy",
          "level": -1,
          "page": 135,
          "reading_order": 6,
          "bbox": [
            125,
            412,
            270,
            492
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_135_order_7",
          "label": "para",
          "text": "Our first challenge is simply to represent the problem: we need to find a way to separate\ntext content from the segmentation. We can do this by annotating each character with\na boolean value to indicate whether or not a word-break appears after the character (an\nidea that will be used heavily for “chunking” in Chapter 7). Let’s assume that the learner\nis given the utterance breaks, since these often correspond to extended pauses. Here is\na possible representation, including the initial and target segmentations:",
          "level": -1,
          "page": 135,
          "reading_order": 7,
          "bbox": [
            97,
            501,
            585,
            601
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_135_order_9",
          "label": "para",
          "text": "Observe that the segmentation strings consist of zeros and ones. They are one character\nshorter than the source text, since a text of length n can be broken up in only n−1 places.\nThe segment() function in Example 3-2 demonstrates that we can get back to the orig-\ninal segmented text from its representation.",
          "level": -1,
          "page": 135,
          "reading_order": 9,
          "bbox": [
            97,
            654,
            585,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_135_order_10",
          "label": "foot",
          "text": "3.8 Segmentation | 113",
          "level": -1,
          "page": 135,
          "reading_order": 10,
          "bbox": [
            476,
            824,
            584,
            842
          ],
          "section_number": "3.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_136_order_0",
          "label": "para",
          "text": "Example 3-2. Reconstruct segmented text from string representation: seg1 and seg2 represent the\ninitial and final segmentations of some hypothetical child-directed speech; the segment() function can\nuse them to reproduce the segmented text.",
          "level": -1,
          "page": 136,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_136_order_2",
          "label": "para",
          "text": "Now the segmentation task becomes a search problem: find the bit string that causes\nthe text string to be correctly segmented into words. We assume the learner is acquiring\nwords and storing them in an internal lexicon. Given a suitable lexicon, it is possible\nto reconstruct the source text as a sequence of lexical items. Following (Brent & Cart-\nwright, 1995) , we can define an objective function , a scoring function whose value\nwe will try to optimize, based on the size of the lexicon and the amount of information\nneeded to reconstruct the source text from the lexicon. We illustrate this in Figure 3 - 6 .",
          "level": -1,
          "page": 136,
          "reading_order": 2,
          "bbox": [
            97,
            367,
            585,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_136_order_3",
          "label": "figure",
          "text": "Figure 3-6. Calculation of objective function: Given a hypothetical segmentation of the source text\n(on the left), derive a lexicon and a derivation table that permit the source text to be reconstructed,\nthen total up the number of characters used by each lexical item (including a boundary marker) and\neach derivation, to serve as a score of the quality of the segmentation; smaller values of the score\nindicate a better segmentation. [IMAGE: ![Figure](figures/NLTK_page_136_figure_003.png)]",
          "level": -1,
          "page": 136,
          "reading_order": 3,
          "bbox": [
            91,
            501,
            583,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_136_figure_003.png)",
              "bbox": [
                91,
                501,
                583,
                672
              ],
              "page": 136,
              "reading_order": 3
            },
            {
              "label": "cap",
              "text": "Figure 3-6. Calculation of objective function: Given a hypothetical segmentation of the source text\n(on the left), derive a lexicon and a derivation table that permit the source text to be reconstructed,\nthen total up the number of characters used by each lexical item (including a boundary marker) and\neach derivation, to serve as a score of the quality of the segmentation; smaller values of the score\nindicate a better segmentation.",
              "bbox": [
                97,
                680,
                585,
                752
              ],
              "page": 136,
              "reading_order": 4
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_136_order_5",
          "label": "para",
          "text": "It is a simple matter to implement this objective function, as shown in Example 3-3.",
          "level": -1,
          "page": 136,
          "reading_order": 5,
          "bbox": [
            98,
            779,
            575,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_136_order_6",
          "label": "foot",
          "text": "114 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 136,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            261,
            842
          ],
          "section_number": "114",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_137_order_0",
          "label": "para",
          "text": "Example 3-3. Computing the cost of storing the lexicon and reconstructing the source text.",
          "level": -1,
          "page": 137,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            539,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_137_order_3",
          "label": "para",
          "text": "The final step is to search for the pattern of zeros and ones that maximizes this objective\nfunction, shown in Example 3-4. Notice that the best segmentation includes “words”\nlike thekitty, since there’s not enough evidence in the data to split this any further.",
          "level": -1,
          "page": 137,
          "reading_order": 3,
          "bbox": [
            97,
            349,
            585,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_137_order_4",
          "label": "para",
          "text": "Example 3-4. Non-deterministic search using simulated annealing: Begin searching with phrase\nsegmentations only; randomly perturb the zeros and ones proportional to the “ temperature ” ; with\neach iteration the temperature is lowered and the perturbation of boundaries is reduced.",
          "level": -1,
          "page": 137,
          "reading_order": 4,
          "bbox": [
            97,
            412,
            585,
            456
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_137_order_9",
          "label": "foot",
          "text": "3.8 Segmentation | 115",
          "level": -1,
          "page": 137,
          "reading_order": 9,
          "bbox": [
            476,
            824,
            585,
            842
          ],
          "section_number": "3.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_138_order_2",
          "label": "para",
          "text": "With enough data, it is possible to automatically segment text into words with a rea-\nsonable degree of accuracy. Such methods can be applied to tokenization for writing\nsystems that don’t have any visual representation of word boundaries.",
          "level": -1,
          "page": 138,
          "reading_order": 2,
          "bbox": [
            97,
            259,
            585,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_139_order_0",
      "label": "sec",
      "text": "Strings and Formats",
      "level": 1,
      "page": 139,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        234,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_139_order_1",
          "label": "para",
          "text": "We have seen that there are two ways to display the contents of an object:",
          "level": -1,
          "page": 139,
          "reading_order": 1,
          "bbox": [
            98,
            98,
            521,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_139_order_3",
          "label": "para",
          "text": "The print command yields Python’s attempt to produce the most human-readable form\nof an object. The second method—naming the variable at a prompt—shows us a string\nthat can be used to recreate this object. It is important to keep in mind that both of\nthese are just strings, displayed for the benefit of you, the user. They do not give us any\nclue as to the actual internal representation of the object.",
          "level": -1,
          "page": 139,
          "reading_order": 3,
          "bbox": [
            97,
            286,
            586,
            371
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_139_order_4",
          "label": "para",
          "text": "There are many other useful ways to display an object as a string of characters. This\nmay be for the benefit of a human reader, or because we want to export our data to a\nparticular file format for use in an external program.",
          "level": -1,
          "page": 139,
          "reading_order": 4,
          "bbox": [
            100,
            376,
            585,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_139_order_5",
          "label": "para",
          "text": "Formatted output typically contains a combination of variables and pre-specified\nstrings. For example, given a frequency distribution fdist , we could do:",
          "level": -1,
          "page": 139,
          "reading_order": 5,
          "bbox": [
            97,
            430,
            584,
            468
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_139_order_7",
          "label": "para",
          "text": "Apart from the problem of unwanted whitespace, print statements that contain alter-\nnating variables and constants can be difficult to read and maintain. A better solution\nis to use string formatting expressions.",
          "level": -1,
          "page": 139,
          "reading_order": 7,
          "bbox": [
            97,
            537,
            585,
            584
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_139_order_9",
          "label": "para",
          "text": "To understand what is going on here, let’s test out the string formatting expression\non ts own. (By now this will be your usual method of exploring new syntax.)",
          "level": -1,
          "page": 139,
          "reading_order": 9,
          "bbox": [
            100,
            636,
            584,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_139_order_11",
          "label": "foot",
          "text": "3.9 Formatting: From Lists to Strings | 117",
          "level": -1,
          "page": 139,
          "reading_order": 11,
          "bbox": [
            403,
            824,
            585,
            842
          ],
          "section_number": "3.9",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_140_order_0",
          "label": "para",
          "text": "The special symbols %s and %d are placeholders for strings and (decimal) integers. We\ncan embed these inside a string, then use the % operator to combine them. Let’s unpack\nthis code further, in order to see this behavior up close:",
          "level": -1,
          "page": 140,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_140_order_2",
          "label": "para",
          "text": "We can have a number of placeholders, but following the % operator we need to specify\na tuple with exactly the same number of values:",
          "level": -1,
          "page": 140,
          "reading_order": 2,
          "bbox": [
            97,
            215,
            585,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_140_order_4",
          "label": "para",
          "text": "We can also provide the values for the placeholders indirectly. Here’s an example using\na for loop:",
          "level": -1,
          "page": 140,
          "reading_order": 4,
          "bbox": [
            97,
            286,
            585,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_140_order_6",
          "label": "para",
          "text": "The %s and %d symbols are called conversion specifiers. They start with the % character\nand end with a conversion character such as s (for string) or d (for decimal integer) The\nstring containing conversion specifiers is called a format string. We combine a format\nstring with the % operator and a tuple of values to create a complete string formatting\nexpression.",
          "level": -1,
          "page": 140,
          "reading_order": 6,
          "bbox": [
            97,
            439,
            585,
            524
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_140_order_7",
      "label": "sec",
      "text": "Lining Things Up",
      "level": 1,
      "page": 140,
      "reading_order": 7,
      "bbox": [
        98,
        537,
        208,
        559
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_140_order_8",
          "label": "para",
          "text": "So far our formatting strings generated output of arbitrary width on the page (or screen),\nsuch as %s and %d . We can specify a width as well, such as %s , producing a string that\nis padded to width 6. It is right-justified by default ❶ , but we can include a minus sign\nto make it left-justified ❷ . In case we don't know in advance how wide a displayed\nvalue should be, the width value can be replaced with a star in the formatting string,\nthen specified using a variable ❸ .",
          "level": -1,
          "page": 140,
          "reading_order": 8,
          "bbox": [
            97,
            564,
            585,
            664
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_140_order_10",
          "label": "foot",
          "text": "118 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 140,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            261,
            842
          ],
          "section_number": "118",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_141_order_0",
          "label": "para",
          "text": "Other control characters are used for decimal integers and floating-point numbers.\nSince the percent character % has a special interpretation in formatting strings, we have\nto precede it with another % to get it in the output.",
          "level": -1,
          "page": 141,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_141_order_2",
          "label": "para",
          "text": "An important use of formatting strings is for tabulating data. Recall that in Sec-\ntion 2.1 we saw data being tabulated from a conditional frequency distribution. Let's\nperform the tabulation ourselves, exercising full control of headings and column\nwidths, as shown in Example 3 - 5 . Note the clear separation between the language\nprocessing work, and the tabulation of results.",
          "level": -1,
          "page": 141,
          "reading_order": 2,
          "bbox": [
            97,
            179,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_141_order_3",
          "label": "para",
          "text": "Example 3-5. Frequency of modals in different sections of the Brown Corpu",
          "level": -1,
          "page": 141,
          "reading_order": 3,
          "bbox": [
            97,
            268,
            467,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_141_order_6",
          "label": "para",
          "text": "Recall from the listing in Example 3-1 that we used a formatting string \"%*s\". This\nallows us to specify the width of a field using a variable.",
          "level": -1,
          "page": 141,
          "reading_order": 6,
          "bbox": [
            97,
            636,
            585,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_141_order_8",
          "label": "para",
          "text": "We could use this to automatically customize the column to be just wide enough to\naccommodate all the words, using width = max(len(w) for w in words). Remember\nthat the comma at the end of print statements adds an extra space, and this is sufficient\nto prevent the column headings from running into each other.",
          "level": -1,
          "page": 141,
          "reading_order": 8,
          "bbox": [
            97,
            716,
            585,
            780
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_141_order_9",
          "label": "foot",
          "text": "3.9 Formatting: From Lists to Strings | 119",
          "level": -1,
          "page": 141,
          "reading_order": 9,
          "bbox": [
            403,
            824,
            585,
            842
          ],
          "section_number": "3.9",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_142_order_0",
      "label": "sec",
      "text": "Writing Results to a File",
      "level": 1,
      "page": 142,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        255,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_142_order_8",
          "label": "sub_sec",
          "text": "Caution!",
          "level": 2,
          "page": 142,
          "reading_order": 8,
          "bbox": [
            171,
            474,
            209,
            485
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_142_order_9",
              "label": "para",
              "text": "You should avoid filenames that contain space characters, such as\noutput file.txt , or that are identical except for case distinctions, e.g.,\nOutput.txt and output.TXT .",
              "level": -1,
              "page": 142,
              "reading_order": 9,
              "bbox": [
                171,
                492,
                530,
                537
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_142_order_1",
          "label": "para",
          "text": "We have seen how to read text from files (Section 3.1). It is often useful to write output\nto files as well. The following code opens a file output.txt for writing, and saves the\nprogram output to the file.",
          "level": -1,
          "page": 142,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_142_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_142_figure_003.png)",
          "level": -1,
          "page": 142,
          "reading_order": 3,
          "bbox": [
            118,
            224,
            164,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_142_order_4",
          "label": "para",
          "text": "Your Turn: What is the effect of appending \\n to each string before we\nwrite it to the file? If you’re using a Windows machine, you may want\nto use word + \"\\r\\n\" instead. What happens if we do",
          "level": -1,
          "page": 142,
          "reading_order": 4,
          "bbox": [
            171,
            232,
            530,
            280
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_142_order_5",
          "label": "para",
          "text": "output_file.write(word)",
          "level": -1,
          "page": 142,
          "reading_order": 5,
          "bbox": [
            197,
            286,
            307,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_142_order_6",
          "label": "para",
          "text": "When we write non-text data to a file, we must convert it to a string first. We can do\nthis conversion using formatting strings, as we saw earlier. Let’s write the total number\nof words to our file, before closing it.",
          "level": -1,
          "page": 142,
          "reading_order": 6,
          "bbox": [
            97,
            322,
            585,
            371
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_142_order_10",
      "label": "sec",
      "text": "Text Wrapping",
      "level": 1,
      "page": 142,
      "reading_order": 10,
      "bbox": [
        97,
        555,
        198,
        582
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_143_order_2",
          "label": "sub_sec",
          "text": "3.10 Summary",
          "level": 2,
          "page": 143,
          "reading_order": 2,
          "bbox": [
            97,
            250,
            217,
            277
          ],
          "section_number": "3.10",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_143_order_3",
              "label": "list_group",
              "text": "• In this book we view a text as a list of words. A “ raw text ” is a potentially long\nstring containing words and whitespace formatting, and is how we typically store\nand visualize a text.\n• A string is specified in Python using single or double quotes: 'Monty Python',\n\"Monty Python\".",
              "level": -1,
              "page": 143,
              "reading_order": 3,
              "bbox": [
                106,
                284,
                585,
                331
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "• In this book we view a text as a list of words. A “ raw text ” is a potentially long\nstring containing words and whitespace formatting, and is how we typically store\nand visualize a text.",
                  "bbox": [
                    106,
                    284,
                    585,
                    331
                  ],
                  "page": 143,
                  "reading_order": 3
                },
                {
                  "label": "list",
                  "text": "• A string is specified in Python using single or double quotes: 'Monty Python',\n\"Monty Python\".",
                  "bbox": [
                    106,
                    338,
                    584,
                    369
                  ],
                  "page": 143,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "• The characters of a string are accessed using indexes, counting from zero: 'Monty\nPython'[0] gives the value M. The length of a string is found using len().",
                  "bbox": [
                    106,
                    375,
                    585,
                    406
                  ],
                  "page": 143,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "• Substrings are accessed using slice notation: 'Monty Python'[1:5] gives the value\nonty. If the start index is omitted, the substring begins at the start of the string; if\nthe end index is omitted, the slice continues to the end of the string.",
                  "bbox": [
                    106,
                    412,
                    585,
                    460
                  ],
                  "page": 143,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "• Strings can be split into lists: 'Monty Python'.split() gives ['Monty', 'Python'].\nLists can be joined into strings: '/'.join(['Monty', 'Python']) gives 'Monty/\nPython'.",
                  "bbox": [
                    106,
                    465,
                    584,
                    514
                  ],
                  "page": 143,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "• We can read text from a file f using text = open(f).read(). We can read text from\na URL u using text = urlopen(u).read(). We can iterate over the lines of a text file\nusing for line_in_open(f).",
                  "bbox": [
                    106,
                    519,
                    585,
                    567
                  ],
                  "page": 143,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "• Texts found on the Web may contain unwanted material (such as headers, footers,\nand markup), that need to be removed before we do any linguistic processing.",
                  "bbox": [
                    106,
                    573,
                    584,
                    609
                  ],
                  "page": 143,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "• Tokenization is the segmentation of a text into basic units—or tokens—such as\nwords and punctuation. Tokenization based on whitespace is inadequate for many\napplications because it bundles punctuation together with words. NLTK provides\nan off-the-shelf tokenizer nltk.word_tokenize() .",
                  "bbox": [
                    106,
                    609,
                    585,
                    675
                  ],
                  "page": 143,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "• Lemmatization is a process that maps the various forms of a word (such as ap-\npeared, appears ) to the canonical or citation form of the word, also known as the\nlexeme or lemma (e.g., appear ).",
                  "bbox": [
                    106,
                    680,
                    585,
                    729
                  ],
                  "page": 143,
                  "reading_order": 11
                },
                {
                  "label": "list",
                  "text": "• Regular expressions are a powerful and flexible method of specifying patterns.\nOnce we have imported the re module, we can use re.findall() to find all sub-\nstrings in a string that match a pattern.",
                  "bbox": [
                    106,
                    734,
                    584,
                    783
                  ],
                  "page": 143,
                  "reading_order": 12
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_143_order_13",
              "label": "foot",
              "text": "3.10 Summary | 121",
              "level": -1,
              "page": 143,
              "reading_order": 13,
              "bbox": [
                491,
                824,
                584,
                842
              ],
              "section_number": "3.10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_144_order_0",
              "label": "list_group",
              "text": "If a regular expression string includes a backslash, you should tell Python not to\npreprocess the string, by using a raw string with an r prefix: r'regexp'.\nWhen backslash is used before certain characters, e.g., \\n, this takes on a special\nmeaning (newline character); however, when backslash is used before regular ex-\npression wildcards and operators, e.g., \\.., \\|, \\$, these characters lose their special\nmeaning and are matched literally.",
              "level": -1,
              "page": 144,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "If a regular expression string includes a backslash, you should tell Python not to\npreprocess the string, by using a raw string with an r prefix: r'regexp'.",
                  "bbox": [
                    118,
                    71,
                    584,
                    107
                  ],
                  "page": 144,
                  "reading_order": 0
                },
                {
                  "label": "list",
                  "text": "When backslash is used before certain characters, e.g., \\n, this takes on a special\nmeaning (newline character); however, when backslash is used before regular ex-\npression wildcards and operators, e.g., \\.., \\|, \\$, these characters lose their special\nmeaning and are matched literally.",
                  "bbox": [
                    118,
                    107,
                    584,
                    179
                  ],
                  "page": 144,
                  "reading_order": 1
                },
                {
                  "label": "list",
                  "text": "A string formatting expression template % arg_tuple consists of a format string\ntemplate that contains conversion specifiers like %-6s and %0.2d.",
                  "bbox": [
                    118,
                    179,
                    585,
                    215
                  ],
                  "page": 144,
                  "reading_order": 2
                }
              ],
              "is_merged": true
            }
          ]
        },
        {
          "id": "page_144_order_3",
          "label": "sub_sec",
          "text": "3.11 Further Reading",
          "level": 2,
          "page": 144,
          "reading_order": 3,
          "bbox": [
            97,
            240,
            270,
            263
          ],
          "section_number": "3.11",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_144_order_4",
              "label": "para",
              "text": "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web. Remember to consult the Python reference ma-\nterials at http://docs.python.org/. (For example, this documentation covers “universal\nnewline support,” explaining how to work with the different newline conventions used\nby various operating systems.)",
              "level": -1,
              "page": 144,
              "reading_order": 4,
              "bbox": [
                97,
                268,
                585,
                352
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_144_order_5",
              "label": "para",
              "text": "For more examples of processing words with NLTK, see the tokenization, stemming,\nand corpus HOWTOs at http://www.nltk.org/howto . Chapters 2 and 3 of (Jurafsky &\nMartin, 2008) contain more advanced material on regular expressions and morphology.\nFor more extensive discussion of text processing with Python, see (Mertz, 2003) . For\ninformation about normalizing non-standard words, see (Sproat et al., 2001) .",
              "level": -1,
              "page": 144,
              "reading_order": 5,
              "bbox": [
                97,
                358,
                585,
                441
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_144_order_6",
              "label": "para",
              "text": "There are many references for regular expressions, both practical and theoretical. For\nan introductory tutorial to using regular expressions in Python, see Kuchling's Regular\nExpression HOWTO , http://www.amk.ca/python/howto/regex/ . For a comprehensive\nand detailed manual in using regular expressions, covering their syntax in most major\nprogramming languages, including Python, see (Friedl, 2002) . Other presentations in-\nclude Section 2.1 of (Jurafsky & Martin, 2008) , and Chapter 3 of (Mertz, 2003) .",
              "level": -1,
              "page": 144,
              "reading_order": 6,
              "bbox": [
                97,
                448,
                585,
                548
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_144_order_7",
              "label": "para",
              "text": "There are many online resources for Unicode. Useful discussions of Python’s facilities\nfor handling Unicode are:",
              "level": -1,
              "page": 144,
              "reading_order": 7,
              "bbox": [
                97,
                555,
                585,
                591
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_144_order_8",
              "label": "list_group",
              "text": "PEP-100 http://www.python.org/dev/peps/pep-0100/\n• Jason Orendorff, Unicode for Programmers, http://www.jorendorff.com/articles/uni\ncode/",
              "level": -1,
              "page": 144,
              "reading_order": 8,
              "bbox": [
                118,
                598,
                413,
                612
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "PEP-100 http://www.python.org/dev/peps/pep-0100/",
                  "bbox": [
                    118,
                    598,
                    413,
                    612
                  ],
                  "page": 144,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "• Jason Orendorff, Unicode for Programmers, http://www.jorendorff.com/articles/uni\ncode/",
                  "bbox": [
                    106,
                    618,
                    584,
                    646
                  ],
                  "page": 144,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "· A. M. Kuchling, Unicode HOWTO , http://www.amk.ca/python/howto/unicod",
                  "bbox": [
                    106,
                    654,
                    557,
                    672
                  ],
                  "page": 144,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "Frederik Lundh, Python Unicode Objects, http://effbot.org/zone/unicode-objects\n.htm",
                  "bbox": [
                    118,
                    672,
                    585,
                    707
                  ],
                  "page": 144,
                  "reading_order": 11
                },
                {
                  "label": "list",
                  "text": "• Joel Spolsky, The Absolute Minimum Every Software Developer Absolutely, Posi-\ntively Must Know About Unicode and Character Sets (No Excuses!), http://www.joe\nlonsoftware.com/articles/Unicode.html",
                  "bbox": [
                    106,
                    714,
                    585,
                    761
                  ],
                  "page": 144,
                  "reading_order": 12
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_144_order_13",
              "label": "foot",
              "text": "122 | Chapter3: Processing Raw Text",
              "level": -1,
              "page": 144,
              "reading_order": 13,
              "bbox": [
                97,
                824,
                261,
                842
              ],
              "section_number": "122",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_0",
              "label": "para",
              "text": "The problem of tokenizing Chinese text is a major focus of SIGHAN, the ACL Special\nInterest Group on Chinese Language Processing ( http://sighan.org/ ). Our method for\nsegmenting English text follows (Brent & Cartwright, 1995); this work falls in the area\nof language acquisition (Niyogi, 2006).",
              "level": -1,
              "page": 145,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_1",
              "label": "para",
              "text": "Collocations are a special case of multiword expressions. A multiword expression is\na small phrase whose meaning and other properties cannot be predicted from its words\nalone, e.g., part-of-speech (Baldwin & Kim, 2010).",
              "level": -1,
              "page": 145,
              "reading_order": 1,
              "bbox": [
                97,
                143,
                585,
                197
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_2",
              "label": "para",
              "text": "Simulated annealing is a heuristic for finding a good approximation to the optimum\nvalue of a function in a large, discrete search space, based on an analogy with annealing\nin metallurgy. The technique is described in many Artificial Intelligence texts.",
              "level": -1,
              "page": 145,
              "reading_order": 2,
              "bbox": [
                97,
                205,
                585,
                252
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_3",
              "label": "para",
              "text": "The approach to discovering hyponyms in text using search patterns like x and other\nys is described by (Hearst, 1992).",
              "level": -1,
              "page": 145,
              "reading_order": 3,
              "bbox": [
                96,
                259,
                583,
                295
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_145_order_4",
          "label": "sub_sec",
          "text": "3.12 Exercises",
          "level": 2,
          "page": 145,
          "reading_order": 4,
          "bbox": [
            97,
            313,
            211,
            340
          ],
          "section_number": "3.12",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_145_order_5",
              "label": "para",
              "text": "1. ◦ Define a string s = 'colorless'. Write a Python statement that changes this to\n“colourless” using only the slice and concatenation operations.",
              "level": -1,
              "page": 145,
              "reading_order": 5,
              "bbox": [
                107,
                349,
                584,
                385
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_6",
              "label": "para",
              "text": "2. ◦ We can use the slice notation to remove morphological endings on words. For\nexample, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice\nnotation to remove the affixes from these words (we’ve inserted a hyphen to indi-\ncate the affix boundary, but omit this from your strings): dish-es,run-ning,nation-\nality, un-do, pre-heat.",
              "level": -1,
              "page": 145,
              "reading_order": 6,
              "bbox": [
                105,
                385,
                585,
                469
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_7",
              "label": "para",
              "text": "3. ◦ We saw how we can generate an IndexError by indexing beyond the end of a\nstring. Is it possible to construct an index that goes too far to the left, before the\nstart of the string?",
              "level": -1,
              "page": 145,
              "reading_order": 7,
              "bbox": [
                108,
                474,
                585,
                522
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_8",
              "label": "para",
              "text": "4. ◦ We can specify a “ step ” size for the slice. The following returns every second\ncharacter within the slice: monty[6:11:2]. It also works in the reverse direction:\nmonty[10:5:-2]. Try these for yourself, and then experiment with different step\nvalues.",
              "level": -1,
              "page": 145,
              "reading_order": 8,
              "bbox": [
                105,
                528,
                585,
                591
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_9",
              "label": "para",
              "text": "5. ◦ What happens if you ask the interpreter to evaluate monty[::-1]? Explain why\nthis is a reasonable result.",
              "level": -1,
              "page": 145,
              "reading_order": 9,
              "bbox": [
                105,
                600,
                585,
                627
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_10",
              "label": "para",
              "text": "6. ◦ Describe the class of strings matched by the following regular expressions:",
              "level": -1,
              "page": 145,
              "reading_order": 10,
              "bbox": [
                105,
                636,
                557,
                654
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_11",
              "label": "para",
              "text": "a. [a-zA-Z]+",
              "level": -1,
              "page": 145,
              "reading_order": 11,
              "bbox": [
                126,
                654,
                207,
                672
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_12",
              "label": "para",
              "text": "b. [A-Z][a-z]",
              "level": -1,
              "page": 145,
              "reading_order": 12,
              "bbox": [
                126,
                678,
                207,
                691
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_13",
              "label": "para",
              "text": "c. p[aeiou]{,2}t",
              "level": -1,
              "page": 145,
              "reading_order": 13,
              "bbox": [
                126,
                698,
                226,
                716
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_14",
              "label": "para",
              "text": "d. \\d+(\\.\\d+)",
              "level": -1,
              "page": 145,
              "reading_order": 14,
              "bbox": [
                126,
                716,
                207,
                734
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_15",
              "label": "para",
              "text": "e. ([^aeiou][aeiou][^aeiou])*",
              "level": -1,
              "page": 145,
              "reading_order": 15,
              "bbox": [
                126,
                740,
                307,
                753
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_16",
              "label": "para",
              "text": "f. \\w+|[^\\w\\s]+",
              "level": -1,
              "page": 145,
              "reading_order": 16,
              "bbox": [
                126,
                761,
                220,
                779
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_17",
              "label": "para",
              "text": "Test your answers using nltk.re_show()",
              "level": -1,
              "page": 145,
              "reading_order": 17,
              "bbox": [
                125,
                779,
                350,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_145_order_18",
              "label": "foot",
              "text": "3.12 Exercises | 123",
              "level": -1,
              "page": 145,
              "reading_order": 18,
              "bbox": [
                494,
                824,
                584,
                842
              ],
              "section_number": "3.12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_0",
              "label": "para",
              "text": "7. ◦ Write regular expressions to match the following classes of strings",
              "level": -1,
              "page": 146,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                512,
                89
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_1",
              "label": "para",
              "text": "a. A single determiner (assume that $a, a n$, and the are the only determiners",
              "level": -1,
              "page": 146,
              "reading_order": 1,
              "bbox": [
                126,
                89,
                557,
                116
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_2",
              "label": "para",
              "text": "b. An arithmetic expression using integers, addition, and multiplication, such as\n2*3+8",
              "level": -1,
              "page": 146,
              "reading_order": 2,
              "bbox": [
                126,
                116,
                585,
                145
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_3",
              "label": "para",
              "text": "8. ◦ Write a utility function that takes a URL as its argument, and returns the contents\nof the URL, with all HTML markup removed. Use urllib.urlopen to access the\ncontents of the URL, e.g.:",
              "level": -1,
              "page": 146,
              "reading_order": 3,
              "bbox": [
                100,
                152,
                585,
                201
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_4",
              "label": "para",
              "text": "raw_contents = urllib.urlopen('http://www.nltk.org/').read()",
              "level": -1,
              "page": 146,
              "reading_order": 4,
              "bbox": [
                144,
                206,
                470,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_5",
              "label": "para",
              "text": "9. ◦ Save some text into a file corpus.txt. Define a function load(f) that reads from\nthe file named in its sole argument, and returns a string containing the text of the\nfile.",
              "level": -1,
              "page": 146,
              "reading_order": 5,
              "bbox": [
                100,
                224,
                585,
                273
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_6",
              "label": "para",
              "text": "a. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various\nkinds of punctuation in this text. Use one multiline regular expression inline\ncomments, using the verbose flag (?x).",
              "level": -1,
              "page": 146,
              "reading_order": 6,
              "bbox": [
                126,
                277,
                585,
                331
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_7",
              "label": "para",
              "text": "b. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following\nkinds of expressions: monetary amounts; dates; names of people and\norganizations.",
              "level": -1,
              "page": 146,
              "reading_order": 7,
              "bbox": [
                126,
                331,
                585,
                385
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_8",
              "label": "para",
              "text": "10. ◦ Rewrite the following loop as a list comprehension",
              "level": -1,
              "page": 146,
              "reading_order": 8,
              "bbox": [
                100,
                385,
                423,
                404
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_10",
              "label": "para",
              "text": "11. ◦ Define a string raw containing a sentence of your own choosing. Now, split raw\non some character other than space, such as 's'.",
              "level": -1,
              "page": 146,
              "reading_order": 10,
              "bbox": [
                100,
                510,
                585,
                540
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_11",
              "label": "para",
              "text": "12. ◦ Write a for loop to print out the characters of a string, one per line.",
              "level": -1,
              "page": 146,
              "reading_order": 11,
              "bbox": [
                100,
                546,
                521,
                564
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_12",
              "label": "para",
              "text": "13. ◦ What is the difference between calling split on a string with no argument and\none with ' ' as the argument, e.g., sent.split() versus sent.split(' ')? What\nhappens when the string being split contains tab characters, consecutive space\ncharacters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to\nenter a tab character.)",
              "level": -1,
              "page": 146,
              "reading_order": 12,
              "bbox": [
                100,
                564,
                585,
                645
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_13",
              "label": "para",
              "text": "14. ◦ Create a variable words containing a list of words. Experiment with\nwords.sort() and sorted(words). What is the difference?",
              "level": -1,
              "page": 146,
              "reading_order": 13,
              "bbox": [
                100,
                654,
                584,
                685
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_14",
              "label": "para",
              "text": "15. ◦ Explore the difference between strings and integers by typing the following at a\nPython prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers\nusing int(\"3\") and str(3).",
              "level": -1,
              "page": 146,
              "reading_order": 14,
              "bbox": [
                100,
                689,
                585,
                739
              ],
              "section_number": "15",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_15",
              "label": "para",
              "text": "16. ◦ Earlier, we asked you to use a text editor to create a file called test.py, containing\nthe single line monty = 'Monty Python'. If you haven’t already done this (or can’t\nfind the file), go ahead and do it now. Next, start up a new session with the Python",
              "level": -1,
              "page": 146,
              "reading_order": 15,
              "bbox": [
                100,
                743,
                585,
                797
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_146_order_16",
              "label": "foot",
              "text": "124 | Chapter3: Processing Raw Text",
              "level": -1,
              "page": 146,
              "reading_order": 16,
              "bbox": [
                97,
                824,
                261,
                842
              ],
              "section_number": "124",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_0",
              "label": "para",
              "text": "interpreter, and enter the expression monty at the prompt. You will get an error\nfrom the interpreter. Now, try the following (note that you have to leave off\nthe .py part of the filename):",
              "level": -1,
              "page": 147,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_2",
              "label": "para",
              "text": "This time, Python should return with a value. You can also try import test, in\nwhich case Python should be able to evaluate the expression test.monty at the\nprompt.",
              "level": -1,
              "page": 147,
              "reading_order": 2,
              "bbox": [
                118,
                161,
                585,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_3",
              "label": "para",
              "text": "17. ◦ What happens when the formatting strings %6s and %-6s are used to display\nstrings that are longer than six characters?",
              "level": -1,
              "page": 147,
              "reading_order": 3,
              "bbox": [
                100,
                215,
                585,
                251
              ],
              "section_number": "17",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_4",
              "label": "para",
              "text": "18. o Read in some text from a corpus, tokenize it, and print the list of all wh-word\ntypes that occur. (wh-words in English are used in questions, relative clauses, and\nexclamations: who, which, what, and so on.) Print them in order. Are any words\nduplicated in this list, because of the presence of case distinctions or punctuation?",
              "level": -1,
              "page": 147,
              "reading_order": 4,
              "bbox": [
                100,
                251,
                585,
                322
              ],
              "section_number": "18",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_5",
              "label": "para",
              "text": "19. o Create a file consisting of words and (made up) frequencies, where each line\nconsists of a word, the space character, and a positive integer, e.g., fuzzy 53. Read\nthe file into a Python list using open(filename).readlines(). Next, break each line\ninto its two fields using split(), and convert the number into an integer using\nint(). The result should be a list of the form: [['fuzzy', 53], ...].",
              "level": -1,
              "page": 147,
              "reading_order": 5,
              "bbox": [
                100,
                322,
                585,
                412
              ],
              "section_number": "19",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_6",
              "label": "para",
              "text": "20. Write code to access a favorite web page and extract some text from it. For\nexample, access a weather site and extract the forecast top temperature for your\ntown or city today.",
              "level": -1,
              "page": 147,
              "reading_order": 6,
              "bbox": [
                98,
                412,
                585,
                465
              ],
              "section_number": "20",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_7",
              "label": "para",
              "text": "21. o Write a function unknown() that takes a URL as its argument, and returns a list\nof unknown words that occur on that web page. In order to do this, extract all\nsubstrings consisting of lowercase letters (using re.findall()) and remove any\nitems from this set that occur in the Words Corpus (nltk.corpus.words). Try to\ncategorize these words manually and discuss your findings.",
              "level": -1,
              "page": 147,
              "reading_order": 7,
              "bbox": [
                98,
                465,
                585,
                549
              ],
              "section_number": "21",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_8",
              "label": "para",
              "text": "22. o Examine the results of processing the URL http://news.bbc.co.uk/ using the reg-\nular expressions suggested above. You will see that there is still a fair amount of\nnon-textual data there, particularly JavaScript commands. You may also find that\nsentence breaks have not been properly preserved. Define further regular expres-\nsions that improve the extraction of text from this web page.",
              "level": -1,
              "page": 147,
              "reading_order": 8,
              "bbox": [
                98,
                555,
                585,
                636
              ],
              "section_number": "22",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_9",
              "label": "para",
              "text": "23. o Are you able to write a regular expression to tokenize text in such a way that the\nword don't is tokenized into do and n't? Explain why this regular expression won’t\nwork: «n't|\\w+».",
              "level": -1,
              "page": 147,
              "reading_order": 9,
              "bbox": [
                98,
                643,
                585,
                689
              ],
              "section_number": "23",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_10",
              "label": "para",
              "text": "24. o Try to write code to convert text into hAck3r , using regular expressions and\nsubstitution, where e → 3 , i → 1 , o → 0 , l → | , s → 5 , . → 5w33t! , ate → 8 . Normalize\nthe text to lowercase before converting it. Add more substitutions of your own.\nNow try to map s to two different values: $ for word-initial s , and 5 for word-\ninternal s .",
              "level": -1,
              "page": 147,
              "reading_order": 10,
              "bbox": [
                98,
                689,
                585,
                774
              ],
              "section_number": "24",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_147_order_11",
              "label": "foot",
              "text": "3.12 Exercises | 125",
              "level": -1,
              "page": 147,
              "reading_order": 11,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "3.12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_0",
              "label": "para",
              "text": "25.\n• Pig Latin is a simple transformation of English text. Each word of the text is\nconverted as follows: move any consonant (or consonant cluster) that appears at\nthe start of the word to the end, then append ay, e.g., string → ingstray, idle →\nidleay (see http://en.wikipedia.org/wiki/Pig_Latin).",
              "level": -1,
              "page": 148,
              "reading_order": 0,
              "bbox": [
                98,
                71,
                585,
                143
              ],
              "section_number": "25",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_1",
              "label": "para",
              "text": "a. Write a function to convert a word to Pig Latin",
              "level": -1,
              "page": 148,
              "reading_order": 1,
              "bbox": [
                126,
                143,
                414,
                161
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_2",
              "label": "para",
              "text": "b. Write code that converts text, instead of individual words",
              "level": -1,
              "page": 148,
              "reading_order": 2,
              "bbox": [
                126,
                161,
                476,
                179
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_3",
              "label": "para",
              "text": "c. Extend it further to preserve capitalization, to keep qu together (so that\nquiet becomes jetquay, for example), and to detect when y is used as a con-\nsonant (e.g., yellow) versus a vowel (e.g., style).",
              "level": -1,
              "page": 148,
              "reading_order": 3,
              "bbox": [
                126,
                187,
                585,
                234
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_4",
              "label": "para",
              "text": "26. o Download some text from a language that has vowel harmony (e.g., Hungarian),\nextract the vowel sequences of words, and create a vowel bigram table.",
              "level": -1,
              "page": 148,
              "reading_order": 4,
              "bbox": [
                98,
                240,
                584,
                271
              ],
              "section_number": "26",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_5",
              "label": "para",
              "text": "27. o Python's random module includes a function choice() which randomly chooses\nan item from a sequence; e.g., choice(\"aehh \") will produce one of four possible\ncharacters, with the letter h being twice as frequent as the others. Write a generator\nexpression that produces a sequence of 500 randomly chosen letters drawn from\nthe string \"aehh \", and put this expression inside a call to the ''.join() function,\nto concatenate them into one long string. You should get a result that looks like\nuncontrolled sneezing or maniacal laughter: he haha ee heheeh eha. Use split()\nand join() again to normalize the whitespace in this string.",
              "level": -1,
              "page": 148,
              "reading_order": 5,
              "bbox": [
                98,
                277,
                585,
                412
              ],
              "section_number": "27",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_6",
              "label": "para",
              "text": "28. • Consider the numeric expressions in the following sentence from the MedLine\nCorpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15%\nand 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53\n+/- 0.15% is three words? Or should we say that it’s a single compound word? Or\nshould we say that it is actually nine words, since it’s read “four point five three,\nplus or minus fifteen percent”? Or should we say that it’s not a “real” word at all,\nsince it wouldn’t appear in any dictionary? Discuss these different possibilities. Can\nyou think of application domains that motivate at least two of these answers?",
              "level": -1,
              "page": 148,
              "reading_order": 6,
              "bbox": [
                98,
                412,
                585,
                546
              ],
              "section_number": "28",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_7",
              "label": "para",
              "text": "29. o Readability measures are used to score the reading difficulty of a text, for the\npurposes of selecting texts of appropriate difficulty for language learners. Let us\ndefine μ w to be the average number of letters per word, and μ s to be the average\nnumber of words per sentence, in a given text. The Automated Readability Index\n(ARI) of the text is defined to be: 4.71 μ w + 0.5 μ s - 21.43. Compute the ARI score\nfor various sections of the Brown Corpus, including section f (popular lore) and\nj (learned). Make use of the fact that nltk.corpus.brown.words() produces a se-\nquence of words, whereas nltk.corpus.brown.sents() produces a sequence of\nsentences.",
              "level": -1,
              "page": 148,
              "reading_order": 7,
              "bbox": [
                98,
                546,
                586,
                698
              ],
              "section_number": "29",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_8",
              "label": "para",
              "text": "30. o Use the Porter Stemmer to normalize some tokenized text, calling the stemmer\non each word. Do the same thing with the Lancaster Stemmer, and see if you ob-\nserve any differences.",
              "level": -1,
              "page": 148,
              "reading_order": 8,
              "bbox": [
                98,
                698,
                585,
                752
              ],
              "section_number": "30",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_9",
              "label": "para",
              "text": "31. o Define the variable saying to contain the list ['After', 'all', 'is', 'said',\n'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']. Process the list",
              "level": -1,
              "page": 148,
              "reading_order": 9,
              "bbox": [
                98,
                752,
                585,
                788
              ],
              "section_number": "31",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_148_order_10",
              "label": "foot",
              "text": "126 | Chapter3: Processing Raw Text",
              "level": -1,
              "page": 148,
              "reading_order": 10,
              "bbox": [
                97,
                824,
                261,
                842
              ],
              "section_number": "126",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_0",
              "label": "para",
              "text": "using a for loop, and store the result in a new list lengths. Hint: begin by assigning\nthe empty list to lengths, using lengths = []. Then each time through the loop,\nuse append() to add another length value to the list.",
              "level": -1,
              "page": 149,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_1",
              "label": "para",
              "text": "32. O Define a variable silly to contain the string: 'newly formed bland ideas are\ninexpressible in an infuriating way'. (This happens to be the legitimate inter-\npretation that bilingual English-Spanish speakers can assign to Chomsky’s famous\nnonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now\nwrite code to perform the following tasks:",
              "level": -1,
              "page": 149,
              "reading_order": 1,
              "bbox": [
                98,
                125,
                585,
                209
              ],
              "section_number": "32",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_2",
              "label": "para",
              "text": "a. Split silly into a list of strings, one per word, using Python's split() opera-\ntion, and save this to a variable called bland .",
              "level": -1,
              "page": 149,
              "reading_order": 2,
              "bbox": [
                126,
                215,
                584,
                243
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_3",
              "label": "para",
              "text": "b. Extract the second letter of each word in silly and join them into a string, to\nget 'eoldrnnnna'.",
              "level": -1,
              "page": 149,
              "reading_order": 3,
              "bbox": [
                126,
                250,
                585,
                286
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_4",
              "label": "para",
              "text": "c. Combine the words in bland back into a single string, using join(). Make sure\nthe words in the resulting string are separated with whitespace.",
              "level": -1,
              "page": 149,
              "reading_order": 4,
              "bbox": [
                126,
                286,
                585,
                322
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_5",
              "label": "para",
              "text": "d. Print the words of silly in alphabetical order, one per line.",
              "level": -1,
              "page": 149,
              "reading_order": 5,
              "bbox": [
                126,
                322,
                485,
                341
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_6",
              "label": "para",
              "text": "33. o The index() function can be used to look up items in sequences. For example,\n'inexpressible'.index('e') tells us the index of the first position of the letter e.",
              "level": -1,
              "page": 149,
              "reading_order": 6,
              "bbox": [
                98,
                348,
                584,
                379
              ],
              "section_number": "33",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_7",
              "label": "para",
              "text": "a. What\nhappens\nwhen\nyou\nlook\nup\na\nsubstring,\ne.g.,\n'inexpressi\nble'.index('re')?",
              "level": -1,
              "page": 149,
              "reading_order": 7,
              "bbox": [
                126,
                385,
                584,
                415
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_8",
              "label": "para",
              "text": "b. Define a variable words containing a list of words. Now use words.index() to\nlook up the position of an individual word.",
              "level": -1,
              "page": 149,
              "reading_order": 8,
              "bbox": [
                126,
                421,
                584,
                453
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_9",
              "label": "para",
              "text": "c. Define a variable silly as in Exercise 32. Use the index() function in combi-\nnation with list slicing to build a list phrase consisting of all the words up to\n(but not including) in in silly.",
              "level": -1,
              "page": 149,
              "reading_order": 9,
              "bbox": [
                126,
                456,
                585,
                510
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_10",
              "label": "para",
              "text": "34. Write code to convert nationality adjectives such as Canadian and Australian to\ntheir corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/\nList_of_adjectival_forms_of_place_names).",
              "level": -1,
              "page": 149,
              "reading_order": 10,
              "bbox": [
                98,
                510,
                585,
                564
              ],
              "section_number": "34",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_11",
              "label": "para",
              "text": "35. o Read the LanguageLog post on phrases of the form as best as p can and as best p\ncan, where p is a pronoun. Investigate this phenomenon with the help of a corpus\nand the findall() method for searching tokenized text described in Section 3.5.\nThe post is at http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html.",
              "level": -1,
              "page": 149,
              "reading_order": 11,
              "bbox": [
                98,
                564,
                585,
                631
              ],
              "section_number": "35",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_12",
              "label": "para",
              "text": "36. o Study the lolcat version of the book of Genesis, accessible as nltk.corpus.gene\nsis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://\nwww.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expres-\nsions to convert English words into corresponding lolspeak words.",
              "level": -1,
              "page": 149,
              "reading_order": 12,
              "bbox": [
                98,
                636,
                585,
                701
              ],
              "section_number": "36",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_13",
              "label": "para",
              "text": "37. o Read about the re.sub() function for string substitution using regular expres-\nsions, using help(re.sub) and by consulting the further readings for this chapter.\nUse re.sub in writing code to remove HTML tags from an HTML file, and to\nnormalize whitespace.",
              "level": -1,
              "page": 149,
              "reading_order": 13,
              "bbox": [
                98,
                707,
                585,
                772
              ],
              "section_number": "37",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_149_order_14",
              "label": "foot",
              "text": "3.12 Exercises | 127",
              "level": -1,
              "page": 149,
              "reading_order": 14,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "3.12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_0",
              "label": "para",
              "text": "38. • An interesting challenge for tokenization is words that have been split across a\nlinebreak. E.g., if long-term is split, then we have the string long-\\nterm.",
              "level": -1,
              "page": 150,
              "reading_order": 0,
              "bbox": [
                98,
                71,
                584,
                107
              ],
              "section_number": "38",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_1",
              "label": "para",
              "text": "a. Write a regular expression that identifies words that are hyphenated at a line-\nbreak. The expression will need to include the \\n character.",
              "level": -1,
              "page": 150,
              "reading_order": 1,
              "bbox": [
                126,
                107,
                584,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_2",
              "label": "para",
              "text": "b. Use re.sub() to remove the \\n character from these words",
              "level": -1,
              "page": 150,
              "reading_order": 2,
              "bbox": [
                126,
                149,
                478,
                163
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_3",
              "label": "para",
              "text": "c. How might you identify words that should not remain hyphenated once the\nnewline is removed, e.g., 'encyclo-\\npedia'?",
              "level": -1,
              "page": 150,
              "reading_order": 3,
              "bbox": [
                126,
                170,
                585,
                201
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_4",
              "label": "para",
              "text": "39. • Read the Wikipedia entry on Soundex. Implement this algorithm in Python.",
              "level": -1,
              "page": 150,
              "reading_order": 4,
              "bbox": [
                98,
                206,
                566,
                224
              ],
              "section_number": "39",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_5",
              "label": "para",
              "text": "40. • Obtain raw texts from two or more genres and compute their respective reading\ndifficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC\nRural News and ABC Science News ( nltk.corpus.abc ). Use Punkt to perform sen-\ntence segmentation.",
              "level": -1,
              "page": 150,
              "reading_order": 5,
              "bbox": [
                98,
                224,
                585,
                292
              ],
              "section_number": "40",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_6",
              "label": "para",
              "text": "41. • Rewrite the following nested loop as a nested list comprehension",
              "level": -1,
              "page": 150,
              "reading_order": 6,
              "bbox": [
                98,
                295,
                504,
                313
              ],
              "section_number": "41",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_8",
              "label": "para",
              "text": "42. • Use WordNet to create a semantic index for a text collection. Extend the con-\ncordance search program in Example 3-1, indexing each word using the offset of\nits first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some\nof its ancestors in the hypernym hierarchy).",
              "level": -1,
              "page": 150,
              "reading_order": 8,
              "bbox": [
                98,
                465,
                586,
                537
              ],
              "section_number": "42",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_9",
              "label": "para",
              "text": "43. • With the help of a multilingual corpus such as the Universal Declaration of\nHuman Rights Corpus ( nltk.corpus.udhr ), along with NLTK's frequency distri-\nbution and rank correlation functionality ( nltk.FreqDist , nltk.spearman_correla\ntion ), develop a system that guesses the language of a previously unseen text. For\nsimplicity, work with a single character encoding and just a few languages.",
              "level": -1,
              "page": 150,
              "reading_order": 9,
              "bbox": [
                98,
                537,
                586,
                621
              ],
              "section_number": "43",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_10",
              "label": "para",
              "text": "44. • Write a program that processes a text and discovers cases where a word has been\nused with a novel sense. For each word, compute the WordNet similarity between\nall synsets of the word and all synsets of the words in its context. (Note that this\nis a crude approach; doing it well is a difficult, open research problem.)",
              "level": -1,
              "page": 150,
              "reading_order": 10,
              "bbox": [
                98,
                627,
                585,
                691
              ],
              "section_number": "44",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_11",
              "label": "para",
              "text": "45. • Read the article on normalization of non-standard words (Sproat et al., 2001) ,\nand implement a similar system for text normalization.",
              "level": -1,
              "page": 150,
              "reading_order": 11,
              "bbox": [
                98,
                698,
                584,
                728
              ],
              "section_number": "45",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_150_order_12",
              "label": "foot",
              "text": "128 | Chapter3: Processing Raw Text",
              "level": -1,
              "page": 150,
              "reading_order": 12,
              "bbox": [
                97,
                824,
                261,
                842
              ],
              "section_number": "128",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_142_order_11",
          "label": "para",
          "text": "When the output of our program is text-like, instead of tabular, it will usually be nec-\nessary to wrap it so that it can be displayed conveniently. Consider the following output,\nwhich overflows its line, and which uses a complicated print statement:",
          "level": -1,
          "page": 142,
          "reading_order": 11,
          "bbox": [
            97,
            582,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_142_order_13",
          "label": "para",
          "text": "We can take care of line wrapping with the help of Python’s textwrap module. For\nmaximum clarity we will separate each step onto its own line:",
          "level": -1,
          "page": 142,
          "reading_order": 13,
          "bbox": [
            97,
            724,
            585,
            754
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_142_order_15",
          "label": "foot",
          "text": "120 | Chapter3: Processing Raw Text",
          "level": -1,
          "page": 142,
          "reading_order": 15,
          "bbox": [
            97,
            824,
            261,
            842
          ],
          "section_number": "120",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_143_order_1",
          "label": "para",
          "text": "Notice that there is a linebreak between more and its following number. If we wanted\nto avoid this, we could redefine the formatting string so that it contained no spaces\n(e.g., '%s_(%d),'), then instead of printing the value of wrapped, we could print wrap\nped.replace('_', ' ' ').",
          "level": -1,
          "page": 143,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            585,
            225
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_151_order_0",
      "label": "sec",
      "text": "CHAPTER 4\n\nWriting Structured Programs",
      "level": 1,
      "page": 151,
      "reading_order": 0,
      "bbox": [
        207,
        71,
        585,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_152_order_0",
          "label": "sub_sec",
          "text": "4.1 Back to the Basics",
          "level": 2,
          "page": 152,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            271,
            98
          ],
          "section_number": "4.1",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_152_order_1",
              "label": "sub_sub_sec",
              "text": "Assignment",
              "level": 3,
              "page": 152,
              "reading_order": 1,
              "bbox": [
                97,
                115,
                180,
                134
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_152_order_2",
                  "label": "para",
                  "text": "Assignment would seem to be the most elementary programming concept, not deserv-\ning a separate discussion. However, there are some surprising subtleties here. Consider\nthe following code fragment:",
                  "level": -1,
                  "page": 152,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    143,
                    585,
                    190
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_152_order_4",
                  "label": "para",
                  "text": "This behaves exactly as expected. When we write bar = foo in the code O, the value\nof foo (the string 'Monty') is assigned to bar. That is, bar is a copy of foo, so when we\noverwrite foo with a new string 'Python' on line O, the value of bar is not affected.",
                  "level": -1,
                  "page": 152,
                  "reading_order": 4,
                  "bbox": [
                    100,
                    268,
                    585,
                    322
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_152_order_5",
                  "label": "para",
                  "text": "However, assignment statements do not always involve making copies in this way.\nAssignment always copies the value of an expression, but a value is not always what\nyou might expect it to be. In particular, the “value” of a structured object such as a list\nis actually just a reference to the object. In the following example, ❶ assigns the refer-\nence of foo to the new variable bar . Now when we modify something inside foo on line\n❷ , we can see that the contents of bar have also been changed.",
                  "level": -1,
                  "page": 152,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    329,
                    585,
                    426
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_152_order_7",
                  "label": "para",
                  "text": "The line bar = foo❶ does not copy the contents of the variable, only its “ object refer-\nence. ” To understand what is going on here, we need to know how lists are stored in\nthe computer's memory. In Figure 4 - 1 , we see that a list foo is a reference to an object\nstored at location 3133 (which is itself a series of pointers to other locations holding\nstrings). When we assign bar = foo , it is just the object reference 3133 that gets copied.\nThis behavior extends to other aspects of the language, such as parameter passing\n( Section 4.4 ).",
                  "level": -1,
                  "page": 152,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    508,
                    585,
                    621
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_152_order_8",
                  "label": "foot",
                  "text": "130",
                  "level": -1,
                  "page": 152,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    824,
                    112,
                    842
                  ],
                  "section_number": "130",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_152_order_9",
                  "label": "foot",
                  "text": "Chapter 4: Writing Structured Programs",
                  "level": -1,
                  "page": 152,
                  "reading_order": 9,
                  "bbox": [
                    126,
                    824,
                    297,
                    842
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_153_order_0",
                  "label": "figure",
                  "text": "Figure 4-1. List assignment and computer memory: Two list objects foo and bar reference the same\nlocation in the computer's memory; updating foo will also modify bar, and vice versa. [IMAGE: ![Figure](figures/NLTK_page_153_figure_000.png)]",
                  "level": -1,
                  "page": 153,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    583,
                    241
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_153_figure_000.png)",
                      "bbox": [
                        100,
                        71,
                        583,
                        241
                      ],
                      "page": 153,
                      "reading_order": 0
                    },
                    {
                      "label": "cap",
                      "text": "Figure 4-1. List assignment and computer memory: Two list objects foo and bar reference the same\nlocation in the computer's memory; updating foo will also modify bar, and vice versa.",
                      "bbox": [
                        97,
                        241,
                        585,
                        277
                      ],
                      "page": 153,
                      "reading_order": 1
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_153_order_2",
                  "label": "para",
                  "text": "Let’s experiment some more, by creating a variable empty holding the empty list, then\nusing it three times on the next line.",
                  "level": -1,
                  "page": 153,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    284,
                    584,
                    315
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_153_order_4",
                  "label": "para",
                  "text": "Observe that changing one of the items inside our nested list of lists changed them all.\nThis is because each of the three elements is actually just a reference to one and the\nsame list in memory.",
                  "level": -1,
                  "page": 153,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    421,
                    585,
                    467
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_153_order_5",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_153_figure_005.png)",
                  "level": -1,
                  "page": 153,
                  "reading_order": 5,
                  "bbox": [
                    118,
                    483,
                    164,
                    546
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_153_order_6",
                  "label": "para",
                  "text": "Your Turn: Use multiplication to create a list of lists: nested = [[]] *\n3. Now modify one of the elements of the list, and observe that all the\nelements are changed. Use Python's id() function to find out the nu-\nmerical identifier for any object, and verify that id(nested[0]),\nid(nested[1]), and id(nested[2]) are all the same.",
                  "level": -1,
                  "page": 153,
                  "reading_order": 6,
                  "bbox": [
                    171,
                    499,
                    530,
                    573
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_153_order_7",
                  "label": "para",
                  "text": "Now, notice that when we assign a new value to one of the elements of the list, it does\nnot propagate to the others:",
                  "level": -1,
                  "page": 153,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    591,
                    585,
                    627
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_153_order_9",
                  "label": "para",
                  "text": "We began with a list containing three references to a single empty list object. Then we\nmodified that object by appending 'Python' to it, resulting in a list containing three\nreferences to a single list object ['Python']. Next, we overwrote one of those references\nwith a reference to a new object ['Monty']. This last step modified one of the three\nobject references inside the nested list. However, the ['Python'] object wasn’t changed,",
                  "level": -1,
                  "page": 153,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    707,
                    585,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_153_order_10",
                  "label": "foot",
                  "text": "4.1 Back to the Basics | 131",
                  "level": -1,
                  "page": 153,
                  "reading_order": 10,
                  "bbox": [
                    463,
                    824,
                    584,
                    842
                  ],
                  "section_number": "4.1",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_154_order_0",
                  "label": "para",
                  "text": "and is still referenced from two places in our nested list of lists. It is crucial to appreciate\nthis difference between modifying an object via an object reference and overwriting an\nobject reference.",
                  "level": -1,
                  "page": 154,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_154_order_1",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_154_figure_001.png)",
                  "level": -1,
                  "page": 154,
                  "reading_order": 1,
                  "bbox": [
                    118,
                    141,
                    171,
                    197
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_154_order_2",
                  "label": "para",
                  "text": "Important: To copy the items from a list foo to a new list bar, you can\nwrite bar = foo[:]. This copies the object references inside the list. To\ncopy a structure without copying any object references, use copy.deep\n\ncopy().",
                  "level": -1,
                  "page": 154,
                  "reading_order": 2,
                  "bbox": [
                    171,
                    151,
                    530,
                    208
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_151_order_1",
          "label": "para",
          "text": "By now you will have a sense of the capabilities of the Python programming language\nfor processing natural language. However, if you’re new to Python or to programming,\nyou may still be wrestling with Python and not feel like you are in full control yet. In\nthis chapter we’ll address the following questions:",
          "level": -1,
          "page": 151,
          "reading_order": 1,
          "bbox": [
            97,
            286,
            585,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_151_order_2",
          "label": "para",
          "text": "1. How can you write well-structured, readable programs that you and others will be\nable to reuse easily?",
          "level": -1,
          "page": 151,
          "reading_order": 2,
          "bbox": [
            100,
            358,
            585,
            394
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_151_order_3",
          "label": "para",
          "text": "2. How do the fundamental building blocks work, such as loops, functions, and\nassignment?",
          "level": -1,
          "page": 151,
          "reading_order": 3,
          "bbox": [
            100,
            394,
            584,
            432
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_151_order_4",
          "label": "para",
          "text": "3. What are some of the pitfalls with Python programming, and how can you avoid\nthem?",
          "level": -1,
          "page": 151,
          "reading_order": 4,
          "bbox": [
            100,
            438,
            583,
            466
          ],
          "section_number": "3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_151_order_5",
          "label": "para",
          "text": "Along the way, you will consolidate your knowledge of fundamental programming\nconstructs, learn more about using features of the Python language in a natural and\nconcise way, and learn some useful techniques in visualizing natural language data. As\nbefore, this chapter contains many examples and exercises (and as before, some exer-\ncises introduce new material). Readers new to programming should work through them\ncarefully and consult other introductions to programming if necessary; experienced\nprogrammers can quickly skim this chapter.",
          "level": -1,
          "page": 151,
          "reading_order": 5,
          "bbox": [
            97,
            474,
            585,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_151_order_6",
          "label": "para",
          "text": "In the other chapters of this book, we have organized the programming concepts as\ndictated by the needs of NLP. Here we revert to a more conventional approach, where\nthe material is more closely tied to the structure of the programming language. There's\nnot room for a complete presentation of the language, so we’ll just focus on the language\nconstructs and idioms that are most important for NLP.",
          "level": -1,
          "page": 151,
          "reading_order": 6,
          "bbox": [
            97,
            600,
            585,
            683
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_151_order_7",
          "label": "foot",
          "text": "129",
          "level": -1,
          "page": 151,
          "reading_order": 7,
          "bbox": [
            566,
            824,
            585,
            842
          ],
          "section_number": "129",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_154_order_3",
      "label": "sec",
      "text": "Equality",
      "level": 1,
      "page": 154,
      "reading_order": 3,
      "bbox": [
        98,
        232,
        153,
        252
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_155_order_0",
          "label": "sub_sub_sec",
          "text": "Conditionals",
          "level": 3,
          "page": 155,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            180,
            91
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_155_order_1",
              "label": "para",
              "text": "In the condition part of an if statement, a non-empty string or list is evaluated as true,\nwhile an empty string or list evaluates as false.",
              "level": -1,
              "page": 155,
              "reading_order": 1,
              "bbox": [
                97,
                98,
                584,
                134
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_155_order_3",
              "label": "para",
              "text": "That is, we don’t need to say if len(element) > 0: in the condition.",
              "level": -1,
              "page": 155,
              "reading_order": 3,
              "bbox": [
                100,
                241,
                485,
                259
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_155_order_4",
              "label": "para",
              "text": "What’s the difference between using if...elif as opposed to using a couple of if\nstatements in a row? Well, consider the following situation:",
              "level": -1,
              "page": 155,
              "reading_order": 4,
              "bbox": [
                97,
                259,
                585,
                296
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_155_order_6",
              "label": "para",
              "text": "Since the if clause of the statement is satisfied, Python never tries to evaluate the\nelif clause, so we never get to print out 2. By contrast, if we replaced the elif by an\nif, then we would print out both 1 and 2. So an elif clause potentially gives us more\ninformation than a bare if clause; when it evaluates to true, it tells us not only that the\ncondition is satisfied, but also that the condition of the main if clause was not satisfied.",
              "level": -1,
              "page": 155,
              "reading_order": 6,
              "bbox": [
                97,
                403,
                585,
                483
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_155_order_7",
              "label": "para",
              "text": "The functions all() and any() can be applied to a list (or other sequence) to check\nwhether all or any items meet some condition:",
              "level": -1,
              "page": 155,
              "reading_order": 7,
              "bbox": [
                97,
                492,
                584,
                528
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_155_order_9",
          "label": "sub_sec",
          "text": "4.2 Sequence",
          "level": 2,
          "page": 155,
          "reading_order": 9,
          "bbox": [
            97,
            618,
            207,
            646
          ],
          "section_number": "4.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_155_order_10",
              "label": "para",
              "text": "So far, we have seen two kinds of sequence object: strings and lists. Another kind of\nsequence is called a tuple . Tuples are formed with the comma operator ❶ , and typically\nenclosed using parentheses. We've actually seen them in the previous chapters, and\nsometimes referred to them as “ pairs, ” since there were always two members. However,\ntuples can have any number of members. Like lists and strings, tuples can be indexed\n❸ and sliced ❷ , and have a length ❹ .",
              "level": -1,
              "page": 155,
              "reading_order": 10,
              "bbox": [
                97,
                654,
                586,
                752
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_155_order_12",
              "label": "foot",
              "text": "4.2 Sequences | 133",
              "level": -1,
              "page": 155,
              "reading_order": 12,
              "bbox": [
                492,
                824,
                584,
                842
              ],
              "section_number": "4.2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_156_order_1",
              "label": "para",
              "text": "Caution!",
              "level": -1,
              "page": 156,
              "reading_order": 1,
              "bbox": [
                171,
                152,
                209,
                170
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_156_order_2",
              "label": "para",
              "text": "Tuples are constructed using the comma operator. Parentheses are a\nmore general feature of Python syntax, designed for grouping. A tuple\ncontaining the single element 'snark' is defined by adding a trailing\ncomma, like this: 'snark',. The empty tuple is a special case, and is\ndefined using empty parentheses ().",
              "level": -1,
              "page": 156,
              "reading_order": 2,
              "bbox": [
                171,
                170,
                530,
                250
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_156_order_3",
              "label": "para",
              "text": "Let’s compare strings, lists, and tuples directly, and do the indexing, slice, and length\noperation on each type:",
              "level": -1,
              "page": 156,
              "reading_order": 3,
              "bbox": [
                97,
                268,
                584,
                304
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_156_order_5",
              "label": "para",
              "text": "Notice in this code sample that we computed multiple values on a single line, separated\nby commas. These comma-separated expressions are actually just tuples—Python al-\nlows us to omit the parentheses around tuples if there is no ambiguity. When we print\na tuple, the parentheses are always displayed. By using tuples in this way, we are im-\nplicitly aggregating items together.",
              "level": -1,
              "page": 156,
              "reading_order": 5,
              "bbox": [
                97,
                430,
                585,
                519
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_156_order_6",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_156_figure_006.png)",
              "level": -1,
              "page": 156,
              "reading_order": 6,
              "bbox": [
                118,
                528,
                171,
                591
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_156_order_7",
              "label": "para",
              "text": "Your Turn: Define a set, e.g., using set(text), and see what happens\nwhen you convert it to a list or iterate over its members.",
              "level": -1,
              "page": 156,
              "reading_order": 7,
              "bbox": [
                171,
                545,
                530,
                573
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_154_order_4",
          "label": "para",
          "text": "Python provides two ways to check that a pair of items are the same. The is operator\ntests for object identity. We can use it to verify our earlier observations about objects.\nFirst, we create a list containing several copies of the same object, and demonstrate that\nthey are not only identical according to ==, but also that they are one and the same\nobject:",
          "level": -1,
          "page": 154,
          "reading_order": 4,
          "bbox": [
            97,
            259,
            585,
            341
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_154_order_6",
          "label": "para",
          "text": "Now let's put a new python in this nest. We can easily show that the objects are not\nall identical:",
          "level": -1,
          "page": 154,
          "reading_order": 6,
          "bbox": [
            97,
            448,
            584,
            477
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_154_order_8",
          "label": "para",
          "text": "You can do several pairwise tests to discover which position contains the interloper,\nbut the id() function makes detection is easier:",
          "level": -1,
          "page": 154,
          "reading_order": 8,
          "bbox": [
            97,
            609,
            584,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_154_order_10",
          "label": "para",
          "text": "This reveals that the second item of the list has a distinct identifier. If you try running\nthis code snippet yourself, expect to see different numbers in the resulting list, and\ndon't be surprised if the interloper is in a different position.",
          "level": -1,
          "page": 154,
          "reading_order": 10,
          "bbox": [
            97,
            688,
            585,
            735
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_154_order_11",
          "label": "para",
          "text": "Having two kinds of equality might seem strange. However, it’s really just the type-\ntoken distinction, familiar from natural language, here showing up in a programming\nlanguage.",
          "level": -1,
          "page": 154,
          "reading_order": 11,
          "bbox": [
            97,
            743,
            585,
            792
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_154_order_12",
          "label": "foot",
          "text": "132 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 154,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "132",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_156_order_8",
      "label": "sec",
      "text": "Operating on Sequence Types",
      "level": 1,
      "page": 156,
      "reading_order": 8,
      "bbox": [
        97,
        618,
        297,
        638
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_156_order_9",
          "label": "para",
          "text": "We can iterate over the items in a sequence $\\mathsf{s}$ in a variety of useful ways, as shown in\nTable 4 - 1 .",
          "level": -1,
          "page": 156,
          "reading_order": 9,
          "bbox": [
            97,
            645,
            584,
            674
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_156_order_10",
          "label": "table",
          "text": "Table 4-1. Various ways to iterate over sequences [TABLE: <table><tr><td>Python expression</td><td>Comment</td></tr><tr><td>for item in s</td><td>Iterate over the items of s</td></tr><tr><td>for item in sorted(s)</td><td>Iterate over the items of s in order</td></tr><tr><td>for item in set(s)</td><td>Iterate over unique elements of s</td></tr></table>]",
          "level": -1,
          "page": 156,
          "reading_order": 10,
          "bbox": [
            100,
            707,
            467,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Python expression</td><td>Comment</td></tr><tr><td>for item in s</td><td>Iterate over the items of s</td></tr><tr><td>for item in sorted(s)</td><td>Iterate over the items of s in order</td></tr><tr><td>for item in set(s)</td><td>Iterate over unique elements of s</td></tr></table>",
              "bbox": [
                100,
                707,
                467,
                788
              ],
              "page": 156,
              "reading_order": 10
            },
            {
              "label": "cap",
              "text": "Table 4-1. Various ways to iterate over sequences",
              "bbox": [
                99,
                689,
                342,
                707
              ],
              "page": 156,
              "reading_order": 11
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_156_order_12",
          "label": "foot",
          "text": "134 | Chapter 4: Writing Structured Programs",
          "level": -1,
          "page": 156,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "134",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_157_order_0",
          "label": "tab",
          "text": "<table><tr><td>Python expression</td><td>Comment</td></tr><tr><td>for item in reversed(s)</td><td>Iterate over elements of s in reverse</td></tr><tr><td>for item in set(s).difference(t)</td><td>Iterate over elements of s not in t</td></tr><tr><td>for item in random.shuffle(s)</td><td>Iterate over elements of s in random order</td></tr></table>",
          "level": -1,
          "page": 157,
          "reading_order": 0,
          "bbox": [
            100,
            80,
            458,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_157_order_1",
          "label": "para",
          "text": "The sequence functions illustrated in Table 4-1 can be combined in various ways; for\nexample, to get unique elements of s sorted in reverse, use reversed(sorted(set(s))).",
          "level": -1,
          "page": 157,
          "reading_order": 1,
          "bbox": [
            97,
            170,
            585,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_157_order_2",
          "label": "para",
          "text": "We can convert between these sequence types. For example, tuple(s) converts any\nkind of sequence into a tuple, and list(s) converts any kind of sequence into a list.\nWe can convert a list of strings to a single string using the join() function, e.g.,\n':' -join(words) .",
          "level": -1,
          "page": 157,
          "reading_order": 2,
          "bbox": [
            97,
            214,
            585,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_157_order_3",
          "label": "para",
          "text": "Some other objects, such as a FreqDist, can be converted into a sequence (using\nlist()) and support iteration:",
          "level": -1,
          "page": 157,
          "reading_order": 3,
          "bbox": [
            97,
            286,
            585,
            318
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_157_order_5",
          "label": "para",
          "text": "In the next example, we use tuples to re-arrange the contents of our list. (We can omit\nthe parentheses because the comma has higher precedence than assignment.)",
          "level": -1,
          "page": 157,
          "reading_order": 5,
          "bbox": [
            97,
            448,
            585,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_157_order_7",
          "label": "para",
          "text": "This is an idiomatic and readable way to move items inside a list. It is equivalent to the\nfollowing traditional way of doing such tasks that does not use tuples (notice that this\nmethod needs a temporary variable tmp).",
          "level": -1,
          "page": 157,
          "reading_order": 7,
          "bbox": [
            97,
            546,
            585,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_157_order_9",
          "label": "para",
          "text": "As we have seen, Python has sequence functions such as sorted() and reversed() that\nrearrange the items of a sequence. There are also functions that modify the structure of\na sequence, which can be handy for language processing. Thus, zip() takes the items\nof two or more sequences and “zips” them together into a single list of pairs. Given a\nsequence s, enumerate(s) returns pairs consisting of an index and the item at that index.",
          "level": -1,
          "page": 157,
          "reading_order": 9,
          "bbox": [
            97,
            669,
            586,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_157_order_11",
          "label": "foot",
          "text": "4.2 Sequences | 135",
          "level": -1,
          "page": 157,
          "reading_order": 11,
          "bbox": [
            492,
            824,
            585,
            842
          ],
          "section_number": "4.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_158_order_1",
          "label": "para",
          "text": "For some NLP tasks it is necessary to cut up a sequence into two or more parts. For\ninstance, we might want to “train” a system on 90% of the data and test it on the\nremaining 10%. To do this we decide the location where we want to cut the data ❶ ,\nthen cut the sequence at that location ❷ .",
          "level": -1,
          "page": 158,
          "reading_order": 1,
          "bbox": [
            97,
            134,
            585,
            199
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_158_order_3",
          "label": "para",
          "text": "We can verify that none of the original data is lost during this process, nor is it dupli-\ncated ❸ . We can also verify that the ratio of the sizes of the two pieces is what we\nintended ❸ .",
          "level": -1,
          "page": 158,
          "reading_order": 3,
          "bbox": [
            97,
            304,
            584,
            352
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_158_order_4",
      "label": "sec",
      "text": "Combining Different Sequence Types",
      "level": 1,
      "page": 158,
      "reading_order": 4,
      "bbox": [
        97,
        367,
        342,
        389
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_158_order_5",
          "label": "para",
          "text": "Let’s combine our knowledge of these three sequence types, together with list com-\nprehensions, to perform the task of sorting the words in a string by their length.",
          "level": -1,
          "page": 158,
          "reading_order": 5,
          "bbox": [
            97,
            394,
            584,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_158_order_7",
          "label": "para",
          "text": "Each of the preceding lines of code contains a significant feature. A simple string is\nactually an object with methods defined on it, such as split() ❶ . We use a list com-\nprehension to build a list of tuples ❷ , where each tuple consists of a number (the word\nlength) and the word, e.g., (3, 'the') . We use the sort() method ❸ to sort the list in\nplace. Finally, we discard the length information and join the words back into a single\nstring ❸ . (The underscore ❸ is just a regular Python variable, but we can use underscore\nby convention to indicate that we will not use its value.)",
          "level": -1,
          "page": 158,
          "reading_order": 7,
          "bbox": [
            97,
            510,
            585,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_158_order_8",
          "label": "para",
          "text": "We began by talking about the commonalities in these sequence types, but the previous\ncode illustrates important differences in their roles. First, strings appear at the beginning\nand the end: this is typical in the context where our program is reading in some text\nand producing output for us to read. Lists and tuples are used in the middle, but for\ndifferent purposes. A list is typically a sequence of objects all having the same type , of\narbitrary length . We often use lists to hold sequences of words. In contrast, a tuple is\ntypically a collection of objects of different types , of fixed length . We often use a tuple\nto hold a record , a collection of different fields relating to some entity. This distinction\nbetween the use of lists and tuples takes some getting used to, so here is another\nexample:",
          "level": -1,
          "page": 158,
          "reading_order": 8,
          "bbox": [
            97,
            634,
            586,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_158_order_9",
          "label": "foot",
          "text": "136 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 158,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "136",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_159_order_1",
          "label": "para",
          "text": "Here, a lexicon is represented as a list because it is a collection of objects of a single\ntype — lexical entries — of no predetermined length. An individual entry is represented\nas a tuple because it is a collection of objects with different interpretations, such as the\northographic form, the part-of-speech, and the pronunciations (represented in the\nSAMPA computer-readable phonetic alphabet; see http://www.phon.ucl.ac.uk/home/\nsampa/ ). Note that these pronunciations are stored using a list. (Why?)",
          "level": -1,
          "page": 159,
          "reading_order": 1,
          "bbox": [
            97,
            134,
            585,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_159_order_2",
          "label": "para",
          "text": "A good way to decide when to use tuples versus lists is to ask whether\nthe interpretation of an item depends on its position. For example, a\ntagged token combines two strings having different interpretations, and\nwe choose to interpret the first item as the token and the second item\nas the tag. Thus we use tuples like this: ('grail', 'noun') . A tuple of\nthe form ('noun', 'grail') would be non-sensical since it would be a\nword noun tagged grail . In contrast, the elements of a text are all tokens,\nand position is not significant. Thus we use lists like this: ['venetian',\n'blind'] . A list of the form ['blind', 'venetian'] would be equally\nvalid. The linguistic meaning of the words might be different, but the\ninterpretation of list items as tokens is unchanged.",
          "level": -1,
          "page": 159,
          "reading_order": 2,
          "bbox": [
            171,
            259,
            530,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_159_order_3",
          "label": "para",
          "text": "The distinction between lists and tuples has been described in terms of usage. However,\nthere is a more fundamental difference: in Python, lists are mutable, whereas tuples\nare immutable. In other words, lists can be modified, whereas tuples cannot. Here are\nsome of the operations on lists that do in-place modification of the list:",
          "level": -1,
          "page": 159,
          "reading_order": 3,
          "bbox": [
            97,
            439,
            585,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_159_order_5",
          "label": "para",
          "text": "Your\nTurn:\nConvert\nlexicon\nto\na\ntuple,\nusing\nlexicon\n=\ntuple(lexicon), then try each of the operations, to confirm that none of\nthem is permitted on tuples.",
          "level": -1,
          "page": 159,
          "reading_order": 5,
          "bbox": [
            171,
            582,
            530,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_159_order_6",
      "label": "sec",
      "text": "Generator Expressions",
      "level": 1,
      "page": 159,
      "reading_order": 6,
      "bbox": [
        97,
        654,
        245,
        675
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_160_order_3",
          "label": "sub_sec",
          "text": "4.3 Questions of Style",
          "level": 2,
          "page": 160,
          "reading_order": 3,
          "bbox": [
            97,
            331,
            272,
            355
          ],
          "section_number": "4.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_160_order_5",
              "label": "sub_sub_sec",
              "text": "Python Coding Style",
              "level": 3,
              "page": 160,
              "reading_order": 5,
              "bbox": [
                98,
                492,
                234,
                512
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_160_order_6",
                  "label": "para",
                  "text": "When writing programs you make many subtle choices about names, spacing, com-\nments, and so on. When you look at code written by other people, needless differences\nin style make it harder to interpret the code. Therefore, the designers of the Python\nlanguage have published a style guide for Python code, available at http://www.python\n.org/dev/peps/pep-0008/ . The underlying value presented in the style guide is consis-\ntency , for the purpose of maximizing the readability of code. We briefly review some\nof its key recommendations here, and refer readers to the full guide for detailed dis-\ncussion with examples.",
                  "level": -1,
                  "page": 160,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    519,
                    585,
                    654
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_160_order_7",
                  "label": "para",
                  "text": "Code layout should use four spaces per indentation level. You should make sure that\nwhen you write Python code in a file, you avoid tabs for indentation, since these can\nbe misinterpreted by different text editors and the indentation can be messed up. Lines\nshould be less than 80 characters long; if necessary, you can break a line inside paren-\ntheses, brackets, or braces, because Python is able to detect that the line continues over\nto the next line, as in the following examples:",
                  "level": -1,
                  "page": 160,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    654,
                    585,
                    756
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_160_order_9",
                  "label": "foot",
                  "text": "138 | Chapter4: Writing Structured Programs",
                  "level": -1,
                  "page": 160,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    824,
                    297,
                    842
                  ],
                  "section_number": "138",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_161_order_1",
                  "label": "para",
                  "text": "If you need to break a line outside parentheses, brackets, or braces, you can often add\nextra parentheses, and you can always add a backslash at the end of the line that is\nbroken:",
                  "level": -1,
                  "page": 161,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    187,
                    585,
                    232
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_161_order_3",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_161_figure_003.png)",
                  "level": -1,
                  "page": 161,
                  "reading_order": 3,
                  "bbox": [
                    118,
                    331,
                    171,
                    394
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_161_order_4",
                  "label": "para",
                  "text": "Typing spaces instead of tabs soon becomes a chore. Many program-\nming editors have built-in support for Python, and can automatically\nindent code and highlight any syntax errors (including indentation er-\nrors). For a list of Python-aware editors, please see http://wiki.python\n.org/moin/PythonEditors .",
                  "level": -1,
                  "page": 161,
                  "reading_order": 4,
                  "bbox": [
                    171,
                    348,
                    530,
                    421
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_160_order_4",
              "label": "para",
              "text": "Programming is as much an art as a science. The undisputed “bible” of programming,\na 2,500 page multivolume work by Donald Knuth, is called The Art of Computer Pro-\ngramming. Many books have been written on Literate Programming, recognizing that\nhumans, not just computers, must read and understand programs. Here we pick up on\nsome issues of programming style that have important ramifications for the readability\nof your code, including code layout, procedural versus declarative style, and the use of\nloop variables.",
              "level": -1,
              "page": 160,
              "reading_order": 4,
              "bbox": [
                96,
                358,
                586,
                477
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_159_order_7",
          "label": "para",
          "text": "We’ve been making heavy use of list comprehensions, for compact and readable pro-\ncessing of texts. Here’s an example where we tokenize and normalize a text:",
          "level": -1,
          "page": 159,
          "reading_order": 7,
          "bbox": [
            97,
            680,
            584,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_159_order_9",
          "label": "foot",
          "text": "4.2 Sequences | 137",
          "level": -1,
          "page": 159,
          "reading_order": 9,
          "bbox": [
            492,
            824,
            585,
            842
          ],
          "section_number": "4.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_160_order_0",
          "label": "para",
          "text": "Suppose we now want to process these words further. We can do this by inserting the\npreceding expression inside a call to some other function ❶, but Python allows us to\nomit the brackets ❷.",
          "level": -1,
          "page": 160,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_160_order_2",
          "label": "para",
          "text": "The second line uses a generator expression. This is more than a notational conven-\nience: in many language processing situations, generator expressions will be more ef-\nficient. In ❶ , storage for the list object must be allocated before the value of max() is\ncomputed. If the text is very large, this could be slow. In ❷ , the data is streamed to the\ncalling function. Since the calling function simply has to find the maximum value—the\nword that comes latest in lexicographic sort order—it can process the stream of data\nwithout having to store anything more than the maximum value seen so far.",
          "level": -1,
          "page": 160,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            585,
            305
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_161_order_5",
      "label": "sec",
      "text": "Procedural Versus Declarative Style",
      "level": 1,
      "page": 161,
      "reading_order": 5,
      "bbox": [
        98,
        439,
        331,
        465
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_161_order_6",
          "label": "para",
          "text": "We have just seen how the same task can be performed in different ways, with impli-\ncations for efficiency. Another factor influencing program development is programming\nstyle. Consider the following program to compute the average length of words in the\nBrown Corpus:",
          "level": -1,
          "page": 161,
          "reading_order": 6,
          "bbox": [
            97,
            471,
            585,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_161_order_8",
          "label": "para",
          "text": "In this program we use the variable count to keep track of the number of tokens seen,\nand total to store the combined length of all words. This is a low-level style, not far\nremoved from machine code, the primitive operations performed by the computer's\nCPU. The two variables are just like a CPU's registers, accumulating values at many\nintermediate stages, values that are meaningless until the end. We say that this program\nis written in a procedural style, dictating the machine operations step by step. Now\nconsider the following program that computes the same thing:",
          "level": -1,
          "page": 161,
          "reading_order": 8,
          "bbox": [
            97,
            654,
            585,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_161_order_9",
          "label": "foot",
          "text": "4.3 Questions of Style | 139",
          "level": -1,
          "page": 161,
          "reading_order": 9,
          "bbox": [
            463,
            824,
            585,
            842
          ],
          "section_number": "4.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_162_order_1",
          "label": "para",
          "text": "The first line uses a generator expression to sum the token lengths, while the second\nline computes the average as before. Each line of code performs a complete, meaningful\ntask, which can be understood in terms of high-level properties like: “ total is the sum\nof the lengths of the tokens. ” Implementation details are left to the Python interpreter.\nThe second program uses a built-in function, and constitutes programming at a more\nabstract level; the resulting code is more declarative. Let's look at an extreme example:",
          "level": -1,
          "page": 162,
          "reading_order": 1,
          "bbox": [
            97,
            122,
            585,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_162_order_3",
          "label": "para",
          "text": "The equivalent declarative version uses familiar built-in functions, and its purpose is\nnstantly recognizable:",
          "level": -1,
          "page": 162,
          "reading_order": 3,
          "bbox": [
            100,
            376,
            585,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_162_order_5",
          "label": "para",
          "text": "Another case where a loop counter seems to be necessary is for printing a counter with\neach line of output. Instead, we can use enumerate(), which processes a sequence s and\nproduces a tuple of the form (i, s[i]) for each item in s, starting with (0, s[0]). Here\nwe enumerate the keys of the frequency distribution, and capture the integer-string pair\nin the variables rank and word. We print rank+1 so that the counting appears to start\nfrom 1, as required when producing a list of ranked items.",
          "level": -1,
          "page": 162,
          "reading_order": 5,
          "bbox": [
            97,
            439,
            585,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_162_order_7",
          "label": "para",
          "text": "It’s sometimes tempting to use loop variables to store a maximum or minimum value\nseen so far. Let’s use this method to find the longest word in a text.",
          "level": -1,
          "page": 162,
          "reading_order": 7,
          "bbox": [
            97,
            761,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_162_order_8",
          "label": "foot",
          "text": "140 | Chapter 4: Writing Structured Programs",
          "level": -1,
          "page": 162,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "140",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_163_order_1",
          "label": "para",
          "text": "However, a more transparent solution uses two list comprehensions, both having forms\nthat should be familiar by now:",
          "level": -1,
          "page": 163,
          "reading_order": 1,
          "bbox": [
            97,
            170,
            585,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_163_order_3",
          "label": "para",
          "text": "Note that our first solution found the first word having the longest length, while the\nsecond solution found all of the longest words (which is usually what we would want).\nAlthough there's a theoretical efficiency difference between the two solutions, the main\noverhead is reading the data into main memory; once it's there, a second pass through\nthe data is effectively instantaneous. We also need to balance our concerns about pro-\ngram efficiency with programmer efficiency. A fast but cryptic solution will be harder\nto understand and maintain.",
          "level": -1,
          "page": 163,
          "reading_order": 3,
          "bbox": [
            97,
            259,
            585,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_163_order_4",
      "label": "sec",
      "text": "Some Legitimate Uses for Counters",
      "level": 1,
      "page": 163,
      "reading_order": 4,
      "bbox": [
        97,
        385,
        329,
        412
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_164_order_4",
          "label": "sub_sec",
          "text": "4.4 Functions: The Foundation of Structured Programming",
          "level": 2,
          "page": 164,
          "reading_order": 4,
          "bbox": [
            97,
            348,
            566,
            371
          ],
          "section_number": "4.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_164_order_5",
              "label": "para",
              "text": "Functions provide an effective way to package and reuse program code, as already\nexplained in Section 2.3. For example, suppose we find that we often want to read text\nfrom an HTML file. This involves several steps: opening the file, reading it in, normal-\nizing whitespace, and stripping HTML markup. We can collect these steps into a func-\ntion, and give it a name such as get_text(), as shown in Example 4-1.",
              "level": -1,
              "page": 164,
              "reading_order": 5,
              "bbox": [
                97,
                376,
                585,
                460
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_164_order_6",
              "label": "para",
              "text": "Example 4-1. Read text from a file.",
              "level": -1,
              "page": 164,
              "reading_order": 6,
              "bbox": [
                97,
                474,
                270,
                487
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_164_order_8",
              "label": "para",
              "text": "Now, any time we want to get cleaned-up text from an HTML file, we can just call\nget_text() with the name of the file as its only argument. It will return a string, and we\ncan assign this to a variable, e.g., contents = get_text(\"test.html\"). Each time we\nwant to use this series of steps, we only have to call the function.",
              "level": -1,
              "page": 164,
              "reading_order": 8,
              "bbox": [
                97,
                600,
                585,
                664
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_164_order_9",
              "label": "para",
              "text": "Using functions has the benefit of saving space in our program. More importantly, our\nchoice of name for the function helps make the program readable. In the case of the\npreceding example, whenever our program needs to read cleaned-up text from a file\nwe don’t have to clutter the program with four lines of code; we simply need to call\nget_text(). This naming helps to provide some “semantic interpretation”—it helps a\nreader of our program to see what the program “means.”",
              "level": -1,
              "page": 164,
              "reading_order": 9,
              "bbox": [
                97,
                672,
                585,
                770
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_164_order_10",
              "label": "foot",
              "text": "142 | Chapter 4: Writing Structured Programs",
              "level": -1,
              "page": 164,
              "reading_order": 10,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "142",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_165_order_0",
              "label": "para",
              "text": "Notice that this example function definition contains a string. The first string inside a\nfunction definition is called a docstring. Not only does it document the purpose of the\nfunction to someone reading the code, it is accessible to a programmer who has loaded\nthe code from a file:",
              "level": -1,
              "page": 165,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                136
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_165_order_3",
              "label": "para",
              "text": "We have seen that functions help to make our work reusable and readable. They also\nhelp make it reliable. When we reuse code that has already been developed and tested,\nwe can be more confident that it handles a variety of cases correctly. We also remove\nthe risk of forgetting some important step or introducing a bug. The program that calls\nour function also has increased reliability. The author of that program is dealing with\na shorter program, and its components behave transparently.",
              "level": -1,
              "page": 165,
              "reading_order": 3,
              "bbox": [
                97,
                232,
                585,
                331
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_165_order_4",
              "label": "para",
              "text": "To summarize, as its name suggests, a function captures functionality. It is a segment\nof code that can be given a meaningful name and which performs a well-defined task.\nFunctions allow us to abstract away from the details, to see a bigger picture, and to\nprogram more effectively.",
              "level": -1,
              "page": 165,
              "reading_order": 4,
              "bbox": [
                97,
                340,
                584,
                404
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_165_order_5",
              "label": "para",
              "text": "The rest of this section takes a closer look at functions, exploring the mechanics and\ndiscussing ways to make your programs easier to read.",
              "level": -1,
              "page": 165,
              "reading_order": 5,
              "bbox": [
                97,
                412,
                584,
                448
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_163_order_5",
          "label": "para",
          "text": "There are cases where we still want to use loop variables in a list comprehension. For\nexample, we need to use a loop variable to extract successive overlapping n-grams from\na list:",
          "level": -1,
          "page": 163,
          "reading_order": 5,
          "bbox": [
            97,
            418,
            585,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_163_order_7",
          "label": "para",
          "text": "It is quite tricky to get the range of the loop variable right. Since this is a common\noperation\nin\nNLP,\nNLTK\nsupports\nit\nwith\nfunctions\nbigrams(text)\nand\ntrigrams(text) , and a general-purpose ngrams(text, n) .",
          "level": -1,
          "page": 163,
          "reading_order": 7,
          "bbox": [
            97,
            573,
            584,
            621
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_163_order_8",
          "label": "para",
          "text": "Here’s an example of how we can use loop variables in building multidimensional\nstructures. For example, to build an array with m rows and n columns, where each cell\nis a set, we could use a nested list comprehension:",
          "level": -1,
          "page": 163,
          "reading_order": 8,
          "bbox": [
            97,
            627,
            584,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_163_order_10",
          "label": "foot",
          "text": "4.3 Questions of Style | 141",
          "level": -1,
          "page": 163,
          "reading_order": 10,
          "bbox": [
            463,
            824,
            584,
            842
          ],
          "section_number": "4.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_164_order_0",
          "label": "para",
          "text": "Observe that the loop variables i and j are not used anywhere in the resulting object;\nthey are just needed for a syntactically correct for statement. As another example of\nthis usage, observe that the expression ['very' for i in range(3)] produces a list\ncontaining three instances of 'very', with no integers in sight.",
          "level": -1,
          "page": 164,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            586,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_164_order_1",
          "label": "para",
          "text": "Note that it would be incorrect to do this work using multiplication, for reasons con-\ncerning object copying that were discussed earlier in this section.",
          "level": -1,
          "page": 164,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            584,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_164_order_3",
          "label": "para",
          "text": "Iteration is an important programming device. It is tempting to adopt idioms from other\nlanguages. However, Python offers some elegant and highly readable alternatives, as\nwe have seen.",
          "level": -1,
          "page": 164,
          "reading_order": 3,
          "bbox": [
            97,
            268,
            585,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_165_order_6",
      "label": "sec",
      "text": "Function Inputs and Outputs",
      "level": 1,
      "page": 165,
      "reading_order": 6,
      "bbox": [
        98,
        456,
        288,
        479
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_165_order_7",
          "label": "para",
          "text": "We pass information to functions using a function’s parameters, the parenthesized list\nof variables and constants following the function’s name in the function definition.\nHere’s a complete example:",
          "level": -1,
          "page": 165,
          "reading_order": 7,
          "bbox": [
            97,
            483,
            585,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_165_order_9",
          "label": "para",
          "text": "We first define the function to take two parameters, msg and num ❶ . Then, we call the\nfunction and pass it two arguments, monty and 3 ❷ ; these arguments fill the “ place-\nholders” provided by the parameters and provide values for the occurrences of msg and\nnum in the function body.",
          "level": -1,
          "page": 165,
          "reading_order": 9,
          "bbox": [
            97,
            617,
            585,
            681
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_165_order_10",
          "label": "para",
          "text": "It is not necessary to have any parameters, as we see in the following example:",
          "level": -1,
          "page": 165,
          "reading_order": 10,
          "bbox": [
            98,
            689,
            548,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_165_order_12",
          "label": "foot",
          "text": "4.4 Functions: The Foundation of Structured Programming | 143",
          "level": -1,
          "page": 165,
          "reading_order": 12,
          "bbox": [
            315,
            824,
            584,
            842
          ],
          "section_number": "4.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_166_order_0",
          "label": "para",
          "text": "A function usually communicates its results back to the calling program via the\nreturn statement, as we have just seen. To the calling program, it looks as if the function\ncall had been replaced with the function’s result:",
          "level": -1,
          "page": 166,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_166_order_2",
          "label": "para",
          "text": "A Python function is not required to have a return statement. Some functions do their\nwork as a side effect, printing a result, modifying a file, or updating the contents of a\nparameter to the function (such functions are called “procedures” in some other\nprogramming languages).",
          "level": -1,
          "page": 166,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_166_order_3",
          "label": "para",
          "text": "Consider the following three sort functions. The third one is dangerous because a pro-\ngrammer could use it without realizing that it had modified its input. In general, func-\ntions should modify the contents of a parameter (my_sort1()), or return a value\n(my_sort2()), but not both (my_sort3()).",
          "level": -1,
          "page": 166,
          "reading_order": 3,
          "bbox": [
            97,
            259,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_166_order_5",
      "label": "sec",
      "text": "Parameter Passing",
      "level": 1,
      "page": 166,
      "reading_order": 5,
      "bbox": [
        98,
        439,
        225,
        461
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_166_order_6",
          "label": "para",
          "text": "Back in Section 4.1 , you saw that assignment works on values, but that the value of a\nstructured object is a reference to that object. The same is true for functions. Python\ninterprets function parameters as values (this is known as call-by-value) . In the fol-\nlowing code, set_up() has two parameters, both of which are modified inside the func-\ntion. We begin by assigning an empty string to w and an empty dictionary to p . After\ncalling the function, w is unchanged, while p is changed:",
          "level": -1,
          "page": 166,
          "reading_order": 6,
          "bbox": [
            97,
            465,
            585,
            566
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_166_order_8",
          "label": "para",
          "text": "Notice that w was not changed by the function. When we called set_up(w, p), the value\nof w (an empty string) was assigned to a new variable word. Inside the function, the value",
          "level": -1,
          "page": 166,
          "reading_order": 8,
          "bbox": [
            97,
            734,
            585,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_166_order_9",
          "label": "foot",
          "text": "144 | Chapter 4: Writing Structured Programs",
          "level": -1,
          "page": 166,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "144",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_167_order_0",
          "label": "para",
          "text": "of word was modified. However, that change did not propagate to w. This parameter\npassing is identical to the following sequence of assignments:",
          "level": -1,
          "page": 167,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_167_order_2",
          "label": "para",
          "text": "Let's look at what happened with the list p . When we called set_up(w, p) , the value of\np (a reference to an empty list) was assigned to a new local variable properties , so both\nvariables now reference the same memory location. The function modifies\nproperties , and this change is also reflected in the value of p , as we saw. The function\nalso assigned a new value to properties (the number 5); this did not modify the contents\nat that memory location, but created a new local variable. This behavior is just as if we\nhad done the following sequence of assignments:",
          "level": -1,
          "page": 167,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            586,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_167_order_4",
          "label": "para",
          "text": "Thus, to understand Python’s call-by-value parameter passing, it is enough to under-\nstand how assignment works. Remember that you can use the id() function and is\noperator to check your understanding of object identity after each statement.",
          "level": -1,
          "page": 167,
          "reading_order": 4,
          "bbox": [
            97,
            394,
            584,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_167_order_5",
      "label": "sec",
      "text": "Variable Scope",
      "level": 1,
      "page": 167,
      "reading_order": 5,
      "bbox": [
        97,
        456,
        198,
        479
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_167_order_8",
          "label": "sub_sec",
          "text": "Caution!",
          "level": 2,
          "page": 167,
          "reading_order": 8,
          "bbox": [
            171,
            689,
            209,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_167_order_9",
              "label": "para",
              "text": "A function can create a new global variable, using the global declaration.\nHowever, this practice should be avoided as much as possible. Defining\nglobal variables inside a function introduces dependencies on context\nand limits the portability (or reusability) of the function. In general you\nshould use parameters for function inputs and return values for function\noutputs.",
              "level": -1,
              "page": 167,
              "reading_order": 9,
              "bbox": [
                171,
                707,
                530,
                798
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_167_order_10",
              "label": "foot",
              "text": "4.4 Functions: The Foundation of Structured Programming | 145",
              "level": -1,
              "page": 167,
              "reading_order": 10,
              "bbox": [
                315,
                824,
                585,
                842
              ],
              "section_number": "4.4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_167_order_6",
          "label": "para",
          "text": "Function definitions create a new local scope for variables. When you assign to a new\nvariable inside the body of a function, the name is defined only within that function.\nThe name is not visible outside the function, or in other functions. This behavior means\nyou can choose variable names without being concerned about collisions with names\nused in your other function definitions.",
          "level": -1,
          "page": 167,
          "reading_order": 6,
          "bbox": [
            97,
            483,
            585,
            565
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_167_order_7",
          "label": "para",
          "text": "When you refer to an existing name from within the body of a function, the Python\ninterpreter first tries to resolve the name with respect to the names that are local to the\nfunction. If nothing is found, the interpreter checks whether it is a global name within\nthe module. Finally, if that does not succeed, the interpreter checks whether the name\nis a Python built-in. This is the so-called LGB rule of name resolution: local, then\nglobal, then built-in.",
          "level": -1,
          "page": 167,
          "reading_order": 7,
          "bbox": [
            97,
            573,
            585,
            674
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_168_order_0",
      "label": "sec",
      "text": "Checking Parameter Types",
      "level": 1,
      "page": 168,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        273,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_168_order_1",
          "label": "para",
          "text": "Python does not force us to declare the type of a variable when we write a program,\nand this permits us to define functions that are flexible about the type of their argu-\nments. For example, a tagger might expect a sequence of words, but it wouldn't care\nwhether this sequence is expressed as a list, a tuple, or an iterator (a new sequence type\nthat we’ll discuss later).",
          "level": -1,
          "page": 168,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            181
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_168_order_2",
          "label": "para",
          "text": "However, often we want to write programs for later use by others, and want to program\nin a defensive style, providing useful warnings when functions have not been invoked\ncorrectly. The author of the following tag() function assumed that its argument would\nalways be a string.",
          "level": -1,
          "page": 168,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            584,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_168_order_4",
          "label": "para",
          "text": "The function returns sensible values for the arguments 'the' and 'knight' , but look\nwhat happens when it is passed a list ❶ —it fails to complain, even though the result\nwhich it returns is clearly incorrect. The author of this function could take some extra\nsteps to ensure that the word parameter of the tag() function is a string. A naive ap-\nproach would be to check the type of the argument using if not type(word) is str ,\nand if word is not a string, to simply return Python's special empty value, None . This is\na slight improvement, because the function is checking the type of the argument, and\ntrying to return a “special” diagnostic value for the wrong input. However, it is also\ndangerous because the calling program may not detect that None is intended as a “spe-\ncial” value, and this diagnostic return value may then be propagated to other parts of\nthe program with unpredictable consequences. This approach also fails if the word is\na Unicode string, which has type unicode , not str . Here’s a better solution, using an\nassert statement together with Python’s basestring type that generalizes over both\nunicode and str .",
          "level": -1,
          "page": 168,
          "reading_order": 4,
          "bbox": [
            97,
            430,
            586,
            657
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_168_order_6",
          "label": "para",
          "text": "If the assert statement fails, it will produce an error that cannot be ignored, since it\nhalts program execution. Additionally, the error message is easy to interpret. Adding",
          "level": -1,
          "page": 168,
          "reading_order": 6,
          "bbox": [
            97,
            752,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_168_order_7",
          "label": "foot",
          "text": "146 | Chapter 4: Writing Structured Programs",
          "level": -1,
          "page": 168,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "146",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_169_order_0",
          "label": "para",
          "text": "assertions to a program helps you find logical errors, and is a kind of defensive pro-\ngramming. A more fundamental approach is to document the parameters to each\nfunction using docstrings, as described later in this section.",
          "level": -1,
          "page": 169,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_169_order_1",
      "label": "sec",
      "text": "Functional Decomposition",
      "level": 1,
      "page": 169,
      "reading_order": 1,
      "bbox": [
        98,
        134,
        271,
        161
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_169_order_2",
          "label": "para",
          "text": "Well-structured programs usually make extensive use of functions. When a block of\nprogram code grows longer than 10–20 lines, it is a great help to readability if the code\nis broken up into one or more functions, each one having a clear purpose. This is\nanalogous to the way a good essay is divided into paragraphs, each expressing one main\nidea.",
          "level": -1,
          "page": 169,
          "reading_order": 2,
          "bbox": [
            97,
            161,
            586,
            243
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_169_order_3",
          "label": "para",
          "text": "Functions provide an important kind of abstraction. They allow us to group multiple\nactions into a single, complex action, and associate a name with it. (Compare this with\nthe way we combine the actions of go and bring back into a single more complex action\nfetch.) When we use functions, the main program can be written at a higher level of\nabstraction, making its structure transparent, as in the following:",
          "level": -1,
          "page": 169,
          "reading_order": 3,
          "bbox": [
            96,
            250,
            585,
            335
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_169_order_5",
          "label": "para",
          "text": "Appropriate use of functions makes programs more readable and maintainable. Addi-\ntionally, it becomes possible to reimplement a function—replacing the function’s body\nwith more efficient code—without having to be concerned with the rest of the program.",
          "level": -1,
          "page": 169,
          "reading_order": 5,
          "bbox": [
            97,
            392,
            585,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_169_order_6",
          "label": "para",
          "text": "Consider the freq_words function in Example 4-2. It updates the contents of a frequency\ndistribution that is passed in as a parameter, and it also prints a list of the n most\nfrequent words.",
          "level": -1,
          "page": 169,
          "reading_order": 6,
          "bbox": [
            97,
            448,
            585,
            495
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_169_order_7",
          "label": "para",
          "text": "Example 4-2. Poorly designed function to compute frequent words",
          "level": -1,
          "page": 169,
          "reading_order": 7,
          "bbox": [
            97,
            510,
            422,
            523
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_169_order_9",
          "label": "para",
          "text": "This function has a number of problems. The function has two side effects: it modifies\nthe contents of its second parameter, and it prints a selection of the results it has com-\nputed. The function would be easier to understand and to reuse elsewhere if we initialize\nthe FreqDist() object inside the function (in the same place it is populated), and if we\nmoved the selection and display of results to the calling program. In Example 4-3 we\nrefactor this function, and simplify its interface by providing a single url parameter.",
          "level": -1,
          "page": 169,
          "reading_order": 9,
          "bbox": [
            97,
            696,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_169_order_10",
          "label": "foot",
          "text": "4.4 Functions: The Foundation of Structured Programming | 147",
          "level": -1,
          "page": 169,
          "reading_order": 10,
          "bbox": [
            315,
            824,
            585,
            842
          ],
          "section_number": "4.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_170_order_0",
          "label": "cap",
          "text": "Example 4-3. Well-designed function to compute frequent words",
          "level": -1,
          "page": 170,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            413,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_170_order_2",
          "label": "para",
          "text": "Note that we have now simplified the work of freq_words to the point that we can do\nits work with three lines of code:",
          "level": -1,
          "page": 170,
          "reading_order": 2,
          "bbox": [
            97,
            248,
            584,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_170_order_4",
      "label": "sec",
      "text": "Documenting Functions",
      "level": 1,
      "page": 170,
      "reading_order": 4,
      "bbox": [
        100,
        366,
        255,
        385
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_171_order_5",
          "label": "sub_sec",
          "text": "4.5 Doing More with Functions",
          "level": 2,
          "page": 171,
          "reading_order": 5,
          "bbox": [
            97,
            519,
            345,
            546
          ],
          "section_number": "4.5",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_171_order_7",
              "label": "sub_sub_sec",
              "text": "Functions As Arguments",
              "level": 3,
              "page": 171,
              "reading_order": 7,
              "bbox": [
                98,
                600,
                261,
                620
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_171_order_8",
                  "label": "para",
                  "text": "So far the arguments we have passed into functions have been simple objects, such as\nstrings, or structured objects, such as lists. Python also lets us pass a function as an\nargument to another function. Now we can abstract out the operation, and apply a\ndifferent operation on the same data . As the following examples show, we can pass the\nbuilt-in function len() or a user-defined function last_letter() as arguments to an-\nother function:",
                  "level": -1,
                  "page": 171,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    627,
                    585,
                    725
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_171_order_10",
                  "label": "foot",
                  "text": "4.5 Doing More with Functions | 149",
                  "level": -1,
                  "page": 171,
                  "reading_order": 10,
                  "bbox": [
                    422,
                    824,
                    585,
                    842
                  ],
                  "section_number": "4.5",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_172_order_1",
                  "label": "para",
                  "text": "The objects len and last_letter can be passed around like lists and dictionaries. Notice\nthat parentheses are used after a function name only if we are invoking the function;\nwhen we are simply treating the function as an object, these are omitted.",
                  "level": -1,
                  "page": 172,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    161,
                    585,
                    208
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_172_order_2",
                  "label": "para",
                  "text": "Python provides us with one more way to define functions as arguments to other func-\ntions, so-called lambda expressions. Supposing there was no need to use the last_let\nter() function in multiple places, and thus no need to give it a name. Let’s suppose we\ncan equivalently write the following:",
                  "level": -1,
                  "page": 172,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    215,
                    585,
                    281
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_172_order_4",
                  "label": "para",
                  "text": "Our next example illustrates passing a function to the sorted() function. When we call\nthe latter with a single argument (the list to be sorted), it uses the built-in comparison\nfunction cmp(). However, we can supply our own sort function, e.g., to sort by de-\ncreasing length.",
                  "level": -1,
                  "page": 172,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    322,
                    584,
                    389
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_171_order_6",
              "label": "para",
              "text": "This section discusses more advanced features, which you may prefer to skip on the\nfirst time through this chapter.",
              "level": -1,
              "page": 171,
              "reading_order": 6,
              "bbox": [
                97,
                555,
                585,
                585
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_170_order_5",
          "label": "para",
          "text": "If we have done a good job at decomposing our program into functions, then it should\nbe easy to describe the purpose of each function in plain language, and provide this in\nthe docstring at the top of the function definition. This statement should not explain\nhow the functionality is implemented; in fact, it should be possible to reimplement the\nfunction using a different method without changing this statement.",
          "level": -1,
          "page": 170,
          "reading_order": 5,
          "bbox": [
            97,
            394,
            585,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_170_order_6",
          "label": "para",
          "text": "For the simplest functions, a one-line docstring is usually adequate (see Example 4 -1).\nYou should provide a triple-quoted string containing a complete sentence on a single\nline. For non-trivial functions, you should still provide a one-sentence summary on the\nfirst line, since many docstring processing tools index this string. This should be fol-\nlowed by a blank line, then a more detailed description of the functionality (see http://\nwww.python.org/dev/peps/pep-0257/ for more information on docstring conventions).",
          "level": -1,
          "page": 170,
          "reading_order": 6,
          "bbox": [
            97,
            483,
            585,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_170_order_7",
          "label": "para",
          "text": "Docstrings can include a doctest block , illustrating the use of the function and the\nexpected output. These can be tested automatically using Python's docutils module.\nDocstrings should document the type of each parameter to the function, and the return\ntype. At a minimum, that can be done in plain text. However, note that NLTK uses the\n“ epytext ” markup language to document parameters. This format can be automatically\nconverted into richly structured API documentation (see http://www.nltk.org/ ), and in-\ncludes special handling of certain “ fields, ” such as @param , which allow the inputs and\noutputs of functions to be clearly documented. Example 4-4 illustrates a complete\ndocstring.",
          "level": -1,
          "page": 170,
          "reading_order": 7,
          "bbox": [
            97,
            590,
            585,
            736
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_170_order_8",
          "label": "foot",
          "text": "148 | Chapter 4: Writing Structured Programs",
          "level": -1,
          "page": 170,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "148",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_171_order_0",
          "label": "para",
          "text": "Example 4-4. Illustration of a complete docstring, consisting of a one-line summary, a more detailed\nexplanation, a doctest example, and epytext markup specifying the parameters, types, return type,\nand exceptions.",
          "level": -1,
          "page": 171,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_171_order_2",
          "label": "para",
          "text": "Given a list of reference values and a corresponding list of test values,\nreturn the fraction of corresponding values that are equal.\nIn particular, return the fraction of indexes\n{0<i<=len(test)} such that C{test[i] == reference[i]}.\n>>> accuracy(['ADJ', 'N', 'V', 'N'], ['N', 'N', 'V', 'ADJ'])\n0.5",
          "level": -1,
          "page": 171,
          "reading_order": 2,
          "bbox": [
            118,
            178,
            513,
            261
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_172_order_6",
      "label": "sec",
      "text": "Accumulative Functions",
      "level": 1,
      "page": 172,
      "reading_order": 6,
      "bbox": [
        97,
        528,
        255,
        546
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_172_order_7",
          "label": "para",
          "text": "These functions start by initializing some storage, and iterate over input to build it up,\nbefore returning some final object (a large structure or aggregated result). A standard\nway to do this is to initialize an empty list, accumulate the material, then return the\nlist, as shown in function search1() in Example 4-5.",
          "level": -1,
          "page": 172,
          "reading_order": 7,
          "bbox": [
            97,
            555,
            585,
            619
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_172_order_8",
          "label": "cap",
          "text": "Example 4-5. Accumulating output into a list.",
          "level": -1,
          "page": 172,
          "reading_order": 8,
          "bbox": [
            97,
            634,
            324,
            647
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_172_order_11",
          "label": "foot",
          "text": "150 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 172,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "150",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_173_order_1",
          "label": "para",
          "text": "The function search2() is a generator. The first time this function is called, it gets as\nfar as the yield statement and pauses. The calling program gets the first word and does\nany necessary processing. Once the calling program is ready for another word, execu-\ntion of the function is continued from where it stopped, until the next time it encounters\na yield statement. This approach is typically more efficient, as the function only gen-\nerates the data as it is required by the calling program, and does not need to allocate\nadditional memory to store the output (see the earlier discussion of generator expres-\nsions).",
          "level": -1,
          "page": 173,
          "reading_order": 1,
          "bbox": [
            97,
            178,
            585,
            306
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_173_order_2",
          "label": "para",
          "text": "Here’s a more sophisticated example of a generator which produces all permutations\nof a list of words. In order to force the permutations() function to generate all its output,\nwe wrap it with a call to list() ❶.",
          "level": -1,
          "page": 173,
          "reading_order": 2,
          "bbox": [
            97,
            313,
            585,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_173_order_4",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_173_figure_004.png)",
          "level": -1,
          "page": 173,
          "reading_order": 4,
          "bbox": [
            118,
            546,
            162,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_173_order_5",
          "label": "para",
          "text": "The permutations function uses a technique called recursion, discussed\nlater in Section 4.7 . The ability to generate permutations of a set of words\nis useful for creating data to test a grammar ( Chapter 8 ).",
          "level": -1,
          "page": 173,
          "reading_order": 5,
          "bbox": [
            171,
            555,
            530,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_173_order_6",
      "label": "sec",
      "text": "Higher-Order Functions",
      "level": 1,
      "page": 173,
      "reading_order": 6,
      "bbox": [
        98,
        627,
        253,
        649
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_173_order_7",
          "label": "para",
          "text": "Python provides some higher-order functions that are standard features of functional\nprogramming languages such as Haskell. We illustrate them here, alongside the equiv-\nalent expression using list comprehensions.",
          "level": -1,
          "page": 173,
          "reading_order": 7,
          "bbox": [
            97,
            654,
            585,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_173_order_8",
          "label": "para",
          "text": "Let's start by defining a function is_content_word() which checks whether a word is\nfrom the open class of content words. We use this function as the first parameter of\nfilter() , which applies the function to each item in the sequence contained in its\nsecond parameter, and retains only the items for which the function returns True .",
          "level": -1,
          "page": 173,
          "reading_order": 8,
          "bbox": [
            97,
            714,
            585,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_173_order_9",
          "label": "foot",
          "text": "4.5 Doing More with Functions | 151",
          "level": -1,
          "page": 173,
          "reading_order": 9,
          "bbox": [
            422,
            824,
            584,
            842
          ],
          "section_number": "4.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_174_order_1",
          "label": "para",
          "text": "Another higher-order function is map() , which applies a function to every item in a\nsequence. It is a general version of the extract_property() function we saw earlier in\nthis section. Here is a simple way to find the average length of a sentence in the news\nsection of the Brown Corpus, followed by an equivalent version with list comprehen-\nsion calculation:",
          "level": -1,
          "page": 174,
          "reading_order": 1,
          "bbox": [
            97,
            187,
            584,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_174_order_3",
          "label": "para",
          "text": "In the previous examples, we specified a user-defined function is_content_word() and\na built-in function len(). We can also provide a lambda expression. Here’s a pair of\nequivalent examples that count the number of vowels in each word.",
          "level": -1,
          "page": 174,
          "reading_order": 3,
          "bbox": [
            97,
            358,
            586,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_174_order_5",
          "label": "para",
          "text": "The solutions based on list comprehensions are usually more readable than the solu-\ntions based on higher-order functions, and we have favored the former approach\nthroughout this book.",
          "level": -1,
          "page": 174,
          "reading_order": 5,
          "bbox": [
            97,
            480,
            584,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_174_order_6",
      "label": "sec",
      "text": "Named Arguments",
      "level": 1,
      "page": 174,
      "reading_order": 6,
      "bbox": [
        98,
        537,
        225,
        564
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_176_order_1",
          "label": "sub_sub_sec",
          "text": "Caution!",
          "level": 3,
          "page": 176,
          "reading_order": 1,
          "bbox": [
            171,
            152,
            209,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_176_order_2",
              "label": "para",
              "text": "Take care not to use a mutable object as the default value of a parameter.\nA series of calls to the function will use the same object, sometimes with\nbizarre results, as we will see in the discussion of debugging later.",
              "level": -1,
              "page": 176,
              "reading_order": 2,
              "bbox": [
                171,
                170,
                530,
                217
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_176_order_3",
          "label": "sub_sec",
          "text": "4.6 Program Development",
          "level": 2,
          "page": 176,
          "reading_order": 3,
          "bbox": [
            97,
            241,
            315,
            268
          ],
          "section_number": "4.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_176_order_4",
              "label": "para",
              "text": "Programming is a skill that is acquired over several years of experience with a variety\nof programming languages and tasks. Key high-level abilities are algorithm design and\nits manifestation in structured programming . Key low-level abilities include familiarity\nwith the syntactic constructs of the language, and knowledge of a variety of diagnostic\nmethods for trouble-shooting a program which does not exhibit the expected behavior.",
              "level": -1,
              "page": 176,
              "reading_order": 4,
              "bbox": [
                97,
                268,
                585,
                358
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_176_order_5",
              "label": "para",
              "text": "This section describes the internal structure of a program module and how to organize\na multi-module program. Then it describes various kinds of error that arise during\nprogram development, what you can do to fix them and, better still, to avoid them in\nthe first place.",
              "level": -1,
              "page": 176,
              "reading_order": 5,
              "bbox": [
                97,
                358,
                585,
                430
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_174_order_7",
          "label": "para",
          "text": "When there are a lot of parameters it is easy to get confused about the correct order.\nInstead we can refer to parameters by name, and even assign them a default value just\nin case one was not provided by the calling program. Now the parameters can be speci-\nfied in any order, and can be omitted.",
          "level": -1,
          "page": 174,
          "reading_order": 7,
          "bbox": [
            97,
            564,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_174_order_9",
          "label": "para",
          "text": "These are called keyword arguments. If we mix these two kinds of parameters, then\nwe must ensure that the unnamed parameters precede the named ones. It has to be this",
          "level": -1,
          "page": 174,
          "reading_order": 9,
          "bbox": [
            97,
            752,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_174_order_10",
          "label": "foot",
          "text": "152 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 174,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "152",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_175_order_0",
          "label": "para",
          "text": "way, since unnamed parameters are defined by position. We can define a function that\ntakes an arbitrary number of unnamed and named parameters, and access them via an\nin-place list of arguments *args and an in-place dictionary of keyword arguments\n**kwargs.",
          "level": -1,
          "page": 175,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            139
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_175_order_2",
          "label": "para",
          "text": "When *args appears as a function parameter, it actually corresponds to all the unnamed\nparameters of the function. As another illustration of this aspect of Python syntax,\nconsider the zip() function, which operates on a variable number of arguments. We’ll\nuse the variable name *song to demonstrate that there’s nothing special about the name\n*args.",
          "level": -1,
          "page": 175,
          "reading_order": 2,
          "bbox": [
            97,
            241,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_175_order_4",
          "label": "para",
          "text": "It should be clear from this example that typing *song is just a convenient shorthand,\nand equivalent to typing out song[0], song[1], song[2].",
          "level": -1,
          "page": 175,
          "reading_order": 4,
          "bbox": [
            97,
            430,
            584,
            467
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_175_order_5",
          "label": "para",
          "text": "Here’s another example of the use of keyword arguments in a function definition, along\nwith three equivalent ways to call the function:",
          "level": -1,
          "page": 175,
          "reading_order": 5,
          "bbox": [
            97,
            474,
            585,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_175_order_7",
          "label": "para",
          "text": "A side effect of having named arguments is that they permit optionality. Thus we can\nleave out any arguments where we are happy with the default value:\nfreq_words('ch01.rst', min=4), freq_words('ch01.rst', 4). Another common use of\noptional arguments is to permit a flag. Here’s a revised version of the same function\nthat reports its progress if a verbose flag is set:",
          "level": -1,
          "page": 175,
          "reading_order": 7,
          "bbox": [
            97,
            627,
            584,
            708
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_175_order_9",
          "label": "foot",
          "text": "4.5 Doing More with Functions | 153",
          "level": -1,
          "page": 175,
          "reading_order": 9,
          "bbox": [
            422,
            824,
            584,
            842
          ],
          "section_number": "4.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_176_order_6",
      "label": "sec",
      "text": "Structure of a Python Module",
      "level": 1,
      "page": 176,
      "reading_order": 6,
      "bbox": [
        97,
        439,
        297,
        465
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_176_order_7",
          "label": "para",
          "text": "The purpose of a program module is to bring logically related definitions and functions\ntogether in order to facilitate reuse and abstraction. Python modules are nothing more\nthan individual .py files. For example, if you were working with a particular corpus\nformat, the functions to read and write the format could be kept together. Constants\nused by both formats, such as field separators, or a EXTN = \".inf\" filename extension,\ncould be shared. If the format was updated, you would know that only one file needed\nto be changed. Similarly, a module could contain code for creating and manipulating\na particular data structure such as syntax trees, or code for performing a particular\nprocessing task such as plotting corpus statistics.",
          "level": -1,
          "page": 176,
          "reading_order": 7,
          "bbox": [
            97,
            472,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_176_order_8",
          "label": "para",
          "text": "When you start writing Python modules, it helps to have some examples to emulate.\nYou can locate the code for any NLTK module on your system using the __file__\nvariable:",
          "level": -1,
          "page": 176,
          "reading_order": 8,
          "bbox": [
            97,
            627,
            585,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_176_order_10",
          "label": "para",
          "text": "This returns the location of the compiled .pyc file for the module, and you’ll probably\nsee a different location on your machine. The file that you will need to open is the\ncorresponding .py source file, and this will be in the same directory as the .pyc file.",
          "level": -1,
          "page": 176,
          "reading_order": 10,
          "bbox": [
            97,
            716,
            585,
            765
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_176_order_11",
          "label": "foot",
          "text": "154 | Chapter 4: Writing Structured Programs",
          "level": -1,
          "page": 176,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "154",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_177_order_0",
          "label": "para",
          "text": "Alternatively, you can view the latest version of this module on the Web at http://code\n.google.com/p/nltk/source/browse/trunk/nltk/nltk/metrics/distance.py.",
          "level": -1,
          "page": 177,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_177_order_1",
          "label": "para",
          "text": "Like every other NLTK module, distance.py begins with a group of comment lines giving\na one-line title of the module and identifying the authors. (Since the code is distributed,\nit also includes the URL where the code is available, a copyright statement, and license\ninformation.) Next is the module-level docstring, a triple-quoted multiline string con-\ntaining information about the module that will be printed when someone types\nhelp(nltk.metrics.distance).",
          "level": -1,
          "page": 177,
          "reading_order": 1,
          "bbox": [
            97,
            115,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_177_order_6",
          "label": "para",
          "text": "After this comes all the import statements required for the module, then any global\nvariables, followed by a series of function definitions that make up most of the module.\nOther modules define “classes,” the main building blocks of object-oriented program-\nming, which falls outside the scope of this book. (Most NLTK modules also include a\ndemo() function, which can be used to see examples of the module in use.)",
          "level": -1,
          "page": 177,
          "reading_order": 6,
          "bbox": [
            97,
            483,
            585,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_177_order_7",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_177_figure_007.png)",
          "level": -1,
          "page": 177,
          "reading_order": 7,
          "bbox": [
            118,
            582,
            171,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_177_order_8",
          "label": "para",
          "text": "Some module variables and functions are only used within the module.\nThese should have names beginning with an underscore, e.g.,\n_helper() , since this will hide the name. If another module imports this\none, using the idiom: from module import *, these names will not be\nimported. You can optionally list the externally accessible names of a\nmodule using a special built-in variable like this: __all__ = ['edit_dis\ntance', 'jaccard_distance'] .",
          "level": -1,
          "page": 177,
          "reading_order": 8,
          "bbox": [
            171,
            591,
            530,
            699
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_177_order_9",
      "label": "sec",
      "text": "Multimodule Programs",
      "level": 1,
      "page": 177,
      "reading_order": 9,
      "bbox": [
        100,
        724,
        252,
        743
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_177_order_10",
          "label": "para",
          "text": "Some programs bring together a diverse range of tasks, such as loading data from a\ncorpus, performing some analysis tasks on the data, then visualizing it. We may already",
          "level": -1,
          "page": 177,
          "reading_order": 10,
          "bbox": [
            97,
            743,
            585,
            782
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_177_order_11",
          "label": "foot",
          "text": "4.6 Program Development | 155",
          "level": -1,
          "page": 177,
          "reading_order": 11,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "4.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_178_order_0",
          "label": "para",
          "text": "have stable modules that take care of loading data and producing visualizations. Our\nwork might involve coding up the analysis task, and just invoking functions from the\nexisting modules. This scenario is depicted in Figure 4-2 .",
          "level": -1,
          "page": 178,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_178_order_1",
          "label": "figure",
          "text": "Figure 4-2. Structure of a multimodule program: The main program my_program.py imports\nfunctions from two other modules; unique analysis tasks are localized to the main program, while\ncommon loading and visualization tasks are kept apart to facilitate reuse and abstraction. [IMAGE: ![Figure](figures/NLTK_page_178_figure_001.png)]",
          "level": -1,
          "page": 178,
          "reading_order": 1,
          "bbox": [
            100,
            143,
            583,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_178_figure_001.png)",
              "bbox": [
                100,
                143,
                583,
                439
              ],
              "page": 178,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Figure 4-2. Structure of a multimodule program: The main program my_program.py imports\nfunctions from two other modules; unique analysis tasks are localized to the main program, while\ncommon loading and visualization tasks are kept apart to facilitate reuse and abstraction.",
              "bbox": [
                96,
                448,
                585,
                494
              ],
              "page": 178,
              "reading_order": 2
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_178_order_3",
          "label": "para",
          "text": "By dividing our work into several modules and using import statements to access func-\ntions defined elsewhere, we can keep the individual modules simple and easy to main-\ntain. This approach will also result in a growing collection of modules, and make it\npossible for us to build sophisticated systems involving a hierarchy of modules. De-\nsigning such systems well is a complex software engineering task, and beyond the scope\nof this book.",
          "level": -1,
          "page": 178,
          "reading_order": 3,
          "bbox": [
            97,
            528,
            585,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_178_order_4",
      "label": "sec",
      "text": "Sources of Error",
      "level": 1,
      "page": 178,
      "reading_order": 4,
      "bbox": [
        97,
        636,
        201,
        656
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_178_order_5",
          "label": "para",
          "text": "Mastery of programming depends on having a variety of problem-solving skills to draw\nupon when the program doesn’t work as expected. Something as trivial as a misplaced\nsymbol might cause the program to behave very differently. We call these “bugs” be-\ncause they are tiny in comparison to the damage they can cause. They creep into our\ncode unnoticed, and it’s only much later when we’re running the program on some\nnew data that their presence is detected. Sometimes, fixing one bug only reveals an-\nother, and we get the distinct impression that the bug is on the move. The only reas-\nsurance we have is that bugs are spontaneous and not the fault of the programmer.",
          "level": -1,
          "page": 178,
          "reading_order": 5,
          "bbox": [
            97,
            663,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_178_order_6",
          "label": "foot",
          "text": "156 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 178,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "156",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_179_order_0",
          "label": "para",
          "text": "Flippancy aside, debugging code is hard because there are so many ways for it to be\nfaulty. Our understanding of the input data, the algorithm, or even the programming\nlanguage, may be at fault. Let’s look at examples of each of these.",
          "level": -1,
          "page": 179,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_179_order_1",
          "label": "para",
          "text": "First, the input data may contain some unexpected characters. For example, WordNet\nsynset names have the form tree.n.01 , with three components separated using periods.\nThe NLTK WordNet module initially decomposed these names using split('.') .\nHowever, this method broke when someone tried to look up the word PhD , which has\nthe synset name ph.d..n.01 , containing four periods instead of the expected two. The\nsolution was to use rsplit('.', 2) to do at most two splits, using the rightmost in-\nstances of the period, and leaving the ph.d. string intact. Although several people had\ntested the module before it was released, it was some weeks before someone detected\nthe problem (see http://code.google.com/p/nltk/issues/detail?id=297 ).",
          "level": -1,
          "page": 179,
          "reading_order": 1,
          "bbox": [
            97,
            132,
            585,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_179_order_2",
          "label": "para",
          "text": "Second, a supplied function might not behave as expected. For example, while testing\nNLTK's interface to WordNet, one of the authors noticed that no synsets had any\nantonyms defined, even though the underlying database provided a large quantity of\nantonym information. What looked like a bug in the WordNet interface turned out to\nbe a misunderstanding about WordNet itself: antonyms are defined for lemmas, not\nfor synsets. The only “bug” was a misunderstanding of the interface (see http://code\n.google.com/p/nltk/issues/detail?id=98 ).",
          "level": -1,
          "page": 179,
          "reading_order": 2,
          "bbox": [
            97,
            286,
            586,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_179_order_3",
          "label": "para",
          "text": "Third, our understanding of Python's semantics may be at fault. It is easy to make the\nwrong assumption about the relative scope of two operators. For example, \"%s.%s.\n%02d\" % \"ph.d.\", \"n\", 1 produces a runtime error TypeError: not enough arguments\nfor format string . This is because the percent operator has higher precedence than\nthe comma operator. The fix is to add parentheses in order to force the required scope.\nAs another example, suppose we are defining a function to collect all tokens of a text\nhaving a given length. The function has parameters for the text and the word length,\nand an extra parameter that allows the initial value of the result to be given as a\nparameter:",
          "level": -1,
          "page": 179,
          "reading_order": 3,
          "bbox": [
            97,
            410,
            585,
            557
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_179_order_5",
          "label": "para",
          "text": "The first time we call find_words() ❶ , we get all three-letter words as expected. The\nsecond time we specify an initial value for the result, a one-element list ['ur'] , and as\nexpected, the result has this word along with the other two-letter word in our text.\nNow, the next time we call find_words() ❸ we use the same parameters as in ❹ , but\nwe get a different result! Each time we call find_words() with no third parameter, the",
          "level": -1,
          "page": 179,
          "reading_order": 5,
          "bbox": [
            97,
            716,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_179_order_6",
          "label": "foot",
          "text": "4.6 Program Development | 157",
          "level": -1,
          "page": 179,
          "reading_order": 6,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "4.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_180_order_0",
          "label": "para",
          "text": "result will simply extend the result of the previous call, rather than start with the empty\nresult list as specified in the function definition. The program’s behavior is not as ex-\npected because we incorrectly assumed that the default value was created at the time\nthe function was invoked. However, it is created just once, at the time the Python\ninterpreter loads the function. This one list object is used whenever no explicit value\nis provided to the function.",
          "level": -1,
          "page": 180,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_180_order_1",
      "label": "sec",
      "text": "Debugging Techniques",
      "level": 1,
      "page": 180,
      "reading_order": 1,
      "bbox": [
        98,
        188,
        252,
        207
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_180_order_2",
          "label": "para",
          "text": "Since most code errors result from the programmer making incorrect assumptions, the\nfirst thing to do when you detect a bug is to check your assumptions. Localize the prob-\nlem by adding print statements to the program, showing the value of important vari-\nables, and showing how far the program has progressed.",
          "level": -1,
          "page": 180,
          "reading_order": 2,
          "bbox": [
            97,
            215,
            585,
            279
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_180_order_3",
          "label": "para",
          "text": "If the program produced an “exception”—a runtime error—the interpreter will print\na stack trace, pinpointing the location of program execution at the time of the error.\nIf the program depends on input data, try to reduce this to the smallest size while still\nproducing the error.",
          "level": -1,
          "page": 180,
          "reading_order": 3,
          "bbox": [
            97,
            286,
            585,
            352
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_180_order_4",
          "label": "para",
          "text": "Once you have localized the problem to a particular function or to a line of code, you\nneed to work out what is going wrong. It is often helpful to recreate the situation using\nthe interactive command line. Define some variables, and then copy-paste the offending\nline of code into the session and see what happens. Check your understanding of the\ncode by reading some documentation and examining other code samples that purport\nto do the same thing that you are trying to do. Try explaining your code to someone\nelse, in case she can see where things are going wrong.",
          "level": -1,
          "page": 180,
          "reading_order": 4,
          "bbox": [
            97,
            358,
            585,
            475
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_180_order_5",
          "label": "para",
          "text": "Python provides a debugger which allows you to monitor the execution of your pro-\ngram, specify line numbers where execution will stop (i.e., breakpoints), and step\nthrough sections of code and inspect the value of variables. You can invoke the debug-\nger on your code as follows:",
          "level": -1,
          "page": 180,
          "reading_order": 5,
          "bbox": [
            97,
            483,
            585,
            548
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_180_order_7",
          "label": "para",
          "text": "It will present you with a prompt (Pdb) where you can type instructions to the debugger.\nType help to see the full list of commands. Typing step (or just s) will execute the\ncurrent line and stop. If the current line calls a function, it will enter the function and\nstop at the first line. Typing next (or just n) is similar, but it stops execution at the next\nline in the current function. The break (or b) command can be used to create or list\nbreakpoints. Type continue (or c) to continue execution as far as the next breakpoint.\nType the name of any variable to inspect its value.",
          "level": -1,
          "page": 180,
          "reading_order": 7,
          "bbox": [
            97,
            600,
            585,
            718
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_180_order_8",
          "label": "para",
          "text": "We can use the Python debugger to locate the problem in our find_words() function.\nRemember that the problem arose the second time the function was called. We’ll start\nby calling the function without using the debugger ❶, using the smallest possible input.\nThe second time, we’ll call it with the debugger ❷.",
          "level": -1,
          "page": 180,
          "reading_order": 8,
          "bbox": [
            97,
            725,
            585,
            791
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_180_order_9",
          "label": "foot",
          "text": "158 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 180,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "158",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_181_order_1",
          "label": "para",
          "text": "Here we typed just two commands into the debugger: step took us inside the function,\nand args showed the values of its arguments (or parameters). We see immediately that\nresult has an initial value of ['cat'], and not the empty list as expected. The debugger\nhas helped us to localize the problem, prompting us to check our understanding of\nPython functions.",
          "level": -1,
          "page": 181,
          "reading_order": 1,
          "bbox": [
            97,
            239,
            585,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_181_order_2",
      "label": "sec",
      "text": "Defensive Programming",
      "level": 1,
      "page": 181,
      "reading_order": 2,
      "bbox": [
        98,
        331,
        261,
        354
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_182_order_2",
          "label": "sub_sec",
          "text": "4.7 Algorithm Design",
          "level": 2,
          "page": 182,
          "reading_order": 2,
          "bbox": [
            97,
            222,
            270,
            245
          ],
          "section_number": "4.7",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_182_order_8",
              "label": "sub_sub_sec",
              "text": "Recursion",
              "level": 3,
              "page": 182,
              "reading_order": 8,
              "bbox": [
                98,
                689,
                162,
                707
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_182_order_9",
                  "label": "para",
                  "text": "The earlier examples of sorting and searching have a striking property: to solve a prob-\nlem of size n, we have to break it in half and then work on one or more problems of\nsize n/2. A common way to implement such methods uses recursion . We define a\nfunction f, which simplifies the problem, and calls itself to solve one or more easier",
                  "level": -1,
                  "page": 182,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    716,
                    585,
                    783
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_182_order_10",
                  "label": "foot",
                  "text": "160 | Chapter4: Writing Structured Programs",
                  "level": -1,
                  "page": 182,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    297,
                    842
                  ],
                  "section_number": "160",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_183_order_0",
                  "label": "figure",
                  "text": "Figure 4-3. Sorting by divide-and-conquer: To sort an array, we split it in half and sort each half\n(recursively); we merge each sorted half back into a whole list (again recursively); this algorithm is\nknown as “ Merge Sort. ” [IMAGE: ![Figure](figures/NLTK_page_183_figure_000.png)]",
                  "level": -1,
                  "page": 183,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    583,
                    331
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_183_figure_000.png)",
                      "bbox": [
                        100,
                        71,
                        583,
                        331
                      ],
                      "page": 183,
                      "reading_order": 0
                    },
                    {
                      "label": "cap",
                      "text": "Figure 4-3. Sorting by divide-and-conquer: To sort an array, we split it in half and sort each half\n(recursively); we merge each sorted half back into a whole list (again recursively); this algorithm is\nknown as “ Merge Sort. ”",
                      "bbox": [
                        97,
                        339,
                        585,
                        380
                      ],
                      "page": 183,
                      "reading_order": 1
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_183_order_2",
                  "label": "para",
                  "text": "instances of the same problem. It then combines the results into a solution for the\noriginal problem.",
                  "level": -1,
                  "page": 183,
                  "reading_order": 2,
                  "bbox": [
                    100,
                    391,
                    585,
                    422
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_183_order_3",
                  "label": "para",
                  "text": "For example, suppose we have a set of n words, and want to calculate how many dif-\nferent ways they can be combined to make a sequence of words. If we have only one\nword (n=1), there is just one way to make it into a sequence. If we have a set of two\nwords, there are two ways to put them into a sequence. For three words there are six\npossibilities. In general, for n words, there are n × n -1 × ... × 2 × 1 ways (i.e., the factorial\nof n ). We can code this up as follows:",
                  "level": -1,
                  "page": 183,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    430,
                    585,
                    528
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_183_order_5",
                  "label": "para",
                  "text": "However, there is also a recursive algorithm for solving this problem, based on the\nfollowing observation. Suppose we have a way to construct all orderings for $n$ -1 distinct\nwords. Then for each such ordering, there are $n$ places where we can insert a new word:\nat the start, the end, or any of the $n$ -2 boundaries between the words. Thus we simply\nmultiply the number of solutions found for $n$ -1 by the value of $n$ . We also need the\nbase case , to say that if we have a single word, there's just one ordering. We can code\nthis up as follows:",
                  "level": -1,
                  "page": 183,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    609,
                    585,
                    725
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_183_order_7",
                  "label": "foot",
                  "text": "4.7 Algorithm Design | 161",
                  "level": -1,
                  "page": 183,
                  "reading_order": 7,
                  "bbox": [
                    464,
                    824,
                    584,
                    842
                  ],
                  "section_number": "4.7",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_184_order_0",
                  "label": "para",
                  "text": "These two algorithms solve the same problem. One uses iteration while the other uses\nrecursion. We can use recursion to navigate a deeply nested object, such as the Word-\nNet hypernym hierarchy. Let’s count the size of the hypernym hierarchy rooted at a\ngiven synset s. We’ll do this by finding the size of each hyponym of s, then adding these\ntogether (we will also add 1 for the synset itself). The following function size1() does\nthis work; notice that the body of the function includes a recursive call to size1():",
                  "level": -1,
                  "page": 184,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    172
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_184_order_2",
                  "label": "para",
                  "text": "We can also design an iterative solution to this problem which processes the hierarchy\nin layers. The first layer is the synset itself ❶, then all the hyponyms of the synset, then\nall the hyponyms of the hyponyms. Each time through the loop it computes the next\nlayer by finding the hyponyms of everything in the last layer ❸. It also maintains a total\nof the number of synsets encountered so far ❷.",
                  "level": -1,
                  "page": 184,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    215,
                    585,
                    296
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_184_order_4",
                  "label": "para",
                  "text": "Not only is the iterative solution much longer, it is harder to interpret. It forces us to\nthink procedurally, and keep track of what is happening with the layer and total\nvariables through time. Let's satisfy ourselves that both solutions give the same result.\nWe’ll use a new form of the import statement, allowing us to abbreviate the name\nwordnet to wn:",
                  "level": -1,
                  "page": 184,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    403,
                    585,
                    483
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_184_order_6",
                  "label": "para",
                  "text": "As a final example of recursion, let's use it to construct a deeply nested object. A letter\ntrie is a data structure that can be used for indexing a lexicon, one letter at a time. (The\nname is based on the word retrieval.) For example, if trie contained a letter trie, then\ntrie['c'] would be a smaller trie which held all words starting with c. Example 4-6\ndemonstrates the recursive process of building a trie, using Python dictionaries ( Sec-\ntion 5.3 ). To insert the word chien (French for dog ), we split off the c and recursively\ninsert hien into the sub-trie trie['c'] . The recursion continues until there are no letters\nremaining in the word, when we store the intended value (in this case, the word dog ).",
                  "level": -1,
                  "page": 184,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    580,
                    585,
                    710
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_184_order_7",
                  "label": "foot",
                  "text": "162 | Chapter4: Writing Structured Programs",
                  "level": -1,
                  "page": 184,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    824,
                    297,
                    842
                  ],
                  "section_number": "162",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_185_order_0",
                  "label": "para",
                  "text": "Example 4-6. Building a letter trie: A recursive function that builds a nested dictionary structure;\neach level of nesting contains all words with a given prefix, and a sub-trie containing all possible\ncontinuations.",
                  "level": -1,
                  "page": 185,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    116
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_182_order_3",
              "label": "para",
              "text": "This section discusses more advanced concepts, which you may prefer to skip on the\nfirst time through this chapter.",
              "level": -1,
              "page": 182,
              "reading_order": 3,
              "bbox": [
                97,
                250,
                585,
                286
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_182_order_4",
              "label": "para",
              "text": "A major part of algorithmic problem solving is selecting or adapting an appropriate\nalgorithm for the problem at hand. Sometimes there are several alternatives, and choos-\ning the best one depends on knowledge about how each alternative performs as the size\nof the data grows. Whole books are written on this topic, and we only have space to\nintroduce some key concepts and elaborate on the approaches that are most prevalent\nin natural language processing.",
              "level": -1,
              "page": 182,
              "reading_order": 4,
              "bbox": [
                97,
                293,
                585,
                394
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_182_order_5",
              "label": "para",
              "text": "The best-known strategy is known as divide-and-conquer. We attack a problem of\nsize n by dividing it into two problems of size n/2, solve these problems, and combine\ntheir results into a solution of the original problem. For example, suppose that we had\na pile of cards with a single word written on each card. We could sort this pile by\nsplitting it in half and giving it to two other people to sort (they could do the same in\nturn). Then, when two sorted piles come back, it is an easy task to merge them into a\nsingle sorted pile. See Figure 4-3 for an illustration of this process.",
              "level": -1,
              "page": 182,
              "reading_order": 5,
              "bbox": [
                97,
                394,
                586,
                513
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_182_order_6",
              "label": "para",
              "text": "Another example is the process of looking up a word in a dictionary. We open the book\nsomewhere around the middle and compare our word with the current page. If it’s\nearlier in the dictionary, we repeat the process on the first half; if it’s later, we use the\nsecond half. This search method is called binary search since it splits the problem in\nhalf at every step.",
              "level": -1,
              "page": 182,
              "reading_order": 6,
              "bbox": [
                97,
                519,
                585,
                603
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_182_order_7",
              "label": "para",
              "text": "In another approach to algorithm design, we attack a problem by transforming it into\nan instance of a problem we already know how to solve. For example, in order to detect\nduplicate entries in a list, we can pre-sort the list, then scan through it once to check\nwhether any adjacent pairs of elements are identical.",
              "level": -1,
              "page": 182,
              "reading_order": 7,
              "bbox": [
                97,
                609,
                585,
                680
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_185_order_3",
          "label": "sub_sec",
          "text": "Caution!",
          "level": 2,
          "page": 185,
          "reading_order": 3,
          "bbox": [
            171,
            421,
            209,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_185_order_4",
              "label": "para",
              "text": "Despite the simplicity of recursive programming, it comes with a cost.\nEach time a function is called, some state information needs to be push-\ned on a stack, so that once the function has completed, execution can\ncontinue from where it left off. For this reason, iterative solutions are\noften more efficient than recursive solutions.",
              "level": -1,
              "page": 185,
              "reading_order": 4,
              "bbox": [
                171,
                439,
                530,
                512
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_181_order_3",
          "label": "para",
          "text": "In order to avoid some of the pain of debugging, it helps to adopt some defensive\nprogramming habits. Instead of writing a 20-line program and then testing it, build the\nprogram bottom-up out of small pieces that are known to work. Each time you combine\nthese pieces to make a larger unit, test it carefully to see that it works as expected.\nConsider adding assert statements to your code, specifying properties of a variable,\ne.g., assert( isinstance(text, list)). If the value of the text variable later becomes a\nstring when your code is used in some larger context, this will raise an\nAssertionError and you will get immediate notification of the problem.",
          "level": -1,
          "page": 181,
          "reading_order": 3,
          "bbox": [
            97,
            358,
            585,
            493
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_181_order_4",
          "label": "para",
          "text": "Once you think you’ve found the bug, view your solution as a hypothesis. Try to predict\nthe effect of your bugfix before re-running the program. If the bug isn’t fixed, don’t fall\ninto the trap of blindly changing the code in the hope that it will magically start working\nagain. Instead, for each change, try to articulate a hypothesis about what is wrong and\nwhy the change will fix the problem. Then undo the change if the problem was not\nresolved.",
          "level": -1,
          "page": 181,
          "reading_order": 4,
          "bbox": [
            97,
            501,
            585,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_181_order_5",
          "label": "para",
          "text": "As you develop your program, extend its functionality, and fix any bugs, it helps to\nmaintain a suite of test cases. This is called regression testing , since it is meant to\ndetect situations where the code “ regresses ”— where a change to the code has an un-\nintended side effect of breaking something that used to work. Python provides a simple\nregression-testing framework in the form of the doctest module. This module searches\na file of code or documentation for blocks of text that look like an interactive Python\nsession, of the form you have already seen many times in this book. It executes the\nPython commands it finds, and tests that their output matches the output supplied in\nthe original file. Whenever there is a mismatch, it reports the expected and actual val-\nues. For details, please consult the doctest documentation at",
          "level": -1,
          "page": 181,
          "reading_order": 5,
          "bbox": [
            97,
            608,
            585,
            771
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_181_order_6",
          "label": "foot",
          "text": "4.6 Program Development | 159",
          "level": -1,
          "page": 181,
          "reading_order": 6,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "4.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_182_order_0",
          "label": "para",
          "text": "http://docs.python.org/library/doctest.html. Apart from its value for regression testing,\nthe doctest module is useful for ensuring that your software documentation stays in\nsync with your code.",
          "level": -1,
          "page": 182,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_182_order_1",
          "label": "para",
          "text": "Perhaps the most important defensive programming strategy is to set out your code\nclearly, choose meaningful variable and function names, and simplify the code wher-\never possible by decomposing it into functions and modules with well-documented\ninterfaces.",
          "level": -1,
          "page": 182,
          "reading_order": 1,
          "bbox": [
            97,
            132,
            585,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_185_order_5",
      "label": "sec",
      "text": "Space-Time Trade-offs",
      "level": 1,
      "page": 185,
      "reading_order": 5,
      "bbox": [
        97,
        537,
        246,
        559
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_185_order_6",
          "label": "para",
          "text": "We can sometimes significantly speed up the execution of a program by building an\nauxiliary data structure, such as an index. The listing in Example 4-7 implements a\nsimple text retrieval system for the Movie Reviews Corpus. By indexing the document\ncollection, it provides much faster lookup.",
          "level": -1,
          "page": 185,
          "reading_order": 6,
          "bbox": [
            97,
            564,
            585,
            631
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_185_order_7",
          "label": "para",
          "text": "Example 4-7. A simple text retrieval system.",
          "level": -1,
          "page": 185,
          "reading_order": 7,
          "bbox": [
            97,
            645,
            315,
            658
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_185_order_10",
          "label": "foot",
          "text": "4.7 Algorithm Design | 163",
          "level": -1,
          "page": 185,
          "reading_order": 10,
          "bbox": [
            464,
            824,
            584,
            842
          ],
          "section_number": "4.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_186_order_1",
          "label": "para",
          "text": "A more subtle example of a space-time trade-off involves replacing the tokens of a\ncorpus with integer identifiers. We create a vocabulary for the corpus, a list in which\neach word is stored once, then invert this list so that we can look up any word to find\nits identifier. Each document is preprocessed, so that a list of words becomes a list of\nintegers. Any language models can now work with integers. See the listing in Exam-\nple 4-8 for an example of how to do this for a tagged corpus.",
          "level": -1,
          "page": 186,
          "reading_order": 1,
          "bbox": [
            97,
            250,
            584,
            353
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_186_order_2",
          "label": "cap",
          "text": "Example 4-8. Preprocess tagged corpus data, converting all words and tags to integers.",
          "level": -1,
          "page": 186,
          "reading_order": 2,
          "bbox": [
            97,
            367,
            521,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_186_order_4",
          "label": "para",
          "text": "Another example of a space-time trade-off is maintaining a vocabulary list. If you need\nto process an input text to check that all words are in an existing vocabulary, the vo-\ncabulary should be stored as a set, not a list. The elements of a set are automatically\nindexed, so testing membership of a large set will be much faster than testing mem-\nbership of the corresponding list.",
          "level": -1,
          "page": 186,
          "reading_order": 4,
          "bbox": [
            97,
            528,
            585,
            613
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_186_order_5",
          "label": "para",
          "text": "We can test this claim using the timeit module. The Timer class has two parameters: a\nstatement that is executed multiple times, and setup code that is executed once at the\nbeginning. We will simulate a vocabulary of 100,000 items using a list O or set Q of\nintegers. The test statement will generate a random item that has a 50% chance of being\nin the vocabulary O .",
          "level": -1,
          "page": 186,
          "reading_order": 5,
          "bbox": [
            97,
            618,
            586,
            703
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_186_order_6",
          "label": "foot",
          "text": "164 | Chapter 4: Writing Structured Programs",
          "level": -1,
          "page": 186,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "164",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_187_order_1",
          "label": "para",
          "text": "Performing 1,000 list membership tests takes a total of 2.8 seconds, whereas the equiv-\nalent tests on a set take a mere 0.0037 seconds, or three orders of magnitude faster!",
          "level": -1,
          "page": 187,
          "reading_order": 1,
          "bbox": [
            97,
            197,
            584,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_187_order_2",
      "label": "sec",
      "text": "Dynamic Programming",
      "level": 1,
      "page": 187,
      "reading_order": 2,
      "bbox": [
        100,
        247,
        252,
        268
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_189_order_4",
          "label": "sub_sec",
          "text": "4.8 A Sample of Python Libraries",
          "level": 2,
          "page": 189,
          "reading_order": 4,
          "bbox": [
            97,
            698,
            359,
            722
          ],
          "section_number": "4.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_189_order_5",
              "label": "para",
              "text": "Python has hundreds of third-party libraries, specialized software packages that extend\nthe functionality of Python. NLTK is one such library. To realize the full power of\nPython programming, you should become familiar with several other libraries. Most\nof these will need to be manually installed on your computer.",
              "level": -1,
              "page": 189,
              "reading_order": 5,
              "bbox": [
                97,
                725,
                585,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_189_order_6",
              "label": "foot",
              "text": "4.8 A Sample of Python Libraries | 167",
              "level": -1,
              "page": 189,
              "reading_order": 6,
              "bbox": [
                413,
                824,
                585,
                842
              ],
              "section_number": "4.8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_187_order_3",
          "label": "para",
          "text": "Dynamic programming is a general technique for designing algorithms which is widely\nused in natural language processing. The term “programming” is used in a different\nsense to what you might expect, to mean planning or scheduling. Dynamic program-\nming is used when a problem contains overlapping subproblems. Instead of computing\nsolutions to these subproblems repeatedly, we simply store them in a lookup table. In\nthe remainder of this section, we will introduce dynamic programming, but in a rather\ndifferent context to syntactic parsing.",
          "level": -1,
          "page": 187,
          "reading_order": 3,
          "bbox": [
            97,
            268,
            585,
            387
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_187_order_4",
          "label": "para",
          "text": "Pingala was an Indian author who lived around the 5th century B.C., and wrote a\ntreatise on Sanskrit prosody called the Chandas Shastra . Virahanka extended this work\naround the 6th century A.D., studying the number of ways of combining short and long\nsyllables to create a meter of length n . Short syllables, marked S , take up one unit of\nlength, while long syllables, marked L , take two. Pingala found, for example, that there\nare five ways to construct a meter of length 4: V 4 = { LL , SSL , SLS , LSS , SSSS } . Observe\nthat we can split V 4 into two subsets, those starting with L and those starting with S ,\nas shown in (1) .",
          "level": -1,
          "page": 187,
          "reading_order": 4,
          "bbox": [
            97,
            394,
            586,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_187_order_6",
          "label": "para",
          "text": "With this observation, we can write a little recursive function called virahanka1() to\ncompute these meters, shown in Example 4-9 . Notice that, in order to compute $V_4$ we\nfirst compute $V_3$ and $V_2$ . But to compute $V_3$ , we need to first compute $V_2$ and $V_1$ . This\ncall structure is depicted in (2) .",
          "level": -1,
          "page": 187,
          "reading_order": 6,
          "bbox": [
            97,
            618,
            585,
            685
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_187_order_7",
          "label": "foot",
          "text": "4.7 Algorithm Design | 165",
          "level": -1,
          "page": 187,
          "reading_order": 7,
          "bbox": [
            464,
            824,
            585,
            842
          ],
          "section_number": "4.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_188_order_0",
          "label": "para",
          "text": "Example 4-9. Four ways to compute Sanskrit meter: (i) iterative, (ii) bottom-up dynamic\nprogramming, (iii) top-down dynamic programming, and (iv) built-in memoization.",
          "level": -1,
          "page": 188,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            583,
            102
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_188_order_6",
          "label": "foot",
          "text": "166 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 188,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "166",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_189_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_189_figure_000.png)",
          "level": -1,
          "page": 189,
          "reading_order": 0,
          "bbox": [
            118,
            71,
            279,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_189_order_1",
          "label": "para",
          "text": "As you can see, $V_2$ is computed twice. This might not seem like a significant problem,\nbut it turns out to be rather wasteful as $n$ gets large: to compute $V_{20}$ using this recursive\ntechnique, we would compute $V_2$ 4,181 times; and for $V_{40}$ we would compute $V_2$\n63,245,986 times! A much better alternative is to store the value of $V_2$ in a table and\nlook it up whenever we need it. The same goes for other values, such as $V_3$ and so on.\nFunction virahanka2() implements a dynamic programming approach to the problem.\nIt works by filling up a table (called lookup ) with solutions to all smaller instances of\nthe problem, stopping as soon as we reach the value we're interested in. At this point\nwe read off the value and return it. Crucially, each subproblem is only ever solved once.",
          "level": -1,
          "page": 189,
          "reading_order": 1,
          "bbox": [
            97,
            214,
            586,
            360
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_189_order_2",
          "label": "para",
          "text": "Notice that the approach taken in virahanka2() is to solve smaller problems on the way\nto solving larger problems. Accordingly, this is known as the bottom-up approach to\ndynamic programming. Unfortunately it turns out to be quite wasteful for some ap-\nplications, since it may compute solutions to sub-problems that are never required for\nsolving the main problem. This wasted computation can be avoided using the top-\ndown approach to dynamic programming, which is illustrated in the function vira\nhanka3() in Example 4-9. Unlike the bottom-up approach, this approach is recursive.\nIt avoids the huge wastage of virahanka1() by checking whether it has previously stored\nthe result. If not, it computes the result recursively and stores it in the table. The last\nstep is to return the stored result. The final method, in virahanka4(), is to use a Python\n“decorator” called memoize, which takes care of the housekeeping work done by\nvirahanka3() without cluttering up the program. This “memoization” process stores\nthe result of each previous call to the function along with the parameters that were\nused. If the function is subsequently called with the same parameters, it returns the\nstored result instead of recalculating it. (This aspect of Python syntax is beyond the\nscope of this book.)",
          "level": -1,
          "page": 189,
          "reading_order": 2,
          "bbox": [
            97,
            367,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_189_order_3",
          "label": "para",
          "text": "This concludes our brief introduction to dynamic programming. We will encounter it\nagain in Section 8.4.",
          "level": -1,
          "page": 189,
          "reading_order": 3,
          "bbox": [
            100,
            636,
            585,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_190_order_0",
      "label": "sec",
      "text": "Matplotlib",
      "level": 1,
      "page": 190,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        171,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_190_order_1",
          "label": "para",
          "text": "Python has some libraries that are useful for visualizing language data. The Matplotlib\npackage supports sophisticated plotting functions with a MATLAB-style interface, and\nis available from http://matplotlib.sourceforge.net/.",
          "level": -1,
          "page": 190,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_190_order_2",
          "label": "para",
          "text": "So far we have focused on textual presentation and the use of formatted print statements\nto get output lined up in columns. It is often very useful to display numerical data in\ngraphical form, since this often makes it easier to detect patterns. For example, in\nExample 3-5 , we saw a table of numbers showing the frequency of particular modal\nverbs in the Brown Corpus, classified by genre. The program in Example 4-10 presents\nthe same information in graphical format. The output is shown in Figure 4-4 (a color\nfigure in the graphical display).",
          "level": -1,
          "page": 190,
          "reading_order": 2,
          "bbox": [
            97,
            160,
            585,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_190_order_3",
          "label": "para",
          "text": "Example 4-10. Frequency of modals in different sections of the Brown Corpus",
          "level": -1,
          "page": 190,
          "reading_order": 3,
          "bbox": [
            97,
            286,
            477,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_190_order_6",
          "label": "para",
          "text": "From the bar chart it is immediately obvious that may and must have almost identical\nrelative frequencies. The same goes for could and might.",
          "level": -1,
          "page": 190,
          "reading_order": 6,
          "bbox": [
            98,
            689,
            584,
            726
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_190_order_7",
          "label": "para",
          "text": "It is also possible to generate such data visualizations on the fly. For example, a web\npage with form input could permit visitors to specify search parameters, submit the\nform, and see a dynamically generated visualization. To do this we have to specify the",
          "level": -1,
          "page": 190,
          "reading_order": 7,
          "bbox": [
            97,
            734,
            585,
            782
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_190_order_8",
          "label": "foot",
          "text": "168 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 190,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "168",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_191_order_0",
          "label": "para",
          "text": "Agg backend for matplotlib , which is a library for producing raster (pixel) images O .\nNext, we use all the same PyLab methods as before, but instead of displaying the result\non a graphical terminal using pylab.show() , we save it to a file using pylab.savefig()\n❷ . We specify the filename and dpi, then print HTML markup that directs the web\nbrowser to load the file.",
          "level": -1,
          "page": 191,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_191_order_2",
          "label": "figure",
          "text": "Figure 4-4. Bar chart showing frequency of modals in different sections of Brown Corpus: This\nvisualization was produced by the program in Example 4-10. [IMAGE: ![Figure](figures/NLTK_page_191_figure_002.png)]",
          "level": -1,
          "page": 191,
          "reading_order": 2,
          "bbox": [
            100,
            277,
            583,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_191_figure_002.png)",
              "bbox": [
                100,
                277,
                583,
                672
              ],
              "page": 191,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Figure 4-4. Bar chart showing frequency of modals in different sections of Brown Corpus: This\nvisualization was produced by the program in Example 4-10.",
              "bbox": [
                97,
                680,
                584,
                711
              ],
              "page": 191,
              "reading_order": 3
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_191_order_4",
      "label": "sec",
      "text": "NetworkX",
      "level": 1,
      "page": 191,
      "reading_order": 4,
      "bbox": [
        98,
        734,
        163,
        755
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_191_order_5",
          "label": "para",
          "text": "The NetworkX package is for defining and manipulating structures consisting of nodes\nand edges, known as graphs. It is available from https://networkx.lanl.gov/. NetworkX",
          "level": -1,
          "page": 191,
          "reading_order": 5,
          "bbox": [
            100,
            768,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_191_order_6",
          "label": "foot",
          "text": "4.8 A Sample of Python Libraries | 169",
          "level": -1,
          "page": 191,
          "reading_order": 6,
          "bbox": [
            413,
            824,
            585,
            842
          ],
          "section_number": "4.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_192_order_0",
          "label": "para",
          "text": "can be used in conjunction with Matplotlib to visualize networks, such as WordNet\n(the semantic network we introduced in Section 2.5 ). The program in Example 4 - 11\ninitializes an empty graph ❸ and then traverses the WordNet hypernym hierarchy\nadding edges to the graph ❸ . Notice that the traversal is recursive ❷ , applying the\nprogramming technique discussed in Section 4.7 . The resulting display is shown in\nFigure 4 - 5 .",
          "level": -1,
          "page": 192,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_192_order_1",
          "label": "para",
          "text": "Example 4-11. Using the NetworkX and Matplotlib libraries.",
          "level": -1,
          "page": 192,
          "reading_order": 1,
          "bbox": [
            97,
            195,
            396,
            208
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_192_order_7",
      "label": "sec",
      "text": "csv",
      "level": 1,
      "page": 192,
      "reading_order": 7,
      "bbox": [
        97,
        564,
        118,
        582
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_192_order_8",
          "label": "para",
          "text": "Language analysis work often involves data tabulations, containing information about\nlexical items, the participants in an empirical study, or the linguistic features extracted\nfrom a corpus. Here's a fragment of a simple lexicon, in CSV format:",
          "level": -1,
          "page": 192,
          "reading_order": 8,
          "bbox": [
            97,
            591,
            585,
            640
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_192_order_10",
          "label": "para",
          "text": "We can use Python's CSV library to read and write files stored in this format. For\nexample, we can open a CSV file called lexicon.csv ❶ and iterate over its rows ❷ :",
          "level": -1,
          "page": 192,
          "reading_order": 10,
          "bbox": [
            97,
            696,
            585,
            727
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_192_order_12",
          "label": "foot",
          "text": "170 | Chapter4: Writing Structured Programs",
          "level": -1,
          "page": 192,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "170",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_193_order_0",
          "label": "para",
          "text": "['walk', 'wo:k', 'v.intr', 'progress by lifting and setting down each foot ...']\n['wake', 'weik', 'intrans', 'cease to sleep']",
          "level": -1,
          "page": 193,
          "reading_order": 0,
          "bbox": [
            118,
            71,
            557,
            99
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_193_order_1",
          "label": "para",
          "text": "Each row is just a list of strings. If any fields contain numerical data, they will appear\nas strings, and will have to be converted using int() or float().",
          "level": -1,
          "page": 193,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_193_order_2",
          "label": "figure",
          "text": "Figure 4-5. Visualization with NetworkX and Matplotlib: Part of the WordNet hypernym hierarchy\nis displayed, starting with dog.n.01 (the darkest node in the middle); node size is based on the number\nof children of the node, and color is based on the distance of the node from dog.n.01; this visualization\nwas produced by the program in Example 4-11. [IMAGE: ![Figure](figures/NLTK_page_193_figure_002.png)]",
          "level": -1,
          "page": 193,
          "reading_order": 2,
          "bbox": [
            100,
            152,
            583,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_193_figure_002.png)",
              "bbox": [
                100,
                152,
                583,
                412
              ],
              "page": 193,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Figure 4-5. Visualization with NetworkX and Matplotlib: Part of the WordNet hypernym hierarchy\nis displayed, starting with dog.n.01 (the darkest node in the middle); node size is based on the number\nof children of the node, and color is based on the distance of the node from dog.n.01; this visualization\nwas produced by the program in Example 4-11.",
              "bbox": [
                97,
                421,
                585,
                483
              ],
              "page": 193,
              "reading_order": 3
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_193_order_4",
      "label": "sec",
      "text": "NumPy",
      "level": 1,
      "page": 193,
      "reading_order": 4,
      "bbox": [
        100,
        507,
        146,
        528
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_193_order_5",
          "label": "para",
          "text": "The NumPy package provides substantial support for numerical processing in Python.\nNumPy has a multidimensional array object, which is easy to initialize and access:",
          "level": -1,
          "page": 193,
          "reading_order": 5,
          "bbox": [
            97,
            528,
            584,
            565
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_193_order_7",
          "label": "para",
          "text": "NumPy includes linear algebra functions. Here we perform singular value decomposi-\ntion on a matrix, an operation used in latent semantic analysis to help identify implicit\nconcepts in a document collection:",
          "level": -1,
          "page": 193,
          "reading_order": 7,
          "bbox": [
            97,
            751,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_193_order_8",
          "label": "foot",
          "text": "4.8 A Sample of Python Libraries | 171",
          "level": -1,
          "page": 193,
          "reading_order": 8,
          "bbox": [
            413,
            824,
            584,
            842
          ],
          "section_number": "4.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_194_order_1",
          "label": "para",
          "text": "NLTK's clustering package nltk.cluster makes extensive use of NumPy arrays, and\nincludes support for k -means clustering, Gaussian EM clustering, group average\nagglomerative clustering, and dendogram plots. For details, type help(nltk.cluster) .",
          "level": -1,
          "page": 194,
          "reading_order": 1,
          "bbox": [
            97,
            224,
            585,
            273
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_194_order_2",
      "label": "sec",
      "text": "Other Python Libraries",
      "level": 1,
      "page": 194,
      "reading_order": 2,
      "bbox": [
        97,
        286,
        247,
        308
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_194_order_4",
          "label": "sub_sec",
          "text": "4.9 Summary",
          "level": 2,
          "page": 194,
          "reading_order": 4,
          "bbox": [
            97,
            439,
            207,
            463
          ],
          "section_number": "4.9",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_194_order_5",
              "label": "list_group",
              "text": "Python’s assignment and parameter passing use object references; e.g., if a is a list\nand we assign b = a, then any operation on a will modify b, and vice versa.\nThe is operation tests whether two objects are identical internal objects, whereas\n== tests whether two objects are equivalent. This distinction parallels the type-\ntoken distinction.",
              "level": -1,
              "page": 194,
              "reading_order": 5,
              "bbox": [
                122,
                473,
                585,
                503
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "Python’s assignment and parameter passing use object references; e.g., if a is a list\nand we assign b = a, then any operation on a will modify b, and vice versa.",
                  "bbox": [
                    122,
                    473,
                    585,
                    503
                  ],
                  "page": 194,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "The is operation tests whether two objects are identical internal objects, whereas\n== tests whether two objects are equivalent. This distinction parallels the type-\ntoken distinction.",
                  "bbox": [
                    121,
                    510,
                    585,
                    555
                  ],
                  "page": 194,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "Strings, lists, and tuples are different kinds of sequence object, supporting common\noperations such as indexing, slicing, len(), sorted(), and membership testing using\nin.",
                  "bbox": [
                    122,
                    564,
                    585,
                    609
                  ],
                  "page": 194,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "We can write text to a file by opening the file for writing\nofile = open('output.txt', 'w'",
                  "bbox": [
                    122,
                    618,
                    443,
                    654
                  ],
                  "page": 194,
                  "reading_order": 8
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_194_order_9",
              "label": "para",
              "text": "hen adding content to the file ofile.write(\"Monty Python\"), and finally closing\nhe file ofile.close().",
              "level": -1,
              "page": 194,
              "reading_order": 9,
              "bbox": [
                126,
                663,
                585,
                693
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_194_order_10",
              "label": "list_group",
              "text": "A declarative programming style usually produces more compact, readable code;\nmanually incremented loop variables are usually unnecessary. When a sequence\nmust be enumerated, use enumerate().\nFunctions are an essential programming abstraction: key concepts to understand\nare parameter passing, variable scope, and docstrings.",
              "level": -1,
              "page": 194,
              "reading_order": 10,
              "bbox": [
                122,
                698,
                585,
                752
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "A declarative programming style usually produces more compact, readable code;\nmanually incremented loop variables are usually unnecessary. When a sequence\nmust be enumerated, use enumerate().",
                  "bbox": [
                    122,
                    698,
                    585,
                    752
                  ],
                  "page": 194,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "Functions are an essential programming abstraction: key concepts to understand\nare parameter passing, variable scope, and docstrings.",
                  "bbox": [
                    122,
                    752,
                    584,
                    788
                  ],
                  "page": 194,
                  "reading_order": 11
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_194_order_12",
              "label": "foot",
              "text": "172 | Chapter4: Writing Structured Programs",
              "level": -1,
              "page": 194,
              "reading_order": 12,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "172",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_195_order_0",
              "label": "list_group",
              "text": "A function serves as a namespace: names defined inside a function are not visible\noutside that function, unless those names are declared to be global.\nModules permit logically related material to be localized in a file. A module serves\nas a namespace: names defined in a module—such as variables and functions—\nare not visible to other modules, unless those names are imported.",
              "level": -1,
              "page": 195,
              "reading_order": 0,
              "bbox": [
                122,
                71,
                585,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "A function serves as a namespace: names defined inside a function are not visible\noutside that function, unless those names are declared to be global.",
                  "bbox": [
                    122,
                    71,
                    585,
                    107
                  ],
                  "page": 195,
                  "reading_order": 0
                },
                {
                  "label": "list",
                  "text": "Modules permit logically related material to be localized in a file. A module serves\nas a namespace: names defined in a module—such as variables and functions—\nare not visible to other modules, unless those names are imported.",
                  "bbox": [
                    122,
                    107,
                    585,
                    161
                  ],
                  "page": 195,
                  "reading_order": 1
                },
                {
                  "label": "list",
                  "text": "Dynamic programming is an algorithm design technique used widely in NLP that\nstores the results of previous computations in order to avoid unnecessary\nrecomputation.",
                  "bbox": [
                    122,
                    161,
                    585,
                    215
                  ],
                  "page": 195,
                  "reading_order": 2
                }
              ],
              "is_merged": true
            }
          ]
        },
        {
          "id": "page_195_order_3",
          "label": "sub_sec",
          "text": "4.10 Further Reading",
          "level": 2,
          "page": 195,
          "reading_order": 3,
          "bbox": [
            97,
            240,
            270,
            263
          ],
          "section_number": "4.10",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_195_order_4",
              "label": "para",
              "text": "This chapter has touched on many topics in programming, some specific to Python,\nand some quite general. We’ve just scratched the surface, and you may want to read\nmore about these topics, starting with the further materials for this chapter available\nat http://www.nltk.org/.",
              "level": -1,
              "page": 195,
              "reading_order": 4,
              "bbox": [
                100,
                268,
                585,
                335
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_195_order_5",
              "label": "para",
              "text": "The Python website provides extensive documentation. It is important to understand\nthe built-in functions and standard types, described at http://docs.python.org/library/\nfunctions.html and http://docs.python.org/library/stdtypes.html . We have learned about\ngenerators and their importance for efficiency; for information about iterators, a closely\nrelated topic, see http://docs.python.org/library/itertools.html . Consult your favorite Py-\nthon book for more information on such topics. An excellent resource for using Python\nfor multimedia processing, including working with sound files, is (Guzdial, 2005).",
              "level": -1,
              "page": 195,
              "reading_order": 5,
              "bbox": [
                96,
                340,
                585,
                458
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_195_order_6",
              "label": "para",
              "text": "When using the online Python documentation, be aware that your installed version\nmight be different from the version of the documentation you are reading. You can\neasily check what version you have, with import sys; sys.version . Version-specific\ndocumentation is available at http://www.python.org/doc/versions/ .",
              "level": -1,
              "page": 195,
              "reading_order": 6,
              "bbox": [
                97,
                465,
                584,
                531
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_195_order_7",
              "label": "para",
              "text": "Algorithm design is a rich field within computer science. Some good starting points are\n(Harel, 2004), (Levitin, 2004), and (Knuth, 2006). Useful guidance on the practice of\nsoftware development is provided in (Hunt & Thomas, 2000) and (McConnell, 2004).",
              "level": -1,
              "page": 195,
              "reading_order": 7,
              "bbox": [
                97,
                537,
                586,
                591
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_195_order_8",
          "label": "sub_sec",
          "text": "4.11 Exercises",
          "level": 2,
          "page": 195,
          "reading_order": 8,
          "bbox": [
            97,
            609,
            211,
            636
          ],
          "section_number": "4.11",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_195_order_9",
              "label": "para",
              "text": "1. ◦ Find out more about sequence objects using Python's help facility. In the inter-\npreter, type help(str), help(list), and help(tuple). This will give you a full list of\nthe functions supported by each type. Some functions have special names flanked\nwith underscores; as the help documentation shows, each such function corre-\nsponds to something more familiar. For example x.__getitem_(y) is just a long-\nwinded way of saying x[y].",
              "level": -1,
              "page": 195,
              "reading_order": 9,
              "bbox": [
                100,
                645,
                586,
                744
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_195_order_10",
              "label": "para",
              "text": "2. ◦ Identify three operations that can be performed on both tuples and lists. Identify\nthree list operations that cannot be performed on tuples. Name a context where\nusing a list instead of a tuple generates a Python error.",
              "level": -1,
              "page": 195,
              "reading_order": 10,
              "bbox": [
                100,
                744,
                585,
                798
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_195_order_11",
              "label": "foot",
              "text": "4.11 Exercises | 173",
              "level": -1,
              "page": 195,
              "reading_order": 11,
              "bbox": [
                493,
                824,
                584,
                842
              ],
              "section_number": "4.11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_0",
              "label": "para",
              "text": "3. ◦ Find out how to create a tuple consisting of a single item. There are at least two\nways to do this.",
              "level": -1,
              "page": 196,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                585,
                107
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_1",
              "label": "para",
              "text": "4. ◦ Create a list words = ['is', 'NLP', 'fun', '?']. Use a series of assignment\nstatements (e.g., words[1] = words[2]) and a temporary variable tmp to transform\nthis list into the list ['NLP', 'is', 'fun', '!']. Now do the same transformation\nusing tuple assignment.",
              "level": -1,
              "page": 196,
              "reading_order": 1,
              "bbox": [
                100,
                107,
                585,
                179
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_2",
              "label": "para",
              "text": "5. ◦ Read about the built-in comparison function cmp, by typing help(cmp). How does\nit differ in behavior from the comparison operators?",
              "level": -1,
              "page": 196,
              "reading_order": 2,
              "bbox": [
                100,
                179,
                585,
                215
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_3",
              "label": "para",
              "text": "6. ◦ Does the method for creating a sliding window of n-grams behave correctly for\nthe two limiting cases: n = 1 and n = len(sent)?",
              "level": -1,
              "page": 196,
              "reading_order": 3,
              "bbox": [
                100,
                215,
                585,
                250
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_4",
              "label": "para",
              "text": "7. ◦ We pointed out that when empty strings and empty lists occur in the condition\npart of an if clause, they evaluate to False. In this case, they are said to be occurring\nin a Boolean context. Experiment with different kinds of non-Boolean expressions\nin Boolean contexts, and see whether they evaluate as True or False.",
              "level": -1,
              "page": 196,
              "reading_order": 4,
              "bbox": [
                100,
                250,
                585,
                322
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_5",
              "label": "para",
              "text": "8. ◦ Use the inequality operators to compare strings, e.g., 'Monty' < 'Python'. What\nhappens when you do 'Z' < 'a'? Try pairs of strings that have a common prefix,\ne.g., 'Monty' < 'Montague'. Read up on “lexicographical sort” in order to under-\nstand what is going on here. Try comparing structured objects, e.g., ('Monty', 1)\n< ('Monty', 2). Does this behave as expected?",
              "level": -1,
              "page": 196,
              "reading_order": 5,
              "bbox": [
                100,
                322,
                585,
                412
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_6",
              "label": "para",
              "text": "9. ◦ Write code that removes whitespace at the beginning and end of a string, and\nnormalizes whitespace between words to be a single-space character.",
              "level": -1,
              "page": 196,
              "reading_order": 6,
              "bbox": [
                100,
                412,
                584,
                448
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_7",
              "label": "para",
              "text": "a. Do this task using split() and join()",
              "level": -1,
              "page": 196,
              "reading_order": 7,
              "bbox": [
                126,
                448,
                360,
                466
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_8",
              "label": "para",
              "text": "b. Do this task using regular expression substitutions.",
              "level": -1,
              "page": 196,
              "reading_order": 8,
              "bbox": [
                126,
                466,
                440,
                486
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_9",
              "label": "para",
              "text": "10. ◦ Write a program to sort words by length. Define a helper function cmp_len which\nuses the cmp comparison function on word lengths.",
              "level": -1,
              "page": 196,
              "reading_order": 9,
              "bbox": [
                100,
                492,
                584,
                523
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_10",
              "label": "para",
              "text": "11. o Create a list of words and store it in a variable sent1. Now assign sent2 =\nsent1. Modify one of the items in sent1 and verify that sent2 has changed.",
              "level": -1,
              "page": 196,
              "reading_order": 10,
              "bbox": [
                100,
                528,
                584,
                564
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_11",
              "label": "para",
              "text": "a. Now try the same exercise, but instead assign sent2 = sent1[:]. Modify\nsent1 again and see what happens to sent2. Explain.",
              "level": -1,
              "page": 196,
              "reading_order": 11,
              "bbox": [
                126,
                564,
                585,
                600
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_12",
              "label": "para",
              "text": "b. Now define text1 to be a list of lists of strings (e.g., to represent a text consisting\nof multiple sentences). Now assign text2 = text1[:], assign a new value to\none of the words, e.g., text1[1][1] = 'Monty'. Check what this did to text2.\nExplain.",
              "level": -1,
              "page": 196,
              "reading_order": 12,
              "bbox": [
                126,
                600,
                585,
                672
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_13",
              "label": "para",
              "text": "c. Load Python’s deepcopy() function (i.e., from copy import deepcopy), consult\nits documentation, and test that it makes a fresh copy of any object.",
              "level": -1,
              "page": 196,
              "reading_order": 13,
              "bbox": [
                126,
                672,
                585,
                707
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_14",
              "label": "para",
              "text": "12. o Initialize an n-by-m list of lists of empty strings using list multiplication, e.g.,\nword_table = [[''] * n] * m. What happens when you set one of its values, e.g.,\nword_table[1][2] = \"hello\"? Explain why this happens. Now write an expression\nusing range() to construct a list of lists, and show that it does not have this problem.",
              "level": -1,
              "page": 196,
              "reading_order": 14,
              "bbox": [
                100,
                707,
                584,
                779
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_196_order_15",
              "label": "foot",
              "text": "174 | Chapter 4: Writing Structured Programs",
              "level": -1,
              "page": 196,
              "reading_order": 15,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "174",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_0",
              "label": "para",
              "text": "13. o Write code to initialize a two-dimensional array of sets called word_vowels and\nprocess a list of words, adding each word to word_vowels[1][v] where l is the length\nof the word and v is the number of vowels it contains.",
              "level": -1,
              "page": 197,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                584,
                125
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_1",
              "label": "para",
              "text": "14. o Write a function novel10(text) that prints any word that appeared in the last\n10% of a text that had not been encountered earlier.",
              "level": -1,
              "page": 197,
              "reading_order": 1,
              "bbox": [
                100,
                125,
                584,
                161
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_2",
              "label": "para",
              "text": "15. o Write a program that takes a sentence expressed as a single string, splits it, and\ncounts up the words. Get it to print out each word and the word’s frequency, one\nper line, in alphabetical order.",
              "level": -1,
              "page": 197,
              "reading_order": 2,
              "bbox": [
                100,
                161,
                585,
                215
              ],
              "section_number": "15",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_3",
              "label": "para",
              "text": "16. o Read up on Gematria, a method for assigning numbers to words, and for mapping\nbetween words having the same number to discover the hidden meaning of texts\n(http://en.wikipedia.org/wiki/Gematria, http://essenes.net/gemcal.htm).",
              "level": -1,
              "page": 197,
              "reading_order": 3,
              "bbox": [
                100,
                215,
                585,
                268
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_4",
              "label": "para",
              "text": "a. Write a function gematria() that sums the numerical values of the letters of a\nword, according to the letter values in letter_vals:",
              "level": -1,
              "page": 197,
              "reading_order": 4,
              "bbox": [
                126,
                268,
                584,
                304
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_6",
              "label": "para",
              "text": "b. Process a corpus (e.g., nltk.corpus.state_union ) and for each document,\ncount how many of its words have the number 666.",
              "level": -1,
              "page": 197,
              "reading_order": 6,
              "bbox": [
                126,
                358,
                584,
                386
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_7",
              "label": "para",
              "text": "c. Write a function decode() to process a text, randomly replacing words with\ntheir Gematria equivalents, in order to discover the “hidden meaning” of the\ntext.",
              "level": -1,
              "page": 197,
              "reading_order": 7,
              "bbox": [
                126,
                394,
                585,
                439
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_8",
              "label": "para",
              "text": "17. o Write a function shorten(text, n) to process a text, omitting the n most fre-\nquently occurring words of the text. How readable is it?",
              "level": -1,
              "page": 197,
              "reading_order": 8,
              "bbox": [
                100,
                448,
                584,
                483
              ],
              "section_number": "17",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_9",
              "label": "para",
              "text": "18. o Write code to print out an index for a lexicon, allowing someone to look up\nwords according to their meanings (or their pronunciations; whatever properties\nare contained in the lexical entries).",
              "level": -1,
              "page": 197,
              "reading_order": 9,
              "bbox": [
                100,
                483,
                585,
                531
              ],
              "section_number": "18",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_10",
              "label": "para",
              "text": "19. o Write a list comprehension that sorts a list of WordNet synsets for proximity to\na given synset. For example, given the synsets minke_whale.n.01, orca.n.01,\nnovel.n.01, and tortoise.n.01, sort them according to their path_distance() from\nright_whale.n.01.",
              "level": -1,
              "page": 197,
              "reading_order": 10,
              "bbox": [
                100,
                537,
                585,
                604
              ],
              "section_number": "19",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_11",
              "label": "para",
              "text": "20. o Write a function that takes a list of words (containing duplicates) and returns a\nlist of words (with no duplicates) sorted by decreasing frequency. E.g., if the input\nlist contained 10 instances of the word table and 9 instances of the word chair,\nthen table would appear before chair in the output list.",
              "level": -1,
              "page": 197,
              "reading_order": 11,
              "bbox": [
                98,
                609,
                585,
                674
              ],
              "section_number": "20",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_12",
              "label": "para",
              "text": "21. o Write a function that takes a text and a vocabulary as its arguments and returns\nthe set of words that appear in the text but not in the vocabulary. Both arguments\ncan be represented as lists of strings. Can you do this in a single line, using set.dif\nference()?",
              "level": -1,
              "page": 197,
              "reading_order": 12,
              "bbox": [
                98,
                680,
                585,
                744
              ],
              "section_number": "21",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_13",
              "label": "para",
              "text": "22. o Import the itemgetter() function from the operator module in Python's standard\nlibrary (i.e., from operator import itemgetter). Create a list words containing sev-",
              "level": -1,
              "page": 197,
              "reading_order": 13,
              "bbox": [
                98,
                744,
                584,
                782
              ],
              "section_number": "22",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_197_order_14",
              "label": "foot",
              "text": "4.11 Exercises | 175",
              "level": -1,
              "page": 197,
              "reading_order": 14,
              "bbox": [
                493,
                824,
                585,
                842
              ],
              "section_number": "4.11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_0",
              "label": "para",
              "text": "eral words. Now try calling: sorted(words, key=itemgetter(1)), and sort\nted(words, key=itemgetter(-1)). Explain what itemgetter() is doing.",
              "level": -1,
              "page": 198,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_1",
              "label": "para",
              "text": "23. o Write a recursive function lookup(trie, key) that looks up a key in a trie, and\nreturns the value it finds. Extend the function to return a word when it is uniquely\ndetermined by its prefix (e.g., vanguard is the only word that starts with vang-, so\nlookup(trie, 'vang') should return the same thing as lookup(trie, 'vanguard')).",
              "level": -1,
              "page": 198,
              "reading_order": 1,
              "bbox": [
                98,
                107,
                585,
                179
              ],
              "section_number": "23",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_2",
              "label": "para",
              "text": "24. o Read up on “keyword linkage” (Chapter 5 of (Scott & Tribble, 2006)). Extract\nkeywords from NLTK’s Shakespeare Corpus and using the NetworkX package,\nplot keyword linkage networks.",
              "level": -1,
              "page": 198,
              "reading_order": 2,
              "bbox": [
                98,
                179,
                584,
                232
              ],
              "section_number": "24",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_3",
              "label": "para",
              "text": "25. o Read about string edit distance and the Levenshtein Algorithm. Try the imple-\nmentation provided in nltk.edit_dist(). In what way is this using dynamic pro-\ngramming? Does it use the bottom-up or top-down approach? (See also http://\nnorvig.com/spell-correct.html.)",
              "level": -1,
              "page": 198,
              "reading_order": 3,
              "bbox": [
                98,
                232,
                585,
                304
              ],
              "section_number": "25",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_4",
              "label": "para",
              "text": "26. • The Catalan numbers arise in many applications of combinatorial mathematics,\nincluding the counting of parse trees ( Section 8.6 ). The series can be defined as\nfollows: $C_0=1$ , and $C_{n+1}=\\Sigma_{0..n}$ ( $C_i C_{n-i}$ ).",
              "level": -1,
              "page": 198,
              "reading_order": 4,
              "bbox": [
                98,
                304,
                585,
                358
              ],
              "section_number": "26",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_5",
              "label": "para",
              "text": "a. Write a recursive function to compute $n$th Catalan number $C_{n}$.",
              "level": -1,
              "page": 198,
              "reading_order": 5,
              "bbox": [
                126,
                358,
                504,
                376
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_6",
              "label": "para",
              "text": "b. Now write another function that does this computation using dynamic pro-\ngramming.",
              "level": -1,
              "page": 198,
              "reading_order": 6,
              "bbox": [
                126,
                376,
                584,
                412
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_7",
              "label": "para",
              "text": "c. Use the timeit module to compare the performance of these functions as n\nincreases.",
              "level": -1,
              "page": 198,
              "reading_order": 7,
              "bbox": [
                126,
                412,
                584,
                448
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_8",
              "label": "para",
              "text": "27. $\\bullet$ Reproduce some of the results of (Zhao & Zobel, 2007) concerning authorship\nidentification.",
              "level": -1,
              "page": 198,
              "reading_order": 8,
              "bbox": [
                98,
                456,
                585,
                483
              ],
              "section_number": "27",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_9",
              "label": "para",
              "text": "28. • Study gender-specific lexical choice, and see if you can reproduce some of the\nresults of http://www.clintoneast.com/articles/words.php.",
              "level": -1,
              "page": 198,
              "reading_order": 9,
              "bbox": [
                98,
                492,
                585,
                523
              ],
              "section_number": "28",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_10",
              "label": "para",
              "text": "29. • Write a recursive function that pretty prints a trie in alphabetically sorted order,\nfor example:",
              "level": -1,
              "page": 198,
              "reading_order": 10,
              "bbox": [
                98,
                528,
                584,
                564
              ],
              "section_number": "29",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_12",
              "label": "para",
              "text": "30. • With the help of the trie data structure, write a recursive function that processes\ntext, locating the uniqueness point in each word, and discarding the remainder of\neach word. How much compression does this give? How readable is the resulting\ntext?",
              "level": -1,
              "page": 198,
              "reading_order": 12,
              "bbox": [
                98,
                627,
                586,
                689
              ],
              "section_number": "30",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_13",
              "label": "para",
              "text": "31. • Obtain some raw text, in the form of a single, long string. Use Python’s text\nwrap module to break it up into multiple lines. Now write code to add extra spaces\nbetween words, in order to justify the output. Each line must have the same width,\nand spaces must be approximately evenly distributed across each line. No line can\nbegin or end with a space.",
              "level": -1,
              "page": 198,
              "reading_order": 13,
              "bbox": [
                98,
                698,
                585,
                779
              ],
              "section_number": "31",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_198_order_14",
              "label": "foot",
              "text": "176 | Chapter4: Writing Structured Programs",
              "level": -1,
              "page": 198,
              "reading_order": 14,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "176",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_199_order_0",
              "label": "para",
              "text": "32. • Develop a simple extractive summarization tool, that prints the sentences of a\ndocument which contain the highest total word frequency. Use FreqDist() to count\nword frequencies, and use sum to sum the frequencies of the words in each sentence.\nRank the sentences according to their score. Finally, print the n highest-scoring\nsentences in document order. Carefully review the design of your program,\nespecially your approach to this double sorting. Make sure the program is written\nas clearly as possible.",
              "level": -1,
              "page": 199,
              "reading_order": 0,
              "bbox": [
                98,
                71,
                585,
                188
              ],
              "section_number": "32",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_199_order_1",
              "label": "para",
              "text": "33. • Develop your own NgramTagger class that inherits from NLTK's class, and which\nencapsulates the method of collapsing the vocabulary of the tagged training and\ntesting data that was described in Chapter 5. Make sure that the unigram and\ndefault backoff taggers have access to the full vocabulary.",
              "level": -1,
              "page": 199,
              "reading_order": 1,
              "bbox": [
                98,
                188,
                584,
                259
              ],
              "section_number": "33",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_199_order_2",
              "label": "para",
              "text": "34. • Read the following article on semantic orientation of adjectives. Use the Net-\nworkX package to visualize a network of adjectives with edges to indicate same\nversus different semantic orientation (see http://www.aclweb.org/anthology/P97\n-1023 ).",
              "level": -1,
              "page": 199,
              "reading_order": 2,
              "bbox": [
                98,
                259,
                585,
                331
              ],
              "section_number": "34",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_199_order_3",
              "label": "para",
              "text": "35. • Design an algorithm to find the “statistically improbable phrases” of a document\ncollection (see http://www.amazon.com/gp/search-inside/sipshelp.html).",
              "level": -1,
              "page": 199,
              "reading_order": 3,
              "bbox": [
                98,
                331,
                585,
                367
              ],
              "section_number": "35",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_199_order_4",
              "label": "para",
              "text": "36. • Write a program to implement a brute-force algorithm for discovering word\nsquares, a kind of n × n: crossword in which the entry in the nth row is the same as the entry in the th column. For discussion, see http://itre.cis.upenn.edu/~myl/\nlanguagelog/archives/002679.html.",
              "level": -1,
              "page": 199,
              "reading_order": 4,
              "bbox": [
                98,
                367,
                585,
                439
              ],
              "section_number": "36",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_199_order_5",
              "label": "foot",
              "text": "4.11 Exercises | 177",
              "level": -1,
              "page": 199,
              "reading_order": 5,
              "bbox": [
                493,
                824,
                585,
                842
              ],
              "section_number": "4.11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_200_order_0",
              "label": "para",
              "text": "_",
              "level": -1,
              "page": 200,
              "reading_order": 0,
              "bbox": [
                153,
                161,
                494,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_194_order_3",
          "label": "para",
          "text": "There are many other Python libraries, and you can search for them with the help of\nthe Python Package Index at http://pypi.python.org/. Many libraries provide an interface\nto external software, such as relational databases (e.g., mysql-python) and large docu-\nment collections (e.g., PyLucene). Many other libraries give access to file formats such\nas PDF, MSWord, and XML ( pypdf, pywin32, xml.etree), RSS feeds (e.g., feedparser),\nand electronic mail (e.g., imaplib, email).",
          "level": -1,
          "page": 194,
          "reading_order": 3,
          "bbox": [
            97,
            313,
            585,
            413
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_201_order_0",
      "label": "sec",
      "text": "CHAPTER 5\n\nCategorizing and Tagging Words",
      "level": 1,
      "page": 201,
      "reading_order": 0,
      "bbox": [
        169,
        78,
        584,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_201_order_7",
          "label": "sub_sec",
          "text": "5.1 Using a Tagger",
          "level": 2,
          "page": 201,
          "reading_order": 7,
          "bbox": [
            97,
            636,
            249,
            659
          ],
          "section_number": "5.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_201_order_8",
              "label": "para",
              "text": "A part-of-speech tagger, or POS tagger, processes a sequence of words, and attaches\na part of speech tag to each word (don’t forget to import nltk):",
              "level": -1,
              "page": 201,
              "reading_order": 8,
              "bbox": [
                97,
                663,
                585,
                698
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_201_order_10",
              "label": "para",
              "text": "Here we see that and is CC, a coordinating conjunction; now and completely are RB, or\nadverbs; for is IN, a preposition; something is NN, a noun; and different is JJ, an adjective.",
              "level": -1,
              "page": 201,
              "reading_order": 10,
              "bbox": [
                97,
                768,
                585,
                798
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_201_order_11",
              "label": "foot",
              "text": "179",
              "level": -1,
              "page": 201,
              "reading_order": 11,
              "bbox": [
                566,
                824,
                585,
                842
              ],
              "section_number": "179",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_202_order_0",
              "label": "para",
              "text": "NLTK provides documentation for each tag, which can be queried using\nthe tag, e.g., nltk.help.upenn_tagset('RB') , or a regular expression,\ne.g., nltk.help.upenn_brown_tagset('NN.*') . Some corpora have RE-\nADME files with tagset documentation; see nltk.name.readme() , sub-\nstituting in the name of the corpus.",
              "level": -1,
              "page": 202,
              "reading_order": 0,
              "bbox": [
                171,
                80,
                530,
                157
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_202_order_1",
              "label": "para",
              "text": "Let’s look at another example, this time including some homonyms:",
              "level": -1,
              "page": 202,
              "reading_order": 1,
              "bbox": [
                98,
                179,
                486,
                197
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_202_order_3",
              "label": "para",
              "text": "Notice that refuse and permit both appear as a present tense verb (VBP) and a noun\n(NN). E.g., refUSE is a verb meaning “deny,” while REFuse is a noun meaning “trash”\n(i.e., they are not homophones). Thus, we need to know which word is being used in\norder to pronounce the text correctly. (For this reason, text-to-speech systems usually\nperform POS tagging.)",
              "level": -1,
              "page": 202,
              "reading_order": 3,
              "bbox": [
                97,
                259,
                585,
                344
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_202_order_4",
              "label": "para",
              "text": "Your Turn: Many words, like ski and race, can be used as nouns or\nverbs with no difference in pronunciation. Can you think of others?\nHint: think of a commonplace object and try to put the word to before\nit to see if it can also be a verb, or think of an action and try to put the\nbefore it to see if it can also be a noun. Now make up a sentence with\nboth uses of this word, and run the POS tagger on this sentence.",
              "level": -1,
              "page": 202,
              "reading_order": 4,
              "bbox": [
                171,
                367,
                530,
                459
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_202_order_5",
              "label": "para",
              "text": "Lexical categories like “ noun ” and part-of-speech tags like NN seem to have their uses,\nbut the details will be obscure to many readers. You might wonder what justification\nthere is for introducing this extra level of information. Many of these categories arise\nfrom superficial analysis of the distribution of words in text. Consider the following\nanalysis involving woman (a noun), bought (a verb), over (a preposition), and the (a\ndeterminer). The text.similar() method takes a word w , finds all contexts w 1 w w 2 ,\nthen finds all words w ' that appear in the same context, i.e. w 1 w 'w 2 .",
              "level": -1,
              "page": 202,
              "reading_order": 5,
              "bbox": [
                97,
                483,
                585,
                600
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_202_order_7",
              "label": "foot",
              "text": "180 | Chapter 5: Categorizing and Tagging Words",
              "level": -1,
              "page": 202,
              "reading_order": 7,
              "bbox": [
                97,
                824,
                308,
                842
              ],
              "section_number": "180",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_203_order_0",
              "label": "para",
              "text": "Observe that searching for woman finds nouns; searching for bought mostly finds verbs;\nsearching for over generally finds prepositions; searching for the finds several deter-\nminers. A tagger can correctly identify the tags on these words in the context of a\nsentence, e.g., The woman bought over $150,000 worth of clothes.",
              "level": -1,
              "page": 203,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                584,
                139
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_203_order_1",
              "label": "para",
              "text": "A tagger can also model our knowledge of unknown words; for example, we can guess\nthat scrobbling is probably a verb, with the root scrobble, and likely to occur in contexts\nlike he was scrobbling.",
              "level": -1,
              "page": 203,
              "reading_order": 1,
              "bbox": [
                97,
                143,
                585,
                197
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_203_order_2",
          "label": "sub_sec",
          "text": "5.2 Tagged Corpora",
          "level": 2,
          "page": 203,
          "reading_order": 2,
          "bbox": [
            97,
            222,
            261,
            245
          ],
          "section_number": "5.2",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_203_order_3",
              "label": "sub_sub_sec",
              "text": "Representing Tagged Tokens",
              "level": 3,
              "page": 203,
              "reading_order": 3,
              "bbox": [
                98,
                259,
                289,
                279
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_203_order_4",
                  "label": "para",
                  "text": "By convention in NLTK, a tagged token is represented using a tuple consisting of the\ntoken and the tag. We can create one of these special tuples from the standard string\nrepresentation of a tagged token, using the function str2tuple():",
                  "level": -1,
                  "page": 203,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    286,
                    585,
                    334
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_203_order_6",
                  "label": "para",
                  "text": "We can construct a list of tagged tokens directly from a string. The first step is to\ntokenize the string to access the individual word/tag strings, and then to convert each\nof these into a tuple (using str2tuple()).",
                  "level": -1,
                  "page": 203,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    439,
                    584,
                    492
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            },
            {
              "id": "page_203_order_8",
              "label": "sub_sub_sec",
              "text": "Reading Tagged Corpora",
              "level": 3,
              "page": 203,
              "reading_order": 8,
              "bbox": [
                98,
                654,
                261,
                675
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_203_order_9",
                  "label": "para",
                  "text": "Several of the corpora included with NLTK have been tagged for their part-of-speech.\nHere’s an example of what you might see if you opened a file from the Brown Corpus\nwith a text editor:",
                  "level": -1,
                  "page": 203,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    680,
                    585,
                    727
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_203_order_10",
                  "label": "para",
                  "text": "The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at inves\ntigation/nn of/in Atlanta’s/np$ recent/jj primary/nn election/nn produced/vbd / no/a\nevidence/nn \"/\" that/cs any/dti irregularities/nns took/vbd place/nn ./.",
                  "level": -1,
                  "page": 203,
                  "reading_order": 10,
                  "bbox": [
                    121,
                    734,
                    557,
                    781
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_203_order_11",
                  "label": "foot",
                  "text": "5.2 Tagged Corpora | 181",
                  "level": -1,
                  "page": 203,
                  "reading_order": 11,
                  "bbox": [
                    467,
                    824,
                    584,
                    842
                  ],
                  "section_number": "5.2",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_204_order_0",
                  "label": "para",
                  "text": "Other corpora use a variety of formats for storing part-of-speech tags. NLTK's corpus\nreaders provide a uniform interface so that you don’t have to be concerned with the\ndifferent file formats. In contrast with the file extract just shown, the corpus reader for\nthe Brown Corpus represents the data as shown next. Note that part-of-speech tags\nhave been converted to uppercase; this has become standard practice since the Brown\nCorpus was published.",
                  "level": -1,
                  "page": 204,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    172
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_204_order_2",
                  "label": "para",
                  "text": "Whenever a corpus contains tagged text, the NLTK corpus interface will have a\ntagged_words() method. Here are some more examples, again using the output format\nillustrated for the Brown Corpus:",
                  "level": -1,
                  "page": 204,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    241,
                    585,
                    289
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_204_order_4",
                  "label": "para",
                  "text": "Not all corpora employ the same set of tags; see the tagset help functionality and the\nreadme() methods mentioned earlier for documentation. Initially we want to avoid the\ncomplications of these tagsets, so we use a built-in mapping to a simplified tagset:",
                  "level": -1,
                  "page": 204,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    384,
                    585,
                    431
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_204_order_6",
                  "label": "para",
                  "text": "Tagged corpora for several other languages are distributed with NLTK, including Chi-\nnese, Hindi, Portuguese, Spanish, Dutch, and Catalan. These usually contain non-\nASCII text, and Python always displays this in hexadecimal when printing a larger\nstructure such as a list.",
                  "level": -1,
                  "page": 204,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    501,
                    585,
                    564
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_204_order_8",
                  "label": "para",
                  "text": "If your environment is set up correctly, with appropriate editors and fonts, you should\nbe able to display individual strings in a human-readable way. For example, Fig-\nure 5-1 shows data accessed using nltk.corpus.indian.",
                  "level": -1,
                  "page": 204,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    734,
                    584,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_204_order_9",
                  "label": "foot",
                  "text": "182 | Chapter 5: Categorizing and Tagging Words",
                  "level": -1,
                  "page": 204,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    824,
                    308,
                    842
                  ],
                  "section_number": "182",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_205_order_0",
                  "label": "para",
                  "text": "If the corpus is also segmented into sentences, it will have a tagged_sents() method\nthat divides up the tagged words into sentences rather than presenting them as one big\nlist. This will be useful when we come to developing automatic taggers, as they are\ntrained and tested on lists of sentences, not words.",
                  "level": -1,
                  "page": 205,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    136
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_201_order_1",
          "label": "para",
          "text": "Back in elementary school you learned the difference between nouns, verbs, adjectives,\nand adverbs. These “word classes” are not just the idle invention of grammarians, but\nare useful categories for many language processing tasks. As we will see, they arise from\nsimple analysis of the distribution of words in text. The goal of this chapter is to answer\nthe following questions:",
          "level": -1,
          "page": 201,
          "reading_order": 1,
          "bbox": [
            97,
            295,
            585,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_201_order_2",
          "label": "list_group",
          "text": "1. What are lexical categories, and how are they used in natural language processing?\n2. What is a good Python data structure for storing words and their categories?",
          "level": -1,
          "page": 201,
          "reading_order": 2,
          "bbox": [
            100,
            385,
            585,
            404
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "1. What are lexical categories, and how are they used in natural language processing?",
              "bbox": [
                100,
                385,
                585,
                404
              ],
              "page": 201,
              "reading_order": 2
            },
            {
              "label": "list",
              "text": "2. What is a good Python data structure for storing words and their categories?",
              "bbox": [
                100,
                411,
                558,
                430
              ],
              "page": 201,
              "reading_order": 3
            },
            {
              "label": "list",
              "text": "3. How can we automatically tag each word of a text with its word class?",
              "bbox": [
                100,
                430,
                530,
                448
              ],
              "page": 201,
              "reading_order": 4
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_201_order_5",
          "label": "para",
          "text": "Along the way, we’ll cover some fundamental techniques in NLP, including sequence\nlabeling, n-gram models, backoff, and evaluation. These techniques are useful in many\nareas, and tagging gives us a simple context in which to present them. We will also see\nhow tagging is the second step in the typical NLP pipeline, following tokenization.",
          "level": -1,
          "page": 201,
          "reading_order": 5,
          "bbox": [
            97,
            456,
            585,
            520
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_201_order_6",
          "label": "para",
          "text": "The process of classifying words into their parts-of-speech and labeling them accord-\ningly is known as part-of-speech tagging , POS tagging , or simply tagging . Parts-\nof-speech are also known as word classes or lexical categories . The collection of tags\nused for a particular task is known as a tagset . Our emphasis in this chapter is on\nexploiting tags, and tagging text automatically.",
          "level": -1,
          "page": 201,
          "reading_order": 6,
          "bbox": [
            97,
            528,
            585,
            610
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_205_order_1",
      "label": "sec",
      "text": "A Simplified Part-of-Speech Tagset",
      "level": 1,
      "page": 205,
      "reading_order": 1,
      "bbox": [
        97,
        152,
        329,
        174
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_205_order_2",
          "label": "para",
          "text": "Tagged corpora use many different conventions for tagging words. To help us get star-\nted, we will be looking at a simplified tagset (shown in Table 5-1).",
          "level": -1,
          "page": 205,
          "reading_order": 2,
          "bbox": [
            97,
            179,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_205_order_3",
          "label": "table",
          "text": "Table 5-1. Simplified part-of-speech tagset [TABLE: <table><tr><td>Tag</td><td>Meaning</td><td>Examples</td></tr><tr><td>ADJ</td><td>adjective</td><td>new, good, high, special, big, local</td></tr><tr><td>ADV</td><td>adverb</td><td>really, already, still, early, now</td></tr><tr><td>CNJ</td><td>conjunction</td><td>and, or, but, if, while, although</td></tr><tr><td>DET</td><td>determiner</td><td>the, a, some, most, every, no</td></tr><tr><td>EX</td><td>existential</td><td>there, there’s</td></tr><tr><td>FW</td><td>foreign word</td><td>dolce, ersatz, esprit, quo, maitre</td></tr><tr><td>MOD</td><td>modal ver</td><td>will, can, would, may, must, should</td></tr><tr><td>N</td><td>noun</td><td>year, home, costs, time, education</td></tr><tr><td>NP</td><td>proper noun</td><td>Alison, Africa, April, Washington</td></tr><tr><td>NUM</td><td>number</td><td>twenty-four, fourth, 1991, 14:24</td></tr><tr><td>PRO</td><td>pronoun</td><td>he, their, her, its, my, I, us</td></tr><tr><td>P</td><td>preposition</td><td>on, of, at, with, by, into, under</td></tr><tr><td>TO</td><td>the word to</td><td>to</td></tr><tr><td>UH</td><td>nterjection</td><td>ah, bang, ha, whee, hmpf, oops</td></tr><tr><td>V</td><td>ver</td><td>s, has, get, do, make, see, run</td></tr><tr><td>VD</td><td>past tense</td><td>said, took, told, made, asked</td></tr><tr><td>VG</td><td>present participle</td><td>making, going, playing, working</td></tr><tr><td>VN</td><td>past participle</td><td>given, taken, begun, sung</td></tr><tr><td>WH</td><td>wh determiner</td><td>who, which, when, what, where, how</td></tr></table>]",
          "level": -1,
          "page": 205,
          "reading_order": 3,
          "bbox": [
            100,
            241,
            359,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Tag</td><td>Meaning</td><td>Examples</td></tr><tr><td>ADJ</td><td>adjective</td><td>new, good, high, special, big, local</td></tr><tr><td>ADV</td><td>adverb</td><td>really, already, still, early, now</td></tr><tr><td>CNJ</td><td>conjunction</td><td>and, or, but, if, while, although</td></tr><tr><td>DET</td><td>determiner</td><td>the, a, some, most, every, no</td></tr><tr><td>EX</td><td>existential</td><td>there, there’s</td></tr><tr><td>FW</td><td>foreign word</td><td>dolce, ersatz, esprit, quo, maitre</td></tr><tr><td>MOD</td><td>modal ver</td><td>will, can, would, may, must, should</td></tr><tr><td>N</td><td>noun</td><td>year, home, costs, time, education</td></tr><tr><td>NP</td><td>proper noun</td><td>Alison, Africa, April, Washington</td></tr><tr><td>NUM</td><td>number</td><td>twenty-four, fourth, 1991, 14:24</td></tr><tr><td>PRO</td><td>pronoun</td><td>he, their, her, its, my, I, us</td></tr><tr><td>P</td><td>preposition</td><td>on, of, at, with, by, into, under</td></tr><tr><td>TO</td><td>the word to</td><td>to</td></tr><tr><td>UH</td><td>nterjection</td><td>ah, bang, ha, whee, hmpf, oops</td></tr><tr><td>V</td><td>ver</td><td>s, has, get, do, make, see, run</td></tr><tr><td>VD</td><td>past tense</td><td>said, took, told, made, asked</td></tr><tr><td>VG</td><td>present participle</td><td>making, going, playing, working</td></tr><tr><td>VN</td><td>past participle</td><td>given, taken, begun, sung</td></tr><tr><td>WH</td><td>wh determiner</td><td>who, which, when, what, where, how</td></tr></table>",
              "bbox": [
                100,
                241,
                359,
                645
              ],
              "page": 205,
              "reading_order": 3
            },
            {
              "label": "cap",
              "text": "Table 5-1. Simplified part-of-speech tagset",
              "bbox": [
                99,
                224,
                306,
                241
              ],
              "page": 205,
              "reading_order": 4
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_205_order_5",
          "label": "foot",
          "text": "5.2 Tagged Corpora | 183",
          "level": -1,
          "page": 205,
          "reading_order": 5,
          "bbox": [
            467,
            824,
            584,
            842
          ],
          "section_number": "5.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_206_order_0",
          "label": "figure",
          "text": "Figure 5-1. POS tagged data from four Indian languages: Bangla, Hindi, Marathi, and Telugu [IMAGE: ![Figure](figures/NLTK_page_206_figure_000.png)]",
          "level": -1,
          "page": 206,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_206_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                206
              ],
              "page": 206,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 5-1. POS tagged data from four Indian languages: Bangla, Hindi, Marathi, and Telugu",
              "bbox": [
                97,
                215,
                557,
                232
              ],
              "page": 206,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_206_order_2",
          "label": "para",
          "text": "Let's see which of these tags are the most common in the news category of the Brown\nCorpus:",
          "level": -1,
          "page": 206,
          "reading_order": 2,
          "bbox": [
            97,
            268,
            584,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_206_order_4",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_206_figure_004.png)",
          "level": -1,
          "page": 206,
          "reading_order": 4,
          "bbox": [
            118,
            385,
            171,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_206_order_5",
          "label": "para",
          "text": "Your Turn: Plot the frequency distribution just shown using\ntag_fd.plot(cumulative=True) . What percentage of words are tagged\nusing the first five tags of the above list?",
          "level": -1,
          "page": 206,
          "reading_order": 5,
          "bbox": [
            171,
            401,
            530,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_206_order_6",
          "label": "para",
          "text": "We can use these tags to do powerful searches using a graphical POS-concordance tool\n\nnltk.app.concordance(). Use it to search for any combination of words and POS tags,\ne.g., N_N_N_N, hit/VD, hit/VN, or the ADJ man.",
          "level": -1,
          "page": 206,
          "reading_order": 6,
          "bbox": [
            97,
            474,
            584,
            522
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_206_order_7",
      "label": "sec",
      "text": "Nouns",
      "level": 1,
      "page": 206,
      "reading_order": 7,
      "bbox": [
        98,
        537,
        139,
        555
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_206_order_8",
          "label": "para",
          "text": "Nouns generally refer to people, places, things, or concepts, e.g., woman, Scotland,\nbook, intelligence . Nouns can appear after determiners and adjectives, and can be the\nsubject or object of the verb, as shown in Table 5-2 .",
          "level": -1,
          "page": 206,
          "reading_order": 8,
          "bbox": [
            97,
            564,
            585,
            612
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_206_order_9",
          "label": "table",
          "text": "Table 5-2. Syntactic patterns involving some nouns [TABLE: <table><tr><td>Word</td><td>After a determiner</td><td>Subject of the verb</td></tr><tr><td>woman</td><td>the woman who I saw yesterday ...</td><td>the woman sat down</td></tr><tr><td>Scotland</td><td>the Scotland I remember as a child ...</td><td>Scotland has five million people</td></tr><tr><td>ook</td><td>the book I bought yesterday ...</td><td>this book recounts the colonization of Australia</td></tr><tr><td>ntelligence</td><td>the intelligence displayed by the child ...</td><td>Mary’s intelligence impressed her teachers</td></tr></table>]",
          "level": -1,
          "page": 206,
          "reading_order": 9,
          "bbox": [
            100,
            645,
            512,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Word</td><td>After a determiner</td><td>Subject of the verb</td></tr><tr><td>woman</td><td>the woman who I saw yesterday ...</td><td>the woman sat down</td></tr><tr><td>Scotland</td><td>the Scotland I remember as a child ...</td><td>Scotland has five million people</td></tr><tr><td>ook</td><td>the book I bought yesterday ...</td><td>this book recounts the colonization of Australia</td></tr><tr><td>ntelligence</td><td>the intelligence displayed by the child ...</td><td>Mary’s intelligence impressed her teachers</td></tr></table>",
              "bbox": [
                100,
                645,
                512,
                743
              ],
              "page": 206,
              "reading_order": 9
            },
            {
              "label": "cap",
              "text": "Table 5-2. Syntactic patterns involving some nouns",
              "bbox": [
                99,
                626,
                350,
                639
              ],
              "page": 206,
              "reading_order": 10
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_206_order_11",
          "label": "para",
          "text": "The simplified noun tags are N for common nouns like book, and NP for proper nouns\nike Scotland.",
          "level": -1,
          "page": 206,
          "reading_order": 11,
          "bbox": [
            100,
            761,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_206_order_12",
          "label": "foot",
          "text": "184 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 206,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "184",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_207_order_0",
          "label": "para",
          "text": "Let's inspect some tagged text to see what parts-of-speech occur before a noun, with\nthe most frequent ones first. To begin with, we construct a list of bigrams whose mem-\nbers are themselves word-tag pairs, such as (('The', 'DET'), ('Fulton', 'NP')) and\n(('Fulton', 'NP'), ('County', 'N')). Then we construct a FreqDist from the tag parts\nof the bigrams.",
          "level": -1,
          "page": 207,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_207_order_2",
          "label": "para",
          "text": "This confirms our assertion that nouns occur after determiners and adjectives, includ-\ning numeral adjectives (tagged as NUM ).",
          "level": -1,
          "page": 207,
          "reading_order": 2,
          "bbox": [
            100,
            212,
            584,
            242
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_207_order_3",
      "label": "sec",
      "text": "Verbs",
      "level": 1,
      "page": 207,
      "reading_order": 3,
      "bbox": [
        99,
        258,
        135,
        277
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_207_order_4",
          "label": "para",
          "text": "Verbs are words that describe events and actions, e.g., fall and eat , as shown in Ta-\nble 5-3 . In the context of a sentence, verbs typically express a relation involving the\nreferents of one or more noun phrases.",
          "level": -1,
          "page": 207,
          "reading_order": 4,
          "bbox": [
            97,
            286,
            585,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_207_order_5",
          "label": "table",
          "text": "Table 5-3. Syntactic patterns involving some verbs [TABLE: <table><tr><td>Word</td><td>Simple</td><td>With modifiers and adjuncts (italicized)</td></tr><tr><td>fall</td><td>Rome fell</td><td>Dot com stocks suddenly fell like a stone</td></tr><tr><td>eat</td><td>Mice eat cheese</td><td>John ate the pizza with gusto</td></tr></table>]",
          "level": -1,
          "page": 207,
          "reading_order": 5,
          "bbox": [
            100,
            367,
            386,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Word</td><td>Simple</td><td>With modifiers and adjuncts (italicized)</td></tr><tr><td>fall</td><td>Rome fell</td><td>Dot com stocks suddenly fell like a stone</td></tr><tr><td>eat</td><td>Mice eat cheese</td><td>John ate the pizza with gusto</td></tr></table>",
              "bbox": [
                100,
                367,
                386,
                430
              ],
              "page": 207,
              "reading_order": 5
            },
            {
              "label": "cap",
              "text": "Table 5-3. Syntactic patterns involving some verbs",
              "bbox": [
                99,
                347,
                343,
                360
              ],
              "page": 207,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_207_order_7",
          "label": "para",
          "text": "What are the most common verbs in news text? Let’\ns sort all the verbs by frequency:",
          "level": -1,
          "page": 207,
          "reading_order": 7,
          "bbox": [
            99,
            445,
            583,
            459
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_207_order_9",
          "label": "para",
          "text": "Note that the items being counted in the frequency distribution are word-tag pairs.\nSince words and tags are paired, we can treat the word as a condition and the tag as an\nevent, and initialize a conditional frequency distribution with a list of condition-event\npairs. This lets us see a frequency-ordered list of tags given a word:",
          "level": -1,
          "page": 207,
          "reading_order": 9,
          "bbox": [
            97,
            564,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_207_order_11",
          "label": "para",
          "text": "We can reverse the order of the pairs, so that the tags are the conditions, and the words\nare the events. Now we can see likely words for a given tag:",
          "level": -1,
          "page": 207,
          "reading_order": 11,
          "bbox": [
            97,
            714,
            585,
            745
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_207_order_12",
          "label": "foot",
          "text": "5.2 Tagged Corpora | 185",
          "level": -1,
          "page": 207,
          "reading_order": 12,
          "bbox": [
            467,
            824,
            585,
            842
          ],
          "section_number": "5.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_208_order_1",
          "label": "para",
          "text": "To clarify the distinction between VD (past tense) and VN (past participle), let’s find\nwords that can be both VD and VN, and see some surrounding text:",
          "level": -1,
          "page": 208,
          "reading_order": 1,
          "bbox": [
            97,
            134,
            584,
            166
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_208_order_3",
          "label": "para",
          "text": "In this case, we see that the past participle of kicked is preceded by a form of the auxiliary\nverb have. Is this generally true?",
          "level": -1,
          "page": 208,
          "reading_order": 3,
          "bbox": [
            97,
            295,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_208_order_4",
          "label": "para",
          "text": "Your\nTurn:\nGiven\nthe\nlist\nof\npast\nparticles\nspecified\nby\ncfd2['VN'].keys(), try to collect a list of all the word-tag pairs that im-\nmediately precede items in that list.",
          "level": -1,
          "page": 208,
          "reading_order": 4,
          "bbox": [
            171,
            358,
            530,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_208_order_5",
      "label": "sec",
      "text": "Adjectives and Adverbs",
      "level": 1,
      "page": 208,
      "reading_order": 5,
      "bbox": [
        97,
        430,
        252,
        452
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_208_order_6",
          "label": "para",
          "text": "Two other important word classes are adjectives and adverbs. Adjectives describe\nnouns, and can be used as modifiers (e.g., large in the large pizza), or as predicates (e.g.,\nthe pizza is large). English adjectives can have internal structure (e.g., fall+ing in the\nfalling stocks). Adverbs modify verbs to specify the time, manner, place, or direction of\nthe event described by the verb (e.g., quickly in the stocks fell quickly). Adverbs may\nalso modify adjectives (e.g., really in Mary’s teacher was really nice).",
          "level": -1,
          "page": 208,
          "reading_order": 6,
          "bbox": [
            96,
            456,
            586,
            558
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_208_order_7",
          "label": "para",
          "text": "English has several categories of closed class words in addition to prepositions, such\nas articles (also often called determiners ) (e.g., the , a ), modals (e.g., should , may ),\nand personal pronouns (e.g., she , they ). Each dictionary and grammar classifies these\nwords differently.",
          "level": -1,
          "page": 208,
          "reading_order": 7,
          "bbox": [
            97,
            564,
            585,
            628
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_208_order_8",
          "label": "para",
          "text": "Your Turn: If you are uncertain about some of these parts-of-speech,\nstudy them using nltk.app.concordance() , or watch some of the School-\nhouse Rock! grammar videos available at YouTube, or consult Sec-\ntion 5.9 .",
          "level": -1,
          "page": 208,
          "reading_order": 8,
          "bbox": [
            171,
            654,
            530,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_208_order_9",
          "label": "foot",
          "text": "186 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 208,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "186",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_209_order_0",
      "label": "sec",
      "text": "Unsimplified Tags",
      "level": 1,
      "page": 209,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        216,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_209_order_1",
          "label": "para",
          "text": "Let's find the most frequent nouns of each noun part-of-speech type. The program in\nExample 5-1 finds all tags starting with NN, and provides a few example words for each\none. You will see that there are many variants of NN; the most important contain $ for\npossessive nouns, S for plural nouns (since plural nouns typically end in s), and P for\nproper nouns. In addition, most of the tags have suffix modifiers: -NC for citations,\n-HL for words in headlines, and -TL for titles (a feature of Brown tags).",
          "level": -1,
          "page": 209,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            200
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_209_order_2",
          "label": "para",
          "text": "Example 5-1. Program to find the most frequent noun tags.",
          "level": -1,
          "page": 209,
          "reading_order": 2,
          "bbox": [
            97,
            206,
            387,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_209_order_4",
          "label": "para",
          "text": "When we come to constructing part-of-speech taggers later in this chapter, we will use\nthe unsimplified tags.",
          "level": -1,
          "page": 209,
          "reading_order": 4,
          "bbox": [
            97,
            546,
            585,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_209_order_5",
      "label": "sec",
      "text": "Exploring Tagged Corpora",
      "level": 1,
      "page": 209,
      "reading_order": 5,
      "bbox": [
        98,
        591,
        270,
        618
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_211_order_2",
          "label": "sub_sec",
          "text": "5.3 Mapping Words to Properties Using Python Dictionaries",
          "level": 2,
          "page": 211,
          "reading_order": 2,
          "bbox": [
            97,
            358,
            574,
            385
          ],
          "section_number": "5.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_211_order_4",
              "label": "sub_sub_sec",
              "text": "Indexing Lists Versus Dictionaries",
              "level": 3,
              "page": 211,
              "reading_order": 4,
              "bbox": [
                100,
                536,
                324,
                555
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_211_order_5",
                  "label": "para",
                  "text": "A text, as we have seen, is treated in Python as a list of words. An important property\nof lists is that we can “look up” a particular item by giving its index, e.g., text1[100].\nNotice how we specify a number and get back a word. We can think of a list as a simple\nkind of table, as shown in Figure 5-2.",
                  "level": -1,
                  "page": 211,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    563,
                    585,
                    627
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_211_order_6",
                  "label": "table",
                  "text": "Figure 5-2. List lookup: We access the contents of a Python list with the help of an integer index. [TABLE: <table><tr><td>0</td><td>Call me</td></tr><tr><td>1</td><td>Ishmael</td></tr></table>]",
                  "level": -1,
                  "page": 211,
                  "reading_order": 6,
                  "bbox": [
                    270,
                    645,
                    413,
                    761
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "tab",
                      "text": "<table><tr><td>0</td><td>Call me</td></tr><tr><td>1</td><td>Ishmael</td></tr></table>",
                      "bbox": [
                        270,
                        645,
                        413,
                        761
                      ],
                      "page": 211,
                      "reading_order": 6
                    },
                    {
                      "label": "cap",
                      "text": "Figure 5-2. List lookup: We access the contents of a Python list with the help of an integer index.",
                      "bbox": [
                        97,
                        779,
                        568,
                        792
                      ],
                      "page": 211,
                      "reading_order": 7
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_211_order_8",
                  "label": "foot",
                  "text": "5.3 Mapping Words to Properties Using Python Dictionaries | 189",
                  "level": -1,
                  "page": 211,
                  "reading_order": 8,
                  "bbox": [
                    313,
                    824,
                    585,
                    842
                  ],
                  "section_number": "5.3",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_212_order_0",
                  "label": "para",
                  "text": "Contrast this situation with frequency distributions (Section 1.3), where we specify a\nword and get back a number, e.g., fdist['monstrous'], which tells us the number of\ntimes a given word has occurred in a text. Lookup using words is familiar to anyone\nwho has used a dictionary. Some more examples are shown in Figure 5-3.",
                  "level": -1,
                  "page": 212,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    586,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_212_order_1",
                  "label": "figure",
                  "text": "Figure 5-3. Dictionary lookup: we access the entry of a dictionary using a key such as someone's name,\na web domain, or an English word; other names for dictionary are map, hashmap, hash, and\nassociative array. [IMAGE: ![Figure](figures/NLTK_page_212_figure_001.png)]",
                  "level": -1,
                  "page": 212,
                  "reading_order": 1,
                  "bbox": [
                    100,
                    152,
                    583,
                    286
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_212_figure_001.png)",
                      "bbox": [
                        100,
                        152,
                        583,
                        286
                      ],
                      "page": 212,
                      "reading_order": 1
                    },
                    {
                      "label": "cap",
                      "text": "Figure 5-3. Dictionary lookup: we access the entry of a dictionary using a key such as someone's name,\na web domain, or an English word; other names for dictionary are map, hashmap, hash, and\nassociative array.",
                      "bbox": [
                        97,
                        295,
                        585,
                        340
                      ],
                      "page": 212,
                      "reading_order": 2
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_212_order_3",
                  "label": "para",
                  "text": "In the case of a phonebook, we look up an entry using a name and get back a number.\nWhen we type a domain name in a web browser, the computer looks this up to get\nback an IP address. A word frequency table allows us to look up a word and find its\nfrequency in a text collection. In all these cases, we are mapping from names to num-\nbers, rather than the other way around as with a list. In general, we would like to be\nable to map between arbitrary types of information. Table 5-4 lists a variety of linguistic\nobjects, along with what they map.",
                  "level": -1,
                  "page": 212,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    358,
                    585,
                    474
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_212_order_4",
                  "label": "table",
                  "text": "Table 5-4. Linguistic objects as mappings from keys to values [TABLE: <table><tr><td>Linguistic object</td><td>Maps from</td><td>Maps to</td></tr><tr><td>Document Index</td><td>Word</td><td>List of pages (where word is found)</td></tr><tr><td>Thesaurus</td><td>Word sense</td><td>List of synonyms</td></tr><tr><td>Dictionary</td><td>Headword</td><td>Entry (part-of-speech, sense definitions, etymology)</td></tr><tr><td>Comparative Wordlist</td><td>Gloss term</td><td>Cognates (list of words, one per language)</td></tr><tr><td>Morph Analyzer</td><td>Surface form</td><td>Morphological analysis (list of component morphemes)</td></tr></table>]",
                  "level": -1,
                  "page": 212,
                  "reading_order": 4,
                  "bbox": [
                    100,
                    501,
                    476,
                    627
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "tab",
                      "text": "<table><tr><td>Linguistic object</td><td>Maps from</td><td>Maps to</td></tr><tr><td>Document Index</td><td>Word</td><td>List of pages (where word is found)</td></tr><tr><td>Thesaurus</td><td>Word sense</td><td>List of synonyms</td></tr><tr><td>Dictionary</td><td>Headword</td><td>Entry (part-of-speech, sense definitions, etymology)</td></tr><tr><td>Comparative Wordlist</td><td>Gloss term</td><td>Cognates (list of words, one per language)</td></tr><tr><td>Morph Analyzer</td><td>Surface form</td><td>Morphological analysis (list of component morphemes)</td></tr></table>",
                      "bbox": [
                        100,
                        501,
                        476,
                        627
                      ],
                      "page": 212,
                      "reading_order": 4
                    },
                    {
                      "label": "cap",
                      "text": "Table 5-4. Linguistic objects as mappings from keys to values",
                      "bbox": [
                        99,
                        483,
                        396,
                        501
                      ],
                      "page": 212,
                      "reading_order": 5
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_212_order_6",
                  "label": "para",
                  "text": "Most often, we are mapping from a “word” to some structured object. For example, a\ndocument index maps from a word (which we can represent as a string) to a list of pages\n(represented as a list of integers). In this section, we will see how to represent such\nmappings in Python.",
                  "level": -1,
                  "page": 212,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    644,
                    585,
                    708
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_211_order_3",
              "label": "para",
              "text": "As we have seen, a tagged word of the form (word, tag) is an association between a\nword and a part-of-speech tag. Once we start doing part-of-speech tagging, we will be\ncreating programs that assign a tag to a word, the tag which is most likely in a given\ncontext. We can think of this process as mapping from words to tags. The most natural\nway to store mappings in Python uses the so-called dictionary data type (also known\nas an associative array or hash array in other programming languages). In this sec-\ntion, we look at dictionaries and see how they can represent a variety of language in-\nformation, including parts-of-speech.",
              "level": -1,
              "page": 211,
              "reading_order": 3,
              "bbox": [
                97,
                385,
                585,
                520
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_209_order_6",
          "label": "para",
          "text": "Let’s briefly return to the kinds of exploration of corpora we saw in previous chapters,\nthis time exploiting POS tags.",
          "level": -1,
          "page": 209,
          "reading_order": 6,
          "bbox": [
            97,
            626,
            584,
            656
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_209_order_7",
          "label": "para",
          "text": "Suppose we’re studying the word often and want to see how it is used in text. We could\nask to see the words that follow often:",
          "level": -1,
          "page": 209,
          "reading_order": 7,
          "bbox": [
            97,
            663,
            584,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_209_order_9",
          "label": "para",
          "text": "However, it’s probably more instructive use the tagged_words() method to look at the\npart-of-speech tag of the following words:",
          "level": -1,
          "page": 209,
          "reading_order": 9,
          "bbox": [
            97,
            761,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_209_order_10",
          "label": "foot",
          "text": "5.2 Tagged Corpora | 187",
          "level": -1,
          "page": 209,
          "reading_order": 10,
          "bbox": [
            467,
            824,
            585,
            842
          ],
          "section_number": "5.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_210_order_1",
          "label": "para",
          "text": "Notice that the most high-frequency parts-of-speech following often are verbs. Nouns\nnever appear in this position (in this particular corpus).",
          "level": -1,
          "page": 210,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            585,
            192
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_210_order_2",
          "label": "para",
          "text": "Next, let’s look at some larger context, and find words involving particular sequences\nof tags and words (in this case \"<Verb> to <Verb>\"). In Example 5-2, we consider each\nthree-word window in the sentence ❶ , and check whether they meet our criterion ❷ .\nIf the tags match, we print the corresponding words ❸ .",
          "level": -1,
          "page": 210,
          "reading_order": 2,
          "bbox": [
            97,
            197,
            585,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_210_order_3",
          "label": "para",
          "text": "Example 5-2. Searching for three-word phrases using POS tags.",
          "level": -1,
          "page": 210,
          "reading_order": 3,
          "bbox": [
            97,
            277,
            413,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_210_order_5",
          "label": "para",
          "text": "Finally, let’\ns look for words that are highly ambiguous as to their part-of-speech tag.\nUnderstanding why such words are tagged as they are in each context can help us clarify\nthe distinctions between the tags.",
          "level": -1,
          "page": 210,
          "reading_order": 5,
          "bbox": [
            97,
            518,
            585,
            565
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_210_order_7",
          "label": "foot",
          "text": "188 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 210,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "188",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_211_order_1",
          "label": "para",
          "text": "Your Turn: Open the POS concordance tool nltk.app.concordance()\nand load the complete Brown Corpus (simplified tagset). Now pick\nsome of the words listed at the end of the previous code example and\nsee how the tag of the word correlates with the context of the word. E.g.,\nsearch for near to see all forms mixed together, near/ADJ to see it used\nas an adjective, near N to see just those cases where a noun follows, and\nso forth.",
          "level": -1,
          "page": 211,
          "reading_order": 1,
          "bbox": [
            171,
            231,
            530,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_212_order_7",
      "label": "sec",
      "text": "Dictionaries in Python",
      "level": 1,
      "page": 212,
      "reading_order": 7,
      "bbox": [
        100,
        724,
        243,
        743
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_212_order_8",
          "label": "para",
          "text": "Python provides a dictionary data type that can be used for mapping between arbitrary\ntypes. It is like a conventional dictionary, in that it gives you an efficient way to look\nthings up. However, as we see from Table 5-4, it has a much wider range of uses.",
          "level": -1,
          "page": 212,
          "reading_order": 8,
          "bbox": [
            97,
            751,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_212_order_9",
          "label": "foot",
          "text": "190 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 212,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "190",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_213_order_0",
          "label": "para",
          "text": "To illustrate, we define pos to be an empty dictionary and then add four entries to it,\nspecifying the part-of-speech of some words. We add entries to a dictionary using the\nfamiliar square bracket notation:",
          "level": -1,
          "page": 213,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_213_order_2",
          "label": "para",
          "text": "So, for example, ❶ says that the part-of-speech of colorless is adjective, or more spe-\ncifically, that the key 'colorless' is assigned the value 'ADJ' in dictionary pos . When\nwe inspect the value of pos ❷ we see a set of key-value pairs. Once we have populated\nthe dictionary in this way, we can employ the keys to retrieve values:",
          "level": -1,
          "page": 213,
          "reading_order": 2,
          "bbox": [
            97,
            283,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_213_order_4",
          "label": "para",
          "text": "Of course, we might accidentally use a key that hasn’t been assigned a value",
          "level": -1,
          "page": 213,
          "reading_order": 4,
          "bbox": [
            97,
            412,
            531,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_213_order_6",
          "label": "para",
          "text": "This raises an important question. Unlike lists and strings, where we can use len() to\nwork out which integers will be legal indexes, how do we work out the legal keys for a\ndictionary? If the dictionary is not too big, we can simply inspect its contents by eval-\nuating the variable pos . As we saw earlier in line ❷ , this gives us the key-value pairs.\nNotice that they are not in the same order they were originally entered; this is because\ndictionaries are not sequences but mappings (see Figure 5 - 3 ), and the keys are not\ninherently ordered.",
          "level": -1,
          "page": 213,
          "reading_order": 6,
          "bbox": [
            97,
            499,
            585,
            610
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_213_order_7",
          "label": "para",
          "text": "Alternatively, to just find the keys, we can either convert the dictionary to a list ❶ or\nuse the dictionary in a context where a list is expected, as the parameter of sorted()\nor in a for loop ❸.",
          "level": -1,
          "page": 213,
          "reading_order": 7,
          "bbox": [
            97,
            618,
            585,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_213_order_9",
          "label": "foot",
          "text": "5.3 Mapping Words to Properties Using Python Dictionaries | 191",
          "level": -1,
          "page": 213,
          "reading_order": 9,
          "bbox": [
            313,
            824,
            584,
            842
          ],
          "section_number": "5.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_214_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_214_figure_000.png)",
          "level": -1,
          "page": 214,
          "reading_order": 0,
          "bbox": [
            118,
            71,
            171,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_214_order_1",
          "label": "para",
          "text": "When you type list(pos), you might see a different order to the one\nshown here. If you want to see the keys in order, just sort them.",
          "level": -1,
          "page": 214,
          "reading_order": 1,
          "bbox": [
            171,
            80,
            530,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_214_order_2",
          "label": "para",
          "text": "As well as iterating over all keys in the dictionary with a for loop, we can use the for\nloop as we did for printing lists:",
          "level": -1,
          "page": 214,
          "reading_order": 2,
          "bbox": [
            97,
            158,
            584,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_214_order_4",
          "label": "para",
          "text": "Finally, the dictionary methods keys() , values() , and items() allow us to access the\nkeys, values, and key-value pairs as separate lists. We can even sort tuples ❶ , which\norders them according to their first element (and if the first elements are the same, it\nuses their second elements).",
          "level": -1,
          "page": 214,
          "reading_order": 4,
          "bbox": [
            97,
            295,
            585,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_214_order_6",
          "label": "para",
          "text": "We want to be sure that when we look something up in a dictionary, we get only one\nvalue for each key. Now suppose we try to use a dictionary to store the fact that the\nword sleep can be used as both a verb and a noun:",
          "level": -1,
          "page": 214,
          "reading_order": 6,
          "bbox": [
            97,
            546,
            585,
            592
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_214_order_8",
          "label": "para",
          "text": "Initially, pos['sleep'] is given the value 'V'. But this is immediately overwritten with\nthe new value, 'N'. In other words, there can be only one entry in the dictionary for\n'sleep'. However, there is a way of storing multiple values in that entry: we use a list\nvalue, e.g., pos['sleep'] = ['N', 'V']. In fact, this is what we saw in Section 2.4 for\nthe CMU Pronouncing Dictionary, which stores multiple pronunciations for a single\nword.",
          "level": -1,
          "page": 214,
          "reading_order": 8,
          "bbox": [
            97,
            689,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_214_order_9",
          "label": "foot",
          "text": "192 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 214,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "192",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_215_order_0",
      "label": "sec",
      "text": "Defining Dictionaries",
      "level": 1,
      "page": 215,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        243,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_215_order_1",
          "label": "para",
          "text": "We can use the same key-value pair format to create a dictionary. There are a couple\nof ways to do this, and we will normally use the first:",
          "level": -1,
          "page": 215,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_215_order_3",
          "label": "para",
          "text": "Note that dictionary keys must be immutable types, such as strings and tuples. If we\ntry to define a dictionary using a mutable key, we get a TypeError:",
          "level": -1,
          "page": 215,
          "reading_order": 3,
          "bbox": [
            97,
            170,
            585,
            208
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_215_order_5",
      "label": "sec",
      "text": "Default Dictionaries",
      "level": 1,
      "page": 215,
      "reading_order": 5,
      "bbox": [
        98,
        277,
        234,
        298
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_215_order_6",
          "label": "para",
          "text": "If we try to access a key that is not in a dictionary, we get an error. However, it’s often\nuseful if a dictionary can automatically create an entry for this new key and give it a\ndefault value, such as zero or the empty list. Since Python 2.5, a special kind of dic-\ntionary called a defaultdict has been available. (It is provided as nltk.defaultdict for\nthe benefit of readers who are using Python 2.4.) In order to use it, we have to supply\na parameter which can be used to create the default value, e.g., int, float, str, list,\ndict, tuple.",
          "level": -1,
          "page": 215,
          "reading_order": 6,
          "bbox": [
            97,
            304,
            585,
            424
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_215_order_8",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_215_figure_008.png)",
          "level": -1,
          "page": 215,
          "reading_order": 8,
          "bbox": [
            118,
            546,
            171,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_215_order_9",
          "label": "para",
          "text": "These default values are actually functions that convert other objects to\nthe specified type (e.g., int(\"2\"), list(\"2\")). When they are called with\nno parameter—say, int(), list()—they return 0 and [] respectively.",
          "level": -1,
          "page": 215,
          "reading_order": 9,
          "bbox": [
            171,
            555,
            530,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_215_order_10",
          "label": "para",
          "text": "The preceding examples specified the default value of a dictionary entry to be the default\nvalue of a particular data type. However, we can specify any default value we like, simply\nby providing the name of a function that can be called with no arguments to create the\nrequired value. Let's return to our part-of-speech example, and create a dictionary\nwhose default value for any entry is 'N' ❶ . When we access a non-existent entry ⊘ , it\nis automatically added to the dictionary ⊘ .",
          "level": -1,
          "page": 215,
          "reading_order": 10,
          "bbox": [
            97,
            636,
            585,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_215_order_12",
          "label": "foot",
          "text": "5.3 Mapping Words to Properties Using Python Dictionaries | 193",
          "level": -1,
          "page": 215,
          "reading_order": 12,
          "bbox": [
            306,
            824,
            584,
            842
          ],
          "section_number": "5.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_216_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_216_figure_001.png)",
          "level": -1,
          "page": 216,
          "reading_order": 1,
          "bbox": [
            118,
            116,
            162,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_216_order_2",
          "label": "para",
          "text": "This example used a lambda expression , introduced in Section 4.4 . This\nlambda expression specifies no parameters, so we call it using paren-\ntheses with no arguments. Thus, the following definitions of f and g are\nequivalent:",
          "level": -1,
          "page": 216,
          "reading_order": 2,
          "bbox": [
            171,
            125,
            530,
            181
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_216_order_4",
          "label": "para",
          "text": "Let's see how default dictionaries could be used in a more substantial language pro-\ncessing task. Many language processing tasks—including tagging—struggle to cor-\nrectly process the hapaxes of a text. They can perform better with a fixed vocabulary\nand a guarantee that no new words will appear. We can preprocess a text to replace\nlow-frequency words with a special “ out of vocabulary ” token, UNK , with the help of a\ndefault dictionary. (Can you work out how to do this without reading on?)",
          "level": -1,
          "page": 216,
          "reading_order": 4,
          "bbox": [
            97,
            295,
            585,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_216_order_5",
          "label": "para",
          "text": "We need to create a default dictionary that maps each word to its replacement. The\nmost frequent n words will be mapped to themselves. Everything else will be mapped\nto UNK.",
          "level": -1,
          "page": 216,
          "reading_order": 5,
          "bbox": [
            97,
            401,
            585,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_216_order_7",
      "label": "sec",
      "text": "Incrementally Updating a Dictionary",
      "level": 1,
      "page": 216,
      "reading_order": 7,
      "bbox": [
        98,
        716,
        342,
        737
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_216_order_8",
          "label": "para",
          "text": "We can employ dictionaries to count occurrences, emulating the method for tallying\nwords shown in Figure 1-3. We begin by initializing an empty defaultdict, then process\neach part-of-speech tag in the text. If the tag hasn’t been seen before, it will have a zero",
          "level": -1,
          "page": 216,
          "reading_order": 8,
          "bbox": [
            97,
            743,
            585,
            792
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_216_order_9",
          "label": "foot",
          "text": "194 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 216,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "194",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_217_order_0",
          "label": "para",
          "text": "count by default. Each time we encounter a tag, we increment its count using the +=\noperator (see Example 5-3).",
          "level": -1,
          "page": 217,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_217_order_1",
          "label": "para",
          "text": "Example 5-3. Incrementally updating a dictionary, and sorting by value.",
          "level": -1,
          "page": 217,
          "reading_order": 1,
          "bbox": [
            97,
            116,
            451,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_217_order_4",
          "label": "para",
          "text": "The listing in Example 5-3 illustrates an important idiom for sorting a dictionary by its\nvalues, to show words in decreasing order of frequency. The first parameter of\nsorted() is the items to sort, which is a list of tuples consisting of a POS tag and a\nfrequency. The second parameter specifies the sort key using a function itemget\nter(). In general, itemgetter(n) returns a function that can be called on some other\nsequence object to obtain the n th element:",
          "level": -1,
          "page": 217,
          "reading_order": 4,
          "bbox": [
            97,
            349,
            586,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_217_order_6",
          "label": "para",
          "text": "The last parameter of sorted() specifies that the items should be returned in reverse\norder, i.e., decreasing values of frequency.",
          "level": -1,
          "page": 217,
          "reading_order": 6,
          "bbox": [
            100,
            528,
            585,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_217_order_7",
          "label": "para",
          "text": "There's a second useful programming idiom at the beginning of Example 5-3, where\nwe initialize a defaultdict and then use a for loop to update its values. Here's a sche-\nmatic version:",
          "level": -1,
          "page": 217,
          "reading_order": 7,
          "bbox": [
            97,
            564,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_217_order_9",
          "label": "para",
          "text": "Here's another instance of this pattern, where we index words according to their last\ntwo letters:",
          "level": -1,
          "page": 217,
          "reading_order": 9,
          "bbox": [
            97,
            672,
            584,
            701
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_217_order_11",
          "label": "foot",
          "text": "5.3 Mapping Words to Properties Using Python Dictionaries | 195",
          "level": -1,
          "page": 217,
          "reading_order": 11,
          "bbox": [
            306,
            824,
            585,
            842
          ],
          "section_number": "5.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_218_order_1",
          "label": "para",
          "text": "The following example uses the same pattern to create an anagram dictionary. (You\nmight experiment with the third line to get an idea of why this program works.)",
          "level": -1,
          "page": 218,
          "reading_order": 1,
          "bbox": [
            100,
            143,
            584,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_218_order_3",
          "label": "para",
          "text": "Since accumulating words like this is such a common task, NLTK provides a more\nconvenient way of creating a defaultdict(list), in the form of nltk.Index():",
          "level": -1,
          "page": 218,
          "reading_order": 3,
          "bbox": [
            97,
            286,
            585,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_218_order_5",
          "label": "para",
          "text": "nltk.Index is a defaultdict(list) with extra support for initialization.\n\n\nSimilarly, nltk.FreqDist is essentially a defaultdict(int) with extra\nsupport for initialization (along with sorting and plotting methods).",
          "level": -1,
          "page": 218,
          "reading_order": 5,
          "bbox": [
            171,
            392,
            530,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_218_order_6",
      "label": "sec",
      "text": "Complex Keys and Values",
      "level": 1,
      "page": 218,
      "reading_order": 6,
      "bbox": [
        97,
        465,
        270,
        485
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_218_order_7",
          "label": "para",
          "text": "We can use default dictionaries with complex keys and values. Let’s study the range of\npossible tags for a word, given the word itself and the tag of the previous word. We will\nsee how this information can be used by a POS tagger.",
          "level": -1,
          "page": 218,
          "reading_order": 7,
          "bbox": [
            97,
            492,
            586,
            541
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_218_order_9",
          "label": "para",
          "text": "This example uses a dictionary whose default value for an entry is a dictionary (whose\ndefault value is int(), i.e., zero). Notice how we iterated over the bigrams of the tagged\ncorpus, processing a pair of word-tag pairs for each iteration O . Each time through the\nloop we updated our pos dictionary's entry for (t1, w2), a tag and its following word\nO . When we look up an item in pos we must specify a compound key O , and we get\nback a dictionary object. A POS tagger could use such information to decide that the\nword right, when preceded by a determiner, should be tagged as ADJ .",
          "level": -1,
          "page": 218,
          "reading_order": 9,
          "bbox": [
            97,
            645,
            585,
            763
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_218_order_10",
          "label": "foot",
          "text": "196 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 218,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "196",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_219_order_0",
      "label": "sec",
      "text": "Inverting a Dictionary",
      "level": 1,
      "page": 219,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        243,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_220_order_2",
          "label": "sub_sec",
          "text": "5.4 Automatic Tagging",
          "level": 2,
          "page": 220,
          "reading_order": 2,
          "bbox": [
            97,
            374,
            282,
            396
          ],
          "section_number": "5.4",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_220_order_5",
              "label": "sub_sub_sec",
              "text": "The Default Tagger",
              "level": 3,
              "page": 220,
              "reading_order": 5,
              "bbox": [
                97,
                528,
                225,
                549
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_220_order_6",
                  "label": "para",
                  "text": "The simplest possible tagger assigns the same tag to each token. This may seem to be\na rather banal step, but it establishes an important baseline for tagger performance. In\norder to get the best result, we tag each word with the most likely tag. Let’s find out\nwhich tag is most likely (now using the unsimplified tagset):",
                  "level": -1,
                  "page": 220,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    555,
                    585,
                    621
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_220_order_8",
                  "label": "para",
                  "text": "Now we can create a tagger that tags everything as NN.",
                  "level": -1,
                  "page": 220,
                  "reading_order": 8,
                  "bbox": [
                    98,
                    678,
                    404,
                    692
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_220_order_10",
                  "label": "foot",
                  "text": "198 | Chapter 5: Categorizing and Tagging Words",
                  "level": -1,
                  "page": 220,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    308,
                    842
                  ],
                  "section_number": "198",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_221_order_1",
                  "label": "para",
                  "text": "Unsurprisingly, this method performs rather poorly. On a typical corpus, it will tag\nonly about an eighth of the tokens correctly, as we see here:",
                  "level": -1,
                  "page": 221,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    107,
                    585,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_221_order_3",
                  "label": "para",
                  "text": "Default taggers assign their tag to every single word, even words that have never been\nencountered before. As it happens, once we have processed several thousand words of\nEnglish text, most new words will be nouns. As we will see, this means that default\ntaggers can help to improve the robustness of a language processing system. We will\nreturn to them shortly.",
                  "level": -1,
                  "page": 221,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    179,
                    584,
                    260
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_220_order_3",
              "label": "para",
              "text": "In the rest of this chapter we will explore various ways to automatically add part-of-\nspeech tags to text. We will see that the tag of a word depends on the word and its\ncontext within a sentence. For this reason, we will be working with data at the level of\n(tagged) sentences rather than words. We’ll begin by loading the data we will be using.",
              "level": -1,
              "page": 220,
              "reading_order": 3,
              "bbox": [
                97,
                403,
                586,
                468
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_219_order_1",
          "label": "para",
          "text": "Dictionaries support efficient lookup, so long as you want to get the value for any key.\nIf d is a dictionary and k is a key, we type d[k] and immediately obtain the value. Finding\na key given a value is slower and more cumbersome:",
          "level": -1,
          "page": 219,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_219_order_3",
          "label": "para",
          "text": "If we expect to do this kind of “reverse lookup\n”\noften, it helps to construct a dictionary\nthat maps values to keys. In the case that no two keys have the same value, this is an\neasy thing to do. We just get all the key-value pairs in the dictionary, and create a new\ndictionary of value-key pairs. The next example also illustrates another way of initial-\nizing a dictionary pos with key-value pairs.",
          "level": -1,
          "page": 219,
          "reading_order": 3,
          "bbox": [
            97,
            259,
            585,
            340
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_219_order_5",
          "label": "para",
          "text": "Let's first make our part-of-speech dictionary a bit more realistic and add some more\nwords to pos using the dictionary update() method, to create the situation where mul-\ntiple keys have the same value. Then the technique just shown for reverse lookup will\nno longer work (why not?). Instead, we have to use append() to accumulate the words\nfor each part-of-speech, as follows:",
          "level": -1,
          "page": 219,
          "reading_order": 5,
          "bbox": [
            97,
            403,
            585,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_219_order_7",
          "label": "para",
          "text": "Now we have inverted the pos dictionary, and can look up any part-of-speech and find\nall words having that part-of-speech. We can do the same thing even more simply using\nNLTK's support for indexing, as follows:",
          "level": -1,
          "page": 219,
          "reading_order": 7,
          "bbox": [
            97,
            591,
            585,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_219_order_9",
          "label": "para",
          "text": "A summary of Python’s dictionary methods is given in Table 5-5.",
          "level": -1,
          "page": 219,
          "reading_order": 9,
          "bbox": [
            97,
            698,
            468,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_219_order_10",
          "label": "foot",
          "text": "5.3 Mapping Words to Properties Using Python Dictionaries | 197",
          "level": -1,
          "page": 219,
          "reading_order": 10,
          "bbox": [
            306,
            824,
            585,
            842
          ],
          "section_number": "5.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_220_order_0",
          "label": "table",
          "text": "Table 5-5. Python's dictionary methods: A summary of commonly used methods and idioms involving\ndictionaries [TABLE: <table><tr><td>Example</td><td>Description</td></tr><tr><td>d = {}</td><td>Create an empty dictionary and assign it to d</td></tr><tr><td>d[key] = value</td><td>Assign a value to a given dictionary key</td></tr><tr><td>d.keys()</td><td>The list of keys of the dictionary</td></tr><tr><td>list(d)</td><td>The list of keys of the dictionary</td></tr><tr><td>sorted(d)</td><td>The keys of the dictionary, sorted</td></tr><tr><td>key in d</td><td>Test whether a particular key is in the dictionary</td></tr><tr><td>for key in d</td><td>Iterate over the keys of the dictionary</td></tr><tr><td>d.values()</td><td>The list of values in the dictionary</td></tr><tr><td>dict([(k1,v1), (k2,v2),...])</td><td>Create a dictionary from a list of key-value pairs</td></tr><tr><td>d1.update(d2)</td><td>Add all items from d2 to d1</td></tr><tr><td>defaultdict(int)</td><td>A dictionary whose default value is zero</td></tr></table>]",
          "level": -1,
          "page": 220,
          "reading_order": 0,
          "bbox": [
            100,
            107,
            467,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Example</td><td>Description</td></tr><tr><td>d = {}</td><td>Create an empty dictionary and assign it to d</td></tr><tr><td>d[key] = value</td><td>Assign a value to a given dictionary key</td></tr><tr><td>d.keys()</td><td>The list of keys of the dictionary</td></tr><tr><td>list(d)</td><td>The list of keys of the dictionary</td></tr><tr><td>sorted(d)</td><td>The keys of the dictionary, sorted</td></tr><tr><td>key in d</td><td>Test whether a particular key is in the dictionary</td></tr><tr><td>for key in d</td><td>Iterate over the keys of the dictionary</td></tr><tr><td>d.values()</td><td>The list of values in the dictionary</td></tr><tr><td>dict([(k1,v1), (k2,v2),...])</td><td>Create a dictionary from a list of key-value pairs</td></tr><tr><td>d1.update(d2)</td><td>Add all items from d2 to d1</td></tr><tr><td>defaultdict(int)</td><td>A dictionary whose default value is zero</td></tr></table>",
              "bbox": [
                100,
                107,
                467,
                349
              ],
              "page": 220,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Table 5-5. Python's dictionary methods: A summary of commonly used methods and idioms involving\ndictionaries",
              "bbox": [
                97,
                71,
                585,
                99
              ],
              "page": 220,
              "reading_order": 1
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_221_order_4",
      "label": "sec",
      "text": "The Regular Expression Tagger",
      "level": 1,
      "page": 221,
      "reading_order": 4,
      "bbox": [
        97,
        277,
        301,
        298
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_221_order_5",
          "label": "para",
          "text": "The regular expression tagger assigns tags to tokens on the basis of matching patterns.\nFor instance, we might guess that any word ending in ed is the past participle of a verb,\nand any word ending with ’s is a possessive noun. We can express these as a list of\nregular expressions:",
          "level": -1,
          "page": 221,
          "reading_order": 5,
          "bbox": [
            97,
            304,
            584,
            370
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_221_order_7",
          "label": "para",
          "text": "Note that these are processed in order, and the first one that matches is applied. Now\nwe can set up a tagger and use it to tag a sentence. After this step, it is correct about a\nfifth of the time.",
          "level": -1,
          "page": 221,
          "reading_order": 7,
          "bbox": [
            97,
            518,
            585,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_221_order_9",
          "label": "para",
          "text": "The final regular expression «.\n*» is a catch-all that tags everything as a noun. This is\nequivalent to the default tagger (only much less efficient). Instead of respecifying this\nas part of the regular expression tagger, is there a way to combine this tagger with the\ndefault tagger? We will see how to do this shortly.",
          "level": -1,
          "page": 221,
          "reading_order": 9,
          "bbox": [
            97,
            687,
            585,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_221_order_10",
          "label": "foot",
          "text": "5.4 Automatic Tagging | 199",
          "level": -1,
          "page": 221,
          "reading_order": 10,
          "bbox": [
            458,
            824,
            585,
            842
          ],
          "section_number": "5.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_222_order_0",
          "label": "para",
          "text": "Your Turn: See if you can come up with patterns to improve the per-\nformance of the regular expression tagger just shown. (Note that Sec-\ntion 6.1 describes a way to partially automate such work.)",
          "level": -1,
          "page": 222,
          "reading_order": 0,
          "bbox": [
            171,
            80,
            530,
            127
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_222_order_1",
      "label": "sec",
      "text": "The Lookup Tagger",
      "level": 1,
      "page": 222,
      "reading_order": 1,
      "bbox": [
        100,
        159,
        225,
        179
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_223_order_0",
          "label": "sub_sec",
          "text": "Example 5-4. Lookup tagger performance with varying model size",
          "level": 2,
          "page": 223,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            422,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_223_order_3",
          "label": "sub_sec",
          "text": ">>> display()",
          "level": 2,
          "page": 223,
          "reading_order": 3,
          "bbox": [
            98,
            313,
            171,
            325
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_223_order_4",
              "label": "para",
              "text": "Observe in Figure 5 - 4 that performance initially increases rapidly as the model size\ngrows, eventually reaching a plateau, when large increases in model size yield little\nimprovement in performance. (This example used the pylab plotting package, dis-\ncussed in Section 4.8 .)",
              "level": -1,
              "page": 223,
              "reading_order": 4,
              "bbox": [
                97,
                339,
                585,
                403
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_222_order_2",
          "label": "para",
          "text": "A lot of high-frequency words do not have the NN tag. Let’s find the hundred most\nfrequent words and store their most likely tag. We can then use this information as the\nmodel for a “lookup tagger” (an NLTK UnigramTagger):",
          "level": -1,
          "page": 222,
          "reading_order": 2,
          "bbox": [
            97,
            186,
            585,
            233
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_222_order_4",
          "label": "para",
          "text": "It should come as no surprise by now that simply knowing the tags for the 100 most\nfrequent words enables us to tag a large fraction of tokens correctly (nearly half, in fact).\nLet’s see what it does on some untagged input text:",
          "level": -1,
          "page": 222,
          "reading_order": 4,
          "bbox": [
            98,
            340,
            584,
            389
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_222_order_6",
          "label": "para",
          "text": "Many words have been assigned a tag of None, because they were not among the 100\nmost frequent words. In these cases we would like to assign the default tag of NN. In\nother words, we want to use the lookup table first, and if it is unable to assign a tag,\nthen use the default tagger, a process known as backoff (Section 5.5). We do this by\nspecifying one tagger as a parameter to the other, as shown next. Now the lookup tagger\nwill only store word-tag pairs for words other than nouns, and whenever it cannot\nassign a tag to a word, it will invoke the default tagger.",
          "level": -1,
          "page": 222,
          "reading_order": 6,
          "bbox": [
            97,
            546,
            585,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_222_order_8",
          "label": "para",
          "text": "Let’s put all this together and write a program to create and evaluate lookup taggers\nhaving a range of sizes ( Example 5-4 ).",
          "level": -1,
          "page": 222,
          "reading_order": 8,
          "bbox": [
            97,
            706,
            585,
            737
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_222_order_9",
          "label": "foot",
          "text": "200 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 222,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "200",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_223_order_5",
      "label": "sec",
      "text": "Evaluation",
      "level": 1,
      "page": 223,
      "reading_order": 5,
      "bbox": [
        98,
        421,
        171,
        440
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_224_order_3",
          "label": "sub_sec",
          "text": "5.5 N-Gram Tagging",
          "level": 2,
          "page": 224,
          "reading_order": 3,
          "bbox": [
            97,
            627,
            261,
            655
          ],
          "section_number": "5.5",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_224_order_4",
              "label": "sub_sub_sec",
              "text": "Unigram Tagging",
              "level": 3,
              "page": 224,
              "reading_order": 4,
              "bbox": [
                100,
                670,
                213,
                689
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_224_order_5",
                  "label": "para",
                  "text": "Unigram taggers are based on a simple statistical algorithm: for each token, assign the\ntag that is most likely for that particular token. For example, it will assign the tag JJ to\nany occurrence of the word frequent , since frequent is used as an adjective (e.g., a fre-\nquent word) more often than it is used as a verb (e.g., I frequent this cafe ). A unigram\ntagger behaves just like a lookup tagger ( Section 5.4 ), except there is a more convenient",
                  "level": -1,
                  "page": 224,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    697,
                    585,
                    779
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_224_order_6",
                  "label": "foot",
                  "text": "202 | Chapter 5: Categorizing and Tagging Words",
                  "level": -1,
                  "page": 224,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    824,
                    308,
                    842
                  ],
                  "section_number": "202",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_225_order_0",
                  "label": "para",
                  "text": "technique for setting it up, called training. In the following code sample, we train a\nunigram tagger, use it to tag a sentence, and then evaluate:",
                  "level": -1,
                  "page": 225,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    584,
                    107
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_225_order_2",
                  "label": "para",
                  "text": "We train a UnigramTagger by specifying tagged sentence data as a parameter when we\ninitialize the tagger. The training process involves inspecting the tag of each word and\nstoring the most likely tag for any word in a dictionary that is stored inside the tagger.",
                  "level": -1,
                  "page": 225,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    277,
                    585,
                    326
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_223_order_6",
          "label": "para",
          "text": "In the previous examples, you will have noticed an emphasis on accuracy scores. In\nfact, evaluating the performance of such tools is a central theme in NLP. Recall the\nprocessing pipeline in Figure 1 - 5 ; any errors in the output of one module are greatly\nmultiplied in the downstream modules.",
          "level": -1,
          "page": 223,
          "reading_order": 6,
          "bbox": [
            97,
            453,
            585,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_223_order_7",
          "label": "para",
          "text": "We evaluate the performance of a tagger relative to the tags a human expert would\nassign. Since we usually don't have access to an expert and impartial human judge, we\nmake do instead with gold standard test data. This is a corpus which has been man-\nually annotated and accepted as a standard against which the guesses of an automatic\nsystem are assessed. The tagger is regarded as being correct if the tag it guesses for a\ngiven word is the same as the gold standard tag.",
          "level": -1,
          "page": 223,
          "reading_order": 7,
          "bbox": [
            97,
            526,
            585,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_223_order_8",
          "label": "para",
          "text": "Of course, the humans who designed and carried out the original gold standard anno-\ntation were only human. Further analysis might show mistakes in the gold standard,\nor may eventually lead to a revised tagset and more elaborate guidelines. Nevertheless,\nthe gold standard is by definition “ correct ” as far as the evaluation of an automatic\ntagger is concerned.",
          "level": -1,
          "page": 223,
          "reading_order": 8,
          "bbox": [
            97,
            632,
            585,
            712
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_223_order_9",
          "label": "foot",
          "text": "5.4 Automatic Tagging | 201",
          "level": -1,
          "page": 223,
          "reading_order": 9,
          "bbox": [
            458,
            824,
            584,
            842
          ],
          "section_number": "5.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_224_order_0",
          "label": "figure",
          "text": "Figure 5-4. Lookup tagger [IMAGE: ![Figure](figures/NLTK_page_224_figure_000.png)]",
          "level": -1,
          "page": 224,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_224_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                465
              ],
              "page": 224,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 5-4. Lookup tagger",
              "bbox": [
                97,
                472,
                225,
                485
              ],
              "page": 224,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_224_order_2",
          "label": "para",
          "text": "Developing an annotated corpus is a major undertaking. Apart from the\ndata, it generates sophisticated tools, documentation, and practices for\nensuring high-quality annotation. The tagsets and other coding schemes\ninevitably depend on some theoretical position that is not shared by all.\nHowever, corpus creators often go to great lengths to make their work\nas theory-neutral as possible in order to maximize the usefulness of their\nwork. We will discuss the challenges of creating a corpus in Chapter 11 .",
          "level": -1,
          "page": 224,
          "reading_order": 2,
          "bbox": [
            171,
            501,
            530,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_225_order_3",
      "label": "sec",
      "text": "Separating the Training and Testing Data",
      "level": 1,
      "page": 225,
      "reading_order": 3,
      "bbox": [
        97,
        340,
        371,
        361
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_225_order_4",
          "label": "para",
          "text": "Now that we are training a tagger on some data, we must be careful not to test it on\nthe same data, as we did in the previous example. A tagger that simply memorized its\ntraining data and made no attempt to construct a general model would get a perfect\nscore, but would be useless for tagging new text. Instead, we should split the data,\ntraining on 90% and testing on the remaining 10%:",
          "level": -1,
          "page": 225,
          "reading_order": 4,
          "bbox": [
            97,
            367,
            585,
            450
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_225_order_6",
          "label": "para",
          "text": "Although the score is worse, we now have a better picture of the usefulness of this\ntagger, i.e., its performance on previously unseen text.",
          "level": -1,
          "page": 225,
          "reading_order": 6,
          "bbox": [
            97,
            564,
            585,
            602
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_225_order_7",
      "label": "sec",
      "text": "General N-Gram Tagging",
      "level": 1,
      "page": 225,
      "reading_order": 7,
      "bbox": [
        97,
        618,
        261,
        637
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_227_order_1",
          "label": "sub_sec",
          "text": "Caution!",
          "level": 2,
          "page": 227,
          "reading_order": 1,
          "bbox": [
            171,
            170,
            209,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_227_order_2",
              "label": "para",
              "text": "N-gram taggers should not consider context that crosses a sentence\nboundary. Accordingly, NLTK taggers are designed to work with lists\nof sentences, where each sentence is a list of words. At the start of a\nsentence, $t_{n\\text{-}1}$ and preceding tags are set to $\\texttt{None}$ .",
              "level": -1,
              "page": 227,
              "reading_order": 2,
              "bbox": [
                171,
                188,
                530,
                250
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_225_order_8",
          "label": "para",
          "text": "When we perform a language processing task based on unigrams, we are using one\nitem of context. In the case of tagging, we consider only the current token, in isolation\nfrom any larger context. Given such a model, the best we can do is tag each word with\nits a priori most likely tag. This means we would tag a word such as wind with the same\ntag, regardless of whether it appears in the context the wind or to wind .",
          "level": -1,
          "page": 225,
          "reading_order": 8,
          "bbox": [
            97,
            645,
            585,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_225_order_9",
          "label": "para",
          "text": "An n-gram tagger is a generalization of a unigram tagger whose context is the current\nword together with the part-of-speech tags of the n -1 preceding tokens, as shown in\nFigure 5 - 5 . The tag to be chosen, $t_n$ , is circled, and the context is shaded in grey. In the\nexample of an n-gram tagger shown in Figure 5 - 5 , we have n =3; that is, we consider",
          "level": -1,
          "page": 225,
          "reading_order": 9,
          "bbox": [
            97,
            734,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_225_order_10",
          "label": "foot",
          "text": "5.5 N-Gram Tagging | 203",
          "level": -1,
          "page": 225,
          "reading_order": 10,
          "bbox": [
            467,
            824,
            584,
            842
          ],
          "section_number": "5.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_226_order_0",
          "label": "figure",
          "text": "Figure 5-5. Tagger context. [IMAGE: ![Figure](figures/NLTK_page_226_figure_000.png)]",
          "level": -1,
          "page": 226,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_226_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                224
              ],
              "page": 226,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 5-5. Tagger context.",
              "bbox": [
                97,
                224,
                234,
                241
              ],
              "page": 226,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_226_order_2",
          "label": "para",
          "text": "the tags of the two preceding words in addition to the current word. An n-gram tagger\npicks the tag that is most likely in the given context.",
          "level": -1,
          "page": 226,
          "reading_order": 2,
          "bbox": [
            97,
            250,
            585,
            282
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_226_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_226_figure_003.png)",
          "level": -1,
          "page": 226,
          "reading_order": 3,
          "bbox": [
            118,
            304,
            171,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_226_order_4",
          "label": "para",
          "text": "A 1-gram tagger is another term for a unigram tagger: i.e., the context\nused to tag a token is just the text of the token itself. 2-gram taggers are\nalso called bigram taggers, and 3-gram taggers are called trigram taggers.",
          "level": -1,
          "page": 226,
          "reading_order": 4,
          "bbox": [
            171,
            313,
            530,
            359
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_226_order_5",
          "label": "para",
          "text": "The NgramTagger class uses a tagged training corpus to determine which part-of-speech\ntag is most likely for each context. Here we see a special case of an n-gram tagger,\nnamely a bigram tagger. First we train it, then use it to tag untagged sentences:",
          "level": -1,
          "page": 226,
          "reading_order": 5,
          "bbox": [
            97,
            385,
            584,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_226_order_7",
          "label": "para",
          "text": "Notice that the bigram tagger manages to tag every word in a sentence it saw during\ntraining, but does badly on an unseen sentence. As soon as it encounters a new word\n(i.e., 13.5 ), it is unable to assign a tag. It cannot tag the following word (i.e., million ),\neven if it was seen during training, simply because it never saw it during training with\na None tag on the previous word. Consequently, the tagger fails to tag the rest of the\nsentence. Its overall accuracy score is very low:",
          "level": -1,
          "page": 226,
          "reading_order": 7,
          "bbox": [
            97,
            645,
            585,
            744
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_226_order_9",
          "label": "foot",
          "text": "204 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 226,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "204",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_227_order_0",
          "label": "para",
          "text": "As n gets larger, the specificity of the contexts increases, as does the chance that the\ndata we wish to tag contains contexts that were not present in the training data. This\nis known as the sparse data problem, and is quite pervasive in NLP. As a consequence,\nthere is a trade-off between the accuracy and the coverage of our results (and this is\nrelated to the precision/recall trade-off in information retrieval).",
          "level": -1,
          "page": 227,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_227_order_3",
      "label": "sec",
      "text": "Combining Taggers",
      "level": 1,
      "page": 227,
      "reading_order": 3,
      "bbox": [
        97,
        275,
        226,
        295
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_228_order_0",
          "label": "sub_sec",
          "text": "Tagging Unknown Words",
          "level": 2,
          "page": 228,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            270,
            98
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_228_order_1",
              "label": "para",
              "text": "Our approach to tagging unknown words still uses backoff to a regular expression\ntagger or a default tagger. These are unable to make use of context. Thus, if our tagger\nencountered the word blog, not seen during training, it would assign it the same tag,\nregardless of whether this word appeared in the context the blog or to blog. How can\nwe do better with these unknown words, or out-of-vocabulary items?",
              "level": -1,
              "page": 228,
              "reading_order": 1,
              "bbox": [
                97,
                98,
                585,
                184
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_228_order_2",
              "label": "para",
              "text": "A useful method to tag unknown words based on context is to limit the vocabulary of\na tagger to the most frequent n words, and to replace every other word with a special\nword UNK using the method shown in Section 5.3. During training, a unigram tagger\nwill probably learn that UNK is usually a noun. However, the n-gram taggers will detect\ncontexts in which it has some other tag. For example, if the preceding word is to (tagged\nT0), then UNK will probably be tagged as a verb.",
              "level": -1,
              "page": 228,
              "reading_order": 2,
              "bbox": [
                97,
                188,
                586,
                290
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_228_order_3",
          "label": "sub_sec",
          "text": "Storing Taggers",
          "level": 2,
          "page": 228,
          "reading_order": 3,
          "bbox": [
            97,
            304,
            207,
            325
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_228_order_4",
              "label": "para",
              "text": "Training a tagger on a large corpus may take a significant time. Instead of training a\ntagger every time we need one, it is convenient to save a trained tagger in a file for later\nreuse. Let's save our tagger t2 to a file t2.pkl:",
              "level": -1,
              "page": 228,
              "reading_order": 4,
              "bbox": [
                97,
                331,
                585,
                380
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_228_order_6",
              "label": "para",
              "text": "Now, in a separate Python process, we can load our saved tagger",
              "level": -1,
              "page": 228,
              "reading_order": 6,
              "bbox": [
                98,
                448,
                467,
                465
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_228_order_8",
              "label": "para",
              "text": "Now let’s check that it can be used for tagging",
              "level": -1,
              "page": 228,
              "reading_order": 8,
              "bbox": [
                98,
                528,
                362,
                547
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_228_order_10",
          "label": "sub_sec",
          "text": "Performance Limitations",
          "level": 2,
          "page": 228,
          "reading_order": 10,
          "bbox": [
            98,
            672,
            270,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_228_order_11",
              "label": "para",
              "text": "What is the upper limit to the performance of an n-gram tagger? Consider the case of\na trigram tagger. How many cases of part-of-speech ambiguity does it encounter? We\ncan determine the answer to this question empirically:",
              "level": -1,
              "page": 228,
              "reading_order": 11,
              "bbox": [
                97,
                698,
                586,
                748
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_228_order_12",
              "label": "foot",
              "text": "206 | Chapter 5: Categorizing and Tagging Words",
              "level": -1,
              "page": 228,
              "reading_order": 12,
              "bbox": [
                97,
                824,
                308,
                842
              ],
              "section_number": "206",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_229_order_1",
              "label": "para",
              "text": "Thus, 1 out of 20 trigrams is ambiguous. Given the current word and the previous two\ntags, in 5% of cases there is more than one tag that could be legitimately assigned to\nthe current word according to the training data. Assuming we always pick the most\nlikely tag in such ambiguous contexts, we can derive a lower bound on the performance\nof a trigram tagger.",
              "level": -1,
              "page": 229,
              "reading_order": 1,
              "bbox": [
                97,
                170,
                585,
                254
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_229_order_2",
              "label": "para",
              "text": "Another way to investigate the performance of a tagger is to study its mistakes. Some\ntags may be harder than others to assign, and it might be possible to treat them specially\nby pre- or post-processing the data. A convenient way to look at tagging errors is the\nconfusion matrix. It charts expected tags (the gold standard) against actual tags gen-\nerated by a tagger:",
              "level": -1,
              "page": 229,
              "reading_order": 2,
              "bbox": [
                97,
                259,
                585,
                344
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_229_order_4",
              "label": "para",
              "text": "Based on such analysis we may decide to modify the tagset. Perhaps a distinction be-\ntween tags that is difficult to make can be dropped, since it is not important in the\ncontext of some larger processing task.",
              "level": -1,
              "page": 229,
              "reading_order": 4,
              "bbox": [
                97,
                412,
                584,
                465
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_229_order_5",
              "label": "para",
              "text": "Another way to analyze the performance bound on a tagger comes from the less than\n100% agreement between human annotators.",
              "level": -1,
              "page": 229,
              "reading_order": 5,
              "bbox": [
                97,
                465,
                584,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_229_order_6",
              "label": "para",
              "text": "In general, observe that the tagging process collapses distinctions: e.g., lexical identity\nis usually lost when all personal pronouns are tagged PRP . At the same time, the tagging\nprocess introduces new distinctions and removes ambiguities: e.g., deal tagged as VB or\nNN . This characteristic of collapsing certain distinctions and introducing new distinc-\ntions is an important feature of tagging which facilitates classification and prediction.\nWhen we introduce finer distinctions in a tagset, an n-gram tagger gets more detailed\ninformation about the left-context when it is deciding what tag to assign to a particular\nword. However, the tagger simultaneously has to do more work to classify the current\ntoken, simply because there are more tags to choose from. Conversely, with fewer dis-\ntinctions (as with the simplified tagset), the tagger has less information about context,\nand it has a smaller range of choices in classifying the current token.",
              "level": -1,
              "page": 229,
              "reading_order": 6,
              "bbox": [
                97,
                510,
                585,
                690
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_229_order_7",
              "label": "para",
              "text": "We have seen that ambiguity in the training data leads to an upper limit in tagger\nperformance. Sometimes more context will resolve the ambiguity. In other cases, how-\never, as noted by (Abney, 1996) , the ambiguity can be resolved only with reference to\nsyntax or to world knowledge. Despite these imperfections, part-of-speech tagging has\nplayed a central role in the rise of statistical approaches to natural language processing.\nIn the early 1990s, the surprising accuracy of statistical taggers was a striking",
              "level": -1,
              "page": 229,
              "reading_order": 7,
              "bbox": [
                97,
                698,
                585,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_229_order_8",
              "label": "foot",
              "text": "5.5 N-Gram Tagging | 207",
              "level": -1,
              "page": 229,
              "reading_order": 8,
              "bbox": [
                467,
                824,
                585,
                842
              ],
              "section_number": "5.5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_230_order_0",
              "label": "para",
              "text": "demonstration that it was possible to solve one small part of the language understand-\ning problem, namely part-of-speech disambiguation, without reference to deeper sour-\nces of linguistic knowledge. Can this idea be pushed further? In Chapter 7 , we will see\nthat it can.",
              "level": -1,
              "page": 230,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                136
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_227_order_4",
          "label": "para",
          "text": "One way to address the trade-off between accuracy and coverage is to use the more\naccurate algorithms when we can, but to fall back on algorithms with wider coverage\nwhen necessary. For example, we could combine the results of a bigram tagger, a\nunigram tagger, and a default tagger, as follows:",
          "level": -1,
          "page": 227,
          "reading_order": 4,
          "bbox": [
            97,
            302,
            585,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_227_order_5",
          "label": "list_group",
          "text": "1. Try tagging the token with the bigram tagger.\n2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger.",
          "level": -1,
          "page": 227,
          "reading_order": 5,
          "bbox": [
            100,
            376,
            386,
            394
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "1. Try tagging the token with the bigram tagger.",
              "bbox": [
                100,
                376,
                386,
                394
              ],
              "page": 227,
              "reading_order": 5
            },
            {
              "label": "list",
              "text": "2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger.",
              "bbox": [
                100,
                394,
                575,
                412
              ],
              "page": 227,
              "reading_order": 6
            },
            {
              "label": "list",
              "text": "3. If the unigram tagger is also unable to find a tag, use a default tagger",
              "bbox": [
                100,
                418,
                512,
                432
              ],
              "page": 227,
              "reading_order": 7
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_227_order_8",
          "label": "para",
          "text": "Most NLTK taggers permit a backoff tagger to be specified. The backoff tagger may\nitself have a backoff tagger:",
          "level": -1,
          "page": 227,
          "reading_order": 8,
          "bbox": [
            97,
            439,
            585,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_227_order_10",
          "label": "para",
          "text": "Your Turn: Extend the preceding example by defining a TrigramTag\nger called t3, which backs off to t2.",
          "level": -1,
          "page": 227,
          "reading_order": 10,
          "bbox": [
            171,
            573,
            530,
            601
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_227_order_11",
          "label": "para",
          "text": "Note that we specify the backoff tagger when the tagger is initialized so that training\ncan take advantage of the backoff tagger. Thus, if the bigram tagger would assign the\nsame tag as its unigram backoff tagger in a certain context, the bigram tagger discards\nthe training instance. This keeps the bigram tagger model as small as possible. We can\nfurther specify that a tagger needs to see more than one instance of a context in order\nto retain it. For example, nltk.BigramTagger(sents, cutoff=2, backoff=t1) will dis-\ncard contexts that have only been seen once or twice.",
          "level": -1,
          "page": 227,
          "reading_order": 11,
          "bbox": [
            97,
            645,
            585,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_227_order_12",
          "label": "foot",
          "text": "5.5 N-Gram Tagging | 205",
          "level": -1,
          "page": 227,
          "reading_order": 12,
          "bbox": [
            467,
            824,
            585,
            842
          ],
          "section_number": "5.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_230_order_1",
      "label": "sec",
      "text": "Tagging Across Sentence Boundaries",
      "level": 1,
      "page": 230,
      "reading_order": 1,
      "bbox": [
        97,
        152,
        342,
        174
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_230_order_9",
          "label": "sub_sec",
          "text": "5.6 Transformation-Based Tagging",
          "level": 2,
          "page": 230,
          "reading_order": 9,
          "bbox": [
            97,
            528,
            378,
            555
          ],
          "section_number": "5.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_230_order_10",
              "label": "para",
              "text": "A potential issue with n-gram taggers is the size of their n-gram table (or language\nmodel). If tagging is to be employed in a variety of language technologies deployed on\nmobile computing devices, it is important to strike a balance between model size and\ntagger performance. An n-gram tagger with backoff may store trigram and bigram ta-\nbles, which are large, sparse arrays that may have hundreds of millions of entries.",
              "level": -1,
              "page": 230,
              "reading_order": 10,
              "bbox": [
                97,
                555,
                585,
                640
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_230_order_11",
              "label": "para",
              "text": "A second issue concerns context. The only information an n-gram tagger considers\nfrom prior context is tags, even though words themselves might be a useful source of\ninformation. It is simply impractical for n-gram models to be conditioned on the iden-\ntities of words in the context. In this section, we examine Brill tagging, an inductive\ntagging method which performs very well using models that are only a tiny fraction of\nthe size of n-gram taggers.",
              "level": -1,
              "page": 230,
              "reading_order": 11,
              "bbox": [
                97,
                645,
                586,
                746
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_230_order_12",
              "label": "para",
              "text": "Brill tagging is a kind of transformation-based learning, named after its inventor. The\ngeneral idea is very simple: guess the tag of each word, then go back and fix the mistakes.",
              "level": -1,
              "page": 230,
              "reading_order": 12,
              "bbox": [
                97,
                752,
                585,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_230_order_13",
              "label": "foot",
              "text": "208 | Chapter 5: Categorizing and Tagging Words",
              "level": -1,
              "page": 230,
              "reading_order": 13,
              "bbox": [
                97,
                824,
                308,
                842
              ],
              "section_number": "208",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_231_order_0",
              "label": "para",
              "text": "In this way, a Brill tagger successively transforms a bad tagging of a text into a better\none. As with n-gram tagging, this is a supervised learning method, since we need an-\nnotated training data to figure out whether the tagger’s guess is a mistake or not. How-\never, unlike n-gram tagging, it does not count observations but compiles a list of trans-\nformational correction rules.",
              "level": -1,
              "page": 231,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                161
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_231_order_1",
              "label": "para",
              "text": "The process of Brill tagging is usually explained by analogy with painting. Suppose we\nwere painting a tree, with all its details of boughs, branches, twigs, and leaves, against\na uniform sky-blue background. Instead of painting the tree first and then trying to\npaint blue in the gaps, it is simpler to paint the whole canvas blue, then “correct” the\ntree section by over-painting the blue background. In the same fashion, we might paint\nthe trunk a uniform brown before going back to over-paint further details with even\nfiner brushes. Brill tagging uses the same idea: begin with broad brush strokes, and\nthen fix up the details, with successively finer changes. Let’s look at an example in-\nvolving the following sentence:",
              "level": -1,
              "page": 231,
              "reading_order": 1,
              "bbox": [
                97,
                161,
                585,
                313
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_231_order_2",
              "label": "para",
              "text": "(1) The President said he will ask Congress to increase grants to states for voca-\ntional rehabilitation.",
              "level": -1,
              "page": 231,
              "reading_order": 2,
              "bbox": [
                118,
                322,
                575,
                358
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_231_order_3",
              "label": "para",
              "text": "We will examine the operation of two rules: (a) replace NN with VB when the previous\nword is TO; (b) replace TO with IN when the next tag is NNS. Table 5-6 illustrates this\nprocess, first tagging with the unigram tagger, then applying the rules to fix the errors.",
              "level": -1,
              "page": 231,
              "reading_order": 3,
              "bbox": [
                97,
                367,
                585,
                421
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_231_order_4",
              "label": "table",
              "text": "Table 5-6. Steps in Brill tagging [TABLE: <table><tr><td>Phrase</td><td>to</td><td>increase</td><td>grants</td><td>to</td><td>states</td><td>for</td><td>vocational</td><td>rehabilitation</td></tr><tr><td>Unigram</td><td>TO</td><td>NW</td><td>NNS</td><td>TO</td><td>NNS</td><td>IN</td><td>JJ</td><td>NN</td></tr><tr><td>Rule 1</td><td></td><td>VB</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Rule 2</td><td></td><td></td><td></td><td>IN</td><td></td><td></td><td></td><td></td></tr><tr><td>Output</td><td>TO</td><td>VB</td><td>NNS</td><td>IN</td><td>NNS</td><td>IN</td><td>JJ</td><td>NN</td></tr><tr><td>Gold</td><td>TO</td><td>VB</td><td>NNS</td><td>IN</td><td>NNS</td><td>IN</td><td>JJ</td><td>NN</td></tr></table>]",
              "level": -1,
              "page": 231,
              "reading_order": 4,
              "bbox": [
                100,
                448,
                468,
                573
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "tab",
                  "text": "<table><tr><td>Phrase</td><td>to</td><td>increase</td><td>grants</td><td>to</td><td>states</td><td>for</td><td>vocational</td><td>rehabilitation</td></tr><tr><td>Unigram</td><td>TO</td><td>NW</td><td>NNS</td><td>TO</td><td>NNS</td><td>IN</td><td>JJ</td><td>NN</td></tr><tr><td>Rule 1</td><td></td><td>VB</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Rule 2</td><td></td><td></td><td></td><td>IN</td><td></td><td></td><td></td><td></td></tr><tr><td>Output</td><td>TO</td><td>VB</td><td>NNS</td><td>IN</td><td>NNS</td><td>IN</td><td>JJ</td><td>NN</td></tr><tr><td>Gold</td><td>TO</td><td>VB</td><td>NNS</td><td>IN</td><td>NNS</td><td>IN</td><td>JJ</td><td>NN</td></tr></table>",
                  "bbox": [
                    100,
                    448,
                    468,
                    573
                  ],
                  "page": 231,
                  "reading_order": 4
                },
                {
                  "label": "cap",
                  "text": "Table 5-6. Steps in Brill tagging",
                  "bbox": [
                    99,
                    430,
                    252,
                    448
                  ],
                  "page": 231,
                  "reading_order": 5
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_231_order_6",
              "label": "para",
              "text": "In this table, we see two rules. All such rules are generated from a template of the\nfollowing form: “ replace $T_1$ with $T_2$ in the context C . ” Typical contexts are the identity\nor the tag of the preceding or following word, or the appearance of a specific tag within\ntwo to three words of the current word. During its training phase, the tagger guesses\nvalues for T 1 , T 2 , and C , to create thousands of candidate rules. Each rule is scored\naccording to its net benefit: the number of incorrect tags that it corrects, less the number\nof correct tags it incorrectly modifies.",
              "level": -1,
              "page": 231,
              "reading_order": 6,
              "bbox": [
                97,
                591,
                585,
                707
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_231_order_7",
              "label": "para",
              "text": "Brill taggers have another interesting property: the rules are linguistically interpretable.\nCompare this with the n-gram taggers, which employ a potentially massive table of n-\ngrams. We cannot learn much from direct inspection of such a table, in comparison to\nthe rules learned by the Brill tagger. Example 5-6 demonstrates NLTK’s Brill tagger.",
              "level": -1,
              "page": 231,
              "reading_order": 7,
              "bbox": [
                97,
                707,
                585,
                779
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_231_order_8",
              "label": "foot",
              "text": "5.6 Transformation-Based Tagging | 209",
              "level": -1,
              "page": 231,
              "reading_order": 8,
              "bbox": [
                410,
                824,
                585,
                842
              ],
              "section_number": "5.6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_232_order_0",
              "label": "para",
              "text": "Example 5-6. Brill tagger demonstration: The tagger has a collection of templates of the form X → Y\nif the preceding word is Z; the variables in these templates are instantiated to particular words and\ntags to create “ rules”; the score for a rule is the number of broken examples it corrects minus the\nnumber of correct cases it breaks; apart from training a tagger, the demonstration displays residual\nerrors.",
              "level": -1,
              "page": 232,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_232_order_2",
              "label": "tab",
              "text": "<table><tr><td>S</td><td>F</td><td>r</td><td>O</td><td>Score = Fixed – Broken</td></tr><tr><td>c</td><td>i</td><td>o</td><td>t</td><td>R</td><td>Fixed = num tags changed incorrect -&gt; correct</td></tr><tr><td>o</td><td>x</td><td>k</td><td>h</td><td>u</td><td>Broken = num tags changed correct -&gt; incorrect</td></tr><tr><td>r</td><td>e</td><td>e</td><td>l</td><td>Other = num tags changed incorrect -&gt; incorrect</td></tr><tr><td>e</td><td>d</td><td>n</td><td>r</td><td>e</td></tr></table>",
              "level": -1,
              "page": 232,
              "reading_order": 2,
              "bbox": [
                109,
                215,
                503,
                306
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_232_order_5",
              "label": "tab",
              "text": "<table><tr><td>|</td><td>Then/NN-&gt;RB</td><td>| ,/, in/IN the/DT guests/N</td></tr><tr><td>, in/IN the/DT guests/NNS |</td><td>'/VBD-&gt;POS</td><td>| honor/NN ,/, the/DT speed</td></tr><tr><td>'/POS honor/NN ,/, the/DT |</td><td>speedway/JJ-&gt;NN</td><td>hauled/VBD out/RP four/CD</td></tr><tr><td>NN ,/, the/DT speedway/NN |</td><td>hauled/NN-&gt;VBD</td><td>out/RP four/CD drivers/NN</td></tr><tr><td>DT speedway/NN hauled/VBD |</td><td>out/NNP-&gt;RP</td><td>four/CD drivers/NNS ,/, c</td></tr><tr><td>dway/NN hauled/VBD out/RP |</td><td>four/NNP-&gt;CD</td><td>drivers/NNS ,/, crews/NNS</td></tr><tr><td>hauled/VBD out/RP four/CD |</td><td>drivers/NNP-&gt;NNS</td><td>,/, crews/NNS and/CC even</td></tr><tr><td>P four/CD drivers/NNS ,/, |</td><td>crews/NN-&gt;NNS</td><td>and/CC even/RB the/DT off</td></tr><tr><td>NNS and/CC even/RB the/DT |</td><td>official/NNP-&gt;JJ</td><td>Indianapolis/NNP 500/CD a</td></tr><tr><td>|</td><td>After/VBD-&gt;IN</td><td>the/DT race/NN ,/, Fortun</td></tr><tr><td>ter/IN the/DT race/NN ,/, |</td><td>Fortune/IN-&gt;NNP</td><td>500/CD executives/NNS dro</td></tr><tr><td>s/NNS drooled/VBD like/IN |</td><td>schoolboys/NNP-&gt;NNS</td><td>over/IN the/DT cars/NNS a</td></tr><tr><td>olboys/NNS over/IN the/DT |</td><td>cars/NN-&gt;NNS</td><td>and/CC drivers/NNS ./.</td></tr></table>",
              "level": -1,
              "page": 232,
              "reading_order": 5,
              "bbox": [
                97,
                443,
                521,
                627
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_232_order_6",
          "label": "sub_sec",
          "text": "5.7 How to Determine the Category of a Word",
          "level": 2,
          "page": 232,
          "reading_order": 6,
          "bbox": [
            97,
            645,
            467,
            672
          ],
          "section_number": "5.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_232_order_7",
              "label": "para",
              "text": "Now that we have examined word classes in detail, we turn to a more basic question:\nhow do we decide what category a word belongs to in the first place? In general, linguists\nuse morphological, syntactic, and semantic clues to determine the category of a word.",
              "level": -1,
              "page": 232,
              "reading_order": 7,
              "bbox": [
                97,
                680,
                585,
                727
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_232_order_8",
              "label": "foot",
              "text": "210 | Chapter 5: Categorizing and Tagging Words",
              "level": -1,
              "page": 232,
              "reading_order": 8,
              "bbox": [
                97,
                824,
                308,
                842
              ],
              "section_number": "210",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_230_order_2",
          "label": "para",
          "text": "An n-gram tagger uses recent tags to guide the choice of tag for the current word. When\ntagging the first word of a sentence, a trigram tagger will be using the part-of-speech\ntag of the previous two tokens, which will normally be the last word of the previous\nsentence and the sentence-ending punctuation. However, the lexical category that\nclosed the previous sentence has no bearing on the one that begins the next sentence.",
          "level": -1,
          "page": 230,
          "reading_order": 2,
          "bbox": [
            97,
            179,
            585,
            262
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_230_order_3",
          "label": "para",
          "text": "To deal with this situation, we can train, run, and evaluate taggers using lists of tagged\nsentences, as shown in Example 5-5.",
          "level": -1,
          "page": 230,
          "reading_order": 3,
          "bbox": [
            97,
            268,
            585,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_230_order_4",
          "label": "para",
          "text": "Example 5-5. N-gram tagging at the sentence level.",
          "level": -1,
          "page": 230,
          "reading_order": 4,
          "bbox": [
            97,
            313,
            350,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_233_order_0",
      "label": "sec",
      "text": "Morphological Clues",
      "level": 1,
      "page": 233,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        234,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_233_order_1",
          "label": "para",
          "text": "The internal structure of a word may give useful clues as to the word’s category. For\nexample, -ness is a suffix that combines with an adjective to produce a noun, e.g., happy\n→ happiness, ill → illness. So if we encounter a word that ends in -ness, this is very likely\nto be a noun. Similarly, -ment is a suffix that combines with some verbs to produce a\nnoun, e.g., govern → government and establish → establishment.",
          "level": -1,
          "page": 233,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_233_order_2",
          "label": "para",
          "text": "English verbs can also be morphologically complex. For instance, the present par-\nticiple of a verb ends in -ing, and expresses the idea of ongoing, incomplete action (e.g.,\nfalling, eating). The -ing suffix also appears on nouns derived from verbs, e.g., the falling\nof the leaves (this is known as the gerund).",
          "level": -1,
          "page": 233,
          "reading_order": 2,
          "bbox": [
            96,
            188,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_233_order_3",
      "label": "sec",
      "text": "Syntactic Clues",
      "level": 1,
      "page": 233,
      "reading_order": 3,
      "bbox": [
        97,
        268,
        198,
        295
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_233_order_4",
          "label": "para",
          "text": "Another source of information is the typical contexts in which a word can occur. For\nexample, assume that we have already determined the category of nouns. Then we\nmight say that a syntactic criterion for an adjective in English is that it can occur im-\nmediately before a noun, or immediately following the words be or very. According to\nthese tests, near should be categorized as an adjective:",
          "level": -1,
          "page": 233,
          "reading_order": 4,
          "bbox": [
            97,
            295,
            585,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_233_order_5",
          "label": "list_group",
          "text": "(2) a. the near window\nb. The end is (very) near.",
          "level": -1,
          "page": 233,
          "reading_order": 5,
          "bbox": [
            118,
            394,
            270,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "(2) a. the near window",
              "bbox": [
                118,
                394,
                270,
                412
              ],
              "page": 233,
              "reading_order": 5
            },
            {
              "label": "list",
              "text": "b. The end is (very) near.",
              "bbox": [
                144,
                412,
                297,
                430
              ],
              "page": 233,
              "reading_order": 6
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_233_order_7",
      "label": "sec",
      "text": "Semantic Clues",
      "level": 1,
      "page": 233,
      "reading_order": 7,
      "bbox": [
        97,
        439,
        198,
        465
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_233_order_8",
          "label": "para",
          "text": "Finally, the meaning of a word is a useful clue as to its lexical category. For example,\nthe best-known definition of a noun is semantic: “the name of a person, place, or thing.”\nWithin modern linguistics, semantic criteria for word classes are treated with suspicion,\nmainly because they are hard to formalize. Nevertheless, semantic criteria underpin\nmany of our intuitions about word classes, and enable us to make a good guess about\nthe categorization of words in languages with which we are unfamiliar. For example,\nif all we know about the Dutch word verjaardag is that it means the same as the English\nword birthday, then we can guess that verjaardag is a noun in Dutch. However, some\ncare is needed: although we might translate zij is vandaag jarig as it’s her birthday to-\nday, the word jarig is in fact an adjective in Dutch, and has no exact equivalent in\nEnglish.",
          "level": -1,
          "page": 233,
          "reading_order": 8,
          "bbox": [
            97,
            465,
            585,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_233_order_9",
      "label": "sec",
      "text": "New Words",
      "level": 1,
      "page": 233,
      "reading_order": 9,
      "bbox": [
        98,
        663,
        173,
        684
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_233_order_10",
          "label": "para",
          "text": "All languages acquire new lexical items. A list of words recently added to the Oxford\nDictionary of English includes cyberslacker, fatoush, blamestorm, SARS, cantopop,\nbupkis, noughties, muggle, and robata. Notice that all these new words are nouns, and\nthis is reflected in calling nouns an open class. By contrast, prepositions are regarded\nas a closed class. That is, there is a limited set of words belonging to the class (e.g.,\nabove, along, at, below, beside, between, during, for, from, in, near, on, outside, over,",
          "level": -1,
          "page": 233,
          "reading_order": 10,
          "bbox": [
            97,
            689,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_233_order_11",
          "label": "foot",
          "text": "5.7 How to Determine the Category of a Word | 211",
          "level": -1,
          "page": 233,
          "reading_order": 11,
          "bbox": [
            359,
            824,
            584,
            842
          ],
          "section_number": "5.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_234_order_0",
          "label": "para",
          "text": "past, through, towards, under, up, with), and membership of the set only changes very\ngradually over time.",
          "level": -1,
          "page": 234,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_234_order_1",
      "label": "sec",
      "text": "Morphology in Part-of-Speech Tagsets",
      "level": 1,
      "page": 234,
      "reading_order": 1,
      "bbox": [
        97,
        116,
        351,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_235_order_2",
          "label": "sub_sec",
          "text": "5.8 Summary",
          "level": 2,
          "page": 235,
          "reading_order": 2,
          "bbox": [
            97,
            304,
            207,
            331
          ],
          "section_number": "5.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_235_order_3",
              "label": "list_group",
              "text": "Words can be grouped into classes, such as nouns, verbs, adjectives, and adverbs.\nThese classes are known as lexical categories or parts-of-speech. Parts-of-speech\nare assigned short labels, or tags, such as NN and VB .\nThe process of automatically assigning parts-of-speech to words in text is called\npart-of-speech tagging, POS tagging, or just tagging.",
              "level": -1,
              "page": 235,
              "reading_order": 3,
              "bbox": [
                121,
                338,
                584,
                385
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "Words can be grouped into classes, such as nouns, verbs, adjectives, and adverbs.\nThese classes are known as lexical categories or parts-of-speech. Parts-of-speech\nare assigned short labels, or tags, such as NN and VB .",
                  "bbox": [
                    121,
                    338,
                    584,
                    385
                  ],
                  "page": 235,
                  "reading_order": 3
                },
                {
                  "label": "list",
                  "text": "The process of automatically assigning parts-of-speech to words in text is called\npart-of-speech tagging, POS tagging, or just tagging.",
                  "bbox": [
                    122,
                    391,
                    583,
                    422
                  ],
                  "page": 235,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "Automatic tagging is an important step in the NLP pipeline, and is useful in a variety\nof situations, including predicting the behavior of previously unseen words, ana-\nlyzing word usage in corpora, and text-to-speech systems.",
                  "bbox": [
                    122,
                    429,
                    585,
                    476
                  ],
                  "page": 235,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "Some linguistic corpora, such as the Brown Corpus, have been POS tagged.",
                  "bbox": [
                    122,
                    482,
                    557,
                    496
                  ],
                  "page": 235,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "A variety of tagging methods are possible, e.g., default tagger, regular expression\ntagger, unigram tagger, and n-gram taggers. These can be combined using a tech-\nnique known as backoff.",
                  "bbox": [
                    122,
                    501,
                    585,
                    550
                  ],
                  "page": 235,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "Taggers can be trained and evaluated using tagged corpora",
                  "bbox": [
                    125,
                    555,
                    458,
                    573
                  ],
                  "page": 235,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "Backoff is a method for combining models: when a more specialized model (such\nas a bigram tagger) cannot assign a tag in a given context, we back off to a more\ngeneral model (such as a unigram tagger).",
                  "bbox": [
                    121,
                    573,
                    585,
                    627
                  ],
                  "page": 235,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "Part-of-speech tagging is an important, early example of a sequence classification\ntask in NLP: a classification decision at any one point in the sequence makes use\nof words and tags in the local context.",
                  "bbox": [
                    126,
                    627,
                    585,
                    680
                  ],
                  "page": 235,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "A dictionary is used to map between arbitrary types of information, such as a string\nand a number: freq['cat'] = 12. We create dictionaries using the brace notation:\npos = {}, pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}.",
                  "bbox": [
                    122,
                    680,
                    585,
                    734
                  ],
                  "page": 235,
                  "reading_order": 11
                },
                {
                  "label": "list",
                  "text": "N-gram taggers can be defined for large values of n, but once n is larger than 3, we\nusually encounter the sparse data problem; even with a large quantity of training\ndata, we see only a tiny fraction of possible contexts.",
                  "bbox": [
                    122,
                    734,
                    585,
                    788
                  ],
                  "page": 235,
                  "reading_order": 12
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_235_order_13",
              "label": "foot",
              "text": "5.8 Summary|213",
              "level": -1,
              "page": 235,
              "reading_order": 13,
              "bbox": [
                494,
                824,
                584,
                842
              ],
              "section_number": "5.8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_236_order_0",
              "label": "list",
              "text": "Transformation-based tagging involves learning a series of repair rules of the form\n‘change tag s to tag t in context c,” where each rule fixes mistakes and possibly\nntroduces a (smaller) number of errors.",
              "level": -1,
              "page": 236,
              "reading_order": 0,
              "bbox": [
                125,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_236_order_1",
          "label": "sub_sec",
          "text": "5.9 Further Reading",
          "level": 2,
          "page": 236,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            261,
            172
          ],
          "section_number": "5.9",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_236_order_2",
              "label": "para",
              "text": "Extra materials for this chapter are posted at http://www.nltk.org/ , including links to\nfreely available resources on the Web. For more examples of tagging with NLTK, please\nsee the Tagging HOWTO at http://www.nltk.org/howto . Chapters 4 and 5 of (Jurafsky\n& Martin, 2008) contain more advanced material on n-grams and part-of-speech tag-\nging. Other approaches to tagging involve machine learning methods (Chapter 6 ). In\nChapter 7 , we will see a generalization of tagging called chunking in which a contiguous\nsequence of words is assigned a single tag.",
              "level": -1,
              "page": 236,
              "reading_order": 2,
              "bbox": [
                97,
                179,
                585,
                295
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_236_order_3",
              "label": "para",
              "text": "For tagset documentation, see nltk.help.upenn_tagset() and nltk.help.brown_tag\nset(). Lexical categories are introduced in linguistics textbooks, including those listed\nin Chapter 1 of this book.",
              "level": -1,
              "page": 236,
              "reading_order": 3,
              "bbox": [
                97,
                303,
                585,
                350
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_236_order_4",
              "label": "para",
              "text": "There are many other kinds of tagging. Words can be tagged with directives to a speech\nsynthesizer, indicating which words should be emphasized. Words can be tagged with\nsense numbers, indicating which sense of the word was used. Words can also be tagged\nwith morphological features. Examples of each of these kinds of tags are shown in the\nfollowing list. For space reasons, we only show the tag for a single word. Note also that\nthe first two examples use XML-style tags, where elements in angle brackets enclose\nthe word that is tagged.",
              "level": -1,
              "page": 236,
              "reading_order": 4,
              "bbox": [
                97,
                358,
                585,
                474
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_236_order_5",
              "label": "para",
              "text": "Speech Synthesis Markup Language (W3C SSML)\nThat is a <emphasis>big</emphasis> car!",
              "level": -1,
              "page": 236,
              "reading_order": 5,
              "bbox": [
                97,
                483,
                377,
                514
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_236_order_6",
              "label": "para",
              "text": "SemCor: Brown Corpus tagged with WordNet senses\nSpace in any <wf pos=\"NN\" lemma=\"form\" wnsn=\"4\">form</wf> is completely meas\nured by the three dimensions. (Wordnet form/nn sense 4: “shape, form, config-\nuration, contour, conformation”)",
              "level": -1,
              "page": 236,
              "reading_order": 6,
              "bbox": [
                97,
                519,
                585,
                583
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_236_order_7",
              "label": "para",
              "text": "Morphological tagging, from the Turin University Italian Treebank\nE' italiano , come progetto e realizzazione , il primo (PRIMO ADJ ORDIN M\nSING) porto turistico dell' Albania.",
              "level": -1,
              "page": 236,
              "reading_order": 7,
              "bbox": [
                97,
                591,
                585,
                639
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_236_order_8",
              "label": "para",
              "text": "Note that tagging is also performed at higher levels. Here is an example of dialogue act\ntagging, from the NPS Chat Corpus (Forsyth & Martell, 2007) included with NLTK.\nEach turn of the dialogue is categorized as to its communicative function:",
              "level": -1,
              "page": 236,
              "reading_order": 8,
              "bbox": [
                97,
                645,
                585,
                698
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_236_order_10",
              "label": "foot",
              "text": "214 | Chapter 5: Categorizing and Tagging Words",
              "level": -1,
              "page": 236,
              "reading_order": 10,
              "bbox": [
                97,
                824,
                308,
                842
              ],
              "section_number": "214",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_237_order_0",
          "label": "sub_sec",
          "text": "5.10 Exercises",
          "level": 2,
          "page": 237,
          "reading_order": 0,
          "bbox": [
            97,
            116,
            211,
            134
          ],
          "section_number": "5.10",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_237_order_1",
              "label": "para",
              "text": "1. ◦ Search the Web for “spoof newspaper headlines,” to find such gems as: British\nLeft Waffles on Falkland Islands, and Juvenile Court to Try Shooting Defendant.\nManually tag these headlines to see whether knowledge of the part-of-speech tags\nremoves the ambiguity.",
              "level": -1,
              "page": 237,
              "reading_order": 1,
              "bbox": [
                100,
                143,
                585,
                215
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_2",
              "label": "para",
              "text": "2. ◦ Working with someone else, take turns picking a word that can be either a noun\nor a verb (e.g., contest); the opponent has to predict which one is likely to be the\nmost frequent in the Brown Corpus. Check the opponent's prediction, and tally\nthe score over several turns.",
              "level": -1,
              "page": 237,
              "reading_order": 2,
              "bbox": [
                100,
                215,
                585,
                286
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_3",
              "label": "para",
              "text": "3. ◦ Tokenize and tag the following sentence: They wind back the clock, while we\nchase after the wind. What different pronunciations and parts-of-speech are\ninvolved?",
              "level": -1,
              "page": 237,
              "reading_order": 3,
              "bbox": [
                100,
                286,
                585,
                333
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_4",
              "label": "para",
              "text": "4. ◦ Review the mappings in Table 5-4. Discuss any other examples of mappings you\ncan think of. What type of information do they map from and to?",
              "level": -1,
              "page": 237,
              "reading_order": 4,
              "bbox": [
                100,
                340,
                584,
                376
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_5",
              "label": "para",
              "text": "5. ◦ Using the Python interpreter in interactive mode, experiment with the dictionary\nexamples in this chapter. Create a dictionary d, and add some entries. What hap­\npens whether you try to access a non­existent entry, e.g., d['xyz']?",
              "level": -1,
              "page": 237,
              "reading_order": 5,
              "bbox": [
                100,
                376,
                585,
                430
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_6",
              "label": "para",
              "text": "6. ◦ Try deleting an element from a dictionary d, using the syntax del d['abc']. Check\nthat the item was deleted.",
              "level": -1,
              "page": 237,
              "reading_order": 6,
              "bbox": [
                100,
                430,
                585,
                465
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_7",
              "label": "para",
              "text": "7. ◦ Create two dictionaries, d1 and d2, and add some entries to each. Now issue the\ncommand d1.update(d2). What did this do? What might it be useful for?",
              "level": -1,
              "page": 237,
              "reading_order": 7,
              "bbox": [
                100,
                465,
                585,
                502
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_8",
              "label": "para",
              "text": "8. ◦ Create a dictionary e, to represent a single lexical entry for some word of your\nchoice. Define keys such as headword, part-of-speech, sense, and example, and as-\nsign them suitable values.",
              "level": -1,
              "page": 237,
              "reading_order": 8,
              "bbox": [
                100,
                502,
                585,
                556
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_9",
              "label": "para",
              "text": "9. ◦ Satisfy yourself that there are restrictions on the distribution of go and went, in\nthe sense that they cannot be freely interchanged in the kinds of contexts illustrated\nin (3), Section 5.7.",
              "level": -1,
              "page": 237,
              "reading_order": 9,
              "bbox": [
                100,
                556,
                584,
                609
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_10",
              "label": "para",
              "text": "10. ◦ Train a unigram tagger and run it on some new text. Observe that some words\nare not assigned a tag. Why not?",
              "level": -1,
              "page": 237,
              "reading_order": 10,
              "bbox": [
                100,
                609,
                585,
                647
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_11",
              "label": "para",
              "text": "11. ◦ Learn about the affix tagger (type help(nltk.AffixTagger)). Train an affix tagger\nand run it on some new text. Experiment with different settings for the affix length\nand the minimum word length. Discuss your findings.",
              "level": -1,
              "page": 237,
              "reading_order": 11,
              "bbox": [
                100,
                647,
                585,
                700
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_12",
              "label": "para",
              "text": "12. ◦ Train a bigram tagger with no backoff tagger, and run it on some of the training\ndata. Next, run it on some new data. What happens to the performance of the\ntagger? Why?",
              "level": -1,
              "page": 237,
              "reading_order": 12,
              "bbox": [
                100,
                707,
                585,
                754
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_13",
              "label": "para",
              "text": "13. ◦ We can use a dictionary to specify the values to be substituted into a formatting\nstring. Read Python’s library documentation for formatting strings (http://docs.py",
              "level": -1,
              "page": 237,
              "reading_order": 13,
              "bbox": [
                100,
                761,
                585,
                797
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_237_order_14",
              "label": "foot",
              "text": "5.10 Exercises | 215",
              "level": -1,
              "page": 237,
              "reading_order": 14,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "5.10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_0",
              "label": "para",
              "text": "thon.org/lib/typesseq-strings.html) and use this method to display today’s date in\ntwo different formats.",
              "level": -1,
              "page": 238,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_1",
              "label": "para",
              "text": "14. o Use sorted() and set() to get a sorted list of tags used in the Brown Corpus,\nremoving duplicates.",
              "level": -1,
              "page": 238,
              "reading_order": 1,
              "bbox": [
                100,
                107,
                584,
                143
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_2",
              "label": "para",
              "text": "15. o Write programs to process the Brown Corpus and find answers to the following\nquestions:",
              "level": -1,
              "page": 238,
              "reading_order": 2,
              "bbox": [
                100,
                143,
                585,
                180
              ],
              "section_number": "15",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_3",
              "label": "para",
              "text": "a. Which nouns are more common in their plural form, rather than their singular\nform? (Only consider regular plurals, formed with the -s suffix.)",
              "level": -1,
              "page": 238,
              "reading_order": 3,
              "bbox": [
                126,
                187,
                585,
                217
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_4",
              "label": "para",
              "text": "b. Which word has the greatest number of distinct tags? What are they, and what\ndo they represent?",
              "level": -1,
              "page": 238,
              "reading_order": 4,
              "bbox": [
                126,
                224,
                585,
                254
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_5",
              "label": "para",
              "text": "c. List tags in order of decreasing frequency. What do the 20 most frequent tags\nrepresent?",
              "level": -1,
              "page": 238,
              "reading_order": 5,
              "bbox": [
                126,
                259,
                585,
                295
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_6",
              "label": "para",
              "text": "d. Which tags are nouns most commonly found after? What do these tags\nrepresent?",
              "level": -1,
              "page": 238,
              "reading_order": 6,
              "bbox": [
                126,
                295,
                585,
                331
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_7",
              "label": "para",
              "text": "16. o Explore the following issues that arise in connection with the lookup tagger",
              "level": -1,
              "page": 238,
              "reading_order": 7,
              "bbox": [
                100,
                331,
                566,
                350
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_8",
              "label": "para",
              "text": "a. What happens to the tagger performance for the various model sizes when a\nbackoff tagger is omitted?",
              "level": -1,
              "page": 238,
              "reading_order": 8,
              "bbox": [
                126,
                356,
                584,
                387
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_9",
              "label": "para",
              "text": "b. Consider the curve in Figure 5-4 ; suggest a good size for a lookup tagger that\nbalances memory and performance. Can you come up with scenarios where it\nwould be preferable to minimize memory usage, or to maximize performance\nwith no regard for memory usage?",
              "level": -1,
              "page": 238,
              "reading_order": 9,
              "bbox": [
                126,
                393,
                585,
                457
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_10",
              "label": "para",
              "text": "17. o What is the upper limit of performance for a lookup tagger, assuming no limit\nto the size of its table? (Hint: write a program to work out what percentage of tokens\nof a word are assigned the most likely tag for that word, on average.)",
              "level": -1,
              "page": 238,
              "reading_order": 10,
              "bbox": [
                100,
                464,
                585,
                511
              ],
              "section_number": "17",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_11",
              "label": "para",
              "text": "18. o Generate some statistics for tagged data to answer the following questions",
              "level": -1,
              "page": 238,
              "reading_order": 11,
              "bbox": [
                100,
                518,
                557,
                532
              ],
              "section_number": "18",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_12",
              "label": "para",
              "text": "a. What proportion of word types are always assigned the same part-of-speech\ntag?",
              "level": -1,
              "page": 238,
              "reading_order": 12,
              "bbox": [
                126,
                537,
                584,
                569
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_13",
              "label": "para",
              "text": "b. How many words are ambiguous, in the sense that they appear with at least\ntwo tags?",
              "level": -1,
              "page": 238,
              "reading_order": 13,
              "bbox": [
                126,
                573,
                584,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_14",
              "label": "para",
              "text": "c. What percentage of word tokens in the Brown Corpus involve these ambiguous\nwords?",
              "level": -1,
              "page": 238,
              "reading_order": 14,
              "bbox": [
                126,
                609,
                585,
                645
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_15",
              "label": "para",
              "text": "19. o The evaluate() method works out how accurately the tagger performs on this\ntext. For example, if the supplied tagged text was [('the', 'DT'), ('dog',\n'NN')] and the tagger produced the output [('the', 'NN'), ('dog', 'NN')], then\nthe score would be 0.5. Let’s try to figure out how the evaluation method works:",
              "level": -1,
              "page": 238,
              "reading_order": 15,
              "bbox": [
                100,
                645,
                585,
                716
              ],
              "section_number": "19",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_16",
              "label": "para",
              "text": "a. A tagger t takes a list of words as input, and produces a list of tagged words\nas output. However, t.evaluate() is given correctly tagged text as its only\nparameter. What must it do with this input before performing the tagging?",
              "level": -1,
              "page": 238,
              "reading_order": 16,
              "bbox": [
                126,
                716,
                585,
                770
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_238_order_17",
              "label": "foot",
              "text": "216 | Chapter 5: Categorizing and Tagging Words",
              "level": -1,
              "page": 238,
              "reading_order": 17,
              "bbox": [
                97,
                824,
                308,
                842
              ],
              "section_number": "216",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_0",
              "label": "para",
              "text": "b. Once the tagger has created newly tagged text, how might the evaluate()\nmethod go about comparing it with the original tagged text and computing\nthe accuracy score?",
              "level": -1,
              "page": 239,
              "reading_order": 0,
              "bbox": [
                126,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_1",
              "label": "para",
              "text": "c. Now examine the source code to see how the method is implemented. Inspect\nnltk.tag.api.__file__ to discover the location of the source code, and open\nthis file using an editor (be sure to use the api.py file and not the compiled\napi.pyc binary file).",
              "level": -1,
              "page": 239,
              "reading_order": 1,
              "bbox": [
                126,
                125,
                585,
                197
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_2",
              "label": "para",
              "text": "20. • Write code to search the Brown Corpus for particular words and phrases ac-\ncording to tags, to answer the following questions:",
              "level": -1,
              "page": 239,
              "reading_order": 2,
              "bbox": [
                98,
                197,
                584,
                232
              ],
              "section_number": "20",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_3",
              "label": "para",
              "text": "a. Produce an alphabetically sorted list of the distinct words tagged as MD.",
              "level": -1,
              "page": 239,
              "reading_order": 3,
              "bbox": [
                126,
                232,
                549,
                250
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_4",
              "label": "para",
              "text": "b. Identify words that can be plural nouns or third person singular verbs (e.g.,\ndeals, flies).",
              "level": -1,
              "page": 239,
              "reading_order": 4,
              "bbox": [
                126,
                250,
                584,
                288
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_5",
              "label": "para",
              "text": "c. Identify three-word prepositional phrases of the form IN + DET + NN (e.g.,\nin the lab).",
              "level": -1,
              "page": 239,
              "reading_order": 5,
              "bbox": [
                126,
                294,
                584,
                322
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_6",
              "label": "para",
              "text": "d. What is the ratio of masculine to feminine pronouns?",
              "level": -1,
              "page": 239,
              "reading_order": 6,
              "bbox": [
                126,
                331,
                451,
                349
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_7",
              "label": "para",
              "text": "21. o In Table 3-1, we saw a table involving frequency counts for the verbs adore, love,\nlike, and prefer, and preceding qualifiers such as really. Investigate the full range\nof qualifiers (Brown tag QL) that appear before these four verbs.",
              "level": -1,
              "page": 239,
              "reading_order": 7,
              "bbox": [
                98,
                349,
                585,
                403
              ],
              "section_number": "21",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_8",
              "label": "para",
              "text": "22. o We defined the regexp_tagger that can be used as a fall-back tagger for unknown\nwords. This tagger only checks for cardinal numbers. By testing for particular prefix\nor suffix strings, it should be possible to guess other tags. For example, we could\ntag any word that ends with -s as a plural noun. Define a regular expression tagger\n(using RegexpTagger()) that tests for at least five other patterns in the spelling of\nwords. (Use inline documentation to explain the rules.)",
              "level": -1,
              "page": 239,
              "reading_order": 8,
              "bbox": [
                98,
                403,
                586,
                503
              ],
              "section_number": "22",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_9",
              "label": "para",
              "text": "23. o Consider the regular expression tagger developed in the exercises in the previous\nsection. Evaluate the tagger using its accuracy() method, and try to come up with\nways to improve its performance. Discuss your findings. How does objective eval-\nuation help in the development process?",
              "level": -1,
              "page": 239,
              "reading_order": 9,
              "bbox": [
                98,
                509,
                585,
                573
              ],
              "section_number": "23",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_10",
              "label": "para",
              "text": "24. o How serious is the sparse data problem? Investigate the performance of n-gram\ntaggers as n increases from 1 to 6. Tabulate the accuracy score. Estimate the training\ndata required for these taggers, assuming a vocabulary size of $10^5$ and a tagset size\nof $10^2$ .",
              "level": -1,
              "page": 239,
              "reading_order": 10,
              "bbox": [
                98,
                580,
                585,
                645
              ],
              "section_number": "24",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_11",
              "label": "para",
              "text": "25. o Obtain some tagged data for another language, and train and evaluate a variety\nof taggers on it. If the language is morphologically complex, or if there are any\northographic clues (e.g., capitalization) to word classes, consider developing a reg-\nular expression tagger for it (ordered after the unigram tagger, and before the de-\nfault tagger). How does the accuracy of your tagger(s) compare with the same\ntaggers run on English data? Discuss any issues you encounter in applying these\nmethods to the language.",
              "level": -1,
              "page": 239,
              "reading_order": 11,
              "bbox": [
                98,
                645,
                585,
                764
              ],
              "section_number": "25",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_239_order_12",
              "label": "foot",
              "text": "5.10 Exercises | 217",
              "level": -1,
              "page": 239,
              "reading_order": 12,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "5.10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_0",
              "label": "para",
              "text": "26. o Example 5-4 plotted a curve showing change in the performance of a lookup\ntagger as the model size was increased. Plot the performance curve for a unigram\ntagger, as the amount of training data is varied.",
              "level": -1,
              "page": 240,
              "reading_order": 0,
              "bbox": [
                98,
                71,
                585,
                125
              ],
              "section_number": "26",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_1",
              "label": "para",
              "text": "27. o Inspect the confusion matrix for the bigram tagger t2 defined in Section 5.5, and\nidentify one or more sets of tags to collapse. Define a dictionary to do the mapping,\nand evaluate the tagger on the simplified data.",
              "level": -1,
              "page": 240,
              "reading_order": 1,
              "bbox": [
                98,
                125,
                584,
                179
              ],
              "section_number": "27",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_2",
              "label": "para",
              "text": "28. o Experiment with taggers using the simplified tagset (or make one of your own\nby discarding all but the first character of each tag name). Such a tagger has fewer\ndistinctions to make, but much less information on which to base its work. Discuss\nyour findings.",
              "level": -1,
              "page": 240,
              "reading_order": 2,
              "bbox": [
                98,
                179,
                585,
                250
              ],
              "section_number": "28",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_3",
              "label": "para",
              "text": "29. o Recall the example of a bigram tagger which encountered a word it hadn’t seen\nduring training, and tagged the rest of the sentence as None. It is possible for a\nbigram tagger to fail partway through a sentence even if it contains no unseen words\n(even if the sentence was used during training). In what circumstance can this\nhappen? Can you write a program to find some examples of this?",
              "level": -1,
              "page": 240,
              "reading_order": 3,
              "bbox": [
                98,
                250,
                585,
                333
              ],
              "section_number": "29",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_4",
              "label": "para",
              "text": "30. o Preprocess the Brown News data by replacing low-frequency words with UNK,\nbut leaving the tags untouched. Now train and evaluate a bigram tagger on this\ndata. How much does this help? What is the contribution of the unigram tagger\nand default tagger now?",
              "level": -1,
              "page": 240,
              "reading_order": 4,
              "bbox": [
                98,
                340,
                585,
                404
              ],
              "section_number": "30",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_5",
              "label": "para",
              "text": "31. o Modify the program in Example 5-4 to use a logarithmic scale on the x-axis, by\nreplacing pylab.plot() with pylab.semilogx(). What do you notice about the\nshape of the resulting plot? Does the gradient tell you anything?",
              "level": -1,
              "page": 240,
              "reading_order": 5,
              "bbox": [
                100,
                410,
                585,
                457
              ],
              "section_number": "31",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_6",
              "label": "para",
              "text": "32. • Consult the documentation for the Brill tagger demo function, using\nhelp(nltk.tag.brill.demo) . Experiment with the tagger by setting different values\nfor the parameters. Is there any trade-off between training time (corpus size) and\nperformance?",
              "level": -1,
              "page": 240,
              "reading_order": 6,
              "bbox": [
                98,
                464,
                585,
                528
              ],
              "section_number": "32",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_7",
              "label": "para",
              "text": "33. Write code that builds a dictionary of dictionaries of sets. Use it to store the set\nof POS tags that can follow a given word having a given POS tag, i.e., wordi → tagi →\ntagi+1.",
              "level": -1,
              "page": 240,
              "reading_order": 7,
              "bbox": [
                98,
                528,
                585,
                582
              ],
              "section_number": "33",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_8",
              "label": "para",
              "text": "34. • There are 264 distinct words in the Brown Corpus having exactly three possible\ntags.",
              "level": -1,
              "page": 240,
              "reading_order": 8,
              "bbox": [
                98,
                582,
                585,
                619
              ],
              "section_number": "34",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_9",
              "label": "para",
              "text": "a. Print a table with the integers 1..10 in one column, and the number of distinct\nwords in the corpus having 1..10 distinct tags in the other column.",
              "level": -1,
              "page": 240,
              "reading_order": 9,
              "bbox": [
                126,
                626,
                585,
                656
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_10",
              "label": "para",
              "text": "b. For the word with the greatest number of distinct tags, print out sentences\nfrom the corpus containing the word, one for each possible tag.",
              "level": -1,
              "page": 240,
              "reading_order": 10,
              "bbox": [
                126,
                663,
                585,
                693
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_11",
              "label": "para",
              "text": "35. • Write a program to classify contexts involving the word must according to the\ntag of the following word. Can this be used to discriminate between the epistemic\nand deontic uses of must?",
              "level": -1,
              "page": 240,
              "reading_order": 11,
              "bbox": [
                98,
                698,
                585,
                744
              ],
              "section_number": "35",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_12",
              "label": "para",
              "text": "36. e Create a regular expression tagger and various unigram and n-gram taggers,\nincorporating backoff, and train them on part of the Brown Corpus.",
              "level": -1,
              "page": 240,
              "reading_order": 12,
              "bbox": [
                98,
                752,
                584,
                788
              ],
              "section_number": "36",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_240_order_13",
              "label": "foot",
              "text": "218 | Chapter 5: Categorizing and Tagging Words",
              "level": -1,
              "page": 240,
              "reading_order": 13,
              "bbox": [
                97,
                824,
                308,
                842
              ],
              "section_number": "218",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_0",
              "label": "para",
              "text": "a. Create three different combinations of the taggers. Test the accuracy of each\ncombined tagger. Which combination works best?",
              "level": -1,
              "page": 241,
              "reading_order": 0,
              "bbox": [
                126,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_1",
              "label": "para",
              "text": "b. Try varying the size of the training corpus. How does it affect your results?",
              "level": -1,
              "page": 241,
              "reading_order": 1,
              "bbox": [
                126,
                107,
                574,
                126
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_2",
              "label": "para",
              "text": "37. • Our approach for tagging an unknown word has been to consider the letters of\nthe word (using RegexpTagger() ), or to ignore the word altogether and tag it as a\nnoun (using nltk.DefaultTagger() ). These methods will not do well for texts hav-\ning new words that are not nouns. Consider the sentence I like to blog on Kim's\nblog . If blog is a new word, then looking at the previous tag ( T0 versus NP$ ) would\nprobably be helpful, i.e., we need a default tagger that is sensitive to the preceding\ntag.",
              "level": -1,
              "page": 241,
              "reading_order": 2,
              "bbox": [
                98,
                133,
                585,
                246
              ],
              "section_number": "37",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_3",
              "label": "para",
              "text": "a. Create a new kind of unigram tagger that looks at the tag of the previous word,\nand ignores the current word. (The best way to do this is to modify the source\ncode for UnigramTagger(), which presumes knowledge of object-oriented pro-\ngramming in Python.)",
              "level": -1,
              "page": 241,
              "reading_order": 3,
              "bbox": [
                126,
                250,
                585,
                317
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_4",
              "label": "para",
              "text": "b. Add this tagger to the sequence of backoff taggers (including ordinary trigram\nand bigram taggers that look at words), right before the usual default tagger.",
              "level": -1,
              "page": 241,
              "reading_order": 4,
              "bbox": [
                126,
                322,
                584,
                358
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_5",
              "label": "para",
              "text": "c. Evaluate the contribution of this new unigram tagger",
              "level": -1,
              "page": 241,
              "reading_order": 5,
              "bbox": [
                126,
                358,
                449,
                376
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_6",
              "label": "para",
              "text": "38. • Consider the code in Section 5.5, which determines the upper bound for accuracy\nof a trigram tagger. Review Abney's discussion concerning the impossibility of\nexact tagging (Abney, 2006). Explain why correct tagging of these examples re-\nquires access to other kinds of information than just words and tags. How might\nyou estimate the scale of this problem?",
              "level": -1,
              "page": 241,
              "reading_order": 6,
              "bbox": [
                98,
                376,
                586,
                465
              ],
              "section_number": "38",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_7",
              "label": "para",
              "text": "39. • Use some of the estimation techniques in nltk.probability, such as Lidstone or\nLaplace estimation, to develop a statistical tagger that does a better job than n-\ngram backoff taggers in cases where contexts encountered during testing were not\nseen during training.",
              "level": -1,
              "page": 241,
              "reading_order": 7,
              "bbox": [
                98,
                465,
                585,
                532
              ],
              "section_number": "39",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_8",
              "label": "para",
              "text": "40. • Inspect the diagnostic files created by the Brill tagger rules.out and\nerrors.out. Obtain the demonstration code by accessing the source code (at http:\n//www.nltk.org/code) and create your own version of the Brill tagger. Delete some\nof the rule templates, based on what you learned from inspecting rules.out. Add\nsome new rule templates which employ contexts that might help to correct the\nerrors you saw in errors.out.",
              "level": -1,
              "page": 241,
              "reading_order": 8,
              "bbox": [
                98,
                537,
                585,
                636
              ],
              "section_number": "40",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_9",
              "label": "para",
              "text": "41. • Develop an n-gram backoff tagger that permits “ anti-n-grams ” such as [\"the\",\n\"the\"] to be specified when a tagger is initialized. An anti-n-gram is assigned a\ncount of zero and is used to prevent backoff for this n-gram (e.g., to avoid esti-\nmating P(the | the) as just P(the)).",
              "level": -1,
              "page": 241,
              "reading_order": 9,
              "bbox": [
                98,
                636,
                584,
                707
              ],
              "section_number": "41",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_10",
              "label": "para",
              "text": "42. • Investigate three different ways to define the split between training and testing\ndata when developing a tagger using the Brown Corpus: genre ( category ), source\n( fileid ), and sentence. Compare their relative performance and discuss which\nmethod is the most legitimate. (You might use n-fold cross validation, discussed\nin Section 6.3 , to improve the accuracy of the evaluations.)",
              "level": -1,
              "page": 241,
              "reading_order": 10,
              "bbox": [
                98,
                707,
                585,
                797
              ],
              "section_number": "42",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_241_order_11",
              "label": "foot",
              "text": "5.10 Exercises | 219",
              "level": -1,
              "page": 241,
              "reading_order": 11,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "5.10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_242_order_0",
              "label": "para",
              "text": "_",
              "level": -1,
              "page": 242,
              "reading_order": 0,
              "bbox": [
                153,
                161,
                494,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_234_order_2",
          "label": "para",
          "text": "Common tagsets often capture some morphosyntactic information, that is, informa-\ntion about the kind of morphological markings that words receive by virtue of their\nsyntactic role. Consider, for example, the selection of distinct grammatical forms of the\nword go illustrated in the following sentences:",
          "level": -1,
          "page": 234,
          "reading_order": 2,
          "bbox": [
            97,
            143,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_234_order_3",
          "label": "list_group",
          "text": "(3) a. Go away\nb. He sometimes goes to the cafe.",
          "level": -1,
          "page": 234,
          "reading_order": 3,
          "bbox": [
            118,
            224,
            216,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "(3) a. Go away",
              "bbox": [
                118,
                224,
                216,
                241
              ],
              "page": 234,
              "reading_order": 3
            },
            {
              "label": "list",
              "text": "b. He sometimes goes to the cafe.",
              "bbox": [
                144,
                241,
                342,
                262
              ],
              "page": 234,
              "reading_order": 4
            },
            {
              "label": "list",
              "text": "c. All the cakes have gone",
              "bbox": [
                144,
                268,
                298,
                286
              ],
              "page": 234,
              "reading_order": 5
            },
            {
              "label": "list",
              "text": "d. We went on the excursion.",
              "bbox": [
                144,
                286,
                324,
                304
              ],
              "page": 234,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_234_order_7",
          "label": "para",
          "text": "Each of these forms—go, goes, gone, and went—is morphologically distinct from the\nothers. Consider the form goes. This occurs in a restricted set of grammatical contexts,\nand requires a third person singular subject. Thus, the following sentences are\nungrammatical.",
          "level": -1,
          "page": 234,
          "reading_order": 7,
          "bbox": [
            97,
            313,
            585,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_234_order_8",
          "label": "list_group",
          "text": "(4) a. *They sometimes goes to the cafe\nb. *I sometimes goes to the cafe.",
          "level": -1,
          "page": 234,
          "reading_order": 8,
          "bbox": [
            118,
            394,
            354,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "(4) a. *They sometimes goes to the cafe",
              "bbox": [
                118,
                394,
                354,
                412
              ],
              "page": 234,
              "reading_order": 8
            },
            {
              "label": "list",
              "text": "b. *I sometimes goes to the cafe.",
              "bbox": [
                144,
                412,
                333,
                431
              ],
              "page": 234,
              "reading_order": 9
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_234_order_10",
          "label": "para",
          "text": "By contrast, gone is the past participle form; it is required after have (and cannot be\nreplaced in this context by goes), and cannot occur as the main verb of a clause.",
          "level": -1,
          "page": 234,
          "reading_order": 10,
          "bbox": [
            98,
            439,
            585,
            476
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_234_order_11",
          "label": "list_group",
          "text": "(5) a.\n*All the cakes have goes\nb. *He sometimes gone to the cafe.",
          "level": -1,
          "page": 234,
          "reading_order": 11,
          "bbox": [
            118,
            491,
            300,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "(5) a.\n*All the cakes have goes",
              "bbox": [
                118,
                491,
                300,
                510
              ],
              "page": 234,
              "reading_order": 11
            },
            {
              "label": "list",
              "text": "b. *He sometimes gone to the cafe.",
              "bbox": [
                144,
                510,
                350,
                528
              ],
              "page": 234,
              "reading_order": 12
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_234_order_13",
          "label": "para",
          "text": "We can easily imagine a tagset in which the four distinct grammatical forms just dis-\ncussed were all tagged as VB . Although this would be adequate for some purposes, a\nmore fine-grained tagset provides useful information about these forms that can help\nother processors that try to detect patterns in tag sequences. The Brown tagset captures\nthese distinctions, as summarized in Table 5 - 7 .",
          "level": -1,
          "page": 234,
          "reading_order": 13,
          "bbox": [
            97,
            537,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_234_order_14",
          "label": "table",
          "text": "Table 5-7. Some morphosyntactic distinctions in the Brown tages [TABLE: <table><tr><td>Form</td><td>Category</td><td>Tag</td></tr><tr><td>go</td><td>ase</td><td>VB</td></tr><tr><td>goes</td><td>third singular present</td><td>VBZ</td></tr><tr><td>gone</td><td>past participle</td><td>VBN</td></tr><tr><td>going</td><td>gerund</td><td>VBG</td></tr><tr><td>went</td><td>simple past</td><td>VBD</td></tr></table>]",
          "level": -1,
          "page": 234,
          "reading_order": 14,
          "bbox": [
            100,
            654,
            270,
            771
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Form</td><td>Category</td><td>Tag</td></tr><tr><td>go</td><td>ase</td><td>VB</td></tr><tr><td>goes</td><td>third singular present</td><td>VBZ</td></tr><tr><td>gone</td><td>past participle</td><td>VBN</td></tr><tr><td>going</td><td>gerund</td><td>VBG</td></tr><tr><td>went</td><td>simple past</td><td>VBD</td></tr></table>",
              "bbox": [
                100,
                654,
                270,
                771
              ],
              "page": 234,
              "reading_order": 14
            },
            {
              "label": "cap",
              "text": "Table 5-7. Some morphosyntactic distinctions in the Brown tages",
              "bbox": [
                99,
                635,
                413,
                648
              ],
              "page": 234,
              "reading_order": 15
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_234_order_16",
          "label": "foot",
          "text": "212 | Chapter 5: Categorizing and Tagging Words",
          "level": -1,
          "page": 234,
          "reading_order": 16,
          "bbox": [
            97,
            824,
            308,
            842
          ],
          "section_number": "212",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_235_order_0",
          "label": "para",
          "text": "In addition to this set of verb tags, the various forms of the verb to be have special tags:\nbe/BE, being/BEG, am/BEM, are/BER, is/BEZ, been/BEN, were/BED, and was/BEDZ (plus extra\ntags for negative forms of the verb). All told, this fine-grained tagging of verbs means\nthat an automatic tagger that uses this tagset is effectively carrying out a limited amount\nof morphological analysis.",
          "level": -1,
          "page": 235,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_235_order_1",
          "label": "para",
          "text": "Most part-of-speech tagsets make use of the same basic categories, such as noun, verb,\nadjective, and preposition. However, tagsets differ both in how finely they divide words\ninto categories, and in how they define their categories. For example, is might be tagged\nsimply as a verb in one tagset, but as a distinct form of the lexeme be in another tagset\n(as in the Brown Corpus). This variation in tagsets is unavoidable, since part-of-speech\ntags are used in different ways for different tasks. In other words, there is no one “right\nway” to assign tags, only more or less useful ways depending on one’s goals.",
          "level": -1,
          "page": 235,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            585,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_243_order_0",
      "label": "sec",
      "text": "CHAPTER 6\nLearning to Classify Text",
      "level": 1,
      "page": 243,
      "reading_order": 0,
      "bbox": [
        270,
        78,
        585,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_243_order_7",
          "label": "sub_sec",
          "text": "6.1 Supervised Classification",
          "level": 2,
          "page": 243,
          "reading_order": 7,
          "bbox": [
            97,
            643,
            326,
            666
          ],
          "section_number": "6.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_243_order_8",
              "label": "para",
              "text": "Classification is the task of choosing the correct class label for a given input. In basic\nclassification tasks, each input is considered in isolation from all other inputs, and the\nset of labels is defined in advance. Some examples of classification tasks are:",
              "level": -1,
              "page": 243,
              "reading_order": 8,
              "bbox": [
                97,
                672,
                585,
                725
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_243_order_9",
              "label": "foot",
              "text": "221",
              "level": -1,
              "page": 243,
              "reading_order": 9,
              "bbox": [
                566,
                824,
                584,
                837
              ],
              "section_number": "221",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_244_order_0",
              "label": "list_group",
              "text": "Deciding whether an email is spam or not\nDeciding what the topic of a news article is, from a fixed list of topic areas such as\n‘sports,\n’” “technology,\n” and “politics.\n”",
              "level": -1,
              "page": 244,
              "reading_order": 0,
              "bbox": [
                126,
                71,
                360,
                89
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "Deciding whether an email is spam or not",
                  "bbox": [
                    126,
                    71,
                    360,
                    89
                  ],
                  "page": 244,
                  "reading_order": 0
                },
                {
                  "label": "list",
                  "text": "Deciding what the topic of a news article is, from a fixed list of topic areas such as\n‘sports,\n’” “technology,\n” and “politics.\n”",
                  "bbox": [
                    125,
                    96,
                    585,
                    126
                  ],
                  "page": 244,
                  "reading_order": 1
                },
                {
                  "label": "list",
                  "text": "Deciding whether a given occurrence of the word bank is used to refer to a river\nbank, a financial institution, the act of tilting to the side, or the act of depositing\nsomething in a financial institution.",
                  "bbox": [
                    122,
                    133,
                    585,
                    180
                  ],
                  "page": 244,
                  "reading_order": 2
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_244_order_3",
              "label": "para",
              "text": "The basic classification task has a number of interesting variants. For example, in multi-\nclass classification, each instance may be assigned multiple labels; in open-class clas-\nsification, the set of labels is not defined in advance; and in sequence classification, a\nlist of inputs are jointly classified.",
              "level": -1,
              "page": 244,
              "reading_order": 3,
              "bbox": [
                97,
                188,
                584,
                254
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_244_order_4",
              "label": "para",
              "text": "A classifier is called supervised if it is built based on training corpora containing the\ncorrect label for each input. The framework used by supervised classification is shown\nin Figure 6-1.",
              "level": -1,
              "page": 244,
              "reading_order": 4,
              "bbox": [
                97,
                259,
                585,
                313
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_244_order_5",
              "label": "figure",
              "text": "Figure 6-1. Supervised classification. (a) During training, a feature extractor is used to convert each\ninput value to a feature set. These feature sets, which capture the basic information about each input\nthat should be used to classify it, are discussed in the next section. Pairs of feature sets and labels are\nfed into the machine learning algorithm to generate a model. (b) During prediction, the same feature\nextractor is used to convert unseen inputs to feature sets. These feature sets are then fed into the model,\nwhich generates predicted labels. [IMAGE: ![Figure](figures/NLTK_page_244_figure_005.png)]",
              "level": -1,
              "page": 244,
              "reading_order": 5,
              "bbox": [
                126,
                322,
                557,
                546
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_244_figure_005.png)",
                  "bbox": [
                    126,
                    322,
                    557,
                    546
                  ],
                  "page": 244,
                  "reading_order": 5
                },
                {
                  "label": "cap",
                  "text": "Figure 6-1. Supervised classification. (a) During training, a feature extractor is used to convert each\ninput value to a feature set. These feature sets, which capture the basic information about each input\nthat should be used to classify it, are discussed in the next section. Pairs of feature sets and labels are\nfed into the machine learning algorithm to generate a model. (b) During prediction, the same feature\nextractor is used to convert unseen inputs to feature sets. These feature sets are then fed into the model,\nwhich generates predicted labels.",
                  "bbox": [
                    96,
                    555,
                    585,
                    645
                  ],
                  "page": 244,
                  "reading_order": 6
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_244_order_7",
              "label": "para",
              "text": "In the rest of this section, we will look at how classifiers can be employed to solve a\nwide variety of tasks. Our discussion is not intended to be comprehensive, but to give\na representative sample of tasks that can be performed with the help of text classifiers.",
              "level": -1,
              "page": 244,
              "reading_order": 7,
              "bbox": [
                97,
                661,
                585,
                708
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_243_order_1",
          "label": "para",
          "text": "Detecting patterns is a central part of Natural Language Processing. Words ending in\n-ed tend to be past tense verbs ( Chapter 5 ). Frequent use of will is indicative of news\ntext ( Chapter 3 ). These observable patterns—word structure and word frequency—\nhappen to correlate with particular aspects of meaning, such as tense and topic. But\nhow did we know where to start looking, which aspects of form to associate with which\naspects of meaning?",
          "level": -1,
          "page": 243,
          "reading_order": 1,
          "bbox": [
            97,
            286,
            585,
            386
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_243_order_2",
          "label": "para",
          "text": "The goal of this chapter is to answer the following questions",
          "level": -1,
          "page": 243,
          "reading_order": 2,
          "bbox": [
            100,
            394,
            441,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_243_order_3",
          "label": "para",
          "text": "1. How can we identify particular features of language data that are salient for clas-\nsifying it?",
          "level": -1,
          "page": 243,
          "reading_order": 3,
          "bbox": [
            100,
            420,
            584,
            451
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_243_order_4",
          "label": "para",
          "text": "2. How can we construct models of language that can be used to perform language\nprocessing tasks automatically?",
          "level": -1,
          "page": 243,
          "reading_order": 4,
          "bbox": [
            100,
            456,
            585,
            488
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_243_order_5",
          "label": "para",
          "text": "3. What can we learn about language from these models?",
          "level": -1,
          "page": 243,
          "reading_order": 5,
          "bbox": [
            100,
            492,
            440,
            510
          ],
          "section_number": "3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_243_order_6",
          "label": "para",
          "text": "Along the way we will study some important machine learning techniques, including\ndecision trees, naive Bayes classifiers, and maximum entropy classifiers. We will gloss\nover the mathematical and statistical underpinnings of these techniques, focusing in-\nstead on how and when to use them (see Section 6.9 for more technical background).\nBefore looking at these methods, we first need to appreciate the broad scope of this\ntopic.",
          "level": -1,
          "page": 243,
          "reading_order": 6,
          "bbox": [
            97,
            519,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_244_order_8",
      "label": "sec",
      "text": "Gender Identification",
      "level": 1,
      "page": 244,
      "reading_order": 8,
      "bbox": [
        97,
        724,
        243,
        743
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_244_order_9",
          "label": "para",
          "text": "In Section 2.4, we saw that male and female names have some distinctive characteristics.\nNames ending in a, e, and i are likely to be female, while names ending in k, o, r, s, and\nt are likely to be male. Let’s build a classifier to model these differences more precisely.",
          "level": -1,
          "page": 244,
          "reading_order": 9,
          "bbox": [
            97,
            751,
            584,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_244_order_10",
          "label": "foot",
          "text": "222 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 244,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "222",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_0",
          "label": "para",
          "text": "The first step in creating a classifier is deciding what features of the input are relevant,\nand how to encode those features. For this example, we’ll start by just looking at the\nfinal letter of a given name. The following feature extractor function builds a dic-\ntionary containing relevant information about a given name:",
          "level": -1,
          "page": 245,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_2",
          "label": "para",
          "text": "The dictionary that is returned by this function is called a feature set and maps from\nfeatures' names to their values. Feature names are case-sensitive strings that typically\nprovide a short human-readable description of the feature. Feature values are values\nwith simple types, such as Booleans, numbers, and strings.",
          "level": -1,
          "page": 245,
          "reading_order": 2,
          "bbox": [
            97,
            206,
            585,
            272
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_245_figure_003.png)",
          "level": -1,
          "page": 245,
          "reading_order": 3,
          "bbox": [
            118,
            286,
            171,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_4",
          "label": "para",
          "text": "Most classification methods require that features be encoded using sim-\nple value types, such as Booleans, numbers, and strings. But note that\njust because a feature has a simple type, this does not necessarily mean\nthat the feature's value is simple to express or compute; indeed, it is\neven possible to use very complex and informative values, such as the\noutput of a second supervised classifier, as features.",
          "level": -1,
          "page": 245,
          "reading_order": 4,
          "bbox": [
            171,
            295,
            530,
            387
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_5",
          "label": "para",
          "text": "Now that we’ve defined a feature extractor, we need to prepare a list of examples and\ncorresponding class labels:",
          "level": -1,
          "page": 245,
          "reading_order": 5,
          "bbox": [
            97,
            410,
            583,
            441
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_7",
          "label": "para",
          "text": "Next, we use the feature extractor to process the names data, and divide the resulting\nlist of feature sets into a training set and a test set. The training set is used to train a\nnew “naive Bayes” classifier.",
          "level": -1,
          "page": 245,
          "reading_order": 7,
          "bbox": [
            97,
            519,
            585,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_9",
          "label": "para",
          "text": "We will learn more about the naive Bayes classifier later in the chapter. For now, let’s\njust test it out on some names that did not appear in its training data:",
          "level": -1,
          "page": 245,
          "reading_order": 9,
          "bbox": [
            97,
            627,
            584,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_11",
          "label": "para",
          "text": "Observe that these character names from The Matrix are correctly classified. Although\nthis science fiction movie is set in 2199, it still conforms with our expectations about\nnames and genders. We can systematically evaluate the classifier on a much larger\nquantity of unseen data:",
          "level": -1,
          "page": 245,
          "reading_order": 11,
          "bbox": [
            97,
            725,
            585,
            791
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_245_order_12",
          "label": "foot",
          "text": "6.1 Supervised Classification | 223",
          "level": -1,
          "page": 245,
          "reading_order": 12,
          "bbox": [
            436,
            824,
            584,
            842
          ],
          "section_number": "6.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_246_order_1",
          "label": "para",
          "text": "Finally, we can examine the classifier to determine which features it found most effec-\ntive for distinguishing the names’ genders:",
          "level": -1,
          "page": 246,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            584,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_246_order_3",
          "label": "para",
          "text": "This listing shows that the names in the training set that end in a are female 38 times\nmore often than they are male, but names that end in k are male 31 times more often\nthan they are female. These ratios are known as likelihood ratios, and can be useful\nfor comparing different feature-outcome relationships.",
          "level": -1,
          "page": 246,
          "reading_order": 3,
          "bbox": [
            97,
            248,
            585,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_246_order_4",
          "label": "para",
          "text": "Your Turn: Modify the gender _ features() function to provide the clas-\nsifier with features encoding the length of the name, its first letter, and\nany other features that seem like they might be informative. Retrain the\nclassifier with these new features, and test its accuracy.",
          "level": -1,
          "page": 246,
          "reading_order": 4,
          "bbox": [
            171,
            340,
            530,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_246_order_5",
          "label": "para",
          "text": "When working with large corpora, constructing a single list that contains the features\nof every instance can use up a large amount of memory. In these cases, use the function\nnltk.classify.apply_features, which returns an object that acts like a list but does not\nstore all the feature sets in memory:",
          "level": -1,
          "page": 246,
          "reading_order": 5,
          "bbox": [
            97,
            421,
            585,
            485
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_246_order_7",
      "label": "sec",
      "text": "Choosing the Right Features",
      "level": 1,
      "page": 246,
      "reading_order": 7,
      "bbox": [
        97,
        546,
        288,
        566
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_246_order_8",
          "label": "para",
          "text": "Selecting relevant features and deciding how to encode them for a learning method can\nhave an enormous impact on the learning method’s ability to extract a good model.\nMuch of the interesting work in building a classifier is deciding what features might be\nrelevant, and how we can represent them. Although it’s often possible to get decent\nperformance by using a fairly simple and obvious set of features, there are usually sig-\nnificant gains to be had by using carefully constructed features based on a thorough\nunderstanding of the task at hand.",
          "level": -1,
          "page": 246,
          "reading_order": 8,
          "bbox": [
            97,
            573,
            585,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_246_order_9",
          "label": "para",
          "text": "Typically, feature extractors are built through a process of trial-and-error, guided by\nintuitions about what information is relevant to the problem. It's common to start with\na “ kitchen sink ” approach, including all the features that you can think of, and then\nchecking to see which features actually are helpful. We take this approach for name\ngender features in Example 6 - 1 .",
          "level": -1,
          "page": 246,
          "reading_order": 9,
          "bbox": [
            97,
            697,
            585,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_246_order_10",
          "label": "foot",
          "text": "224 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 246,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "224",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_247_order_0",
          "label": "para",
          "text": "Example 6-1. A feature extractor that overfits gender features. The featuresets returned by this feature\nextractor contain a large number of specific features, leading to overfitting for the relatively small\nNames Corpus.",
          "level": -1,
          "page": 247,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_247_order_2",
          "label": "para",
          "text": "However, there are usually limits to the number of features that you should use with a\ngiven learning algorithm — if you provide too many features, then the algorithm will\nhave a higher chance of relying on idiosyncrasies of your training data that don't gen-\neralize well to new examples. This problem is known as overfitting , and can be espe-\ncially problematic when working with small training sets. For example, if we train a\nnaive Bayes classifier using the feature extractor shown in Example 6-1 , it will overfit\nthe relatively small training set, resulting in a system whose accuracy is about $1\\%$ lower\nthan the accuracy of a classifier that only pays attention to the final letter of each name:",
          "level": -1,
          "page": 247,
          "reading_order": 2,
          "bbox": [
            97,
            277,
            585,
            407
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_247_order_4",
          "label": "para",
          "text": "Once an initial set of features has been chosen, a very productive method for refining\nthe feature set is error analysis . First, we select a development set , containing the\ncorpus data for creating the model. This development set is then subdivided into the\ntraining set and the dev-test set.",
          "level": -1,
          "page": 247,
          "reading_order": 4,
          "bbox": [
            97,
            490,
            585,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_247_order_6",
          "label": "para",
          "text": "The training set is used to train the model, and the dev-test set is used to perform error\nanalysis. The test set serves in our final evaluation of the system. For reasons discussed\nlater, it is important that we employ a separate dev-test set for error analysis, rather\nthan just using the test set. The division of the corpus data into different subsets is\nshown in Figure 6 - 2 .",
          "level": -1,
          "page": 247,
          "reading_order": 6,
          "bbox": [
            97,
            609,
            585,
            690
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_247_order_7",
          "label": "para",
          "text": "Having divided the corpus into appropriate datasets, we train a model using the training\nset ❶ , and then run it on the dev-test set ❷ .",
          "level": -1,
          "page": 247,
          "reading_order": 7,
          "bbox": [
            97,
            698,
            585,
            728
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_247_order_9",
          "label": "foot",
          "text": "6.1 Supervised Classification | 225",
          "level": -1,
          "page": 247,
          "reading_order": 9,
          "bbox": [
            436,
            824,
            585,
            842
          ],
          "section_number": "6.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_248_order_1",
          "label": "figure",
          "text": "Figure 6-2. Organization of corpus data for training supervised classifiers. The corpus data is divided\ninto two sets: the development set and the test set. The development set is often further subdivided into\na training set and a dev-test set. [IMAGE: ![Figure](figures/NLTK_page_248_figure_001.png)]",
          "level": -1,
          "page": 248,
          "reading_order": 1,
          "bbox": [
            100,
            107,
            583,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_248_figure_001.png)",
              "bbox": [
                100,
                107,
                583,
                331
              ],
              "page": 248,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Figure 6-2. Organization of corpus data for training supervised classifiers. The corpus data is divided\ninto two sets: the development set and the test set. The development set is often further subdivided into\na training set and a dev-test set.",
              "bbox": [
                97,
                338,
                585,
                380
              ],
              "page": 248,
              "reading_order": 2
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_248_order_3",
          "label": "para",
          "text": "Using the dev-test set, we can generate a list of the errors that the classifier makes when\npredicting name genders:",
          "level": -1,
          "page": 248,
          "reading_order": 3,
          "bbox": [
            97,
            403,
            584,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_248_order_5",
          "label": "para",
          "text": "We can then examine individual error cases where the model predicted the wrong label,\nand try to determine what additional pieces of information would allow it to make the\nright decision (or which existing pieces of information are tricking it into making the\nwrong decision). The feature set can then be adjusted accordingly. The names classifier\nthat we have built generates about 100 errors on the dev-test corpus:",
          "level": -1,
          "page": 248,
          "reading_order": 5,
          "bbox": [
            97,
            510,
            585,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_248_order_7",
          "label": "foot",
          "text": "226 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 248,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "226",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_249_order_0",
          "label": "para",
          "text": "Looking through this list of errors makes it clear that some suffixes that are more than\none letter can be indicative of name genders. For example, names ending in yn appear\nto be predominantly female, despite the fact that names ending in n tend to be male;\nand names ending in ch are usually male, even though names that end in h tend to be\nfemale. We therefore adjust our feature extractor to include features for two-letter\nsuffixes:",
          "level": -1,
          "page": 249,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_249_order_2",
          "label": "para",
          "text": "Rebuilding the classifier with the new feature extractor, we see that the performance\non the dev-test dataset improves by almost three percentage points (from 76.5 % to\n78.2 % ):",
          "level": -1,
          "page": 249,
          "reading_order": 2,
          "bbox": [
            97,
            224,
            585,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_249_order_4",
          "label": "para",
          "text": "This error analysis procedure can then be repeated, checking for patterns in the errors\nthat are made by the newly improved classifier. Each time the error analysis procedure\nis repeated, we should select a different dev-test/training split, to ensure that the clas-\nsifier does not start to reflect idiosyncrasies in the dev-test set.",
          "level": -1,
          "page": 249,
          "reading_order": 4,
          "bbox": [
            97,
            358,
            585,
            422
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_249_order_5",
          "label": "para",
          "text": "But once we’ve used the dev-test set to help us develop the model, we can no longer\ntrust that it will give us an accurate idea of how well the model would perform on new\ndata. It is therefore important to keep the test set separate, and unused, until our model\ndevelopment is complete. At that point, we can use the test set to evaluate how well\nour model will perform on new input values.",
          "level": -1,
          "page": 249,
          "reading_order": 5,
          "bbox": [
            97,
            430,
            585,
            511
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_249_order_6",
      "label": "sec",
      "text": "Document Classification",
      "level": 1,
      "page": 249,
      "reading_order": 6,
      "bbox": [
        100,
        527,
        261,
        546
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_249_order_7",
          "label": "para",
          "text": "In Section 2.1 , we saw several examples of corpora where documents have been labeled\nwith categories. Using these corpora, we can build classifiers that will automatically\ntag new documents with appropriate category labels. First, we construct a list of docu-\nments, labeled with the appropriate categories. For this example, we've chosen the\nMovie Reviews Corpus, which categorizes each review as positive or negative.",
          "level": -1,
          "page": 249,
          "reading_order": 7,
          "bbox": [
            97,
            555,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_249_order_9",
          "label": "para",
          "text": "Next, we define a feature extractor for documents, so the classifier will know which\naspects of the data it should pay attention to (see Example 6-2 ). For document topic\nidentification, we can define a feature for each word, indicating whether the document\ncontains that word. To limit the number of features that the classifier needs to process,\nwe begin by constructing a list of the 2,000 most frequent words in the overall",
          "level": -1,
          "page": 249,
          "reading_order": 9,
          "bbox": [
            97,
            716,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_249_order_10",
          "label": "foot",
          "text": "6.1 Supervised Classification | 227",
          "level": -1,
          "page": 249,
          "reading_order": 10,
          "bbox": [
            436,
            824,
            585,
            842
          ],
          "section_number": "6.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_250_order_0",
          "label": "para",
          "text": "corpus ❶. We can then define a feature extractor ❷ that simply checks whether each\nof these words is present in a given document.",
          "level": -1,
          "page": 250,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_250_order_1",
          "label": "para",
          "text": "Example 6-2. A feature extractor for document classification, whose features indicate whether or not\nindividual words are present in a given document.",
          "level": -1,
          "page": 250,
          "reading_order": 1,
          "bbox": [
            97,
            116,
            584,
            147
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_250_order_5",
          "label": "figure",
          "text": "We compute the set of all words in a document in ❸ , rather than just\nchecking if word in document , because checking whether a word occurs\nin a set is much faster than checking whether it occurs in a list (see\nSection 4.7 ). [IMAGE: ![Figure](figures/NLTK_page_250_figure_005.png)]",
          "level": -1,
          "page": 250,
          "reading_order": 5,
          "bbox": [
            118,
            322,
            171,
            378
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_250_figure_005.png)",
              "bbox": [
                118,
                322,
                171,
                378
              ],
              "page": 250,
              "reading_order": 5
            },
            {
              "label": "cap",
              "text": "We compute the set of all words in a document in ❸ , rather than just\nchecking if word in document , because checking whether a word occurs\nin a set is much faster than checking whether it occurs in a list (see\nSection 4.7 ).",
              "bbox": [
                171,
                331,
                530,
                394
              ],
              "page": 250,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_250_order_7",
          "label": "para",
          "text": "Now that we've defined our feature extractor, we can use it to train a classifier to label\nnew movie reviews ( Example 6-3 ). To check how reliable the resulting classifier is, we\ncompute its accuracy on the test set ❶ . And once again, we can use show_most_infor\nmative_features() to find out which features the classifier found to be most\ninformative ❷ .",
          "level": -1,
          "page": 250,
          "reading_order": 7,
          "bbox": [
            97,
            412,
            585,
            495
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_250_order_8",
          "label": "para",
          "text": "Example 6-3. Training and testing a classifier for document classification.",
          "level": -1,
          "page": 250,
          "reading_order": 8,
          "bbox": [
            98,
            510,
            458,
            524
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_250_order_11",
          "label": "para",
          "text": "Apparently in this corpus, a review that mentions Seagal is almost 8 times more likely\nto be negative than positive, while a review that mentions Damon is about 6 times more\nlikely to be positive.",
          "level": -1,
          "page": 250,
          "reading_order": 11,
          "bbox": [
            97,
            707,
            585,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_250_order_12",
          "label": "foot",
          "text": "228 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 250,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "228",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_251_order_0",
      "label": "sec",
      "text": "Part-of-Speech Tagging",
      "level": 1,
      "page": 251,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        254,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_251_order_1",
          "label": "para",
          "text": "In Chapter 5, we built a regular expression tagger that chooses a part-of-speech tag for\na word by looking at the internal makeup of the word. However, this regular expression\ntagger had to be handcrafted. Instead, we can train a classifier to work out which suf-\nfixes are most informative. Let’s begin by finding the most common suffixes:",
          "level": -1,
          "page": 251,
          "reading_order": 1,
          "bbox": [
            97,
            103,
            585,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_251_order_3",
          "label": "para",
          "text": "Next, we’ll define a feature extractor function that checks a given word for these\nsuffixes:",
          "level": -1,
          "page": 251,
          "reading_order": 3,
          "bbox": [
            97,
            358,
            585,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_251_order_5",
          "label": "para",
          "text": "Feature extraction functions behave like tinted glasses, highlighting some of the prop-\nerties (colors) in our data and making it impossible to see other properties. The classifier\nwill rely exclusively on these highlighted properties when determining how to label\ninputs. In this case, the classifier will make its decisions based only on information\nabout which of the common suffixes (if any) a given word has.",
          "level": -1,
          "page": 251,
          "reading_order": 5,
          "bbox": [
            97,
            474,
            585,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_251_order_6",
          "label": "para",
          "text": "Now that we've defined our feature extractor, we can use it to train a new “ decision\ntree ” classifier (to be discussed in Section 6.4 ):",
          "level": -1,
          "page": 251,
          "reading_order": 6,
          "bbox": [
            97,
            564,
            584,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_251_order_8",
          "label": "para",
          "text": "One nice feature of decision tree models is that they are often fairly easy to interpret.\nWe can even instruct NLTK to print them out as pseudocode:",
          "level": -1,
          "page": 251,
          "reading_order": 8,
          "bbox": [
            97,
            752,
            584,
            784
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_251_order_9",
          "label": "foot",
          "text": "6.1 Supervised Classification | 229",
          "level": -1,
          "page": 251,
          "reading_order": 9,
          "bbox": [
            436,
            824,
            585,
            842
          ],
          "section_number": "6.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_252_order_1",
          "label": "para",
          "text": "Here, we can see that the classifier begins by checking whether a word ends with a\ncomma—if so, then it will receive the special tag \",\" . Next, the classifier checks whether\nthe word ends in \"the\" , in which case it's almost certainly a determiner. This “suffix”\ngets used early by the decision tree because the word the is so common. Continuing\non, the classifier checks if the word ends in s. If so, then it's most likely to receive the\nverb tag VBZ (unless it's the word is, which has the special tag BEZ), and if not, then it's\nmost likely a noun (unless it's the punctuation mark “ .”). The actual classifier contains\nfurther nested if-then statements below the ones shown here, but the depth=4 argument\njust displays the top portion of the decision tree.",
          "level": -1,
          "page": 252,
          "reading_order": 1,
          "bbox": [
            97,
            224,
            585,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_252_order_2",
      "label": "sec",
      "text": "Exploiting Context",
      "level": 1,
      "page": 252,
      "reading_order": 2,
      "bbox": [
        98,
        385,
        220,
        408
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_252_order_3",
          "label": "para",
          "text": "By augmenting the feature extraction function, we could modify this part-of-speech\ntagger to leverage a variety of other word-internal features, such as the length of the\nword, the number of syllables it contains, or its prefix. However, as long as the feature\nextractor just looks at the target word, we have no way to add features that depend on\nthe context in which the word appears. But contextual features often provide powerful\nclues about the correct tag — for example, when tagging the word fly , knowing that the\nprevious word is a will allow us to determine that it is functioning as a noun, not a verb.",
          "level": -1,
          "page": 252,
          "reading_order": 3,
          "bbox": [
            97,
            412,
            585,
            529
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_252_order_4",
          "label": "para",
          "text": "In order to accommodate features that depend on a word's context, we must revise the\npattern that we used to define our feature extractor. Instead of just passing in the word\nto be tagged, we will pass in a complete (untagged) sentence, along with the index of\nthe target word. This approach is demonstrated in Example 6-4 , which employs a con-\ntext-dependent feature extractor to define a part-of-speech tag classifier.",
          "level": -1,
          "page": 252,
          "reading_order": 4,
          "bbox": [
            97,
            537,
            585,
            619
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_252_order_5",
          "label": "foot",
          "text": "230 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 252,
          "reading_order": 5,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "230",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_253_order_0",
          "label": "para",
          "text": "Example 6-4. A part-of-speech classifier whose feature detector examines the context in which a word\nappears in order to determine which part-of-speech tag should be assigned. In particular, the identity\nof the previous word is included as a feature.",
          "level": -1,
          "page": 253,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_253_order_5",
          "label": "para",
          "text": "It's clear that exploiting contextual features improves the performance of our part-of-\nspeech tagger. For example, the classifier learns that a word is likely to be a noun if it\ncomes immediately after the word large or the word gubernatorial . However, it is unable\nto learn the generalization that a word is probably a noun if it follows an adjective,\nbecause it doesn't have access to the previous word's part-of-speech tag. In general,\nsimple classifiers always treat each input as independent from all other inputs. In many\ncontexts, this makes perfect sense. For example, decisions about whether names tend\nto be male or female can be made on a case-by-case basis. However, there are often\ncases, such as part-of-speech tagging, where we are interested in solving classification\nproblems that are closely related to one another.",
          "level": -1,
          "page": 253,
          "reading_order": 5,
          "bbox": [
            97,
            472,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_253_order_6",
      "label": "sec",
      "text": "Sequence Classification",
      "level": 1,
      "page": 253,
      "reading_order": 6,
      "bbox": [
        97,
        645,
        252,
        672
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_255_order_1",
          "label": "sub_sub_sec",
          "text": "Other Methods for Sequence Classification",
          "level": 3,
          "page": 255,
          "reading_order": 1,
          "bbox": [
            97,
            213,
            377,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_255_order_2",
              "label": "para",
              "text": "One shortcoming of this approach is that we commit to every decision that we make.\nFor example, if we decide to label a word as a noun, but later find evidence that it should\nhave been a verb, there’s no way to go back and fix our mistake. One solution to this\nproblem is to adopt a transformational strategy instead. Transformational joint classi-\nfiers work by creating an initial assignment of labels for the inputs, and then iteratively\nrefining that assignment in an attempt to repair inconsistencies between related inputs.\nThe Brill tagger, described in Section 5.6 , is a good example of this strategy.",
              "level": -1,
              "page": 255,
              "reading_order": 2,
              "bbox": [
                97,
                241,
                585,
                354
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_255_order_3",
              "label": "para",
              "text": "Another solution is to assign scores to all of the possible sequences of part-of-speech\ntags, and to choose the sequence whose overall score is highest. This is the approach\ntaken by Hidden Markov Models . Hidden Markov Models are similar to consecutive\nclassifiers in that they look at both the inputs and the history of predicted tags. How-\never, rather than simply finding the single best tag for a given word, they generate a\nprobability distribution over tags. These probabilities are then combined to calculate\nprobability scores for tag sequences, and the tag sequence with the highest probability\nis chosen. Unfortunately, the number of possible tag sequences is quite large. Given a\ntag set with 30 tags, there are about 600 trillion (30 $^{10}$ ) ways to label a 10-word sentence.\nIn order to avoid considering all these possible sequences separately, Hidden Markov\nModels require that the feature extractor only look at the most recent tag (or the most\nrecent n tags, where n is fairly small). Given that restriction, it is possible to use dynamic\nprogramming ( Section 4.7 ) to efficiently find the most likely tag sequence. In particular,\nfor each consecutive word index i , a score is computed for each possible current and\nprevious tag. This same basic approach is taken by two more advanced models, called\nMaximum Entropy Markov Models and Linear-Chain Conditional Random\nField Models ; but different algorithms are used to find scores for tag sequences.",
              "level": -1,
              "page": 255,
              "reading_order": 3,
              "bbox": [
                97,
                358,
                585,
                645
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_255_order_4",
          "label": "sub_sec",
          "text": "6.2 Further Examples of Supervised Classification",
          "level": 2,
          "page": 255,
          "reading_order": 4,
          "bbox": [
            97,
            669,
            494,
            692
          ],
          "section_number": "6.2",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_255_order_5",
              "label": "sub_sub_sec",
              "text": "Sentence Segmentation",
              "level": 3,
              "page": 255,
              "reading_order": 5,
              "bbox": [
                97,
                707,
                261,
                726
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_255_order_6",
                  "label": "para",
                  "text": "Sentence segmentation can be viewed as a classification task for punctuation: whenever\nwe encounter a symbol that could possibly end a sentence, such as a period or a question\nmark, we have to decide whether it terminates the preceding sentence.",
                  "level": -1,
                  "page": 255,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    734,
                    585,
                    782
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_255_order_7",
                  "label": "foot",
                  "text": "6.2 Further Examples of Supervised Classification | 233",
                  "level": -1,
                  "page": 255,
                  "reading_order": 7,
                  "bbox": [
                    350,
                    824,
                    584,
                    842
                  ],
                  "section_number": "6.2",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_256_order_0",
                  "label": "para",
                  "text": "The first step is to obtain some data that has already been segmented into sentences\nand convert it into a form that is suitable for extracting features:",
                  "level": -1,
                  "page": 256,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    585,
                    107
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_256_order_2",
                  "label": "para",
                  "text": "Here, tokens is a merged list of tokens from the individual sentences, and boundaries\nis a set containing the indexes of all sentence-boundary tokens. Next, we need to specify\nthe features of the data that will be used in order to decide whether punctuation indi-\ncates a sentence boundary:",
                  "level": -1,
                  "page": 256,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    224,
                    585,
                    288
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_256_order_4",
                  "label": "para",
                  "text": "Based on this feature extractor, we can create a list of labeled featuresets by selecting\nall the punctuation tokens, and tagging whether they are boundary tokens or not:",
                  "level": -1,
                  "page": 256,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    373,
                    585,
                    404
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_256_order_6",
                  "label": "para",
                  "text": "Using these featuresets, we can train and evaluate a punctuation classifier:",
                  "level": -1,
                  "page": 256,
                  "reading_order": 6,
                  "bbox": [
                    98,
                    456,
                    521,
                    474
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_256_order_8",
                  "label": "para",
                  "text": "To use this classifier to perform sentence segmentation, we simply check each punc-\ntuation mark to see whether it’s labeled as a boundary, and divide the list of words at\nthe boundary marks. The listing in Example 6-6 shows how this can be done.",
                  "level": -1,
                  "page": 256,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    555,
                    584,
                    604
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_256_order_9",
                  "label": "para",
                  "text": "Example 6-6. Classification-based sentence segmenter",
                  "level": -1,
                  "page": 256,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    618,
                    360,
                    631
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_256_order_11",
                  "label": "foot",
                  "text": "234 | Chapter 6: Learning to Classify Text",
                  "level": -1,
                  "page": 256,
                  "reading_order": 11,
                  "bbox": [
                    97,
                    824,
                    275,
                    842
                  ],
                  "section_number": "234",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_253_order_7",
          "label": "para",
          "text": "In order to capture the dependencies between related classification tasks, we can use\njoint classifier models, which choose an appropriate labeling for a collection of related\ninputs. In the case of part-of-speech tagging, a variety of different sequence\nclassifier models can be used to jointly choose part-of-speech tags for all the words in\na given sentence.",
          "level": -1,
          "page": 253,
          "reading_order": 7,
          "bbox": [
            96,
            679,
            585,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_253_order_8",
          "label": "foot",
          "text": "6.1 Supervised Classification | 231",
          "level": -1,
          "page": 253,
          "reading_order": 8,
          "bbox": [
            436,
            824,
            584,
            842
          ],
          "section_number": "6.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_254_order_0",
          "label": "para",
          "text": "One sequence classification strategy, known as consecutive classification or greedy\nsequence classification , is to find the most likely class label for the first input, then\nto use that answer to help find the best label for the next input. The process can then\nbe repeated until all of the inputs have been labeled. This is the approach that was taken\nby the bigram tagger from Section 5.5 , which began by choosing a part-of-speech tag\nfor the first word in the sentence, and then chose the tag for each subsequent word\nbased on the word itself and the predicted tag for the previous word.",
          "level": -1,
          "page": 254,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_254_order_1",
          "label": "para",
          "text": "This strategy is demonstrated in Example 6-5 . First, we must augment our feature\nextractor function to take a history argument, which provides a list of the tags that\nwe've predicted for the sentence so far O . Each tag in history corresponds with a word\nin sentence . But note that history will only contain tags for words we've already clas-\nsified, that is, words to the left of the target word. Thus, although it is possible to look\nat some features of words to the right of the target word, it is not possible to look at\nthe tags for those words (since we haven’t generated them yet).",
          "level": -1,
          "page": 254,
          "reading_order": 1,
          "bbox": [
            97,
            197,
            585,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_254_order_2",
          "label": "para",
          "text": "Having defined a feature extractor, we can proceed to build our sequence\nclassifier ❷ . During training, we use the annotated tags to provide the appropriate\nhistory to the feature extractor, but when tagging new sentences, we generate the his-\ntory list based on the output of the tagger itself.",
          "level": -1,
          "page": 254,
          "reading_order": 2,
          "bbox": [
            97,
            321,
            585,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_254_order_3",
          "label": "cap",
          "text": "Example 6-5. Part-of-speech tagging with a consecutive classifier",
          "level": -1,
          "page": 254,
          "reading_order": 3,
          "bbox": [
            97,
            394,
            413,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_254_order_7",
          "label": "foot",
          "text": "232 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 254,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "232",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_257_order_0",
      "label": "sec",
      "text": "Identifying Dialogue Act Types",
      "level": 1,
      "page": 257,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        300,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_257_order_1",
          "label": "para",
          "text": "When processing dialogue, it can be useful to think of utterances as a type of action\nperformed by the speaker. This interpretation is most straightforward for performative\nstatements such as I forgive you or I bet you can't climb that hill . But greetings, questions,\nanswers, assertions, and clarifications can all be thought of as types of speech-based\nactions. Recognizing the dialogue acts underlying the utterances in a dialogue can be\nan important first step in understanding the conversation.",
          "level": -1,
          "page": 257,
          "reading_order": 1,
          "bbox": [
            97,
            103,
            585,
            200
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_257_order_2",
          "label": "para",
          "text": "The NPS Chat Corpus, which was demonstrated in Section 2.1 , consists of over 10,000\nposts from instant messaging sessions. These posts have all been labeled with one of\n15 dialogue act types, such as “Statement,” “Emotion,” “ynQuestion,” and “Contin-\nuer.” We can therefore use this data to build a classifier that can identify the dialogue\nact types for new instant messaging posts. The first step is to extract the basic messaging\ndata. We will call xml_posts() to get a data structure representing the XML annotation\nfor each post:",
          "level": -1,
          "page": 257,
          "reading_order": 2,
          "bbox": [
            97,
            206,
            586,
            323
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_257_order_4",
          "label": "para",
          "text": "Next, we\n’ll define a simple feature extractor that checks what words the post contains:",
          "level": -1,
          "page": 257,
          "reading_order": 4,
          "bbox": [
            98,
            354,
            584,
            368
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_257_order_6",
          "label": "para",
          "text": "Finally, we construct the training and testing data by applying the feature extractor to\neach post (using post.get('class') to get a post’s dialogue act type), and create a new\nclassifier:",
          "level": -1,
          "page": 257,
          "reading_order": 6,
          "bbox": [
            97,
            448,
            585,
            494
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_257_order_8",
      "label": "sec",
      "text": "Recognizing Textual Entailment",
      "level": 1,
      "page": 257,
      "reading_order": 8,
      "bbox": [
        98,
        609,
        310,
        630
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_259_order_3",
          "label": "sub_sub_sec",
          "text": "Scaling Up to Large Datasets",
          "level": 3,
          "page": 259,
          "reading_order": 3,
          "bbox": [
            97,
            367,
            288,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_259_order_4",
              "label": "para",
              "text": "Python provides an excellent environment for performing basic text processing and\nfeature extraction. However, it is not able to perform the numerically intensive calcu-\nlations required by machine learning methods nearly as quickly as lower-level languages\nsuch as C. Thus, if you attempt to use the pure-Python machine learning implemen-\ntations (such as nltk.NaiveBayesClassifier) on large datasets, you may find that the\nlearning algorithm takes an unreasonable amount of time and memory to complete.",
              "level": -1,
              "page": 259,
              "reading_order": 4,
              "bbox": [
                97,
                394,
                585,
                496
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_259_order_5",
              "label": "para",
              "text": "If you plan to train classifiers with large amounts of training data or a large number of\nfeatures, we recommend that you explore NLTK’s facilities for interfacing with external\nmachine learning packages. Once these packages have been installed, NLTK can trans-\nparently invoke them (via system calls) to train classifier models significantly faster than\nthe pure-Python classifier implementations. See the NLTK web page for a list of rec-\nommended machine learning packages that are supported by NLTK.",
              "level": -1,
              "page": 259,
              "reading_order": 5,
              "bbox": [
                97,
                501,
                586,
                602
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_259_order_6",
          "label": "sub_sec",
          "text": "6.3 Evaluation",
          "level": 2,
          "page": 259,
          "reading_order": 6,
          "bbox": [
            97,
            627,
            216,
            647
          ],
          "section_number": "6.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_259_order_8",
              "label": "sub_sub_sec",
              "text": "The Test Set",
              "level": 3,
              "page": 259,
              "reading_order": 8,
              "bbox": [
                97,
                734,
                180,
                755
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_259_order_9",
                  "label": "para",
                  "text": "Most evaluation techniques calculate a score for a model by comparing the labels that\nt generates for the inputs in a test set (or evaluation set) with the correct labels for",
                  "level": -1,
                  "page": 259,
                  "reading_order": 9,
                  "bbox": [
                    100,
                    767,
                    585,
                    798
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_259_order_10",
                  "label": "foot",
                  "text": "6.3 Evaluation | 237",
                  "level": -1,
                  "page": 259,
                  "reading_order": 10,
                  "bbox": [
                    492,
                    824,
                    585,
                    842
                  ],
                  "section_number": "6.3",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_260_order_0",
                  "label": "para",
                  "text": "those inputs. This test set typically has the same format as the training set. However,\nit is very important that the test set be distinct from the training corpus: if we simply\nreused the training set as the test set, then a model that simply memorized its input,\nwithout learning how to generalize to new examples, would receive misleadingly high\nscores.",
                  "level": -1,
                  "page": 260,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    152
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_260_order_1",
                  "label": "para",
                  "text": "When building the test set, there is often a trade-off between the amount of data avail-\nable for testing and the amount available for training. For classification tasks that have\na small number of well-balanced labels and a diverse test set, a meaningful evaluation\ncan be performed with as few as 100 evaluation instances. But if a classification task\nhas a large number of labels or includes very infrequent labels, then the size of the test\nset should be chosen to ensure that the least frequent label occurs at least 50 times.\nAdditionally, if the test set contains many closely related instances—such as instances\ndrawn from a single document—then the size of the test set should be increased to\nensure that this lack of diversity does not skew the evaluation results. When large\namounts of annotated data are available, it is common to err on the side of safety by\nusing 10% of the overall data for evaluation.",
                  "level": -1,
                  "page": 260,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    161,
                    585,
                    344
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_260_order_2",
                  "label": "para",
                  "text": "Another consideration when choosing the test set is the degree of similarity between\ninstances in the test set and those in the development set. The more similar these two\ndatasets are, the less confident we can be that evaluation results will generalize to other\ndatasets. For example, consider the part-of-speech tagging task. At one extreme, we\ncould create the training set and test set by randomly assigning sentences from a data\nsource that reflects a single genre, such as news:",
                  "level": -1,
                  "page": 260,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    349,
                    585,
                    451
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_260_order_4",
                  "label": "para",
                  "text": "In this case, our test set will be very similar to our training set. The training set and test\nset are taken from the same genre, and so we cannot be confident that evaluation results\nwould generalize to other genres. What's worse, because of the call to\nrandom.shuffle() , the test set contains sentences that are taken from the same docu-\nments that were used for training. If there is any consistent pattern within a document\n(say, if a given word appears with a particular part-of-speech tag especially frequently),\nthen that difference will be reflected in both the development set and the test set. A\nsomewhat better approach is to ensure that the training set and test set are taken from\ndifferent documents:",
                  "level": -1,
                  "page": 260,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    546,
                    585,
                    690
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_260_order_6",
                  "label": "para",
                  "text": "If we want to perform a more stringent evaluation, we can draw the test set from docu-\nments that are less closely related to those in the training set:",
                  "level": -1,
                  "page": 260,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    761,
                    585,
                    797
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_260_order_7",
                  "label": "foot",
                  "text": "238 | Chapter 6: Learning to Classify Text",
                  "level": -1,
                  "page": 260,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    824,
                    275,
                    842
                  ],
                  "section_number": "238",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_261_order_1",
                  "label": "para",
                  "text": "If we build a classifier that performs well on this test set, then we can be confident that\nit has the power to generalize well beyond the data on which it was trained.",
                  "level": -1,
                  "page": 261,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    107,
                    585,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_259_order_7",
              "label": "para",
              "text": "In order to decide whether a classification model is accurately capturing a pattern, we\nmust evaluate that model. The result of this evaluation is important for deciding how\ntrustworthy the model is, and for what purposes we can use it. Evaluation can also be\nan effective tool for guiding us in making future improvements to the model.",
              "level": -1,
              "page": 259,
              "reading_order": 7,
              "bbox": [
                97,
                660,
                585,
                725
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_257_order_9",
          "label": "para",
          "text": "Recognizing textual entailment (RTE) is the task of determining whether a given piece\nof text T entails another text called the “hypothesis” (as already discussed in Sec-\ntion 1.5 ). To date, there have been four RTE Challenges, where shared development\nand test data is made available to competing teams. Here are a couple of examples of\ntext/hypothesis pairs from the Challenge 3 development dataset. The label True indi-\ncates that the entailment holds, and False indicates that it fails to hold.",
          "level": -1,
          "page": 257,
          "reading_order": 9,
          "bbox": [
            97,
            636,
            586,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_257_order_10",
          "label": "foot",
          "text": "6.2 Further Examples of Supervised Classification | 235",
          "level": -1,
          "page": 257,
          "reading_order": 10,
          "bbox": [
            350,
            824,
            585,
            842
          ],
          "section_number": "6.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_0",
          "label": "para",
          "text": "Challenge 3, Pair 34 (True)",
          "level": -1,
          "page": 258,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            252,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_1",
          "label": "para",
          "text": "T : Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation\nOrganisation (SCO), the fledgling association that binds Russia, China and four\nformer Soviet republics of central Asia together to fight terrorism.",
          "level": -1,
          "page": 258,
          "reading_order": 1,
          "bbox": [
            118,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_2",
          "label": "para",
          "text": "H: China is a member of SCO.",
          "level": -1,
          "page": 258,
          "reading_order": 2,
          "bbox": [
            118,
            152,
            297,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_3",
          "label": "para",
          "text": "Challenge 3, Pair 81 (False)",
          "level": -1,
          "page": 258,
          "reading_order": 3,
          "bbox": [
            97,
            178,
            253,
            192
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_4",
          "label": "para",
          "text": "T: According to NC Articles of Organization, the members of LLC company are\nH. Nelson Beavers, III, H. Chester Beavers and Jennie Beavers Stewart.",
          "level": -1,
          "page": 258,
          "reading_order": 4,
          "bbox": [
            121,
            197,
            585,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_5",
          "label": "para",
          "text": "H: Jennie Beavers Stewart is a share-holder of Carolina Analytical Laboratory.",
          "level": -1,
          "page": 258,
          "reading_order": 5,
          "bbox": [
            118,
            241,
            566,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_6",
          "label": "para",
          "text": "It should be emphasized that the relationship between text and hypothesis is not in-\ntended to be logical entailment, but rather whether a human would conclude that the\ntext provides reasonable evidence for taking the hypothesis to be true.",
          "level": -1,
          "page": 258,
          "reading_order": 6,
          "bbox": [
            97,
            259,
            585,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_7",
          "label": "para",
          "text": "We can treat RTE as a classification task, in which we try to predict the True/False label\nfor each pair. Although it seems likely that successful approaches to this task will in-\nvolve a combination of parsing, semantics, and real-world knowledge, many early at-\ntempts at RTE achieved reasonably good results with shallow analysis, based on sim-\nilarity between the text and hypothesis at the word level. In the ideal case, we would\nexpect that if there is an entailment, then all the information expressed by the hypoth-\nesis should also be present in the text. Conversely, if there is information found in the\nhypothesis that is absent from the text, then there will be no entailment.",
          "level": -1,
          "page": 258,
          "reading_order": 7,
          "bbox": [
            97,
            321,
            585,
            451
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_8",
          "label": "para",
          "text": "In our RTE feature detector ( Example 6-7 ), we let words (i.e., word types) serve as\nproxies for information, and our features count the degree of word overlap, and the\ndegree to which there are words in the hypothesis but not in the text (captured by the\nmethod hyp_extra() ). Not all words are equally important—named entity mentions,\nsuch as the names of people, organizations, and places, are likely to be more significant,\nwhich motivates us to extract distinct information for words and nes (named entities).\nIn addition, some high-frequency function words are filtered out as “ stopwords. ”",
          "level": -1,
          "page": 258,
          "reading_order": 8,
          "bbox": [
            97,
            456,
            585,
            574
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_9",
          "label": "para",
          "text": "Example 6-7. “Recognizing Text Entailment” feature extractor: The RTEFeatureExtractor class\nbuilds a bag of words for both the text and the hypothesis after throwing away some stopwords, then\ncalculates overlap and difference.",
          "level": -1,
          "page": 258,
          "reading_order": 9,
          "bbox": [
            97,
            582,
            584,
            629
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_11",
          "label": "para",
          "text": "To illustrate the content of these features, we examine some attributes of the text/\nhypothesis Pair 34 shown earlier:",
          "level": -1,
          "page": 258,
          "reading_order": 11,
          "bbox": [
            100,
            752,
            584,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_258_order_12",
          "label": "foot",
          "text": "236 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 258,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "236",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_259_order_1",
          "label": "para",
          "text": "These features indicate that all important words in the hypothesis are contained in the\ntext, and thus there is some evidence for labeling this as True.",
          "level": -1,
          "page": 259,
          "reading_order": 1,
          "bbox": [
            97,
            250,
            585,
            283
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_259_order_2",
          "label": "para",
          "text": "The module nltk.classify.rte_classify reaches just over 58% accuracy on the com-\nbined RTE test data using methods like these. Although this figure is not very\nimpressive, it requires significant effort, and more linguistic processing, to achieve\nmuch better results.",
          "level": -1,
          "page": 259,
          "reading_order": 2,
          "bbox": [
            100,
            292,
            585,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_261_order_2",
      "label": "sec",
      "text": "Accuracy",
      "level": 1,
      "page": 261,
      "reading_order": 2,
      "bbox": [
        97,
        152,
        155,
        179
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_261_order_3",
          "label": "para",
          "text": "The simplest metric that can be used to evaluate a classifier, accuracy, measures the\npercentage of inputs in the test set that the classifier correctly labeled. For example, a\nname gender classifier that predicts the correct name 60 times in a test set containing\n80 names would have an accuracy of 60/80 = 75%. The function nltk.classify.accu\nracy() will calculate the accuracy of a classifier model on a given test set:",
          "level": -1,
          "page": 261,
          "reading_order": 3,
          "bbox": [
            97,
            179,
            585,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_261_order_5",
          "label": "para",
          "text": "When interpreting the accuracy score of a classifier, it is important to consider the\nfrequencies of the individual class labels in the test set. For example, consider a classifier\nthat determines the correct word sense for each occurrence of the word bank . If we\nevaluate this classifier on financial newswire text, then we may find that the financial-\ninstitution sense appears 19 times out of 20. In that case, an accuracy of 95 % would\nhardly be impressive, since we could achieve that accuracy with a model that always\nreturns the financial-institution sense. However, if we instead evaluate the classifier\non a more balanced corpus, where the most frequent word sense has a frequency of\n40 % , then a 95 % accuracy score would be a much more positive result. (A similar issue\narises when measuring inter-annotator agreement in Section 11.2 .)",
          "level": -1,
          "page": 261,
          "reading_order": 5,
          "bbox": [
            97,
            320,
            586,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_261_order_6",
      "label": "sec",
      "text": "Precision and Recall",
      "level": 1,
      "page": 261,
      "reading_order": 6,
      "bbox": [
        100,
        499,
        234,
        519
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_261_order_7",
          "label": "para",
          "text": "Another instance where accuracy scores can be misleading is in “ search ” tasks, such as\ninformation retrieval, where we are attempting to find documents that are relevant to\na particular task. Since the number of irrelevant documents far outweighs the number\nof relevant documents, the accuracy score for a model that labels every document as\nirrelevant would be very close to 100 % .",
          "level": -1,
          "page": 261,
          "reading_order": 7,
          "bbox": [
            97,
            526,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_261_order_8",
          "label": "para",
          "text": "It is therefore conventional to employ a different set of measures for search tasks, based\non the number of items in each of the four categories shown in Figure 6-3:",
          "level": -1,
          "page": 261,
          "reading_order": 8,
          "bbox": [
            97,
            616,
            583,
            646
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_261_order_9",
          "label": "list_group",
          "text": "• True positives are relevant items that we correctly identified as relevant.\n• True negatives are irrelevant items that we correctly identified as irrelevant.",
          "level": -1,
          "page": 261,
          "reading_order": 9,
          "bbox": [
            106,
            654,
            539,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "• True positives are relevant items that we correctly identified as relevant.",
              "bbox": [
                106,
                654,
                539,
                672
              ],
              "page": 261,
              "reading_order": 9
            },
            {
              "label": "list",
              "text": "• True negatives are irrelevant items that we correctly identified as irrelevant.",
              "bbox": [
                106,
                678,
                559,
                692
              ],
              "page": 261,
              "reading_order": 10
            },
            {
              "label": "list",
              "text": "• False positives (or Type I errors ) are irrelevant items that we incorrectly identi-\nfied as relevant.",
              "bbox": [
                106,
                698,
                584,
                726
              ],
              "page": 261,
              "reading_order": 11
            },
            {
              "label": "list",
              "text": "• False negatives (or Type II errors ) are relevant items that we incorrectly identi-\nfied as irrelevant.",
              "bbox": [
                106,
                734,
                584,
                763
              ],
              "page": 261,
              "reading_order": 12
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_261_order_13",
          "label": "foot",
          "text": "6.3 Evaluation | 239",
          "level": -1,
          "page": 261,
          "reading_order": 13,
          "bbox": [
            492,
            824,
            585,
            842
          ],
          "section_number": "6.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_262_order_0",
          "label": "figure",
          "text": "Figure 6-3. True and false positives and negatives. [IMAGE: ![Figure](figures/NLTK_page_262_figure_000.png)]",
          "level": -1,
          "page": 262,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_262_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                331
              ],
              "page": 262,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 6-3. True and false positives and negatives.",
              "bbox": [
                97,
                331,
                342,
                349
              ],
              "page": 262,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_262_order_2",
          "label": "para",
          "text": "Given these four numbers, we can define the following metrics:",
          "level": -1,
          "page": 262,
          "reading_order": 2,
          "bbox": [
            97,
            358,
            458,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_262_order_3",
          "label": "list_group",
          "text": "• Precision, which indicates how many of the items that we identified were relevant,\nis TP/(TP+FP).\n• Recall , which indicates how many of the relevant items that we identified, is\nTP/(TP+FN) .",
          "level": -1,
          "page": 262,
          "reading_order": 3,
          "bbox": [
            100,
            376,
            584,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "• Precision, which indicates how many of the items that we identified were relevant,\nis TP/(TP+FP).",
              "bbox": [
                100,
                376,
                584,
                412
              ],
              "page": 262,
              "reading_order": 3
            },
            {
              "label": "list",
              "text": "• Recall , which indicates how many of the relevant items that we identified, is\nTP/(TP+FN) .",
              "bbox": [
                100,
                421,
                585,
                449
              ],
              "page": 262,
              "reading_order": 4
            },
            {
              "label": "list",
              "text": "• The F-Measure (or F-Score ), which combines the precision and recall to give a\nsingle score, is defined to be the harmonic mean of the precision and recall\n(2 × Precision × Recall )/(Precision+Recall ).",
              "bbox": [
                106,
                456,
                584,
                503
              ],
              "page": 262,
              "reading_order": 5
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_262_order_6",
      "label": "sec",
      "text": "Confusion Matrices",
      "level": 1,
      "page": 262,
      "reading_order": 6,
      "bbox": [
        97,
        519,
        225,
        537
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_262_order_7",
          "label": "para",
          "text": "When performing classification tasks with three or more labels, it can be informative\nto subdivide the errors made by the model based on which types of mistake it made. A\nconfusion matrix is a table where each cell $[i,j]$ indicates how often label $j$ was pre-\ndicted when the correct label was $i$ . Thus, the diagonal entries (i.e., cells $[i,j]$ ) indicate\nlabels that were correctly predicted, and the off-diagonal entries indicate errors. In the\nfollowing example, we generate a confusion matrix for the unigram tagger developed\nin Section 5.4 :",
          "level": -1,
          "page": 262,
          "reading_order": 7,
          "bbox": [
            97,
            546,
            585,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_262_order_8",
          "label": "foot",
          "text": "240 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 262,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "240",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_263_order_2",
          "label": "para",
          "text": "The confusion matrix indicates that common errors include a substitution of NN for\nJJ (for 1.6% of words), and of NN for NNS (for 1.5% of words). Note that periods (.)\nindicate cells whose value is 0, and that the diagonal entries—which correspond to\ncorrect classifications—are marked with angle brackets.",
          "level": -1,
          "page": 263,
          "reading_order": 2,
          "bbox": [
            97,
            367,
            585,
            433
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_263_order_3",
      "label": "sec",
      "text": "Cross-Validation",
      "level": 1,
      "page": 263,
      "reading_order": 3,
      "bbox": [
        97,
        448,
        207,
        465
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_264_order_2",
          "label": "sub_sec",
          "text": "6.4 Decision Trees",
          "level": 2,
          "page": 264,
          "reading_order": 2,
          "bbox": [
            97,
            340,
            243,
            367
          ],
          "section_number": "6.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_264_order_3",
              "label": "para",
              "text": "In the next three sections, we'll take a closer look at three machine learning methods\nthat can be used to automatically build classification models: decision trees, naive Bayes\nclassifiers, and Maximum Entropy classifiers. As we've seen, it's possible to treat these\nlearning methods as black boxes, simply training models and using them for prediction\nwithout understanding how they work. But there's a lot to be learned from taking a\ncloser look at how these learning methods select models based on the data in a training\nset. An understanding of these methods can help guide our selection of appropriate\nfeatures, and especially our decisions about how those features should be encoded.\nAnd an understanding of the generated models can allow us to extract information\nabout which features are most informative, and how those features relate to one an-\nother.",
              "level": -1,
              "page": 264,
              "reading_order": 3,
              "bbox": [
                97,
                376,
                585,
                555
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_264_order_4",
              "label": "para",
              "text": "A decision tree is a simple flowchart that selects labels for input values. This flowchart\nconsists of decision nodes, which check feature values, and leaf nodes, which assign\nlabels. To choose the label for an input value, we begin at the flowchart's initial decision\nnode, known as its root node . This node contains a condition that checks one of the\ninput value's features, and selects a branch based on that feature's value. Following the\nbranch that describes our input value, we arrive at a new decision node, with a new\ncondition on the input value's features. We continue following the branch selected by\neach node's condition, until we arrive at a leaf node which provides a label for the input\nvalue. Figure 6-4 shows an example decision tree model for the name gender task.",
              "level": -1,
              "page": 264,
              "reading_order": 4,
              "bbox": [
                97,
                564,
                585,
                716
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_264_order_5",
              "label": "para",
              "text": "Once we have a decision tree, it is straightforward to use it to assign labels to new input\nvalues. What’s less straightforward is how we can build a decision tree that models a\ngiven training set. But before we look at the learning algorithm for building decision\ntrees, we’ll consider a simpler task: picking the best “decision stump” for a corpus. A",
              "level": -1,
              "page": 264,
              "reading_order": 5,
              "bbox": [
                97,
                716,
                585,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_264_order_6",
              "label": "foot",
              "text": "242 | Chapter 6: Learning to Classify Text",
              "level": -1,
              "page": 264,
              "reading_order": 6,
              "bbox": [
                97,
                824,
                275,
                842
              ],
              "section_number": "242",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_265_order_0",
              "label": "para",
              "text": "decision stump is a decision tree with a single node that decides how to classify inputs\nbased on a single feature. It contains one leaf for each possible feature value, specifying\nthe class label that should be assigned to inputs whose features have that value. In order\nto build a decision stump, we must first decide which feature should be used. The\nsimplest method is to just build a decision stump for each possible feature, and see\nwhich one achieves the highest accuracy on the training data, although there are other\nalternatives that we will discuss later. Once we’ve picked a feature, we can build the\ndecision stump by assigning a label to each leaf based on the most frequent label for\nthe selected examples in the training set (i.e., the examples where the selected feature\nhas that value).",
              "level": -1,
              "page": 265,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                236
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_265_order_1",
              "label": "para",
              "text": "Given the algorithm for choosing decision stumps, the algorithm for growing larger\ndecision trees is straightforward. We begin by selecting the overall best decision stump\nfor the classification task. We then check the accuracy of each of the leaves on the\ntraining set. Leaves that do not achieve sufficient accuracy are then replaced by new\ndecision stumps, trained on the subset of the training corpus that is selected by the path\nto the leaf. For example, we could grow the decision tree in Figure 6 - 4 by replacing the\nleftmost leaf with a new decision stump, trained on the subset of the training set names\nthat do not start with a $k$ or end with a vowel or an $l$ .",
              "level": -1,
              "page": 265,
              "reading_order": 1,
              "bbox": [
                97,
                241,
                585,
                376
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_263_order_4",
          "label": "para",
          "text": "In order to evaluate our models, we must reserve a portion of the annotated data for\nthe test set. As we already mentioned, if the test set is too small, our evaluation may\nnot be accurate. However, making the test set larger usually means making the training\nset smaller, which can have a significant impact on performance if a limited amount of\nannotated data is available.",
          "level": -1,
          "page": 263,
          "reading_order": 4,
          "bbox": [
            97,
            474,
            586,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_263_order_5",
          "label": "para",
          "text": "One solution to this problem is to perform multiple evaluations on different test sets,\nthen to combine the scores from those evaluations, a technique known as cross-\nvalidation . In particular, we subdivide the original corpus into N subsets called\nfolds . For each of these folds, we train a model using all of the data except the data in\nthat fold, and then test that model on the fold. Even though the individual folds might\nbe too small to give accurate evaluation scores on their own, the combined evaluation\nscore is based on a large amount of data and is therefore quite reliable.",
          "level": -1,
          "page": 263,
          "reading_order": 5,
          "bbox": [
            97,
            564,
            585,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_263_order_6",
          "label": "para",
          "text": "A second, and equally important, advantage of using cross-validation is that it allows\nus to examine how widely the performance varies across different training sets. If we\nget very similar scores for all N training sets, then we can be fairly confident that the\nscore is accurate. On the other hand, if scores vary widely across the N training sets,\nthen we should probably be skeptical about the accuracy of the evaluation score.",
          "level": -1,
          "page": 263,
          "reading_order": 6,
          "bbox": [
            97,
            689,
            585,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_263_order_7",
          "label": "foot",
          "text": "6.3 Evaluation | 241",
          "level": -1,
          "page": 263,
          "reading_order": 7,
          "bbox": [
            492,
            824,
            584,
            842
          ],
          "section_number": "6.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_264_order_0",
          "label": "figure",
          "text": "Figure 6-4. Decision Tree model for the name gender task. Note that tree diagrams are conventionally\ndrawn “ upside down,\n” with the root at the top, and the leaves at the bottom. [IMAGE: ![Figure](figures/NLTK_page_264_figure_000.png)]",
          "level": -1,
          "page": 264,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_264_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                295
              ],
              "page": 264,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 6-4. Decision Tree model for the name gender task. Note that tree diagrams are conventionally\ndrawn “ upside down,\n” with the root at the top, and the leaves at the bottom.",
              "bbox": [
                97,
                304,
                583,
                332
              ],
              "page": 264,
              "reading_order": 1
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_265_order_2",
      "label": "sec",
      "text": "Entropy and Information Gain",
      "level": 1,
      "page": 265,
      "reading_order": 2,
      "bbox": [
        100,
        393,
        297,
        412
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_267_order_6",
          "label": "sub_sec",
          "text": "6.5 Naive Bayes Classifiers",
          "level": 2,
          "page": 267,
          "reading_order": 6,
          "bbox": [
            97,
            725,
            309,
            753
          ],
          "section_number": "6.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_267_order_7",
              "label": "para",
              "text": "In naive Bayes classifiers, every feature gets a say in determining which label should\nbe assigned to a given input value. To choose a label for an input value, the naive Bayes",
              "level": -1,
              "page": 267,
              "reading_order": 7,
              "bbox": [
                97,
                761,
                585,
                792
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_267_order_8",
              "label": "foot",
              "text": "6.5 Naive Bayes Classifiers | 245",
              "level": -1,
              "page": 267,
              "reading_order": 8,
              "bbox": [
                440,
                824,
                585,
                842
              ],
              "section_number": "6.5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_268_order_0",
              "label": "para",
              "text": "classifier begins by calculating the prior probability of each label, which is determined\nby checking the frequency of each label in the training set. The contribution from each\nfeature is then combined with this prior probability, to arrive at a likelihood estimate\nfor each label. The label whose likelihood estimate is the highest is then assigned to the\ninput value. Figure 6-6 illustrates this process.",
              "level": -1,
              "page": 268,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                161
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_268_order_1",
              "label": "figure",
              "text": "Figure 6-6. An abstract illustration of the procedure used by the naive Bayes classifier to choose the\ntopic for a document. In the training corpus, most documents are automotive, so the classifier starts\nout at a point closer to the “automotive” label. But it then considers the effect of each feature. In this\nexample, the input document contains the word dark, which is a weak indicator for murder mysteries,\nbut it also contains the word football, which is a strong indicator for sports documents. After every\nfeature has made its contribution, the classifier checks which label it is closest to, and assigns that\nlabel to the input. [IMAGE: ![Figure](figures/NLTK_page_268_figure_001.png)]",
              "level": -1,
              "page": 268,
              "reading_order": 1,
              "bbox": [
                100,
                170,
                583,
                394
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_268_figure_001.png)",
                  "bbox": [
                    100,
                    170,
                    583,
                    394
                  ],
                  "page": 268,
                  "reading_order": 1
                },
                {
                  "label": "cap",
                  "text": "Figure 6-6. An abstract illustration of the procedure used by the naive Bayes classifier to choose the\ntopic for a document. In the training corpus, most documents are automotive, so the classifier starts\nout at a point closer to the “automotive” label. But it then considers the effect of each feature. In this\nexample, the input document contains the word dark, which is a weak indicator for murder mysteries,\nbut it also contains the word football, which is a strong indicator for sports documents. After every\nfeature has made its contribution, the classifier checks which label it is closest to, and assigns that\nlabel to the input.",
                  "bbox": [
                    96,
                    402,
                    585,
                    503
                  ],
                  "page": 268,
                  "reading_order": 2
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_268_order_3",
              "label": "para",
              "text": "Individual features make their contribution to the overall decision by “voting against”\nlabels that don’t occur with that feature very often. In particular, the likelihood score\nfor each label is reduced by multiplying it by the probability that an input value with\nthat label would have the feature. For example, if the word run occurs in 12 % of the\nsports documents, 10 % of the murder mystery documents, and 2 % of the automotive\ndocuments, then the likelihood score for the sports label will be multiplied by 0.12, the\nlikelihood score for the murder mystery label will be multiplied by 0.1, and the likeli-\nhood score for the automotive label will be multiplied by 0.02. The overall effect will\nbe to reduce the score of the murder mystery label slightly more than the score of the\nsports label, and to significantly reduce the automotive label with respect to the other\ntwo labels. This process is illustrated in Figures 6 - 7 and 6 - 8 .",
              "level": -1,
              "page": 268,
              "reading_order": 3,
              "bbox": [
                97,
                527,
                585,
                707
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_268_order_4",
              "label": "foot",
              "text": "246 | Chapter 6: Learning to Classify Text",
              "level": -1,
              "page": 268,
              "reading_order": 4,
              "bbox": [
                97,
                824,
                275,
                842
              ],
              "section_number": "246",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_269_order_0",
              "label": "figure",
              "text": "Figure 6-7. Calculating label likelihoods with naive Bayes. Naive Bayes begins by calculating the prior\nprobability of each label, based on how frequently each label occurs in the training data. Every feature\nthen contributes to the likelihood estimate for each label, by multiplying it by the probability that\ninput values with that label will have that feature. The resulting likelihood score can be thought of as\nan estimate of the probability that a randomly selected value from the training set would have both\nthe given label and the set of features, assuming that the feature probabilities are all independent. [IMAGE: ![Figure](figures/NLTK_page_269_figure_000.png)]",
              "level": -1,
              "page": 269,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                583,
                295
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_269_figure_000.png)",
                  "bbox": [
                    100,
                    71,
                    583,
                    295
                  ],
                  "page": 269,
                  "reading_order": 0
                },
                {
                  "label": "cap",
                  "text": "Figure 6-7. Calculating label likelihoods with naive Bayes. Naive Bayes begins by calculating the prior\nprobability of each label, based on how frequently each label occurs in the training data. Every feature\nthen contributes to the likelihood estimate for each label, by multiplying it by the probability that\ninput values with that label will have that feature. The resulting likelihood score can be thought of as\nan estimate of the probability that a randomly selected value from the training set would have both\nthe given label and the set of features, assuming that the feature probabilities are all independent.",
                  "bbox": [
                    97,
                    303,
                    585,
                    389
                  ],
                  "page": 269,
                  "reading_order": 1
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_269_order_2",
              "label": "figure",
              "text": "Figure 6-8. A Bayesian Network Graph illustrating the generative process that is assumed by the naive\nBayes classifier. To generate a labeled input, the model first chooses a label for the input, and then it\ngenerates each of the input's features based on that label. Every feature is assumed to be entirely\nindependent of every other feature, given the label. [IMAGE: ![Figure](figures/NLTK_page_269_figure_002.png)]",
              "level": -1,
              "page": 269,
              "reading_order": 2,
              "bbox": [
                100,
                412,
                583,
                564
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_269_figure_002.png)",
                  "bbox": [
                    100,
                    412,
                    583,
                    564
                  ],
                  "page": 269,
                  "reading_order": 2
                },
                {
                  "label": "cap",
                  "text": "Figure 6-8. A Bayesian Network Graph illustrating the generative process that is assumed by the naive\nBayes classifier. To generate a labeled input, the model first chooses a label for the input, and then it\ngenerates each of the input's features based on that label. Every feature is assumed to be entirely\nindependent of every other feature, given the label.",
                  "bbox": [
                    96,
                    564,
                    585,
                    627
                  ],
                  "page": 269,
                  "reading_order": 3
                }
              ],
              "is_merged": true
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_265_order_3",
          "label": "para",
          "text": "As was mentioned before, there are several methods for identifying the most informa-\ntive feature for a decision stump. One popular alternative, called information gain ,\nmeasures how much more organized the input values become when we divide them up\nusing a given feature. To measure how disorganized the original set of input values are,\nwe calculate entropy of their labels, which will be high if the input values have highly\nvaried labels, and low if many input values all have the same label. In particular, entropy\nis defined as the sum of the probability of each label times the log probability of that\nsame label:",
          "level": -1,
          "page": 265,
          "reading_order": 3,
          "bbox": [
            97,
            421,
            585,
            548
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_265_order_4",
          "label": "para",
          "text": "(1) $H=\\Sigma_{l \\in \\text { labels }} P(l) \\times \\log _{2} P(l)$.",
          "level": -1,
          "page": 265,
          "reading_order": 4,
          "bbox": [
            118,
            564,
            297,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_265_order_5",
          "label": "para",
          "text": "For example, Figure 6-5 shows how the entropy of labels in the name gender prediction\ntask depends on the ratio of male to female names. Note that if most input values have\nthe same label (e.g., if P (male) is near 0 or near 1), then entropy is low. In particular,\nlabels that have low frequency do not contribute much to the entropy (since P ( l ) is\nsmall), and labels with high frequency also do not contribute much to the entropy (since\nlog 2 P ( l ) is small). On the other hand, if the input values have a wide variety of labels,\nthen there are many labels with a “ medium ” frequency, where neither P ( l ) nor\nlog 2 P ( l ) is small, so the entropy is high. Example 6-8 demonstrates how to calculate\nthe entropy of a list of labels.",
          "level": -1,
          "page": 265,
          "reading_order": 5,
          "bbox": [
            97,
            591,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_265_order_6",
          "label": "foot",
          "text": "6.4 Decision Trees | 243",
          "level": -1,
          "page": 265,
          "reading_order": 6,
          "bbox": [
            476,
            824,
            584,
            842
          ],
          "section_number": "6.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_266_order_0",
          "label": "figure",
          "text": "Figure 6-5. The entropy of labels in the name gender prediction task, as a function of the percentage\nof names in a given set that are male. [IMAGE: ![Figure](figures/NLTK_page_266_figure_000.png)]",
          "level": -1,
          "page": 266,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_266_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                286
              ],
              "page": 266,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 6-5. The entropy of labels in the name gender prediction task, as a function of the percentage\nof names in a given set that are male.",
              "bbox": [
                97,
                292,
                583,
                322
              ],
              "page": 266,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_266_order_2",
          "label": "para",
          "text": "Example 6-8. Calculating the entropy of a list of labels.",
          "level": -1,
          "page": 266,
          "reading_order": 2,
          "bbox": [
            97,
            347,
            368,
            360
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_266_order_5",
          "label": "para",
          "text": "Once we have calculated the entropy of the labels of the original set of input values, we\ncan determine how much more organized the labels become once we apply the decision\nstump. To do so, we calculate the entropy for each of the decision stump’s leaves, and\ntake the average of those leaf entropy values (weighted by the number of samples in\neach leaf). The information gain is then equal to the original entropy minus this new,\nreduced entropy. The higher the information gain, the better job the decision stump\ndoes of dividing the input values into coherent groups, so we can build decision trees\nby selecting the decision stumps with the highest information gain.",
          "level": -1,
          "page": 266,
          "reading_order": 5,
          "bbox": [
            97,
            609,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_266_order_6",
          "label": "para",
          "text": "Another consideration for decision trees is efficiency. The simple algorithm for selecting\ndecision stumps described earlier must construct a candidate decision stump for every\npossible feature, and this process must be repeated for every node in the constructed",
          "level": -1,
          "page": 266,
          "reading_order": 6,
          "bbox": [
            97,
            751,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_266_order_7",
          "label": "foot",
          "text": "244 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 266,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "244",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_267_order_0",
          "label": "para",
          "text": "decision tree. A number of algorithms have been developed to cut down on the training\ntime by storing and reusing information about previously evaluated examples.",
          "level": -1,
          "page": 267,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_267_order_1",
          "label": "para",
          "text": "Decision trees have a number of useful qualities. To begin with, they're simple to un-\nderstand, and easy to interpret. This is especially true near the top of the decision tree,\nwhere it is usually possible for the learning algorithm to find very useful features. De-\ncision trees are especially well suited to cases where many hierarchical categorical dis-\ntinctions can be made. For example, decision trees can be very effective at capturing\nphylogeny trees.",
          "level": -1,
          "page": 267,
          "reading_order": 1,
          "bbox": [
            97,
            115,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_267_order_2",
          "label": "para",
          "text": "However, decision trees also have a few disadvantages. One problem is that, since each\nbranch in the decision tree splits the training data, the amount of training data available\nto train nodes lower in the tree can become quite small. As a result, these lower decision\nnodes may overfit the training set, learning patterns that reflect idiosyncrasies of the\ntraining set rather than linguistically significant patterns in the underlying problem.\nOne solution to this problem is to stop dividing nodes once the amount of training data\nbecomes too small. Another solution is to grow a full decision tree, but then to\nprune decision nodes that do not improve performance on a dev-test.",
          "level": -1,
          "page": 267,
          "reading_order": 2,
          "bbox": [
            97,
            221,
            585,
            351
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_267_order_3",
          "label": "para",
          "text": "A second problem with decision trees is that they force features to be checked in a\nspecific order, even when features may act relatively independently of one another. For\nexample, when classifying documents into topics (such as sports, automotive, or mur-\nder mystery), features such as hasword(football) are highly indicative of a specific label,\nregardless of what the other feature values are. Since there is limited space near the top\nof the decision tree, most of these features will need to be repeated on many different\nbranches in the tree. And since the number of branches increases exponentially as we\ngo down the tree, the amount of repetition can be very large.",
          "level": -1,
          "page": 267,
          "reading_order": 3,
          "bbox": [
            97,
            358,
            585,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_267_order_4",
          "label": "para",
          "text": "A related problem is that decision trees are not good at making use of features that are\nweak predictors of the correct label. Since these features make relatively small\nincremental improvements, they tend to occur very low in the decision tree. But by the\ntime the decision tree learner has descended far enough to use these features, there is\nnot enough training data left to reliably determine what effect they should have. If we\ncould instead look at the effect of these features across the entire training set, then we\nmight be able to make some conclusions about how they should affect the choice of\nlabel.",
          "level": -1,
          "page": 267,
          "reading_order": 4,
          "bbox": [
            97,
            500,
            586,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_267_order_5",
          "label": "para",
          "text": "The fact that decision trees require that features be checked in a specific order limits\ntheir ability to exploit features that are relatively independent of one another. The naive\nBayes classification method, which we’ll discuss next, overcomes this limitation by\nallowing all features to act “in parallel.”",
          "level": -1,
          "page": 267,
          "reading_order": 5,
          "bbox": [
            97,
            636,
            585,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_269_order_4",
      "label": "sec",
      "text": "Underlying Probabilistic Mode",
      "level": 1,
      "page": 269,
      "reading_order": 4,
      "bbox": [
        98,
        654,
        298,
        677
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_269_order_5",
          "label": "para",
          "text": "Another way of understanding the naive Bayes classifier is that it chooses the most likely\nlabel for an input, under the assumption that every input value is generated by first\nchoosing a class label for that input value, and then generating each feature, entirely\nindependent of every other feature. Of course, this assumption is unrealistic; features\nare often highly dependent on one another. We’ll return to some of the consequences\nof this assumption at the end of this section. This simplifying assumption, known as\nthe naive Bayes assumption (or independence assumption), makes it much easier",
          "level": -1,
          "page": 269,
          "reading_order": 5,
          "bbox": [
            97,
            680,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_269_order_6",
          "label": "foot",
          "text": "6.5 Naive Bayes Classifiers | 247",
          "level": -1,
          "page": 269,
          "reading_order": 6,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "6.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_0",
          "label": "para",
          "text": "to combine the contributions of the different features, since we don’t need to worry\nabout how they should interact with one another.",
          "level": -1,
          "page": 270,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_1",
          "label": "para",
          "text": "Based on this assumption, we can calculate an expression for P(label|features), the\nprobability that an input will have a particular label given that it has a particular set of\nfeatures. To choose a label for a new input, we can then simply pick the label l that\nmaximizes P(l|features).",
          "level": -1,
          "page": 270,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            586,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_2",
          "label": "para",
          "text": "To begin, we note that P(label|features) is equal to the probability that an input has a\nparticular label and the specified set of features, divided by the probability that it has\nthe specified set of features:",
          "level": -1,
          "page": 270,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            585,
            235
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_3",
          "label": "para",
          "text": "( 2 ) P ( l a b e l | f e a t u r e s ) = P ( f e a t u r e s , \\, l a b e l ) / P ( f e a t u r e s )",
          "level": -1,
          "page": 270,
          "reading_order": 3,
          "bbox": [
            118,
            241,
            413,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_4",
          "label": "para",
          "text": "Next, we note that P(features) will be the same for every choice of label, so if we are\nsimply interested in finding the most likely label, it suffices to calculate P(features,\nlabel), which we'll call the label likelihood.",
          "level": -1,
          "page": 270,
          "reading_order": 4,
          "bbox": [
            97,
            277,
            585,
            324
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_5",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_270_figure_005.png)",
          "level": -1,
          "page": 270,
          "reading_order": 5,
          "bbox": [
            118,
            340,
            171,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_6",
          "label": "para",
          "text": "(3) P(features) = $\\sum_{label \\in labels}P(features,label)$",
          "level": -1,
          "page": 270,
          "reading_order": 6,
          "bbox": [
            118,
            411,
            351,
            424
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_7",
          "label": "para",
          "text": "The label likelihood can be expanded out as the probability of the label times the prob-\nability of the features given the label:",
          "level": -1,
          "page": 270,
          "reading_order": 7,
          "bbox": [
            100,
            448,
            585,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_8",
          "label": "para",
          "text": "(4) P (features, label) = P (label) × P (features|label)",
          "level": -1,
          "page": 270,
          "reading_order": 8,
          "bbox": [
            118,
            483,
            404,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_9",
          "label": "para",
          "text": "Furthermore, since the features are all independent of one another (given the label), we\ncan separate out the probability of each individual feature:",
          "level": -1,
          "page": 270,
          "reading_order": 9,
          "bbox": [
            97,
            519,
            585,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_10",
          "label": "para",
          "text": "(5) P(features, label) $=P(\\text { label }) \\times \\mathrm{n}_{f \\in \\text { features }} P(f \\mid \\text { label })$",
          "level": -1,
          "page": 270,
          "reading_order": 10,
          "bbox": [
            118,
            564,
            413,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_11",
          "label": "para",
          "text": "This is exactly the equation we discussed earlier for calculating the label likelihood:\nP(label) is the prior probability for a given label, and each P(f|label) is the contribution\nof a single feature to the label likelihood.",
          "level": -1,
          "page": 270,
          "reading_order": 11,
          "bbox": [
            97,
            591,
            584,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_270_order_12",
      "label": "sec",
      "text": "Zero Counts and Smoothing",
      "level": 1,
      "page": 270,
      "reading_order": 12,
      "bbox": [
        97,
        654,
        281,
        680
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_270_order_13",
          "label": "para",
          "text": "The simplest way to calculate P(f|label), the contribution of a feature f toward the label\nikelihood for a label label, is to take the percentage of training instances with the given\nabel that also have the given feature:",
          "level": -1,
          "page": 270,
          "reading_order": 13,
          "bbox": [
            100,
            680,
            584,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_14",
          "label": "para",
          "text": "(6) P(f|label) = count(f, label)/count(label)",
          "level": -1,
          "page": 270,
          "reading_order": 14,
          "bbox": [
            118,
            743,
            359,
            763
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_270_order_15",
          "label": "foot",
          "text": "248 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 270,
          "reading_order": 15,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "248",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_271_order_0",
          "label": "para",
          "text": "However, this simple approach can become problematic when a feature never occurs\nwith a given label in the training set. In this case, our calculated value for P(f|label) will\nbe zero, which will cause the label likelihood for the given label to be zero. Thus, the\ninput will never be assigned this label, regardless of how well the other features fit the\nlabel.",
          "level": -1,
          "page": 271,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_271_order_1",
          "label": "para",
          "text": "The basic problem here is with our calculation of P(f|label), the probability that an\ninput will have a feature, given a label. In particular, just because we haven't seen a\nfeature/label combination occur in the training set, doesn't mean it's impossible for\nthat combination to occur. For example, we may not have seen any murder mystery\ndocuments that contained the word football , but we wouldn't want to conclude that\nit's completely impossible for such documents to exist.",
          "level": -1,
          "page": 271,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            585,
            262
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_271_order_2",
          "label": "para",
          "text": "Thus, although $count(f,label)/count(label)$ is a good estimate for P( f|label ) when $count(f$ ,\nlabel ) is relatively high, this estimate becomes less reliable when $count(f)$ becomes\nsmaller. Therefore, when building naive Bayes models, we usually employ more so-\nphisticated techniques, known as smoothing techniques, for calculating P( f|label ), the\nprobability of a feature given a label. For example, the Expected Likelihood Estima-\ntion for the probability of a feature given a label basically adds 0.5 to each\n$count(f,label)$ value, and the Heldout Estimation uses a heldout corpus to calculate\nthe relationship between feature frequencies and feature probabilities. The nltk.prob\nability module provides support for a wide variety of smoothing techniques.",
          "level": -1,
          "page": 271,
          "reading_order": 2,
          "bbox": [
            97,
            268,
            585,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_271_order_3",
      "label": "sec",
      "text": "Non-Binary Features",
      "level": 1,
      "page": 271,
      "reading_order": 3,
      "bbox": [
        98,
        430,
        234,
        452
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_271_order_4",
          "label": "para",
          "text": "We have assumed here that each feature is binary, i.e., that each input either has a\nfeature or does not. Label-valued features (e.g., a color feature, which could be red,\ngreen, blue, white, or orange) can be converted to binary features by replacing them\nwith binary features, such as “color-is-red”. Numeric features can be converted to bi-\nnary features by binning, which replaces them with features such as “4<x<6.”",
          "level": -1,
          "page": 271,
          "reading_order": 4,
          "bbox": [
            96,
            456,
            584,
            541
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_271_order_5",
          "label": "para",
          "text": "Another alternative is to use regression methods to model the probabilities of numeric\nfeatures. For example, if we assume that the height feature has a bell curve distribution,\nthen we could estimate P(height|label) by finding the mean and variance of the heights\nof the inputs with each label. In this case, P(f=v|label) would not be a fixed value, but\nwould vary depending on the value of $\\nu$ .",
          "level": -1,
          "page": 271,
          "reading_order": 5,
          "bbox": [
            97,
            546,
            585,
            631
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_271_order_6",
      "label": "sec",
      "text": "The Naivete of Independence",
      "level": 1,
      "page": 271,
      "reading_order": 6,
      "bbox": [
        97,
        645,
        297,
        666
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_272_order_3",
          "label": "sub_sub_sec",
          "text": "The Cause of Double-Counting",
          "level": 3,
          "page": 272,
          "reading_order": 3,
          "bbox": [
            97,
            349,
            297,
            369
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_272_order_4",
              "label": "para",
              "text": "The reason for the double-counting problem is that during training, feature contribu-\ntions are computed separately; but when using the classifier to choose labels for new\ninputs, those feature contributions are combined. One solution, therefore, is to con-\nsider the possible interactions between feature contributions during training. We could\nthen use those interactions to adjust the contributions that individual features make.",
              "level": -1,
              "page": 272,
              "reading_order": 4,
              "bbox": [
                97,
                376,
                585,
                456
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_272_order_5",
              "label": "para",
              "text": "To make this more precise, we can rewrite the equation used to calculate the likelihood\nof a label, separating out the contribution made by each feature (or label):",
              "level": -1,
              "page": 272,
              "reading_order": 5,
              "bbox": [
                100,
                465,
                585,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_272_order_6",
              "label": "para",
              "text": "(7) P(features, label) $=w[\\text { label }] \\times \\mathrm{n}_{f \\in \\text { features }} w[f$, label",
              "level": -1,
              "page": 272,
              "reading_order": 6,
              "bbox": [
                118,
                510,
                422,
                528
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_272_order_7",
              "label": "para",
              "text": "Here, w[label] is the “starting score” for a given label, and w[f, label] is the contribution\nmade by a given feature towards a label’s likelihood. We call these values w[label] and\nw[f, label] the parameters or weights for the model. Using the naive Bayes algorithm,\nwe set each of these parameters independently:",
              "level": -1,
              "page": 272,
              "reading_order": 7,
              "bbox": [
                97,
                537,
                585,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_272_order_8",
              "label": "para",
              "text": "(8) w[label]=P(label)",
              "level": -1,
              "page": 272,
              "reading_order": 8,
              "bbox": [
                118,
                618,
                245,
                636
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_272_order_9",
              "label": "para",
              "text": "(9) w[f, label]=P(f|label)",
              "level": -1,
              "page": 272,
              "reading_order": 9,
              "bbox": [
                118,
                645,
                270,
                663
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_272_order_10",
              "label": "para",
              "text": "However, in the next section, we’ll look at a classifier that considers the possible in-\nteractions between these parameters when choosing their values.",
              "level": -1,
              "page": 272,
              "reading_order": 10,
              "bbox": [
                97,
                672,
                584,
                708
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_272_order_11",
          "label": "sub_sec",
          "text": "6.6 Maximum Entropy Classifiers",
          "level": 2,
          "page": 272,
          "reading_order": 11,
          "bbox": [
            97,
            734,
            360,
            761
          ],
          "section_number": "6.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_272_order_12",
              "label": "para",
              "text": "The Maximum Entropy classifier uses a model that is very similar to the model em-\nployed by the naive Bayes classifier. But rather than using probabilities to set the",
              "level": -1,
              "page": 272,
              "reading_order": 12,
              "bbox": [
                100,
                761,
                585,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_272_order_13",
              "label": "foot",
              "text": "250 | Chapter 6: Learning to Classify Text",
              "level": -1,
              "page": 272,
              "reading_order": 13,
              "bbox": [
                97,
                824,
                275,
                842
              ],
              "section_number": "250",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_273_order_0",
              "label": "para",
              "text": "model’s parameters, it uses search techniques to find a set of parameters that will max-\nimize the performance of the classifier. In particular, it looks for the set of parameters\nthat maximizes the total likelihood of the training corpus, which is defined as:",
              "level": -1,
              "page": 273,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_273_order_1",
              "label": "para",
              "text": "(10) P(features) =$\\sum_{x\\in corpus}P(label(x)| features(x)",
              "level": -1,
              "page": 273,
              "reading_order": 1,
              "bbox": [
                109,
                134,
                395,
                152
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_273_order_2",
              "label": "para",
              "text": "Where P(label|features), the probability that an input whose features are features will\nhave class label label, is defined as:",
              "level": -1,
              "page": 273,
              "reading_order": 2,
              "bbox": [
                97,
                161,
                584,
                197
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_273_order_3",
              "label": "para",
              "text": "(11) P(label|features) =P(label, features)/Σlabel P(label, features",
              "level": -1,
              "page": 273,
              "reading_order": 3,
              "bbox": [
                109,
                206,
                471,
                225
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_273_order_4",
              "label": "para",
              "text": "Because of the potentially complex interactions between the effects of related features,\nthere is no way to directly calculate the model parameters that maximize the likelihood\nof the training set. Therefore, Maximum Entropy classifiers choose the model param-\neters using iterative optimization techniques, which initialize the model's parameters\nto random values, and then repeatedly refine those parameters to bring them closer to\nthe optimal solution. These iterative optimization techniques guarantee that each re-\nfinement of the parameters will bring them closer to the optimal values, but do not\nnecessarily provide a means of determining when those optimal values have been\nreached. Because the parameters for Maximum Entropy classifiers are selected using\niterative optimization techniques, they can take a long time to learn. This is especially\ntrue when the size of the training set, the number of features, and the number of labels\nare all large.",
              "level": -1,
              "page": 273,
              "reading_order": 4,
              "bbox": [
                97,
                240,
                585,
                439
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_273_order_5",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_273_figure_005.png)",
              "level": -1,
              "page": 273,
              "reading_order": 5,
              "bbox": [
                109,
                448,
                171,
                519
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_273_order_6",
              "label": "para",
              "text": "Some iterative optimization techniques are much faster than others.\nWhen training Maximum Entropy models, avoid the use of Generalized\nIterative Scaling (GIS) or Improved Iterative Scaling (IIS), which are both\nconsiderably slower than the Conjugate Gradient (CG) and the BFGS\noptimization methods.",
              "level": -1,
              "page": 273,
              "reading_order": 6,
              "bbox": [
                171,
                465,
                530,
                537
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_271_order_7",
          "label": "para",
          "text": "The reason that naive Bayes classifiers are called “naive” is that it’s unreasonable to\nassume that all features are independent of one another (given the label). In particular,\nalmost all real-world problems contain features with varying degrees of dependence on\none another. If we had to avoid any features that were dependent on one another, it\nwould be very difficult to construct good feature sets that provide the required infor-\nmation to the machine learning algorithm.",
          "level": -1,
          "page": 271,
          "reading_order": 7,
          "bbox": [
            97,
            672,
            585,
            771
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_271_order_8",
          "label": "foot",
          "text": "6.5 Naive Bayes Classifiers | 249",
          "level": -1,
          "page": 271,
          "reading_order": 8,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "6.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_272_order_0",
          "label": "para",
          "text": "So what happens when we ignore the independence assumption, and use the naive\nBayes classifier with features that are not independent? One problem that arises is that\nthe classifier can end up “ double-counting” the effect of highly correlated features,\npushing the classifier closer to a given label than is justified.",
          "level": -1,
          "page": 272,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_272_order_1",
          "label": "para",
          "text": "To see how this can occur, consider a name gender classifier that contains two identical\nfeatures, $f_1$ and $f_2$ . In other words, $f_2$ is an exact copy of $f_1$ , and contains no new infor-\nmation. When the classifier is considering an input, it will include the contribution of\nboth $f_1$ and $f_2$ when deciding which label to choose. Thus, the information content of\nthese two features will be given more weight than it deserves.",
          "level": -1,
          "page": 272,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            586,
            228
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_272_order_2",
          "label": "para",
          "text": "Of course, we don’t usually build naive Bayes classifiers that contain two identical\nfeatures. However, we do build classifiers that contain features which are dependent\non one another. For example, the features ends-with(a) and ends-with(vowel) are de-\npendent on one another, because if an input value has the first feature, then it must\nalso have the second feature. For features like these, the duplicated information may\nbe given more weight than is justified by the training set.",
          "level": -1,
          "page": 272,
          "reading_order": 2,
          "bbox": [
            97,
            232,
            585,
            335
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_273_order_7",
      "label": "sec",
      "text": "The Maximum Entropy Model",
      "level": 1,
      "page": 273,
      "reading_order": 7,
      "bbox": [
        97,
        555,
        297,
        582
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_273_order_8",
          "label": "para",
          "text": "The Maximum Entropy classifier model is a generalization of the model used by the\nnaive Bayes classifier. Like the naive Bayes model, the Maximum Entropy classifier\ncalculates the likelihood of each label for a given input value by multiplying together\nthe parameters that are applicable for the input value and label. The naive Bayes clas-\nsifier model defines a parameter for each label, specifying its prior probability, and a\nparameter for each (feature, label) pair, specifying the contribution of individual fea-\ntures toward a label's likelihood.",
          "level": -1,
          "page": 273,
          "reading_order": 8,
          "bbox": [
            97,
            582,
            585,
            699
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_273_order_9",
          "label": "para",
          "text": "In contrast, the Maximum Entropy classifier model leaves it up to the user to decide\nwhat combinations of labels and features should receive their own parameters. In par-\nticular, it is possible to use a single parameter to associate a feature with more than one\nlabel; or to associate more than one feature with a given label. This will sometimes",
          "level": -1,
          "page": 273,
          "reading_order": 9,
          "bbox": [
            97,
            707,
            585,
            775
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_273_order_10",
          "label": "foot",
          "text": "6.6 Maximum Entropy Classifiers | 251",
          "level": -1,
          "page": 273,
          "reading_order": 10,
          "bbox": [
            413,
            824,
            584,
            842
          ],
          "section_number": "6.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_0",
          "label": "para",
          "text": "allow the model to “generalize” over some of the differences between related labels or\nfeatures.",
          "level": -1,
          "page": 274,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_1",
          "label": "para",
          "text": "Each combination of labels and features that receives its own parameter is called a\njoint-feature . Note that joint-features are properties of labeled values, whereas (sim-\nple) features are properties of unlabeled values.",
          "level": -1,
          "page": 274,
          "reading_order": 1,
          "bbox": [
            96,
            115,
            584,
            162
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_2",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_274_figure_002.png)",
          "level": -1,
          "page": 274,
          "reading_order": 2,
          "bbox": [
            109,
            179,
            171,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_3",
          "label": "para",
          "text": "In literature that describes and discusses Maximum Entropy models,\nthe term “features” often refers to joint-features; the term “contexts”\nrefers to what we have been calling (simple) features.",
          "level": -1,
          "page": 274,
          "reading_order": 3,
          "bbox": [
            171,
            188,
            530,
            233
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_4",
          "label": "para",
          "text": "Typically, the joint-features that are used to construct Maximum Entropy models ex-\ntactly mirror those that are used by the naive Bayes model. In particular, a joint-feature\nis defined for each label, corresponding to w[label] , and for each combination of (sim-\nple) feature and label, corresponding to w[f, label] . Given the joint-features for a Max-\nimum Entropy model, the score assigned to a label for a given input is simply the\nproduct of the parameters associated with the joint-features that apply to that input\nand label:",
          "level": -1,
          "page": 274,
          "reading_order": 4,
          "bbox": [
            100,
            259,
            585,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_5",
          "label": "para",
          "text": "(12) P $($ input, label $)=\\mathrm{n}_{\\text {joint-features }(\\text { input,label })} w[$ joint-feature $]$",
          "level": -1,
          "page": 274,
          "reading_order": 5,
          "bbox": [
            109,
            385,
            440,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_274_order_6",
      "label": "sec",
      "text": "Maximizing Entropy",
      "level": 1,
      "page": 274,
      "reading_order": 6,
      "bbox": [
        97,
        421,
        234,
        441
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_276_order_0",
          "label": "sub_sub_sec",
          "text": "Generative Versus Conditional Classifiers",
          "level": 3,
          "page": 276,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            368,
            91
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_276_order_1",
              "label": "para",
              "text": "An important difference between the naive Bayes classifier and the Maximum Entropy\nclassifier concerns the types of questions they can be used to answer. The naive Bayes\nclassifier is an example of a generative classifier, which builds a model that predicts\nP(input, label), the joint probability of an (input, label) pair. As a result, generative\nmodels can be used to answer the following questions:",
              "level": -1,
              "page": 276,
              "reading_order": 1,
              "bbox": [
                97,
                98,
                585,
                188
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_276_order_2",
              "label": "list_group",
              "text": "1. What is the most likely label for a given input?\n2. How likely is a given label for a given input?",
              "level": -1,
              "page": 276,
              "reading_order": 2,
              "bbox": [
                100,
                188,
                387,
                208
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "1. What is the most likely label for a given input?",
                  "bbox": [
                    100,
                    188,
                    387,
                    208
                  ],
                  "page": 276,
                  "reading_order": 2
                },
                {
                  "label": "list",
                  "text": "2. How likely is a given label for a given input?",
                  "bbox": [
                    100,
                    215,
                    377,
                    232
                  ],
                  "page": 276,
                  "reading_order": 3
                },
                {
                  "label": "list",
                  "text": "3. What is the most likely input value?",
                  "bbox": [
                    100,
                    232,
                    327,
                    250
                  ],
                  "page": 276,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "4. How likely is a given input value?",
                  "bbox": [
                    100,
                    250,
                    315,
                    270
                  ],
                  "page": 276,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "5. How likely is a given input value with a given label?",
                  "bbox": [
                    100,
                    277,
                    416,
                    295
                  ],
                  "page": 276,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "6. What is the most likely label for an input that might have one of two values (but\nwe don't know which)?",
                  "bbox": [
                    100,
                    295,
                    585,
                    325
                  ],
                  "page": 276,
                  "reading_order": 7
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_276_order_8",
              "label": "para",
              "text": "The Maximum Entropy classifier, on the other hand, is an example of a conditional\nclassifier. Conditional classifiers build models that predict P(label|input)—the proba-\nbility of a label given the input value. Thus, conditional models can still be used to\nanswer questions 1 and 2. However, conditional models cannot be used to answer the\nremaining questions 3–6.",
              "level": -1,
              "page": 276,
              "reading_order": 8,
              "bbox": [
                97,
                331,
                585,
                421
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_276_order_9",
              "label": "para",
              "text": "In general, generative models are strictly more powerful than conditional models, since\nwe can calculate the conditional probability $P(label|input)$ from the joint probability\n$P(input,\\ label)$ , but not vice versa. However, this additional power comes at a price.\nBecause the model is more powerful, it has more “ free parameters ” that need to be\nlearned. However, the size of the training set is fixed. Thus, when using a more powerful\nmodel, we end up with less data that can be used to train each parameter's value, making\nit harder to find the best parameter values. As a result, a generative model may not do\nas good a job at answering questions 1 and 2 as a conditional model, since the condi-\ntional model can focus its efforts on those two questions. However, if we do need\nanswers to questions like 3 – 6, then we have no choice but to use a generative model.",
              "level": -1,
              "page": 276,
              "reading_order": 9,
              "bbox": [
                97,
                429,
                585,
                592
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_276_order_10",
              "label": "para",
              "text": "The difference between a generative model and a conditional model is analogous to the\ndifference between a topographical map and a picture of a skyline. Although the topo-\ngraphical map can be used to answer a wider variety of questions, it is significantly\nmore difficult to generate an accurate topographical map than it is to generate an ac-\ncurate skyline.",
              "level": -1,
              "page": 276,
              "reading_order": 10,
              "bbox": [
                97,
                600,
                585,
                682
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_276_order_11",
          "label": "sub_sec",
          "text": "6.7 Modeling Linguistic Patterns",
          "level": 2,
          "page": 276,
          "reading_order": 11,
          "bbox": [
            97,
            707,
            359,
            731
          ],
          "section_number": "6.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_276_order_12",
              "label": "para",
              "text": "Classifiers can help us to understand the linguistic patterns that occur in natural lan-\nguage, by allowing us to create explicit models that capture those patterns. Typically,\nthese models are using supervised classification techniques, but it is also possible to",
              "level": -1,
              "page": 276,
              "reading_order": 12,
              "bbox": [
                97,
                734,
                585,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_276_order_13",
              "label": "foot",
              "text": "254 | Chapter 6: Learning to Classify Text",
              "level": -1,
              "page": 276,
              "reading_order": 13,
              "bbox": [
                97,
                824,
                275,
                842
              ],
              "section_number": "254",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_277_order_0",
              "label": "para",
              "text": "build analytically motivated models. Either way, these explicit models serve two im-\nportant purposes: they help us to understand linguistic patterns, and they can be used\nto make predictions about new language data.",
              "level": -1,
              "page": 277,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_277_order_1",
              "label": "para",
              "text": "The extent to which explicit models can give us insights into linguistic patterns depends\nlargely on what kind of model is used. Some models, such as decision trees, are relatively\ntransparent, and give us direct information about which factors are important in mak-\ning decisions and about which factors are related to one another. Other models, such\nas multilevel neural networks, are much more opaque. Although it can be possible to\ngain insight by studying them, it typically takes a lot more work.",
              "level": -1,
              "page": 277,
              "reading_order": 1,
              "bbox": [
                97,
                132,
                585,
                232
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_277_order_2",
              "label": "para",
              "text": "But all explicit models can make predictions about new unseen language data that was\nnot included in the corpus used to build the model. These predictions can be evaluated\nto assess the accuracy of the model. Once a model is deemed sufficiently accurate, it\ncan then be used to automatically predict information about new language data. These\npredictive models can be combined into systems that perform many useful language\nprocessing tasks, such as document classification, automatic translation, and question\nanswering.",
              "level": -1,
              "page": 277,
              "reading_order": 2,
              "bbox": [
                97,
                232,
                585,
                351
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_274_order_7",
          "label": "para",
          "text": "The intuition that motivates Maximum Entropy classification is that we should build\na model that captures the frequencies of individual joint-features, without making any\nunwarranted assumptions. An example will help to illustrate this principle.",
          "level": -1,
          "page": 274,
          "reading_order": 7,
          "bbox": [
            97,
            448,
            585,
            496
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_8",
          "label": "para",
          "text": "Suppose we are assigned the task of picking the correct word sense for a given word,\nfrom a list of 10 possible senses (labeled A – J). At first, we are not told anything more\nabout the word or the senses. There are many probability distributions that we could\nchoose for the 10 senses, such as:",
          "level": -1,
          "page": 274,
          "reading_order": 8,
          "bbox": [
            97,
            501,
            584,
            566
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_9",
          "label": "tab",
          "text": "<table><tr><td></td><td>A</td><td>B</td><td>C</td><td>D</td><td>E</td><td>F</td><td>G</td><td>H</td><td>I</td><td>J</td></tr><tr><td>(i)</td><td>10%</td><td>10%</td><td>10%</td><td>10%</td><td>10%</td><td>10%</td><td>10%</td><td>10%</td><td>10%</td><td>10%</td></tr><tr><td>(ii)</td><td>5%</td><td>15%</td><td>0%</td><td>30%</td><td>0%</td><td>8%</td><td>12%</td><td>0%</td><td>6%</td><td>24%</td></tr><tr><td>(iii)</td><td>0%</td><td>100%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td></tr></table>",
          "level": -1,
          "page": 274,
          "reading_order": 9,
          "bbox": [
            100,
            582,
            467,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_10",
          "label": "para",
          "text": "Although any of these distributions might be correct, we are likely to choose distribution\n(i), because without any more information, there is no reason to believe that any word\nsense is more likely than any other. On the other hand, distributions (ii) and (iii) reflect\nassumptions that are not supported by what we know.",
          "level": -1,
          "page": 274,
          "reading_order": 10,
          "bbox": [
            97,
            680,
            585,
            747
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_11",
          "label": "para",
          "text": "One way to capture this intuition that distribution (i) is more “fair” than the other two\nis to invoke the concept of entropy. In the discussion of decision trees, we described",
          "level": -1,
          "page": 274,
          "reading_order": 11,
          "bbox": [
            97,
            752,
            584,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_274_order_12",
          "label": "foot",
          "text": "252 | Chapter 6: Learning to Classify Text",
          "level": -1,
          "page": 274,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            275,
            842
          ],
          "section_number": "252",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_0",
          "label": "para",
          "text": "entropy as a measure of how “ disorganized ” a set of labels was. In particular, if a single\nlabel dominates then entropy is low, but if the labels are more evenly distributed then\nentropy is high. In our example, we chose distribution (i) because its label probabilities\nare evenly distributed—in other words, because its entropy is high. In general, the\nMaximum Entropy principle states that, among the distributions that are consistent\nwith what we know, we should choose the distribution whose entropy is highest.",
          "level": -1,
          "page": 275,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_1",
          "label": "para",
          "text": "Next, suppose that we are told that sense A appears 55% of the time. Once again, there\nare many distributions that are consistent with this new piece of information, such as:",
          "level": -1,
          "page": 275,
          "reading_order": 1,
          "bbox": [
            97,
            179,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_2",
          "label": "tab",
          "text": "<table><tr><td></td><td>A</td><td>B</td><td>C</td><td>D</td><td>E</td><td>F</td><td>G</td><td>H</td><td>I</td><td>J</td></tr><tr><td>(iv)</td><td>55%</td><td>45%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td></tr><tr><td>(v)</td><td>55%</td><td>5%</td><td>5%</td><td>5%</td><td>5%</td><td>5%</td><td>5%</td><td>5%</td><td>5%</td><td>5%</td></tr><tr><td>(vi)</td><td>55%</td><td>3%</td><td>1%</td><td>2%</td><td>9%</td><td>5%</td><td>0%</td><td>25%</td><td>0%</td><td>0%</td></tr></table>",
          "level": -1,
          "page": 275,
          "reading_order": 2,
          "bbox": [
            100,
            224,
            431,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_3",
          "label": "para",
          "text": "But again, we will likely choose the distribution that makes the fewest unwarranted\nassumptions—in this case, distribution (v).",
          "level": -1,
          "page": 275,
          "reading_order": 3,
          "bbox": [
            97,
            322,
            584,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_4",
          "label": "para",
          "text": "Finally, suppose that we are told that the word up appears in the nearby context 10%\nof the time, and that when it does appear in the context there's an 80% chance that\nsense A or C will be used. In this case, we will have a harder time coming up with an\nappropriate distribution by hand; however, we can verify that the following distribution\nlooks appropriate:",
          "level": -1,
          "page": 275,
          "reading_order": 4,
          "bbox": [
            97,
            366,
            584,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_5",
          "label": "tab",
          "text": "<table><tr><td></td><td>A</td><td>B</td><td>C</td><td>D</td><td>E</td><td>F</td><td>G</td><td>H</td><td>I</td><td>J</td></tr><tr><td rowspan=\"2\">(vii)</td><td>+up</td><td>5.1%</td><td>0.25%</td><td>2.9%</td><td>0.25%</td><td>0.25%</td><td>0.25%</td><td>0.25%</td><td>0.25%</td><td>0.25%</td><td>0.25%</td></tr><tr><td>−up</td><td>49.9%</td><td>4.46%</td><td>4.46%</td><td>4.46%</td><td>4.46%</td><td>4.46%</td><td>4.46%</td><td>4.46%</td><td>4.46%</td><td>4.46%</td></tr></table>",
          "level": -1,
          "page": 275,
          "reading_order": 5,
          "bbox": [
            100,
            456,
            566,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_6",
          "label": "para",
          "text": "In particular, the distribution is consistent with what we know: if we add up the prob-\nabilities in column A, we get 55%; if we add up the probabilities of row 1, we get 10%;\nand if we add up the boxes for senses A and C in the +up row, we get 8% (or 80% of\nthe +up cases). Furthermore, the remaining probabilities appear to be “evenly\ndistributed.”",
          "level": -1,
          "page": 275,
          "reading_order": 6,
          "bbox": [
            97,
            537,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_7",
          "label": "para",
          "text": "Throughout this example, we have restricted ourselves to distributions that are con-\nsistent with what we know; among these, we chose the distribution with the highest\nentropy. This is exactly what the Maximum Entropy classifier does as well. In\nparticular, for each joint-feature, the Maximum Entropy model calculates the “ empir-\nical frequency ” of that feature — i.e., the frequency with which it occurs in the training\nset. It then searches for the distribution which maximizes entropy, while still predicting\nthe correct frequency for each joint-feature.",
          "level": -1,
          "page": 275,
          "reading_order": 7,
          "bbox": [
            97,
            627,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_275_order_8",
          "label": "foot",
          "text": "6.6 Maximum Entropy Classifiers | 253",
          "level": -1,
          "page": 275,
          "reading_order": 8,
          "bbox": [
            413,
            824,
            584,
            842
          ],
          "section_number": "6.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_277_order_3",
      "label": "sec",
      "text": "What Do Models Tell Us?",
      "level": 1,
      "page": 277,
      "reading_order": 3,
      "bbox": [
        97,
        367,
        261,
        385
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_278_order_0",
          "label": "sub_sec",
          "text": "6.8 Summary",
          "level": 2,
          "page": 278,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            207,
            100
          ],
          "section_number": "6.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_278_order_1",
              "label": "list_group",
              "text": "• Modeling the linguistic data found in corpora can help us to understand linguistic\npatterns, and can be used to make predictions about new language data.\n• Supervised classifiers use labeled training corpora to build models that predict the\nlabel of an input based on specific features of that input.",
              "level": -1,
              "page": 278,
              "reading_order": 1,
              "bbox": [
                106,
                107,
                585,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "• Modeling the linguistic data found in corpora can help us to understand linguistic\npatterns, and can be used to make predictions about new language data.",
                  "bbox": [
                    106,
                    107,
                    585,
                    143
                  ],
                  "page": 278,
                  "reading_order": 1
                },
                {
                  "label": "list",
                  "text": "• Supervised classifiers use labeled training corpora to build models that predict the\nlabel of an input based on specific features of that input.",
                  "bbox": [
                    106,
                    143,
                    585,
                    179
                  ],
                  "page": 278,
                  "reading_order": 2
                },
                {
                  "label": "list",
                  "text": "• Supervised classifiers can perform a wide variety of NLP tasks, including document\nclassification, part-of-speech tagging, sentence segmentation, dialogue act type\nidentification, and determining entailment relations, and many other tasks.",
                  "bbox": [
                    106,
                    179,
                    585,
                    232
                  ],
                  "page": 278,
                  "reading_order": 3
                },
                {
                  "label": "list",
                  "text": "• When training a supervised classifier, you should split your corpus into three da-\ntasets: a training set for building the classifier model, a dev-test set for helping select\nand tune the model's features, and a test set for evaluating the final model's\nperformance.",
                  "bbox": [
                    106,
                    232,
                    585,
                    304
                  ],
                  "page": 278,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "• When evaluating a supervised classifier, it is important that you use fresh data that\nwas not included in the training or dev-test set. Otherwise, your evaluation results\nmay be unrealistically optimistic.",
                  "bbox": [
                    106,
                    304,
                    585,
                    358
                  ],
                  "page": 278,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "• Decision trees are automatically constructed tree-structured flowcharts that are\nused to assign labels to input values based on their features. Although they're easy\nto interpret, they are not very good at handling cases where feature values interact\nin determining the proper label.",
                  "bbox": [
                    106,
                    358,
                    585,
                    430
                  ],
                  "page": 278,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "• In naive Bayes classifiers, each feature independently contributes to the decision\nof which label should be used. This allows feature values to interact, but can be\nproblematic when two or more features are highly correlated with one another.",
                  "bbox": [
                    106,
                    430,
                    585,
                    483
                  ],
                  "page": 278,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "• Maximum Entropy classifiers use a basic model that is similar to the model used\nby naive Bayes; however, they employ iterative optimization to find the set of fea-\nture weights that maximizes the probability of the training set.",
                  "bbox": [
                    106,
                    483,
                    585,
                    537
                  ],
                  "page": 278,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "• Most of the models that are automatically constructed from a corpus are descrip-\ntive, that is, they let us know which features are relevant to a given pattern or\nconstruction, but they don't give any information about causal relationships be-\ntween those features and patterns.",
                  "bbox": [
                    106,
                    537,
                    585,
                    609
                  ],
                  "page": 278,
                  "reading_order": 9
                }
              ],
              "is_merged": true
            }
          ]
        },
        {
          "id": "page_278_order_10",
          "label": "sub_sec",
          "text": "6.9 Further Reading",
          "level": 2,
          "page": 278,
          "reading_order": 10,
          "bbox": [
            97,
            627,
            261,
            654
          ],
          "section_number": "6.9",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_278_order_11",
              "label": "para",
              "text": "Please consult http://www.nltk.org/ for further materials on this chapter and on how to\ninstall external machine learning packages, such as Weka, Mallet, TADM, and MegaM.\nFor more examples of classification and machine learning with NLTK, please see the\nclassification HOWTOs at http://www.nltk.org/howto.",
              "level": -1,
              "page": 278,
              "reading_order": 11,
              "bbox": [
                97,
                654,
                585,
                726
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_278_order_12",
              "label": "para",
              "text": "For a general introduction to machine learning, we recommend (Alpaydin, 2004) . For\na more mathematically intense introduction to the theory of machine learning, see\n(Hastie, Tibshirani & Friedman, 2009) . Excellent books on using machine learning",
              "level": -1,
              "page": 278,
              "reading_order": 12,
              "bbox": [
                97,
                734,
                585,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_278_order_13",
              "label": "foot",
              "text": "256 | Chapter 6: Learning to Classify Text",
              "level": -1,
              "page": 278,
              "reading_order": 13,
              "bbox": [
                97,
                824,
                275,
                842
              ],
              "section_number": "256",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_279_order_0",
              "label": "para",
              "text": "techniques for NLP include (Abney, 2008) , (Daelemans & Bosch, 2005) , (Feldman &\nSanger, 2007) , (Segaran, 2007) , and (Weiss et al., 2004) . For more on smoothing tech-\nniques for language problems, see (Manning & Schütze, 1999) . For more on sequence\nmodeling, and especially hidden Markov models, see (Manning & Schütze, 1999) or\n(Jurafsky & Martin, 2008) . Chapter 13 of (Manning, Raghavan & Schütze, 2008) dis-\ncusses the use of naive Bayes for classifying texts.",
              "level": -1,
              "page": 279,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                172
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_279_order_1",
              "label": "para",
              "text": "Many of the machine learning algorithms discussed in this chapter are numerically\nintensive, and as a result, they will run slowly when coded naively in Python. For in-\nformation on increasing the efficiency of numerically intensive algorithms in Python,\nsee (Kiusalaas, 2005) .",
              "level": -1,
              "page": 279,
              "reading_order": 1,
              "bbox": [
                97,
                179,
                585,
                244
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_279_order_2",
              "label": "para",
              "text": "The classification techniques described in this chapter can be applied to a very wide\nvariety of problems. For example, (Agirre & Edmonds, 2007) uses classifiers to perform\nword-sense disambiguation; and (Melamed, 2001) uses classifiers to create parallel\ntexts. Recent textbooks that cover text classification include (Manning, Raghavan &\nSchütze, 2008) and (Croft, Metzler & Strohman, 2009) .",
              "level": -1,
              "page": 279,
              "reading_order": 2,
              "bbox": [
                97,
                250,
                585,
                333
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_279_order_3",
              "label": "para",
              "text": "Much of the current research in the application of machine learning techniques to NLP\nproblems is driven by government-sponsored “challenges,” where a set of research\norganizations are all provided with the same development corpus and asked to build a\nsystem, and the resulting systems are compared based on a reserved test set. Examples\nof these challenge competitions include CoNLL Shared Tasks, the Recognizing Textual\nEntailment competitions, the ACE competitions, and the AQUAINT competitions.\nConsult http://www.nltk.org/ for a list of pointers to the web pages for these challenges.",
              "level": -1,
              "page": 279,
              "reading_order": 3,
              "bbox": [
                97,
                340,
                585,
                457
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_279_order_4",
          "label": "sub_sec",
          "text": "6.10 Exercises",
          "level": 2,
          "page": 279,
          "reading_order": 4,
          "bbox": [
            97,
            483,
            211,
            502
          ],
          "section_number": "6.10",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_279_order_5",
              "label": "para",
              "text": "1. ◦ Read up on one of the language technologies mentioned in this section, such as\nword sense disambiguation, semantic role labeling, question answering, machine\ntranslation, or named entity recognition. Find out what type and quantity of an-\nnotated data is required for developing such systems. Why do you think a large\namount of data is required?",
              "level": -1,
              "page": 279,
              "reading_order": 5,
              "bbox": [
                109,
                517,
                585,
                600
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_279_order_6",
              "label": "para",
              "text": "2. ◦ Using any of the three classifiers described in this chapter, and any features you\ncan think of, build the best name gender classifier you can. Begin by splitting the\nNames Corpus into three subsets: 500 words for the test set, 500 words for the\ndev-test set, and the remaining 6,900 words for the training set. Then, starting with\nthe example name gender classifier, make incremental improvements. Use the dev-\ntest set to check your progress. Once you are satisfied with your classifier, check\nits final performance on the test set. How does the performance on the test set\ncompare to the performance on the dev-test set? Is this what you'd expect?",
              "level": -1,
              "page": 279,
              "reading_order": 6,
              "bbox": [
                105,
                600,
                585,
                734
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_279_order_7",
              "label": "para",
              "text": "3. ◦ The Senseval 2 Corpus contains data intended to train word-sense disambigua-\ntion classifiers. It contains data for four words: hard, interest, line, and serve.\nChoose one of these four words, and load the corresponding data:",
              "level": -1,
              "page": 279,
              "reading_order": 7,
              "bbox": [
                108,
                741,
                584,
                788
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_279_order_8",
              "label": "foot",
              "text": "6.10 Exercises | 257",
              "level": -1,
              "page": 279,
              "reading_order": 8,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "6.10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_280_order_1",
              "label": "para",
              "text": "Using this dataset, build a classifier that predicts the correct sense tag for a given\ninstance. See the corpus HOWTO at http://www.nltk.org/howto for information\non using the instance objects returned by the Senseval 2 Corpus.",
              "level": -1,
              "page": 280,
              "reading_order": 1,
              "bbox": [
                122,
                134,
                584,
                182
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_280_order_2",
              "label": "para",
              "text": "4. ◦ Using the movie review document classifier discussed in this chapter, generate\na list of the 30 features that the classifier finds to be most informative. Can you\nexplain why these particular features are informative? Do you find any of them\nsurprising?",
              "level": -1,
              "page": 280,
              "reading_order": 2,
              "bbox": [
                100,
                188,
                585,
                253
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_280_order_3",
              "label": "para",
              "text": "5. ◦ Select one of the classification tasks described in this chapter, such as name\ngender detection, document classification, part-of-speech tagging, or dialogue act\nclassification. Using the same training and test data, and the same feature extractor,\nbuild three classifiers for the task: a decision tree, a naive Bayes classifier, and a\nMaximum Entropy classifier. Compare the performance of the three classifiers on\nyour selected task. How do you think that your results might be different if you\nused a different feature extractor?",
              "level": -1,
              "page": 280,
              "reading_order": 3,
              "bbox": [
                100,
                259,
                585,
                376
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_280_order_4",
              "label": "para",
              "text": "6. ◦ The synonyms strong and powerful pattern differently (try combining them with\nchip and sales). What features are relevant in this distinction? Build a classifier that\npredicts when each word should be used.",
              "level": -1,
              "page": 280,
              "reading_order": 4,
              "bbox": [
                100,
                376,
                585,
                430
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_280_order_5",
              "label": "para",
              "text": "7. o The dialogue act classifier assigns labels to individual posts, without considering\nthe context in which the post is found. However, dialogue acts are highly depend-\nent on context, and some sequences of dialogue act are much more likely than\nothers. For example, a ynQuestion dialogue act is much more likely to be answered\nby a yanswer than by a greeting. Make use of this fact to build a consecutive clas-\nsifier for labeling dialogue acts. Be sure to consider what features might be useful.\nSee the code for the consecutive classifier for part-of-speech tags in Example 6-5\nto get some ideas.",
              "level": -1,
              "page": 280,
              "reading_order": 5,
              "bbox": [
                100,
                430,
                585,
                564
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_280_order_6",
              "label": "para",
              "text": "8. o Word features can be very useful for performing document classification, since\nthe words that appear in a document give a strong indication about what its se-\nmantic content is. However, many words occur very infrequently, and some of the\nmost informative words in a document may never have occurred in our training\ndata. One solution is to make use of a lexicon, which describes how different words\nrelate to one another. Using the WordNet lexicon, augment the movie review\ndocument classifier presented in this chapter to use features that generalize the\nwords that appear in a document, making it more likely that they will match words\nfound in the training data.",
              "level": -1,
              "page": 280,
              "reading_order": 6,
              "bbox": [
                100,
                564,
                585,
                717
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_280_order_7",
              "label": "para",
              "text": "9. • The PP Attachment Corpus is a corpus describing prepositional phrase attach-\nment decisions. Each instance in the corpus is encoded as a PPAttachment object:",
              "level": -1,
              "page": 280,
              "reading_order": 7,
              "bbox": [
                100,
                717,
                584,
                754
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_280_order_8",
              "label": "foot",
              "text": "258 | Chapter 6: Learning to Classify Text",
              "level": -1,
              "page": 280,
              "reading_order": 8,
              "bbox": [
                97,
                824,
                275,
                842
              ],
              "section_number": "258",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_281_order_1",
              "label": "para",
              "text": "Select only the instances where inst.attachment is N",
              "level": -1,
              "page": 281,
              "reading_order": 1,
              "bbox": [
                122,
                213,
                417,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_281_order_3",
              "label": "para",
              "text": "Using this subcorpus, build a classifier that attempts to predict which preposition\nis used to connect a given pair of nouns. For example, given the pair of nouns\nteam and researchers, the classifier should predict the preposition of. See the corpus\nHOWTO at http://www.nltk.org/howto for more information on using the PP At-\ntachment Corpus.",
              "level": -1,
              "page": 281,
              "reading_order": 3,
              "bbox": [
                122,
                268,
                585,
                352
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_281_order_4",
              "label": "para",
              "text": "10. • Suppose you wanted to automatically generate a prose description of a scene,\nand already had a word to uniquely describe each entity, such as the book , and\nsimply wanted to decide whether to use in or on in relating various items, e.g., the\nbook is in the cupboard versus the book is on the shelf . Explore this issue by looking\nat corpus data and writing programs as needed. Consider the following examples:",
              "level": -1,
              "page": 281,
              "reading_order": 4,
              "bbox": [
                100,
                358,
                585,
                439
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_281_order_5",
              "label": "para",
              "text": "(13) a. in the car versus on the train",
              "level": -1,
              "page": 281,
              "reading_order": 5,
              "bbox": [
                109,
                454,
                327,
                466
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_281_order_6",
              "label": "para",
              "text": "b. in town versus on campus",
              "level": -1,
              "page": 281,
              "reading_order": 6,
              "bbox": [
                150,
                474,
                315,
                492
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_281_order_7",
              "label": "para",
              "text": "c. in the picture versus on the screen",
              "level": -1,
              "page": 281,
              "reading_order": 7,
              "bbox": [
                151,
                492,
                359,
                510
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_281_order_8",
              "label": "para",
              "text": "d. in Macbeth versus on Letterman",
              "level": -1,
              "page": 281,
              "reading_order": 8,
              "bbox": [
                149,
                510,
                350,
                528
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_281_order_9",
              "label": "foot",
              "text": "6.10 Exercises | 259",
              "level": -1,
              "page": 281,
              "reading_order": 9,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "6.10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_282_order_0",
              "label": "para",
              "text": "_",
              "level": -1,
              "page": 282,
              "reading_order": 0,
              "bbox": [
                153,
                161,
                494,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_277_order_4",
          "label": "para",
          "text": "It's important to understand what we can learn about language from an automatically\nconstructed model. One important consideration when dealing with models of lan-\nguage is the distinction between descriptive models and explanatory models. Descrip-\ntive models capture patterns in the data, but they don't provide any information about\nwhy the data contains those patterns. For example, as we saw in Table 3 - 1 , the syno-\nnyms absolutely and definitely are not interchangeable: we say absolutely adore not\ndefinitely adore , and definitely prefer , not absolutely prefer . In contrast, explanatory\nmodels attempt to capture properties and relationships that cause the linguistic pat-\nterns. For example, we might introduce the abstract concept of “ polar adjective\" as an\nadjective that has an extreme meaning, and categorize some adjectives, such as adore\nand detest as polar. Our explanatory model would contain the constraint that abso-\nlutely can combine only with polar adjectives, and definitely can only combine with\nnon-polar adjectives. In summary, descriptive models provide information about cor-\nrelations in the data, while explanatory models go further to postulate causal\nrelationships.",
          "level": -1,
          "page": 277,
          "reading_order": 4,
          "bbox": [
            97,
            394,
            585,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_277_order_5",
          "label": "para",
          "text": "Most models that are automatically constructed from a corpus are descriptive models;\nin other words, they can tell us what features are relevant to a given pattern or con-\nstruction, but they can’t necessarily tell us how those features and patterns relate to\none another. If our goal is to understand the linguistic patterns, then we can use this\ninformation about which features are related as a starting point for further experiments\ndesigned to tease apart the relationships between features and patterns. On the other\nhand, if we’re just interested in using the model to make predictions (e.g., as part of a\nlanguage processing system), then we can use the model to make predictions about\nnew data without worrying about the details of underlying causal relationships.",
          "level": -1,
          "page": 277,
          "reading_order": 5,
          "bbox": [
            97,
            645,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_277_order_6",
          "label": "foot",
          "text": "6.7 Modeling Linguistic Patterns | 255",
          "level": -1,
          "page": 277,
          "reading_order": 6,
          "bbox": [
            413,
            824,
            585,
            842
          ],
          "section_number": "6.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_283_order_0",
      "label": "sec",
      "text": "CHAPTER 7\nExtracting Information from Text",
      "level": 1,
      "page": 283,
      "reading_order": 0,
      "bbox": [
        162,
        78,
        585,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_283_order_6",
          "label": "sub_sec",
          "text": "7.1 Information Extraction",
          "level": 2,
          "page": 283,
          "reading_order": 6,
          "bbox": [
            97,
            600,
            315,
            627
          ],
          "section_number": "7.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_283_order_7",
              "label": "para",
              "text": "Information comes in many shapes and sizes. One important form is structured\ndata , where there is a regular and predictable organization of entities and relationships.\nFor example, we might be interested in the relation between companies and locations.\nGiven a particular company, we would like to be able to identify the locations where\nit does business; conversely, given a location, we would like to discover which com-\npanies do business in that location. If our data is in tabular form, such as the example\nin Table 7 - 1 , then answering these queries is straightforward.",
              "level": -1,
              "page": 283,
              "reading_order": 7,
              "bbox": [
                97,
                635,
                585,
                752
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_283_order_8",
              "label": "foot",
              "text": "261",
              "level": -1,
              "page": 283,
              "reading_order": 8,
              "bbox": [
                566,
                824,
                584,
                842
              ],
              "section_number": "261",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_284_order_0",
              "label": "table",
              "text": "Table 7-1. Locations data [TABLE: <table><tr><td>OrgName</td><td>LocationName</td></tr><tr><td>Omnicom</td><td>New York</td></tr><tr><td>DDB Needham</td><td>New York</td></tr><tr><td>Kaplan Thaler Group</td><td>New York</td></tr><tr><td>BBDO South</td><td>Atlanta</td></tr><tr><td>Georgia-Pacific</td><td>Atlanta</td></tr></table>]",
              "level": -1,
              "page": 284,
              "reading_order": 0,
              "bbox": [
                100,
                89,
                270,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "tab",
                  "text": "<table><tr><td>OrgName</td><td>LocationName</td></tr><tr><td>Omnicom</td><td>New York</td></tr><tr><td>DDB Needham</td><td>New York</td></tr><tr><td>Kaplan Thaler Group</td><td>New York</td></tr><tr><td>BBDO South</td><td>Atlanta</td></tr><tr><td>Georgia-Pacific</td><td>Atlanta</td></tr></table>",
                  "bbox": [
                    100,
                    89,
                    270,
                    215
                  ],
                  "page": 284,
                  "reading_order": 0
                },
                {
                  "label": "cap",
                  "text": "Table 7-1. Locations data",
                  "bbox": [
                    99,
                    71,
                    225,
                    89
                  ],
                  "page": 284,
                  "reading_order": 1
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_284_order_2",
              "label": "para",
              "text": "If this location data was stored in Python as a list of tuples (entity, relation,\nentity) , then the question “Which organizations operate in Atlanta?” could be trans-\nlated as follows:",
              "level": -1,
              "page": 284,
              "reading_order": 2,
              "bbox": [
                97,
                232,
                584,
                277
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_284_order_4",
              "label": "para",
              "text": "Things are more tricky if we try to get similar information out of text. For example,\nconsider the following snippet (from nltk.corpus.ieer, for fileid NYT19980315.0085).",
              "level": -1,
              "page": 284,
              "reading_order": 4,
              "bbox": [
                97,
                322,
                584,
                358
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_284_order_5",
              "label": "para",
              "text": "(1) The fourth Wells account moving to another agency is the packaged paper-\nproducts division of Georgia-Pacific Corp., which arrived at Wells only last fall.\nLike Hertz and the History Channel, it is also leaving for an Omnicom-owned\nagency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta,\nwhich handles corporate advertising for Georgia-Pacific, will assume additional\nduties for brands like Angel Soft toilet tissue and Sparkle paper towels, said\nKen Haldin, a spokesman for Georgia-Pacific in Atlanta.",
              "level": -1,
              "page": 284,
              "reading_order": 5,
              "bbox": [
                118,
                367,
                584,
                483
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_284_order_6",
              "label": "para",
              "text": "If you read through (1) , you will glean the information required to answer the example\nquestion. But how do we get a machine to understand enough about (1) to return the\nlist ['BBDO South', 'Georgia-Pacific'] as an answer? This is obviously a much harder\ntask. Unlike Table 7 - 1 , (1) contains no structure that links organization names with\nlocation names.",
              "level": -1,
              "page": 284,
              "reading_order": 6,
              "bbox": [
                97,
                492,
                585,
                575
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_284_order_7",
              "label": "para",
              "text": "One approach to this problem involves building a very general representation of mean-\ning ( Chapter 10 ) . In this chapter we take a different approach, deciding in advance that\nwe will only look for very specific kinds of information in text, such as the relation\nbetween organizations and locations. Rather than trying to use text like (1) to answer\nthe question directly, we first convert the unstructured data of natural language sen-\ntences into the structured data of Table 7 - 1 . Then we reap the benefits of powerful\nquery tools such as SQL. This method of getting meaning from text is called Infor-\nmation Extraction .",
              "level": -1,
              "page": 284,
              "reading_order": 7,
              "bbox": [
                97,
                582,
                585,
                716
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_284_order_8",
              "label": "para",
              "text": "Information Extraction has many applications, including business intelligence, resume\nharvesting, media analysis, sentiment detection, patent search, and email scanning. A\nparticularly important area of current research involves the attempt to extract",
              "level": -1,
              "page": 284,
              "reading_order": 8,
              "bbox": [
                97,
                725,
                585,
                774
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_284_order_9",
              "label": "foot",
              "text": "262 | Chapter 7: Extracting Information from Text",
              "level": -1,
              "page": 284,
              "reading_order": 9,
              "bbox": [
                97,
                824,
                311,
                842
              ],
              "section_number": "262",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_285_order_0",
              "label": "para",
              "text": "structured data out of electronically available scientific literature, especially in the do-\nmain of biology and medicine.",
              "level": -1,
              "page": 285,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_283_order_1",
          "label": "para",
          "text": "For any given question, it’s likely that someone has written the answer down some-\nwhere. The amount of natural language text that is available in electronic form is truly\nstaggering, and is increasing every day. However, the complexity of natural language\ncan make it very difficult to access the information in that text. The state of the art in\nNLP is still a long way from being able to build general-purpose representations of\nmeaning from unrestricted text. If we instead focus our efforts on a limited set of ques-\ntions or “entity relations,” such as “where are different facilities located” or “who is\nemployed by what company,” we can make significant progress. The goal of this chap-\nter is to answer the following questions:",
          "level": -1,
          "page": 283,
          "reading_order": 1,
          "bbox": [
            97,
            286,
            586,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_283_order_2",
          "label": "para",
          "text": "1. How can we build a system that extracts structured data from unstructured text?",
          "level": -1,
          "page": 283,
          "reading_order": 2,
          "bbox": [
            100,
            447,
            583,
            465
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_283_order_3",
          "label": "para",
          "text": "2. What are some robust methods for identifying the entities and relationships de-\nscribed in a text?",
          "level": -1,
          "page": 283,
          "reading_order": 3,
          "bbox": [
            100,
            465,
            584,
            501
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_283_order_4",
          "label": "para",
          "text": "3. Which corpora are appropriate for this work, and how do we use them for training\nand evaluating our models?",
          "level": -1,
          "page": 283,
          "reading_order": 4,
          "bbox": [
            100,
            501,
            585,
            537
          ],
          "section_number": "3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_283_order_5",
          "label": "para",
          "text": "Along the way, we’ll apply techniques from the last two chapters to the problems of\nchunking and named entity recognition.",
          "level": -1,
          "page": 283,
          "reading_order": 5,
          "bbox": [
            97,
            546,
            584,
            576
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_285_order_1",
      "label": "sec",
      "text": "Information Extraction Architecture",
      "level": 1,
      "page": 285,
      "reading_order": 1,
      "bbox": [
        98,
        116,
        335,
        136
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_286_order_6",
          "label": "sub_sec",
          "text": "7.2 Chunking",
          "level": 2,
          "page": 286,
          "reading_order": 6,
          "bbox": [
            97,
            474,
            207,
            503
          ],
          "section_number": "7.2",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_286_order_9",
              "label": "sub_sub_sec",
              "text": "Noun Phrase Chunking",
              "level": 3,
              "page": 286,
              "reading_order": 9,
              "bbox": [
                100,
                714,
                248,
                734
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_286_order_10",
                  "label": "para",
                  "text": "We will begin by considering the task of noun phrase chunking, or NP-chunking,\nwhere we search for chunks corresponding to individual noun phrases. For example,\nhere is some Wall Street Journal text with NP -chunks marked using brackets:",
                  "level": -1,
                  "page": 286,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    742,
                    584,
                    789
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_286_order_11",
                  "label": "foot",
                  "text": "264 | Chapter 7: Extracting Information from Text",
                  "level": -1,
                  "page": 286,
                  "reading_order": 11,
                  "bbox": [
                    97,
                    824,
                    311,
                    842
                  ],
                  "section_number": "264",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_287_order_0",
                  "label": "para",
                  "text": "(2) [ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/\nIN [ Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB\nthat/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ]\nshould/MD do/VB well/RB there/RB ./.",
                  "level": -1,
                  "page": 287,
                  "reading_order": 0,
                  "bbox": [
                    118,
                    71,
                    585,
                    136
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_287_order_1",
                  "label": "para",
                  "text": "As we can see, NP -chunks are often smaller pieces than complete noun phrases. For\nexample, the market for system-management software for Digital's hardware is a single\nnoun phrase (containing two nested noun phrases), but it is captured in NP -chunks by\nthe simpler chunk the market . One of the motivations for this difference is that NP -\nchunks are defined so as not to contain other NP -chunks. Consequently, any preposi-\ntional phrases or subordinate clauses that modify a nominal will not be included in the\ncorresponding NP -chunk, since they almost certainly contain further noun phrases.",
                  "level": -1,
                  "page": 287,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    152,
                    585,
                    268
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_287_order_2",
                  "label": "para",
                  "text": "One of the most useful sources of information for NP -chunking is part-of-speech tags.\nThis is one of the motivations for performing part-of-speech tagging in our information\nextraction system. We demonstrate this approach using an example sentence that has\nbeen part-of-speech tagged in Example 7 -1 . In order to create an NP -chunker, we will\nfirst define a chunk grammar , consisting of rules that indicate how sentences should\nbe chunked. In this case, we will define a simple grammar with a single regular\nexpression rule ❷ . This rule says that an NP chunk should be formed whenever the\nchunker finds an optional determiner ( DT ) followed by any number of adjectives ( JJ )\nand then a noun ( NN ). Using this grammar, we create a chunk parser ❸ , and test it on\nour example sentence ❹ . The result is a tree, which we can either print ❺ , or display\ngraphically ❻ .",
                  "level": -1,
                  "page": 287,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    276,
                    585,
                    456
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_287_order_3",
                  "label": "para",
                  "text": "Example 7-1. Example of a simple regular expression–based NP chunker.",
                  "level": -1,
                  "page": 287,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    465,
                    458,
                    483
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_287_order_6",
                  "label": "foot",
                  "text": "7.2 Chunking | 265",
                  "level": -1,
                  "page": 287,
                  "reading_order": 6,
                  "bbox": [
                    494,
                    824,
                    585,
                    842
                  ],
                  "section_number": "7.2",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_288_order_0",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_288_figure_000.png)",
                  "level": -1,
                  "page": 288,
                  "reading_order": 0,
                  "bbox": [
                    96,
                    71,
                    368,
                    189
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_286_order_7",
              "label": "para",
              "text": "The basic technique we will use for entity recognition is chunking , which segments\nand labels multitoken sequences as illustrated in Figure 7 - 2 . The smaller boxes show\nthe word-level tokenization and part-of-speech tagging, while the large boxes show\nhigher-level chunking. Each of these larger boxes is called a chunk . Like tokenization,\nwhich omits whitespace, chunking usually selects a subset of the tokens. Also like\ntokenization, the pieces produced by a chunker do not overlap in the source text.",
              "level": -1,
              "page": 286,
              "reading_order": 7,
              "bbox": [
                97,
                510,
                585,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_286_order_8",
              "label": "para",
              "text": "In this section, we will explore chunking in some depth, beginning with the definition\nand representation of chunks. We will see regular expression and n-gram approaches\nto chunking, and will develop and evaluate chunkers using the CoNLL-2000 Chunking\nCorpus. We will then return in Sections 7.5 and 7.6 to the tasks of named entity rec-\nognition and relation extraction.",
              "level": -1,
              "page": 286,
              "reading_order": 8,
              "bbox": [
                97,
                618,
                585,
                698
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_285_order_2",
          "label": "para",
          "text": "Figure 7 -1 shows the architecture for a simple information extraction system. It begins\nby processing a document using several of the procedures discussed in Chapters 3 and\n5 : first, the raw text of the document is split into sentences using a sentence segmenter,\nand each sentence is further subdivided into words using a tokenizer. Next, each sen-\ntence is tagged with part-of-speech tags, which will prove very helpful in the next step,\nnamed entity recognition . In this step, we search for mentions of potentially inter-\nesting entities in each sentence. Finally, we use relation recognition to search for likely\nrelations between different entities in the text.",
          "level": -1,
          "page": 285,
          "reading_order": 2,
          "bbox": [
            97,
            149,
            585,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_285_order_3",
          "label": "figure",
          "text": "Figure 7-1. Simple pipeline architecture for an information extraction system. This system takes the\nraw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its\noutput. For example, given a document that indicates that the company Georgia-Pacific is located in\nAtlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']). [IMAGE: ![Figure](figures/NLTK_page_285_figure_003.png)]",
          "level": -1,
          "page": 285,
          "reading_order": 3,
          "bbox": [
            91,
            295,
            583,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_285_figure_003.png)",
              "bbox": [
                91,
                295,
                583,
                591
              ],
              "page": 285,
              "reading_order": 3
            },
            {
              "label": "cap",
              "text": "Figure 7-1. Simple pipeline architecture for an information extraction system. This system takes the\nraw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its\noutput. For example, given a document that indicates that the company Georgia-Pacific is located in\nAtlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']).",
              "bbox": [
                97,
                600,
                585,
                663
              ],
              "page": 285,
              "reading_order": 4
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_285_order_5",
          "label": "para",
          "text": "To perform the first three tasks, we can define a function that simply connects together\nNLTK's default sentence segmenter ❶ , word tokenizer ❷ , and part-of-speech\ntagger ❸ :",
          "level": -1,
          "page": 285,
          "reading_order": 5,
          "bbox": [
            97,
            689,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_285_order_7",
          "label": "foot",
          "text": "7.1 Information Extraction | 263",
          "level": -1,
          "page": 285,
          "reading_order": 7,
          "bbox": [
            440,
            824,
            584,
            842
          ],
          "section_number": "7.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_286_order_0",
          "label": "figure",
          "text": "Figure 7-2. Segmentation and labeling at both the Token and Chunk levels. [IMAGE: ![Figure](figures/NLTK_page_286_figure_000.png)]",
          "level": -1,
          "page": 286,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_286_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                188
              ],
              "page": 286,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 7-2. Segmentation and labeling at both the Token and Chunk levels.",
              "bbox": [
                97,
                194,
                467,
                207
              ],
              "page": 286,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_286_order_2",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_286_figure_002.png)",
          "level": -1,
          "page": 286,
          "reading_order": 2,
          "bbox": [
            109,
            215,
            171,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_286_order_3",
          "label": "para",
          "text": "Remember that our program samples assume you begin your interactive\nsession or your program with import nltk, re, pprint.",
          "level": -1,
          "page": 286,
          "reading_order": 3,
          "bbox": [
            171,
            224,
            530,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_286_order_4",
          "label": "para",
          "text": "Next, in named entity recognition, we segment and label the entities that might par-\nticipate in interesting relations with one another. Typically, these will be definite noun\nphrases such as the knights who say “ni”, or proper names such as Monty Python. In\nsome tasks it is useful to also consider indefinite nouns or noun chunks, such as every\nstudent or cats, and these do not necessarily refer to entities in the same way as definite\nNPs and proper names.",
          "level": -1,
          "page": 286,
          "reading_order": 4,
          "bbox": [
            97,
            295,
            585,
            397
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_286_order_5",
          "label": "para",
          "text": "Finally, in relation extraction, we search for specific patterns between pairs of entities\nthat occur near one another in the text, and use those patterns to build tuples recording\nthe relationships between the entities.",
          "level": -1,
          "page": 286,
          "reading_order": 5,
          "bbox": [
            97,
            403,
            585,
            456
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_288_order_1",
      "label": "sec",
      "text": "Tag Patterns",
      "level": 1,
      "page": 288,
      "reading_order": 1,
      "bbox": [
        100,
        210,
        181,
        229
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_288_order_2",
          "label": "para",
          "text": "The rules that make up a chunk grammar use tag patterns to describe sequences of\ntagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle\nbrackets, e.g.,<DT>?<JJ>*<NN>. Tag patterns are similar to regular expression patterns\n(Section 3.4). Now, consider the following noun phrases from the Wall Street Journal:",
          "level": -1,
          "page": 288,
          "reading_order": 2,
          "bbox": [
            97,
            232,
            586,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_288_order_4",
          "label": "para",
          "text": "We can match these noun phrases using a slight refinement of the first tag pattern\nabove, i.e., <DT>?<JJ.*>*<NN.*>+. This will chunk any sequence of tokens beginning\nwith an optional determiner, followed by zero or more adjectives of any type (including\nrelative adjectives like earlier/JJR ), followed by one or more nouns of any type. How-\never, it is easy to find many more complicated examples which this rule will not cover:",
          "level": -1,
          "page": 288,
          "reading_order": 4,
          "bbox": [
            97,
            384,
            585,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_288_order_6",
          "label": "para",
          "text": "Your Turn: Try to come up with tag patterns to cover these cases. Test\nthem using the graphical interface nltk.app.chunkparser(). Continue\nto refine your tag patterns with the help of the feedback given by this\ntool.",
          "level": -1,
          "page": 288,
          "reading_order": 6,
          "bbox": [
            171,
            573,
            530,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_288_order_7",
      "label": "sec",
      "text": "Chunking with Regular Expression",
      "level": 1,
      "page": 288,
      "reading_order": 7,
      "bbox": [
        97,
        654,
        324,
        680
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_288_order_8",
          "label": "para",
          "text": "To find the chunk structure for a given sentence, the RegexpParser chunker begins with\na flat structure in which no tokens are chunked. The chunking rules are applied in turn,\nsuccessively updating the chunk structure. Once all of the rules have been invoked, the\nresulting chunk structure is returned.",
          "level": -1,
          "page": 288,
          "reading_order": 8,
          "bbox": [
            97,
            687,
            585,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_288_order_9",
          "label": "para",
          "text": "Example 7-2 shows a simple chunk grammar consisting of two rules. The first rule\nmatches an optional determiner or possessive pronoun, zero or more adjectives, then",
          "level": -1,
          "page": 288,
          "reading_order": 9,
          "bbox": [
            97,
            760,
            585,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_288_order_10",
          "label": "foot",
          "text": "266 | Chapter 7: Extracting Information from Text",
          "level": -1,
          "page": 288,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            311,
            842
          ],
          "section_number": "266",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_289_order_0",
          "label": "para",
          "text": "a noun. The second rule matches one or more proper nouns. We also define an example\nsentence to be chunked ❶, and run the chunker on this input ❸.",
          "level": -1,
          "page": 289,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_289_order_1",
          "label": "para",
          "text": "Example 7-2. Simple noun phrase chunker",
          "level": -1,
          "page": 289,
          "reading_order": 1,
          "bbox": [
            97,
            116,
            306,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_289_order_3",
          "label": "para",
          "text": "The $ symbol is a special character in regular expressions, and must be\nbackslash escaped in order to match the tag PP$.",
          "level": -1,
          "page": 289,
          "reading_order": 3,
          "bbox": [
            171,
            340,
            530,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_289_order_4",
          "label": "para",
          "text": "If a tag pattern matches at overlapping locations, the leftmost match takes precedence.\nFor example, if we apply a rule that matches two consecutive nouns to a text containing\nthree consecutive nouns, then only the first two nouns will be chunked:",
          "level": -1,
          "page": 289,
          "reading_order": 4,
          "bbox": [
            97,
            419,
            585,
            466
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_289_order_6",
          "label": "para",
          "text": "Once we have created the chunk for money market, we have removed the context that\nwould have permitted fund to be included in a chunk. This issue would have been\navoided with a more permissive chunk rule, e.g., NP: {<NN>+}.",
          "level": -1,
          "page": 289,
          "reading_order": 6,
          "bbox": [
            97,
            546,
            585,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_289_order_7",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_289_figure_007.png)",
          "level": -1,
          "page": 289,
          "reading_order": 7,
          "bbox": [
            118,
            609,
            171,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_289_order_8",
          "label": "para",
          "text": "We have added a comment to each of our chunk rules. These are op-\ntional; when they are present, the chunker prints these comments as\npart of its tracing output.",
          "level": -1,
          "page": 289,
          "reading_order": 8,
          "bbox": [
            171,
            625,
            530,
            667
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_289_order_9",
      "label": "sec",
      "text": "Exploring Text Corpora",
      "level": 1,
      "page": 289,
      "reading_order": 9,
      "bbox": [
        98,
        698,
        248,
        718
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_289_order_10",
          "label": "para",
          "text": "In Section 5.2 , we saw how we could interrogate a tagged corpus to extract phrases\nmatching a particular sequence of part-of-speech tags. We can do the same work more\neasily with a chunker, as follows:",
          "level": -1,
          "page": 289,
          "reading_order": 10,
          "bbox": [
            97,
            725,
            585,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_289_order_11",
          "label": "foot",
          "text": "7.2 Chunking | 267",
          "level": -1,
          "page": 289,
          "reading_order": 11,
          "bbox": [
            494,
            824,
            585,
            842
          ],
          "section_number": "7.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_290_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_290_figure_001.png)",
          "level": -1,
          "page": 290,
          "reading_order": 1,
          "bbox": [
            109,
            295,
            171,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_290_order_2",
          "label": "para",
          "text": "Your Turn: Encapsulate the previous example inside a function\nfind_chunks() that takes a chunk string like \"CHUNK: {<V.*> <TO>\n<V.*>}\" as an argument. Use it to search the corpus for several other\npatterns, such as four or more nouns in a row, e.g., \"NOUNS:\n{<N.*>{4,}}\".",
          "level": -1,
          "page": 290,
          "reading_order": 2,
          "bbox": [
            171,
            304,
            530,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_290_order_3",
      "label": "sec",
      "text": "Chinking",
      "level": 1,
      "page": 290,
      "reading_order": 3,
      "bbox": [
        97,
        403,
        156,
        425
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_290_order_4",
          "label": "para",
          "text": "Sometimes it is easier to define what we want to exclude from a chunk. We can define\na chink to be a sequence of tokens that is not included in a chunk. In the following\nexample, barked/VBD at/IN is a chink:",
          "level": -1,
          "page": 290,
          "reading_order": 4,
          "bbox": [
            97,
            430,
            585,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_290_order_5",
          "label": "para",
          "text": "the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]",
          "level": -1,
          "page": 290,
          "reading_order": 5,
          "bbox": [
            126,
            483,
            512,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_290_order_6",
          "label": "para",
          "text": "Chinking is the process of removing a sequence of tokens from a chunk. If the matching\nsequence of tokens spans an entire chunk, then the whole chunk is removed; if the\nsequence of tokens appears in the middle of the chunk, these tokens are removed,\nleaving two chunks where there was only one before. If the sequence is at the periphery\nof the chunk, these tokens are removed, and a smaller chunk remains. These three\npossibilities are illustrated in Table 7-2 .",
          "level": -1,
          "page": 290,
          "reading_order": 6,
          "bbox": [
            97,
            510,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_290_order_7",
          "label": "table",
          "text": "Table 7-2. Three chinking rules applied to the same chunk [TABLE: <table><tr><td></td><td>Entire chunk</td><td>Middle of a chunk</td><td>End of a chunk</td></tr><tr><td>Input</td><td>[a/DT little/JJ dog/NN]</td><td>[a/DT little/JJ dog/NN]</td><td>[a/DT little/JJ dog/NN]</td></tr><tr><td>Operation</td><td>Chink “DT JJ NN”</td><td>Chink “JJ”</td><td>Chink “NN”</td></tr><tr><td>Pattern</td><td>}DT JJ NN{</td><td>}JJ{</td><td>}NN{</td></tr><tr><td>Output</td><td>a/DT little/JJ dog/NN</td><td>[a/DT] little/JJ [dog/NN]</td><td>[a/DT little/JJ] dog/NN</td></tr></table>]",
          "level": -1,
          "page": 290,
          "reading_order": 7,
          "bbox": [
            100,
            645,
            458,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td></td><td>Entire chunk</td><td>Middle of a chunk</td><td>End of a chunk</td></tr><tr><td>Input</td><td>[a/DT little/JJ dog/NN]</td><td>[a/DT little/JJ dog/NN]</td><td>[a/DT little/JJ dog/NN]</td></tr><tr><td>Operation</td><td>Chink “DT JJ NN”</td><td>Chink “JJ”</td><td>Chink “NN”</td></tr><tr><td>Pattern</td><td>}DT JJ NN{</td><td>}JJ{</td><td>}NN{</td></tr><tr><td>Output</td><td>a/DT little/JJ dog/NN</td><td>[a/DT] little/JJ [dog/NN]</td><td>[a/DT little/JJ] dog/NN</td></tr></table>",
              "bbox": [
                100,
                645,
                458,
                743
              ],
              "page": 290,
              "reading_order": 7
            },
            {
              "label": "cap",
              "text": "Table 7-2. Three chinking rules applied to the same chunk",
              "bbox": [
                99,
                618,
                386,
                636
              ],
              "page": 290,
              "reading_order": 8
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_290_order_9",
          "label": "foot",
          "text": "268 | Chapter 7: Extracting Information from Text",
          "level": -1,
          "page": 290,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            311,
            842
          ],
          "section_number": "268",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_291_order_0",
          "label": "para",
          "text": "In Example 7-3, we put the entire sentence into a single chunk, then excise the chinks.",
          "level": -1,
          "page": 291,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            584,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_291_order_1",
          "label": "para",
          "text": "Example 7-3. Simple chinker.",
          "level": -1,
          "page": 291,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            243,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_291_order_3",
      "label": "sec",
      "text": "Representing Chunks: Tags Versus Trees",
      "level": 1,
      "page": 291,
      "reading_order": 3,
      "bbox": [
        100,
        330,
        362,
        349
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_292_order_5",
          "label": "sub_sec",
          "text": "7.3 Developing and Evaluating Chunkers",
          "level": 2,
          "page": 292,
          "reading_order": 5,
          "bbox": [
            97,
            483,
            423,
            519
          ],
          "section_number": "7.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_292_order_7",
              "label": "sub_sub_sec",
              "text": "Reading IOB Format and the CoNLL-2000 Chunking Corpus",
              "level": 3,
              "page": 292,
              "reading_order": 7,
              "bbox": [
                98,
                636,
                485,
                655
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_292_order_8",
                  "label": "para",
                  "text": "Using the corpora module we can load Wall Street Journal text that has been tagged\nthen chunked using the IOB notation. The chunk categories provided in this corpus\nare NP , VP , and PP . As we have seen, each sentence is represented using multiple lines,\nas shown here:",
                  "level": -1,
                  "page": 292,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    663,
                    585,
                    725
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_292_order_10",
                  "label": "foot",
                  "text": "270 | Chapter 7: Extracting Information from Text",
                  "level": -1,
                  "page": 292,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    311,
                    842
                  ],
                  "section_number": "270",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_293_order_0",
                  "label": "para",
                  "text": "A conversion function chunk.conllstr2tree() builds a tree representation from one of\nthese multiline strings. Moreover, it permits us to choose any subset of the three chunk\ntypes to use, here just for NP chunks:",
                  "level": -1,
                  "page": 293,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    586,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_293_order_2",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_293_figure_002.png)",
                  "level": -1,
                  "page": 293,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    392,
                    557,
                    483
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_293_order_3",
                  "label": "para",
                  "text": "We can use the NLTK corpus module to access a larger amount of chunked text. The\nCoNLL-2000 Chunking Corpus contains 270k words of Wall Street Journal text, divi-\nded into “train” and “test” portions, annotated with part-of-speech tags and chunk tags\nin the IOB format. We can access the data using nltk.corpus.conll2000. Here is an\nexample that reads the 100th sentence of the “train” portion of the corpus:",
                  "level": -1,
                  "page": 293,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    492,
                    585,
                    582
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_293_order_5",
                  "label": "para",
                  "text": "As you can see, the CoNLL-2000 Chunking Corpus contains three chunk types: NP\nchunks, which we have already seen; VP chunks, such as has already delivered ; and PP",
                  "level": -1,
                  "page": 293,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    752,
                    585,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_293_order_6",
                  "label": "foot",
                  "text": "7.3 Developing and Evaluating Chunkers | 271",
                  "level": -1,
                  "page": 293,
                  "reading_order": 6,
                  "bbox": [
                    386,
                    824,
                    584,
                    842
                  ],
                  "section_number": "7.3",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_294_order_0",
                  "label": "para",
                  "text": "chunks, such as because of. Since we are only interested in the NP chunks right now, we\ncan use the chunk_types argument to select them:",
                  "level": -1,
                  "page": 294,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    107
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_292_order_6",
              "label": "para",
              "text": "Now you have a taste of what chunking does, but we haven’t explained how to evaluate\nchunkers. As usual, this requires a suitably annotated corpus. We begin by looking at\nthe mechanics of converting IOB format into an NLTK tree, then at how this is done\non a larger scale using a chunked corpus. We will see how to score the accuracy of a\nchunker relative to a corpus, then look at some more data-driven ways to search for\nNP chunks. Our focus throughout will be on expanding the coverage of a chunker.",
              "level": -1,
              "page": 292,
              "reading_order": 6,
              "bbox": [
                97,
                519,
                585,
                620
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_291_order_4",
          "label": "para",
          "text": "As befits their intermediate status between tagging and parsing (Chapter 8 ), chunk\nstructures can be represented using either tags or trees. The most widespread file rep-\nresentation uses IOB tags . In this scheme, each token is tagged with one of three special\nchunk tags, I (inside), O (outside), or B (begin). A token is tagged as B if it marks the\nbeginning of a chunk. Subsequent tokens within the chunk are tagged I . All other\ntokens are tagged O . The B and I tags are suffixed with the chunk type, e.g., B-NP , I-\nNP . Of course, it is not necessary to specify a chunk type for tokens that appear outside\na chunk, so these are just labeled O . An example of this scheme is shown in Figure 7 - 3 .",
          "level": -1,
          "page": 291,
          "reading_order": 4,
          "bbox": [
            97,
            357,
            585,
            487
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_291_order_5",
          "label": "figure",
          "text": "Figure 7-3. Tag representation of chunk structures. [IMAGE: ![Figure](figures/NLTK_page_291_figure_005.png)]",
          "level": -1,
          "page": 291,
          "reading_order": 5,
          "bbox": [
            100,
            510,
            583,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_291_figure_005.png)",
              "bbox": [
                100,
                510,
                583,
                609
              ],
              "page": 291,
              "reading_order": 5
            },
            {
              "label": "cap",
              "text": "Figure 7-3. Tag representation of chunk structures.",
              "bbox": [
                97,
                618,
                350,
                636
              ],
              "page": 291,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_291_order_7",
          "label": "para",
          "text": "IOB tags have become the standard way to represent chunk structures in files, and we\nwill also be using this format. Here is how the information in Figure 7-3 would appear\nin a file:",
          "level": -1,
          "page": 291,
          "reading_order": 7,
          "bbox": [
            97,
            663,
            585,
            711
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_291_order_9",
          "label": "foot",
          "text": "7.2 Chunking | 269",
          "level": -1,
          "page": 291,
          "reading_order": 9,
          "bbox": [
            494,
            824,
            585,
            842
          ],
          "section_number": "7.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_292_order_0",
          "label": "para",
          "text": "In this representation there is one token per line, each with its part-of-speech tag and\nchunk tag. This format permits us to represent more than one chunk type, so long as\nthe chunks do not overlap. As we saw earlier, chunk structures can also be represented\nusing trees. These have the benefit that each chunk is a constituent that can be manip-\nulated directly. An example is shown in Figure 7-4.",
          "level": -1,
          "page": 292,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_292_order_1",
          "label": "figure",
          "text": "Figure 7-4. Tree representation of chunk structures. [IMAGE: ![Figure](figures/NLTK_page_292_figure_001.png)]",
          "level": -1,
          "page": 292,
          "reading_order": 1,
          "bbox": [
            100,
            170,
            583,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_292_figure_001.png)",
              "bbox": [
                100,
                170,
                583,
                358
              ],
              "page": 292,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Figure 7-4. Tree representation of chunk structures.",
              "bbox": [
                97,
                366,
                350,
                379
              ],
              "page": 292,
              "reading_order": 2
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_292_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_292_figure_003.png)",
          "level": -1,
          "page": 292,
          "reading_order": 3,
          "bbox": [
            109,
            394,
            171,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_292_order_4",
          "label": "para",
          "text": "NLTK uses trees for its internal representation of chunks, but provides\nmethods for converting between such trees and the IOB format.",
          "level": -1,
          "page": 292,
          "reading_order": 4,
          "bbox": [
            171,
            412,
            530,
            443
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_294_order_2",
      "label": "sec",
      "text": "Simple Evaluation and Baselines",
      "level": 1,
      "page": 294,
      "reading_order": 2,
      "bbox": [
        97,
        268,
        315,
        290
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_294_order_3",
          "label": "para",
          "text": "Now that we can access a chunked corpus, we can evaluate chunkers. We start off by\nestablishing a baseline for the trivial chunk parser cp that creates no chunks:",
          "level": -1,
          "page": 294,
          "reading_order": 3,
          "bbox": [
            97,
            295,
            584,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_294_order_5",
          "label": "para",
          "text": "The IOB tag accuracy indicates that more than a third of the words are tagged with 0,\ni.e., not in an NP chunk. However, since our tagger did not find any chunks, its precision,\nrecall, and F-measure are all zero. Now let's try a naive regular expression chunker that\nlooks for tags beginning with letters that are characteristic of noun phrase tags (e.g.,\nCD, DT, and JJ).",
          "level": -1,
          "page": 294,
          "reading_order": 5,
          "bbox": [
            97,
            464,
            585,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_294_order_7",
          "label": "para",
          "text": "As you can see, this approach achieves decent results. However, we can improve on it\nby adopting a more data-driven approach, where we use the training corpus to find the\nchunk tag ( I , O , or B ) that is most likely for each part-of-speech tag. In other words, we\ncan build a chunker using a unigram tagger (Section 5.4 ). But rather than trying to\ndetermine the correct part-of-speech tag for each word, we are trying to determine the\ncorrect chunk tag, given each word's part-of-speech tag.",
          "level": -1,
          "page": 294,
          "reading_order": 7,
          "bbox": [
            97,
            663,
            585,
            762
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_294_order_8",
          "label": "foot",
          "text": "272 | Chapter 7: Extracting Information from Text",
          "level": -1,
          "page": 294,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            311,
            842
          ],
          "section_number": "272",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_295_order_0",
          "label": "para",
          "text": "In Example 7-4, we define the UnigramChunker class, which uses a unigram tagger to\nlabel sentences with chunk tags. Most of the code in this class is simply used to convert\nback and forth between the chunk tree representation used by NLTK's ChunkParserI\ninterface, and the IOB representation used by the embedded tagger. The class defines\ntwo methods: a constructor O , which is called when we build a new UnigramChunker;\nand the parse method O , which is used to chunk new sentences.",
          "level": -1,
          "page": 295,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_295_order_1",
          "label": "cap",
          "text": "Example 7-4. Noun phrase chunking with a unigram tagger.",
          "level": -1,
          "page": 295,
          "reading_order": 1,
          "bbox": [
            97,
            186,
            395,
            199
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_295_order_4",
          "label": "para",
          "text": "The constructor ❶ expects a list of training sentences, which will be in the form of\nchunk trees. It first converts training data to a form that’s suitable for training the tagger,\nusing tree2conlltags to map each chunk tree to a list of word,tag,chunk triples. It then\nuses that converted training data to train a unigram tagger, and stores it in self.tag\nger for later use.",
          "level": -1,
          "page": 295,
          "reading_order": 4,
          "bbox": [
            97,
            391,
            585,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_295_order_5",
          "label": "para",
          "text": "The parse method ❶ takes a tagged sentence as its input, and begins by extracting the\npart-of-speech tags from that sentence. It then tags the part-of-speech tags with IOB\nchunk tags, using the tagger self.tagger that was trained in the constructor. Next, it\nextracts the chunk tags, and combines them with the original sentence, to yield\nconlltags. Finally, it uses conlltags2tree to convert the result back into a chunk tree.",
          "level": -1,
          "page": 295,
          "reading_order": 5,
          "bbox": [
            97,
            474,
            585,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_295_order_6",
          "label": "para",
          "text": "Now that we have UnigramChunker, we can train it using the CoNLL-2000 Chunking\nCorpus, and test its resulting performance:",
          "level": -1,
          "page": 295,
          "reading_order": 6,
          "bbox": [
            97,
            564,
            585,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_295_order_8",
          "label": "para",
          "text": "This chunker does reasonably well, achieving an overall F-measure score of 83%. Let’s\ntake a look at what it’s learned, by using its unigram tagger to assign a tag to each of\nthe part-of-speech tags that appear in the corpus:",
          "level": -1,
          "page": 295,
          "reading_order": 8,
          "bbox": [
            97,
            734,
            585,
            782
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_295_order_9",
          "label": "foot",
          "text": "7.3 Developing and Evaluating Chunkers | 273",
          "level": -1,
          "page": 295,
          "reading_order": 9,
          "bbox": [
            386,
            824,
            584,
            842
          ],
          "section_number": "7.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_296_order_1",
          "label": "para",
          "text": "It has discovered that most punctuation marks occur outside of NP chunks, with the\nexception of # and $, both of which are used as currency markers. It has also found that\ndeterminers (DT) and possessives (PRP$ and WP$) occur at the beginnings of NP chunks,\nwhile noun types (NN, NNP, NNPS, NNS) mostly occur inside of NP chunks.",
          "level": -1,
          "page": 296,
          "reading_order": 1,
          "bbox": [
            97,
            250,
            585,
            316
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_296_order_2",
          "label": "para",
          "text": "Having built a unigram chunker, it is quite easy to build a bigram chunker: we simply\nchange the class name to BigramChunker, and modify line ❷ in Example 7-4 to construct\na BigramTagger rather than a UnigramTagger. The resulting chunker has slightly higher\nperformance than the unigram chunker:",
          "level": -1,
          "page": 296,
          "reading_order": 2,
          "bbox": [
            97,
            322,
            585,
            389
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_296_order_4",
      "label": "sec",
      "text": "Training Classifier-Based Chunkers",
      "level": 1,
      "page": 296,
      "reading_order": 4,
      "bbox": [
        97,
        501,
        326,
        522
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_299_order_3",
          "label": "sub_sec",
          "text": "7.4 Recursion in Linguistic Structure",
          "level": 2,
          "page": 299,
          "reading_order": 3,
          "bbox": [
            97,
            680,
            387,
            707
          ],
          "section_number": "7.4",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_299_order_4",
              "label": "sub_sub_sec",
              "text": "Building Nested Structure with Cascaded Chunkers",
              "level": 3,
              "page": 299,
              "reading_order": 4,
              "bbox": [
                98,
                716,
                433,
                739
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_299_order_5",
                  "label": "para",
                  "text": "So far, our chunk structures have been relatively flat. Trees consist of tagged tokens,\noptionally grouped under a chunk node such as NP. However, it is possible to build\nchunk structures of arbitrary depth, simply by creating a multistage chunk grammar",
                  "level": -1,
                  "page": 299,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    743,
                    585,
                    797
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_299_order_6",
                  "label": "foot",
                  "text": "7.4 Recursion in Linguistic Structure | 277",
                  "level": -1,
                  "page": 299,
                  "reading_order": 6,
                  "bbox": [
                    404,
                    824,
                    585,
                    842
                  ],
                  "section_number": "7.4",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_300_order_0",
                  "label": "para",
                  "text": "containing recursive rules. Example 7-6 has patterns for noun phrases, prepositional\nphrases, verb phrases, and sentences. This is a four-stage chunk grammar, and can be\nused to create structures having a depth of at most four.",
                  "level": -1,
                  "page": 300,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_300_order_1",
                  "label": "para",
                  "text": "Example 7-6. A chunker that handles NP, PP, VP, and S.",
                  "level": -1,
                  "page": 300,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    134,
                    377,
                    152
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_300_order_3",
                  "label": "para",
                  "text": "Unfortunately this result misses the VP headed by saw. It has other shortcomings, too.\nLet’s see what happens when we apply this chunker to a sentence having deeper nesting.\nNotice that it fails to identify the VP chunk starting at ❶.",
                  "level": -1,
                  "page": 300,
                  "reading_order": 3,
                  "bbox": [
                    98,
                    385,
                    584,
                    435
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_300_order_5",
                  "label": "para",
                  "text": "The solution to these problems is to get the chunker to loop over its patterns: after\ntrying all of them, it repeats the process. We add an optional second argument loop to\nspecify the number of times the set of patterns should be run:",
                  "level": -1,
                  "page": 300,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    609,
                    585,
                    656
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_300_order_7",
                  "label": "foot",
                  "text": "278 | Chapter 7: Extracting Information from Text",
                  "level": -1,
                  "page": 300,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    824,
                    311,
                    842
                  ],
                  "section_number": "278",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_301_order_1",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_301_figure_001.png)",
                  "level": -1,
                  "page": 301,
                  "reading_order": 1,
                  "bbox": [
                    109,
                    116,
                    171,
                    170
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_301_order_2",
                  "label": "para",
                  "text": "This cascading process enables us to create deep structures. However,\ncreating and debugging a cascade is difficult, and there comes a point\nwhere it is more effective to do full parsing (see Chapter 8 ). Also, the\ncascading process can only produce trees of fixed depth (no deeper than\nthe number of stages in the cascade), and this is insufficient for complete\nsyntactic analysis.",
                  "level": -1,
                  "page": 301,
                  "reading_order": 2,
                  "bbox": [
                    171,
                    125,
                    530,
                    215
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_296_order_5",
          "label": "para",
          "text": "Both the regular expression–based chunkers and the n-gram chunkers decide what\nchunks to create entirely based on part-of-speech tags. However, sometimes part-of-\nspeech tags are insufficient to determine how a sentence should be chunked. For ex-\nample, consider the following two statements:",
          "level": -1,
          "page": 296,
          "reading_order": 5,
          "bbox": [
            97,
            528,
            584,
            594
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_296_order_6",
          "label": "para",
          "text": "(3) a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ../.",
          "level": -1,
          "page": 296,
          "reading_order": 6,
          "bbox": [
            118,
            608,
            458,
            620
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_296_order_7",
          "label": "para",
          "text": "b. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.",
          "level": -1,
          "page": 296,
          "reading_order": 7,
          "bbox": [
            150,
            627,
            513,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_296_order_8",
          "label": "para",
          "text": "These two sentences have the same part-of-speech tags, yet they are chunked differ-\nently. In the first sentence, the farmer and rice are separate chunks, while the corre-\nsponding material in the second sentence, the computer monitor , is a single chunk.\nClearly, we need to make use of information about the content of the words, in addition\nto just their part-of-speech tags, if we wish to maximize chunking performance.",
          "level": -1,
          "page": 296,
          "reading_order": 8,
          "bbox": [
            97,
            654,
            584,
            738
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_296_order_9",
          "label": "para",
          "text": "One way that we can incorporate information about the content of words is to use a\nclassifier-based tagger to chunk the sentence. Like the n-gram chunker considered in\nthe previous section, this classifier-based chunker will work by assigning IOB tags to",
          "level": -1,
          "page": 296,
          "reading_order": 9,
          "bbox": [
            97,
            743,
            584,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_296_order_10",
          "label": "foot",
          "text": "274 | Chapter 7: Extracting Information from Text",
          "level": -1,
          "page": 296,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            311,
            842
          ],
          "section_number": "274",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_297_order_0",
          "label": "para",
          "text": "the words in a sentence, and then converting those tags to chunks. For the classifier-\nbased tagger itself, we will use the same approach that we used in Section 6.1 to build\na part-of-speech tagger.",
          "level": -1,
          "page": 297,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_297_order_1",
          "label": "para",
          "text": "The basic code for the classifier-based NP chunker is shown in Example 7-5 . It consists\nof two classes. The first class ❶ is almost identical to the ConsecutivePosTagger class\nfrom Example 6-5 . The only two differences are that it calls a different feature extractor\n❷ and that it uses a MaxentClassifier rather than a NaiveBayesClassifier ❸ . The sec-\nond class ❹ is basically a wrapper around the tagger class that turns it into a chunker.\nDuring training, this second class maps the chunk trees in the training corpus into tag\nsequences; in the parse() method, it converts the tag sequence provided by the tagger\nback into a chunk tree.",
          "level": -1,
          "page": 297,
          "reading_order": 1,
          "bbox": [
            97,
            132,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_297_order_2",
          "label": "para",
          "text": "Example 7-5. Noun phrase chunking with a consecutive classifier",
          "level": -1,
          "page": 297,
          "reading_order": 2,
          "bbox": [
            97,
            276,
            415,
            289
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_297_order_8",
          "label": "para",
          "text": "The only piece left to fill in is the feature extractor. We begin by defining a simple\nfeature extractor, which just provides the part-of-speech tag of the current token. Using",
          "level": -1,
          "page": 297,
          "reading_order": 8,
          "bbox": [
            97,
            734,
            585,
            771
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_297_order_9",
          "label": "foot",
          "text": "7.3 Developing and Evaluating Chunkers | 275",
          "level": -1,
          "page": 297,
          "reading_order": 9,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "7.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_298_order_0",
          "label": "para",
          "text": "this feature extractor, our classifier-based chunker is very similar to the unigram chunk-\ner, as is reflected in its performance:",
          "level": -1,
          "page": 298,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_298_order_2",
          "label": "para",
          "text": "We can also add a feature for the previous part-of-speech tag. Adding this feature allows\nthe classifier to model interactions between adjacent tags, and results in a chunker that\nis closely related to the bigram chunker.",
          "level": -1,
          "page": 298,
          "reading_order": 2,
          "bbox": [
            97,
            250,
            585,
            300
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_298_order_4",
          "label": "para",
          "text": "Next, we'll try adding a feature for the current word, since we hypothesized that word\ncontent should be useful for chunking. We find that this feature does indeed improve\nthe chunker's performance, by about 1.5 percentage points (which corresponds to\nabout a 10 % reduction in the error rate).",
          "level": -1,
          "page": 298,
          "reading_order": 4,
          "bbox": [
            97,
            500,
            585,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_298_order_6",
          "label": "foot",
          "text": "276 | Chapter 7: Extracting Information from Text",
          "level": -1,
          "page": 298,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            311,
            842
          ],
          "section_number": "276",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_299_order_0",
          "label": "para",
          "text": "Finally, we can try extending the feature extractor with a variety of additional features,\nsuch as lookahead features ❶ , paired features ❷ , and complex contextual features ❸ .\nThis last feature, called tags-since-dt , creates a string describing the set of all part-of-\nspeech tags that have been encountered since the most recent determiner.",
          "level": -1,
          "page": 299,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_299_order_2",
          "label": "para",
          "text": "Your Turn: Try adding different features to the feature extractor func-\ntion npchunk_features, and see if you can further improve the perform-\nance of the NP chunker.",
          "level": -1,
          "page": 299,
          "reading_order": 2,
          "bbox": [
            171,
            600,
            530,
            646
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_301_order_3",
      "label": "sec",
      "text": "Tree",
      "level": 1,
      "page": 301,
      "reading_order": 3,
      "bbox": [
        97,
        232,
        126,
        259
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_301_order_4",
          "label": "para",
          "text": "A tree is a set of connected labeled nodes, each reachable by a unique path from a\ndistinguished root node. Here’s an example of a tree (note that they are standardly\ndrawn upside-down):",
          "level": -1,
          "page": 301,
          "reading_order": 4,
          "bbox": [
            97,
            259,
            585,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_301_order_5",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_301_figure_005.png)",
          "level": -1,
          "page": 301,
          "reading_order": 5,
          "bbox": [
            118,
            322,
            324,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_301_order_6",
          "label": "para",
          "text": "We use a ‘family’ metaphor to talk about the relationships of nodes in a tree: for ex-\nample, S is the parent of VP; conversely VP is a child of S. Also, since NP and VP are both\nchildren of S, they are also siblings. For convenience, there is also a text format for\nspecifying trees:",
          "level": -1,
          "page": 301,
          "reading_order": 6,
          "bbox": [
            97,
            465,
            585,
            531
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_301_order_8",
          "label": "para",
          "text": "Although we will focus on syntactic trees, trees can be used to encode any homogeneous\nhierarchical structure that spans a sequence of linguistic forms (e.g., morphological\nstructure, discourse structure). In the general case, leaves and node values do not have\nto be strings.",
          "level": -1,
          "page": 301,
          "reading_order": 8,
          "bbox": [
            97,
            636,
            585,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_301_order_9",
          "label": "para",
          "text": "In NLTK, we create a tree by giving a node label and a list of children:",
          "level": -1,
          "page": 301,
          "reading_order": 9,
          "bbox": [
            98,
            707,
            503,
            727
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_301_order_10",
          "label": "foot",
          "text": "7.4 Recursion in Linguistic Structure | 279",
          "level": -1,
          "page": 301,
          "reading_order": 10,
          "bbox": [
            404,
            824,
            585,
            842
          ],
          "section_number": "7.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_302_order_1",
          "label": "para",
          "text": "We can incorporate these into successively larger trees as follows",
          "level": -1,
          "page": 302,
          "reading_order": 1,
          "bbox": [
            98,
            161,
            468,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_302_order_3",
          "label": "para",
          "text": "Here are some of the methods available for tree objects:",
          "level": -1,
          "page": 302,
          "reading_order": 3,
          "bbox": [
            98,
            241,
            414,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_302_order_5",
          "label": "para",
          "text": "The bracketed representation for complex trees can be difficult to read. In these cases,\nthe draw method can be very useful. It opens a new window, containing a graphical\nrepresentation of the tree. The tree display window allows you to zoom in and out, to\ncollapse and expand subtrees, and to print the graphical representation to a postscript\nfile (for inclusion in a document).",
          "level": -1,
          "page": 302,
          "reading_order": 5,
          "bbox": [
            97,
            376,
            585,
            458
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_302_order_7",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_302_figure_007.png)",
          "level": -1,
          "page": 302,
          "reading_order": 7,
          "bbox": [
            97,
            492,
            207,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_302_order_8",
      "label": "sec",
      "text": "Tree Traversal",
      "level": 1,
      "page": 302,
      "reading_order": 8,
      "bbox": [
        97,
        591,
        190,
        611
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_303_order_3",
          "label": "sub_sec",
          "text": "7.5 Named Entity Recognition",
          "level": 2,
          "page": 303,
          "reading_order": 3,
          "bbox": [
            97,
            286,
            342,
            313
          ],
          "section_number": "7.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_303_order_4",
              "label": "para",
              "text": "At the start of this chapter, we briefly introduced named entities (NEs). Named entities\nare definite noun phrases that refer to specific types of individuals, such as organiza-\ntions, persons, dates, and so on. Table 7 -3 lists some of the more commonly used types\nof NEs. These should be self-explanatory, except for “ FACILITY ” : human-made arti-\nfacts in the domains of architecture and civil engineering; and “ GPE ” : geo-political\nentities such as city, state/province, and country.",
              "level": -1,
              "page": 303,
              "reading_order": 4,
              "bbox": [
                97,
                320,
                585,
                417
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_303_order_5",
              "label": "table",
              "text": "Table 7-3. Commonly used types of named entity [TABLE: <table><tr><td>NE type</td><td>Examples</td></tr><tr><td>ORGANIZATION</td><td>Georgia-Pacific Corp., WHO</td></tr><tr><td>PERSON</td><td>Eddy Bonte, President Obama</td></tr><tr><td>LOCATION</td><td>Murray River, Mount Everest</td></tr><tr><td>DATE</td><td>June, 2008-06-29</td></tr><tr><td>TIME</td><td>two fifty a m, 1:30 p.m.</td></tr><tr><td>MONEY</td><td>175 million Canadian Dollars, GBP 10.40</td></tr><tr><td>PERCENT</td><td>twenty pct, 18.75 %</td></tr><tr><td>FACILITY</td><td>Washington Monument, Stonehenge</td></tr><tr><td>GPE</td><td>South East Asia, Midlothian</td></tr></table>]",
              "level": -1,
              "page": 303,
              "reading_order": 5,
              "bbox": [
                100,
                448,
                333,
                654
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "tab",
                  "text": "<table><tr><td>NE type</td><td>Examples</td></tr><tr><td>ORGANIZATION</td><td>Georgia-Pacific Corp., WHO</td></tr><tr><td>PERSON</td><td>Eddy Bonte, President Obama</td></tr><tr><td>LOCATION</td><td>Murray River, Mount Everest</td></tr><tr><td>DATE</td><td>June, 2008-06-29</td></tr><tr><td>TIME</td><td>two fifty a m, 1:30 p.m.</td></tr><tr><td>MONEY</td><td>175 million Canadian Dollars, GBP 10.40</td></tr><tr><td>PERCENT</td><td>twenty pct, 18.75 %</td></tr><tr><td>FACILITY</td><td>Washington Monument, Stonehenge</td></tr><tr><td>GPE</td><td>South East Asia, Midlothian</td></tr></table>",
                  "bbox": [
                    100,
                    448,
                    333,
                    654
                  ],
                  "page": 303,
                  "reading_order": 5
                },
                {
                  "label": "cap",
                  "text": "Table 7-3. Commonly used types of named entity",
                  "bbox": [
                    99,
                    430,
                    342,
                    448
                  ],
                  "page": 303,
                  "reading_order": 6
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_303_order_7",
              "label": "para",
              "text": "The goal of a named entity recognition (NER) system is to identify all textual men-\ntions of the named entities. This can be broken down into two subtasks: identifying\nthe boundaries of the NE, and identifying its type. While named entity recognition is\nfrequently a prelude to identifying relations in Information Extraction, it can also con-\ntribute to other tasks. For example, in Question Answering (QA), we try to improve\nthe precision of Information Retrieval by recovering not whole pages, but just those\nparts which contain an answer to the user's question. Most QA systems take the",
              "level": -1,
              "page": 303,
              "reading_order": 7,
              "bbox": [
                97,
                670,
                585,
                783
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_303_order_8",
              "label": "foot",
              "text": "7.5 Named Entity Recognition | 281",
              "level": -1,
              "page": 303,
              "reading_order": 8,
              "bbox": [
                430,
                824,
                584,
                842
              ],
              "section_number": "7.5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_304_order_0",
              "label": "para",
              "text": "documents returned by standard Information Retrieval, and then attempt to isolate\nthe minimal text snippet in the document containing the answer. Now suppose the\nquestion was Who was the first President of the US?, and one of the documents that was\nretrieved contained the following passage:",
              "level": -1,
              "page": 304,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_304_order_1",
              "label": "para",
              "text": "(5) The Washington Monument is the most prominent structure in Washington,\nD.C. and one of the city's early attractions. It was built in honor of George\nWashington, who led the country to independence and then became its first\nPresident.",
              "level": -1,
              "page": 304,
              "reading_order": 1,
              "bbox": [
                118,
                152,
                583,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_304_order_2",
              "label": "para",
              "text": "Analysis of the question leads us to expect that an answer should be of the form X was\nthe first President of the US, where X is not only a noun phrase, but also refers to a\nnamed entity of type PER. This should allow us to ignore the first sentence in the passage.\nAlthough it contains two occurrences of Washington, named entity recognition should\ntell us that neither of them has the correct type.",
              "level": -1,
              "page": 304,
              "reading_order": 2,
              "bbox": [
                97,
                232,
                585,
                313
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_304_order_3",
              "label": "para",
              "text": "How do we go about identifying named entities? One option would be to look up each\nword in an appropriate list of names. For example, in the case of locations, we could\nuse a gazetteer , or geographical dictionary, such as the Alexandria Gazetteer or the\nGetty Gazetteer. However, doing this blindly runs into problems, as shown in Fig-\nure 7 - 5 .",
              "level": -1,
              "page": 304,
              "reading_order": 3,
              "bbox": [
                97,
                322,
                585,
                403
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_304_order_4",
              "label": "figure",
              "text": "Figure 7-5. Location detection by simple lookup for a news story: Looking up every word in a gazetteer\nis error-prone; case distinctions may help, but these are not always present. [IMAGE: ![Figure](figures/NLTK_page_304_figure_004.png)]",
              "level": -1,
              "page": 304,
              "reading_order": 4,
              "bbox": [
                100,
                412,
                583,
                636
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_304_figure_004.png)",
                  "bbox": [
                    100,
                    412,
                    583,
                    636
                  ],
                  "page": 304,
                  "reading_order": 4
                },
                {
                  "label": "cap",
                  "text": "Figure 7-5. Location detection by simple lookup for a news story: Looking up every word in a gazetteer\nis error-prone; case distinctions may help, but these are not always present.",
                  "bbox": [
                    97,
                    636,
                    583,
                    668
                  ],
                  "page": 304,
                  "reading_order": 5
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_304_order_6",
              "label": "para",
              "text": "Observe that the gazetteer has good coverage of locations in many countries, and in-\ncorrectly finds locations like Sanchez in the Dominican Republic and On in Vietnam.\nOf course we could omit such locations from the gazetteer, but then we won’t be able\nto identify them when they do appear in a document.",
              "level": -1,
              "page": 304,
              "reading_order": 6,
              "bbox": [
                97,
                689,
                585,
                761
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_304_order_7",
              "label": "para",
              "text": "It gets even harder in the case of names for people or organizations. Any list of such\nnames will probably have poor coverage. New organizations come into existence every",
              "level": -1,
              "page": 304,
              "reading_order": 7,
              "bbox": [
                97,
                768,
                585,
                798
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_304_order_8",
              "label": "foot",
              "text": "282 | Chapter 7: Extracting Information from Text",
              "level": -1,
              "page": 304,
              "reading_order": 8,
              "bbox": [
                97,
                824,
                311,
                842
              ],
              "section_number": "282",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_305_order_0",
              "label": "para",
              "text": "day, so if we are trying to deal with contemporary newswire or blog entries, it is unlikely\nthat we will be able to recognize many of the entities using gazetteer lookup.",
              "level": -1,
              "page": 305,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_305_order_1",
              "label": "para",
              "text": "Another major source of difficulty is caused by the fact that many named entity terms\nare ambiguous. Thus May and North are likely to be parts of named entities for DATE\nand LOCATION, respectively, but could both be part of a PERSON; conversely Chris-\ntian Dior looks like a PERSON but is more likely to be of type ORGANIZATION. A\nterm like Yankee will be an ordinary modifier in some contexts, but will be marked as\nan entity of type ORGANIZATION in the phrase Yankee infielders.",
              "level": -1,
              "page": 305,
              "reading_order": 1,
              "bbox": [
                97,
                115,
                585,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_305_order_2",
              "label": "para",
              "text": "Further challenges are posed by multiword names like Stanford University , and by\nnames that contain other names, such as Cecil H. Green Library and Escondido Village\nConference Service Center . In named entity recognition, therefore, we need to be able\nto identify the beginning and end of multitoken sequences.",
              "level": -1,
              "page": 305,
              "reading_order": 2,
              "bbox": [
                97,
                221,
                585,
                286
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_305_order_3",
              "label": "para",
              "text": "Named entity recognition is a task that is well suited to the type of classifier-based\napproach that we saw for noun phrase chunking. In particular, we can build a tagger\nthat labels each word in a sentence using the IOB format, where chunks are labeled by\ntheir appropriate type. Here is part of the CONLL 2002 ( conll2002 ) Dutch training\ndata:",
              "level": -1,
              "page": 305,
              "reading_order": 3,
              "bbox": [
                97,
                294,
                585,
                376
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_305_order_5",
              "label": "para",
              "text": "In this representation, there is one token per line, each with its part-of-speech tag and\nits named entity tag. Based on this training corpus, we can construct a tagger that can\nbe used to label new sentences, and use the nltk.chunk.conlltags2tree() function to\nconvert the tag sequences into a chunk tree.",
              "level": -1,
              "page": 305,
              "reading_order": 5,
              "bbox": [
                97,
                492,
                584,
                560
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_305_order_6",
              "label": "para",
              "text": "NLTK provides a classifier that has already been trained to recognize named entities,\naccessed with the function nltk.ne_chunk(). If we set the parameter binary=True ❶,\nthen named entities are just tagged as NE; otherwise, the classifier adds category labels\nsuch as PERSON, ORGANIZATION, and GPE.",
              "level": -1,
              "page": 305,
              "reading_order": 6,
              "bbox": [
                97,
                564,
                585,
                636
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_305_order_8",
              "label": "foot",
              "text": "7.5 Named Entity Recognition | 283",
              "level": -1,
              "page": 305,
              "reading_order": 8,
              "bbox": [
                430,
                824,
                584,
                842
              ],
              "section_number": "7.5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_306_order_1",
          "label": "sub_sec",
          "text": "7.6 Relation Extraction",
          "level": 2,
          "page": 306,
          "reading_order": 1,
          "bbox": [
            97,
            259,
            282,
            286
          ],
          "section_number": "7.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_306_order_2",
              "label": "para",
              "text": "Once named entities have been identified in a text, we then want to extract the relations\nthat exist between them. As indicated earlier, we will typically be looking for relations\nbetween specified types of named entity. One way of approaching this task is to initially\nlook for all triples of the form $(X,\\alpha,Y)$ , where $X$ and $Y$ are named entities of the required\ntypes, and $\\alpha$ is the string of words that intervenes between $X$ and $Y$ . We can then use\nregular expressions to pull out just those instances of $\\alpha$ that express the relation that\nwe are looking for. The following example searches for strings that contain the word\nin . The special regular expression (?!\\b.+ing\\b) is a negative lookahead assertion that\nallows us to disregard strings such as success in supervising the transition of , where in\nis followed by a gerund.",
              "level": -1,
              "page": 306,
              "reading_order": 2,
              "bbox": [
                97,
                295,
                585,
                458
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_306_order_4",
              "label": "para",
              "text": "Searching for the keyword in works reasonably well, though it will also retrieve false\npositives such as [ORG: House Transportation Committee] , secured the most money\nin the [LOC: New York]; there is unlikely to be a simple string-based method of ex-\ncluding filler strings such as this.",
              "level": -1,
              "page": 306,
              "reading_order": 4,
              "bbox": [
                97,
                707,
                585,
                773
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_306_order_5",
              "label": "foot",
              "text": "284 | Chapter 7: Extracting Information from Text",
              "level": -1,
              "page": 306,
              "reading_order": 5,
              "bbox": [
                97,
                824,
                311,
                842
              ],
              "section_number": "284",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_307_order_0",
              "label": "para",
              "text": "As shown earlier, the Dutch section of the CoNLL 2002 Named Entity Corpus contains\nnot just named entity annotation, but also part-of-speech tags. This allows us to devise\npatterns that are sensitive to these tags, as shown in the next example. The method\nshow_clause() prints out the relations in a clausal form, where the binary relation sym-\nbol is specified as the value of parameter relsym ❶.",
              "level": -1,
              "page": 307,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                155
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_307_order_2",
              "label": "para",
              "text": "Your Turn: Replace the last line ❶ with print show_raw_rtuple(rel,\nlcon=True, rcon=True). This will show you the actual words that inter-\nvene between the two NEs and also their left and right context, within\na default 10-word window. With the help of a Dutch dictionary, you\nmight be able to figure out why the result VAN('annie_lennox', 'euryth\nmics') is a false hit.",
              "level": -1,
              "page": 307,
              "reading_order": 2,
              "bbox": [
                171,
                438,
                530,
                524
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_307_order_3",
          "label": "sub_sec",
          "text": "7.7 Summary",
          "level": 2,
          "page": 307,
          "reading_order": 3,
          "bbox": [
            97,
            546,
            207,
            574
          ],
          "section_number": "7.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_307_order_4",
              "label": "list_group",
              "text": "Information extraction systems search large bodies of unrestricted text for specific\ntypes of entities and relations, and use them to populate well-organized databases.\nThese databases can then be used to find answers for specific questions.\nThe typical architecture for an information extraction system begins by segment-\ning, tokenizing, and part-of-speech tagging the text. The resulting data is then\nsearched for specific types of entity. Finally, the information extraction system\nlooks at entities that are mentioned near one another in the text, and tries to de-\ntermine whether specific relationships hold between those entities.",
              "level": -1,
              "page": 307,
              "reading_order": 4,
              "bbox": [
                121,
                582,
                584,
                631
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "Information extraction systems search large bodies of unrestricted text for specific\ntypes of entities and relations, and use them to populate well-organized databases.\nThese databases can then be used to find answers for specific questions.",
                  "bbox": [
                    121,
                    582,
                    584,
                    631
                  ],
                  "page": 307,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "The typical architecture for an information extraction system begins by segment-\ning, tokenizing, and part-of-speech tagging the text. The resulting data is then\nsearched for specific types of entity. Finally, the information extraction system\nlooks at entities that are mentioned near one another in the text, and tries to de-\ntermine whether specific relationships hold between those entities.",
                  "bbox": [
                    121,
                    636,
                    584,
                    718
                  ],
                  "page": 307,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "Entity recognition is often performed using chunkers, which segment multitoken\nsequences, and label them with the appropriate entity type. Common entity types\ninclude ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and\nGPE (geo-political entity).",
                  "bbox": [
                    122,
                    725,
                    585,
                    789
                  ],
                  "page": 307,
                  "reading_order": 6
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_307_order_7",
              "label": "foot",
              "text": "7.7 Summary | 285",
              "level": -1,
              "page": 307,
              "reading_order": 7,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "7.7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_308_order_0",
              "label": "list_group",
              "text": "• Chunkers can be constructed using rule-based systems, such as the RegexpParser\nclass provided by NLTK; or using machine learning techniques, such as the\nConsecutiveNPChunker presented in this chapter. In either case, part-of-speech tags\nare often a very important feature when searching for chunks.\n• Although chunkers are specialized to create relatively flat data structures, where\nno two chunks are allowed to overlap, they can be cascaded together to build nested\nstructures.",
              "level": -1,
              "page": 308,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                585,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "• Chunkers can be constructed using rule-based systems, such as the RegexpParser\nclass provided by NLTK; or using machine learning techniques, such as the\nConsecutiveNPChunker presented in this chapter. In either case, part-of-speech tags\nare often a very important feature when searching for chunks.",
                  "bbox": [
                    100,
                    71,
                    585,
                    143
                  ],
                  "page": 308,
                  "reading_order": 0
                },
                {
                  "label": "list",
                  "text": "• Although chunkers are specialized to create relatively flat data structures, where\nno two chunks are allowed to overlap, they can be cascaded together to build nested\nstructures.",
                  "bbox": [
                    100,
                    143,
                    585,
                    190
                  ],
                  "page": 308,
                  "reading_order": 1
                },
                {
                  "label": "list",
                  "text": "• Relation extraction can be performed using either rule-based systems, which typ-\nically look for specific patterns in the text that connect entities and the intervening\nwords; or using machine-learning systems, which typically attempt to learn such\npatterns automatically from a training corpus.",
                  "bbox": [
                    100,
                    197,
                    585,
                    263
                  ],
                  "page": 308,
                  "reading_order": 2
                }
              ],
              "is_merged": true
            }
          ]
        },
        {
          "id": "page_308_order_3",
          "label": "sub_sec",
          "text": "7.8 Further Reading",
          "level": 2,
          "page": 308,
          "reading_order": 3,
          "bbox": [
            97,
            286,
            261,
            313
          ],
          "section_number": "7.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_308_order_4",
              "label": "para",
              "text": "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web. For more examples of chunking with NLTK,\nplease see the Chunking HOWTO at http://www.nltk.org/howto.",
              "level": -1,
              "page": 308,
              "reading_order": 4,
              "bbox": [
                97,
                313,
                584,
                368
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_308_order_5",
              "label": "para",
              "text": "The popularity of chunking is due in great part to pioneering work by Abney, e.g.,\n(Abney, 1996a). Abney’s Cass chunker is described in http://www.vinartus.net/spa/97a\n.pdf.",
              "level": -1,
              "page": 308,
              "reading_order": 5,
              "bbox": [
                97,
                376,
                584,
                430
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_308_order_6",
              "label": "para",
              "text": "The word chink initially meant a sequence of stopwords, according to a 1975 paper\nby Ross and Tukey (Abney, 1996a).",
              "level": -1,
              "page": 308,
              "reading_order": 6,
              "bbox": [
                100,
                430,
                585,
                465
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_308_order_7",
              "label": "para",
              "text": "The IOB format (or sometimes BIO Format) was developed for NP chunking by (Ram-\nshaw & Marcus, 1995), and was used for the shared NP bracketing task run by the\nConference on Natural Language Learning (CoNLL) in 1999. The same format was\nadopted by CoNLL 2000 for annotating a section of Wall Street Journal text as part of\na shared task on NP chunking.",
              "level": -1,
              "page": 308,
              "reading_order": 7,
              "bbox": [
                97,
                465,
                586,
                555
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_308_order_8",
              "label": "para",
              "text": "Section 13.5 of (Jurafsky & Martin, 2008) contains a discussion of chunking. Chapter\n22 covers information extraction, including named entity recognition. For information\nabout text mining in biology and medicine, see (Ananiadou & McNaught, 2006).",
              "level": -1,
              "page": 308,
              "reading_order": 8,
              "bbox": [
                91,
                555,
                585,
                611
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_308_order_9",
              "label": "para",
              "text": "For more information on the Getty and Alexandria gazetteers, see http://en.wikipedia\n.org/wiki/Getty_Thesaurus_of_Geographic_Names__and__http://www.alexandria.ucsb\n.edu/gazetteer/.",
              "level": -1,
              "page": 308,
              "reading_order": 9,
              "bbox": [
                97,
                618,
                585,
                672
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_308_order_10",
          "label": "sub_sec",
          "text": "7.9 Exercises",
          "level": 2,
          "page": 308,
          "reading_order": 10,
          "bbox": [
            97,
            689,
            207,
            716
          ],
          "section_number": "7.9",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_308_order_11",
              "label": "para",
              "text": "1. ◦ The IOB format categorizes tagged tokens as I, O, and B. Why are three tags\nnecessary? What problem would be caused if we used I and O tags exclusively?",
              "level": -1,
              "page": 308,
              "reading_order": 11,
              "bbox": [
                100,
                725,
                585,
                761
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_308_order_12",
              "label": "foot",
              "text": "286 | Chapter 7: Extracting Information from Text",
              "level": -1,
              "page": 308,
              "reading_order": 12,
              "bbox": [
                97,
                824,
                311,
                842
              ],
              "section_number": "286",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_0",
              "label": "para",
              "text": "2. ◦ Write a tag pattern to match noun phrases containing plural head nouns, e.g.,\nmany/JJ researchers/NNS, two/CD weeks/NNS, both/DT new/JJ positions/NNS. Try\nto do this by generalizing the tag pattern that handled singular noun phrases.",
              "level": -1,
              "page": 309,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                585,
                125
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_1",
              "label": "para",
              "text": "3. ◦ Pick one of the three chunk types in the CoNLL-2000 Chunking Corpus. Inspect\nthe data and try to observe any patterns in the POS tag sequences that make up\nthis kind of chunk. Develop a simple chunker using the regular expression chunker\nnltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably.",
              "level": -1,
              "page": 309,
              "reading_order": 1,
              "bbox": [
                100,
                125,
                585,
                197
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_2",
              "label": "para",
              "text": "4. ◦ An early definition of chunk was the material that occurs between chinks. De-\nvelop a chunker that starts by putting the whole sentence in a single chunk, and\nthen does the rest of its work solely by chinking. Determine which tags (or tag\nsequences) are most likely to make up chinks with the help of your own utility\nprogram. Compare the performance and simplicity of this approach relative to a\nchunker based entirely on chunk rules.",
              "level": -1,
              "page": 309,
              "reading_order": 2,
              "bbox": [
                100,
                197,
                585,
                296
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_3",
              "label": "para",
              "text": "5. o Write a tag pattern to cover noun phrases that contain gerunds, e.g., the/DT\nreceiving/VBG end/NN, assistant/NN managing/VBG editor/NN. Add these patterns\nto the grammar, one per line. Test your work using some tagged sentences of your\nown devising.",
              "level": -1,
              "page": 309,
              "reading_order": 3,
              "bbox": [
                100,
                303,
                585,
                367
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_4",
              "label": "para",
              "text": "6. o Write one or more tag patterns to handle coordinated noun phrases, e.g., July/\nNNP and/CC August/NNP, all/DT your/PRP$ managers/NNS and/CC supervisors/NNS,\ncompany/NN courts/NNS and/CC adjudicators/NNS.",
              "level": -1,
              "page": 309,
              "reading_order": 4,
              "bbox": [
                100,
                367,
                584,
                421
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_5",
              "label": "para",
              "text": "7. • Carry out the following evaluation tasks for any of the chunkers you have de-\nveloped earlier. (Note that most chunking corpora contain some internal incon-\nsistencies, such that any reasonable rule-based approach will produce errors.)",
              "level": -1,
              "page": 309,
              "reading_order": 5,
              "bbox": [
                100,
                421,
                584,
                474
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_6",
              "label": "para",
              "text": "a. Evaluate your chunker on 100 sentences from a chunked corpus, and report\nthe precision, recall, and F-measure.",
              "level": -1,
              "page": 309,
              "reading_order": 6,
              "bbox": [
                126,
                474,
                584,
                511
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_7",
              "label": "para",
              "text": "b. Use the chunkscore.missed() and chunkscore.incorrect() methods to identify\nthe errors made by your chunker. Discuss.",
              "level": -1,
              "page": 309,
              "reading_order": 7,
              "bbox": [
                126,
                518,
                585,
                548
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_8",
              "label": "para",
              "text": "c. Compare the performance of your chunker to the baseline chunker discussed\nin the evaluation section of this chapter.",
              "level": -1,
              "page": 309,
              "reading_order": 8,
              "bbox": [
                126,
                555,
                585,
                586
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_9",
              "label": "para",
              "text": "8. o Develop a chunker for one of the chunk types in the CoNLL Chunking Corpus\nusing a regular expression–based chunk grammar RegexpChunk. Use any combina-\ntion of rules for chunking, chinking, merging, or splitting.",
              "level": -1,
              "page": 309,
              "reading_order": 9,
              "bbox": [
                100,
                591,
                584,
                645
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_10",
              "label": "para",
              "text": "9. o Sometimes a word is incorrectly tagged, e.g., the head noun in 12/CD or/CC so/\nRB cases/VBZ. Instead of requiring manual correction of tagger output, good\nchunkers are able to work with the erroneous output of taggers. Look for other\nexamples of correctly chunked noun phrases with incorrect tags.",
              "level": -1,
              "page": 309,
              "reading_order": 10,
              "bbox": [
                100,
                645,
                585,
                710
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_11",
              "label": "para",
              "text": "10. o The bigram chunker scores about 90% accuracy. Study its errors and try to work\nout why it doesn’t get 100% accuracy. Experiment with trigram chunking. Are you\nable to improve the performance any more?",
              "level": -1,
              "page": 309,
              "reading_order": 11,
              "bbox": [
                100,
                716,
                585,
                764
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_309_order_12",
              "label": "foot",
              "text": "7.9 Exercises | 287",
              "level": -1,
              "page": 309,
              "reading_order": 12,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "7.9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_0",
              "label": "para",
              "text": "11. • Apply the n-gram and Brill tagging methods to IOB chunk tagging. Instead of\nassigning POS tags to words, here we will assign IOB tags to the POS tags. E.g., if\nthe tag DT (determiner) often occurs at the start of a chunk, it will be tagged B\n(begin). Evaluate the performance of these chunking methods relative to the regular\nexpression chunking methods covered in this chapter.",
              "level": -1,
              "page": 310,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                586,
                155
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_1",
              "label": "para",
              "text": "12. • We saw in Chapter 5 that it is possible to establish an upper limit to tagging\nperformance by looking for ambiguous n-grams, which are n-grams that are tagged\nin more than one possible way in the training data. Apply the same method to\ndetermine an upper bound on the performance of an n-gram chunker.",
              "level": -1,
              "page": 310,
              "reading_order": 1,
              "bbox": [
                100,
                161,
                585,
                226
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_2",
              "label": "para",
              "text": "13. • Pick one of the three chunk types in the CoNLL Chunking Corpus. Write func-\ntions to do the following tasks for your chosen type:",
              "level": -1,
              "page": 310,
              "reading_order": 2,
              "bbox": [
                100,
                232,
                585,
                263
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_3",
              "label": "para",
              "text": "a. List all the tag sequences that occur with each instance of this chunk type",
              "level": -1,
              "page": 310,
              "reading_order": 3,
              "bbox": [
                126,
                268,
                566,
                286
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_4",
              "label": "para",
              "text": "b. Count the frequency of each tag sequence, and produce a ranked list in order\nof decreasing frequency; each line should consist of an integer (the frequency)\nand the tag sequence.",
              "level": -1,
              "page": 310,
              "reading_order": 4,
              "bbox": [
                126,
                286,
                585,
                340
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_5",
              "label": "para",
              "text": "c. Inspect the high-frequency tag sequences. Use these as the basis for developing\na better chunker.",
              "level": -1,
              "page": 310,
              "reading_order": 5,
              "bbox": [
                126,
                340,
                585,
                376
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_6",
              "label": "para",
              "text": "14. • The baseline chunker presented in the evaluation section tends to create larger\nchunks than it should. For example, the phrase [every/DT time/NN] [she/PRP]\nsees/VBZ [a/DT newspaper/NN] contains two consecutive chunks, and our baseline\nchunker will incorrectly combine the first two: [every/DT time/NN she/PRP] . Write\na program that finds which of these chunk-internal tags typically occur at the start\nof a chunk, then devise one or more rules that will split up these chunks. Combine\nthese with the existing baseline chunker and re-evaluate it, to see if you have dis-\ncovered an improved baseline.",
              "level": -1,
              "page": 310,
              "reading_order": 6,
              "bbox": [
                100,
                376,
                585,
                511
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_7",
              "label": "para",
              "text": "15. • Develop an NP chunker that converts POS tagged text into a list of tuples, where\neach tuple consists of a verb followed by a sequence of noun phrases and prepo-\nsitions, e.g., the little cat sat on the mat becomes ('sat', 'on', 'NP')...",
              "level": -1,
              "page": 310,
              "reading_order": 7,
              "bbox": [
                100,
                518,
                585,
                565
              ],
              "section_number": "15",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_8",
              "label": "para",
              "text": "16. • The Penn Treebank Corpus sample contains a section of tagged Wall Street\nJournal text that has been chunked into noun phrases. The format uses square\nbrackets, and we have encountered it several times in this chapter. The corpus can\nbe accessed using: for sent in nltk.corpus.treebank_chunk.chunked_sents(fil\neid) . These are flat trees, just as we got using nltk.cor\npus.conll2000.chunked_sents() .",
              "level": -1,
              "page": 310,
              "reading_order": 8,
              "bbox": [
                100,
                572,
                585,
                672
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_9",
              "label": "para",
              "text": "a. The functions nltk.tree.pprint() and nltk.chunk.tree2conllstr() can be\nused to create Treebank and IOB strings from a tree. Write functions\nchunk2brackets() and chunk2iob() that take a single chunk tree as their sole\nargument, and return the required multiline string representation.",
              "level": -1,
              "page": 310,
              "reading_order": 9,
              "bbox": [
                126,
                672,
                585,
                743
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_10",
              "label": "para",
              "text": "b. Write command-line conversion utilities bracket2iob.py and iob2bracket.py\nthat take a file in Treebank or CoNLL format (respectively) and convert it to\nthe other format. (Obtain some raw Treebank or CoNLL data from the NLTK",
              "level": -1,
              "page": 310,
              "reading_order": 10,
              "bbox": [
                126,
                743,
                585,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_310_order_11",
              "label": "foot",
              "text": "288 | Chapter 7: Extracting Information from Text",
              "level": -1,
              "page": 310,
              "reading_order": 11,
              "bbox": [
                97,
                824,
                315,
                842
              ],
              "section_number": "288",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_311_order_0",
              "label": "para",
              "text": "Corpora, save it to a file, and then use for line in open(filename) to access\nit from Python.)",
              "level": -1,
              "page": 311,
              "reading_order": 0,
              "bbox": [
                144,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_311_order_1",
              "label": "para",
              "text": "17. • An n-gram chunker can use information other than the current part-of-speech\ntag and the n- 1 previous chunk tags. Investigate other models of the context, such\nas the n- 1 previous part-of-speech tags, or some combination of previous chunk\ntags along with previous and following part-of-speech tags.",
              "level": -1,
              "page": 311,
              "reading_order": 1,
              "bbox": [
                100,
                107,
                585,
                179
              ],
              "section_number": "17",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_311_order_2",
              "label": "para",
              "text": "18. • Consider the way an n-gram tagger uses recent tags to inform its tagging choice.\nNow observe how a chunker may reuse this sequence information. For example,\nboth tasks will make use of the information that nouns tend to follow adjectives\n(in English). It would appear that the same information is being maintained in two\nplaces. Is this likely to become a problem as the size of the rule sets grows? If so,\nspeculate about any ways that this problem might be addressed.",
              "level": -1,
              "page": 311,
              "reading_order": 2,
              "bbox": [
                100,
                179,
                585,
                279
              ],
              "section_number": "18",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_311_order_3",
              "label": "foot",
              "text": "7.9 Exercises | 289",
              "level": -1,
              "page": 311,
              "reading_order": 3,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "7.9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_312_order_0",
              "label": "para",
              "text": "_",
              "level": -1,
              "page": 312,
              "reading_order": 0,
              "bbox": [
                153,
                161,
                494,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_302_order_9",
          "label": "para",
          "text": "It is standard to use a recursive function to traverse a tree. The listing in Example 7-7\ndemonstrates this.",
          "level": -1,
          "page": 302,
          "reading_order": 9,
          "bbox": [
            97,
            624,
            583,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_302_order_10",
          "label": "para",
          "text": "Example 7-7. A recursive function to traverse a tree",
          "level": -1,
          "page": 302,
          "reading_order": 10,
          "bbox": [
            97,
            669,
            351,
            682
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_302_order_13",
          "label": "foot",
          "text": "280 | Chapter 7: Extracting Information from Text",
          "level": -1,
          "page": 302,
          "reading_order": 13,
          "bbox": [
            97,
            824,
            311,
            842
          ],
          "section_number": "280",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_303_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_303_figure_001.png)",
          "level": -1,
          "page": 303,
          "reading_order": 1,
          "bbox": [
            109,
            197,
            171,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_303_order_2",
          "label": "para",
          "text": "We have used a technique called duck typing to detect that t is a tree\n(i.e., t.node is defined).",
          "level": -1,
          "page": 303,
          "reading_order": 2,
          "bbox": [
            171,
            213,
            530,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_313_order_0",
      "label": "sec",
      "text": "CHAPTER 8\n\nAnalyzing Sentence Structure",
      "level": 1,
      "page": 313,
      "reading_order": 0,
      "bbox": [
        207,
        71,
        584,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_314_order_0",
          "label": "sub_sec",
          "text": "8.1 Some Grammatical Dilemmas",
          "level": 2,
          "page": 314,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            368,
            98
          ],
          "section_number": "8.1",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_314_order_1",
              "label": "sub_sub_sec",
              "text": "Linguistic Data and Unlimited Possibilities",
              "level": 3,
              "page": 314,
              "reading_order": 1,
              "bbox": [
                98,
                107,
                377,
                134
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_314_order_2",
                  "label": "para",
                  "text": "Previous chapters have shown you how to process and analyze text corpora, and we\nhave stressed the challenges for NLP in dealing with the vast amount of electronic\nlanguage data that is growing daily. Let’s consider this data more closely, and make the\nthought experiment that we have a gigantic corpus consisting of everything that has\nbeen either uttered or written in English over, say, the last 50 years. Would we be\njustified in calling this corpus “the language of modern English”? There are a number\nof reasons why we might answer no. Recall that in Chapter 3 , we asked you to search\nthe Web for instances of the pattern the of . Although it is easy to find examples on the\nWeb containing this word sequence, such as New man at the of IMG (see http://www-\n.telegraph.co.uk/sport/2387900/New-man-at-the-of-IMG.html ), speakers of English\nwill say that most such examples are errors, and therefore not part of English after all.",
                  "level": -1,
                  "page": 314,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    134,
                    585,
                    323
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_3",
                  "label": "para",
                  "text": "Accordingly, we can argue that “modern English” is not equivalent to the very big set\nof word sequences in our imaginary corpus. Speakers of English can make judgments\nabout these sequences, and will reject some of them as being ungrammatical.",
                  "level": -1,
                  "page": 314,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    331,
                    585,
                    379
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_4",
                  "label": "para",
                  "text": "Equally, it is easy to compose a new sentence and have speakers agree that it is perfectly\ngood English. For example, sentences have an interesting property that they can be\nembedded inside larger sentences. Consider the following sentences:",
                  "level": -1,
                  "page": 314,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    385,
                    585,
                    439
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_5",
                  "label": "para",
                  "text": "(1) a. Usain Bolt broke the $100 \\mathrm{~m}$ record",
                  "level": -1,
                  "page": 314,
                  "reading_order": 5,
                  "bbox": [
                    118,
                    448,
                    360,
                    465
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_6",
                  "label": "para",
                  "text": "b. The Jamaica Observer reported that Usain Bolt broke the $100 \\mathrm{~m}$ record.",
                  "level": -1,
                  "page": 314,
                  "reading_order": 6,
                  "bbox": [
                    144,
                    465,
                    574,
                    485
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_7",
                  "label": "para",
                  "text": "c. Andre said The Jamaica Observer reported that Usain Bolt broke the 100m\nrecord.",
                  "level": -1,
                  "page": 314,
                  "reading_order": 7,
                  "bbox": [
                    144,
                    492,
                    584,
                    519
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_8",
                  "label": "para",
                  "text": "d. I think Andre said the Jamaica Observer reported that Usain Bolt broke\nthe 100m record.",
                  "level": -1,
                  "page": 314,
                  "reading_order": 8,
                  "bbox": [
                    144,
                    528,
                    574,
                    556
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_9",
                  "label": "para",
                  "text": "If we replaced whole sentences with the symbol S, we would see patterns like Andre\nsaid S and I think S. These are templates for taking a sentence and constructing a bigger\nsentence. There are other templates we can use, such as S but S and S when S. With a\nbit of ingenuity we can construct some really long sentences using these templates.\nHere's an impressive example from a Winnie the Pooh story by A.A. Milne, In Which\nPiglet Is Entirely Surrounded by Water:",
                  "level": -1,
                  "page": 314,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    573,
                    585,
                    672
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_10",
                  "label": "para",
                  "text": "[You can imagine Piglet's joy when at last the ship came in sight of him.] In after-years\nhe liked to think that he had been in Very Great Danger during the Terrible Flood, but\nthe only danger he had really been in was the last half-hour of his imprisonment, when\nOwl, who had just flown up, sat on a branch of his tree to comfort him, and told him a\nvery long story about an aunt who had once laid a seagull’s egg by mistake, and the story\nwent on and on, rather like this sentence, until Piglet who was listening out of his window\nwithout much hope, went to sleep quietly and naturally, slipping slowly out of the win-\ndow towards the water until he was only hanging on by his toes, at which moment,",
                  "level": -1,
                  "page": 314,
                  "reading_order": 10,
                  "bbox": [
                    118,
                    680,
                    560,
                    797
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_314_order_11",
                  "label": "foot",
                  "text": "292 | Chapter8: Analyzing Sentence Structure",
                  "level": -1,
                  "page": 314,
                  "reading_order": 11,
                  "bbox": [
                    97,
                    824,
                    297,
                    842
                  ],
                  "section_number": "292",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_315_order_0",
                  "label": "para",
                  "text": "\"Household, I'm not sure to get a good thing for you and make it more\nsurely, I can tell me that if we have done this one? You know what is your mommy father!\n\"I don't want my mother... And then they are going on here with him when he was doing so much like her baby but she gotta do all right now. But how did you hear about us just be very bad news there were no money at home where you could see any other people who would say of them because I didn't think you shouldn't really give up somebody else or something better than you ever had been getting out again before you come back into a bedroom. It wasn't actually part of the story, being what\nthis aunt said, woke the Piglet up and just gave him time to jerk himself back into safety\nand say, “How interesting, and did she?” when—well, you can imagine his joy when at\nlast he saw the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear) coming\nover the sea to rescue him...",
                  "level": -1,
                  "page": 315,
                  "reading_order": 0,
                  "bbox": [
                    124,
                    71,
                    561,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_315_order_1",
                  "label": "para",
                  "text": "This long sentence actually has a simple structure that begins S but S when S. We can\nsee from this example that language provides us with constructions which seem to allow\nus to extend sentences indefinitely. It is also striking that we can understand sentences\nof arbitrary length that we’ve never heard before: it’s not hard to concoct an entirely\nnovel sentence, one that has probably never been used before in the history of the\nlanguage, yet all speakers of the language will understand it.",
                  "level": -1,
                  "page": 315,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    152,
                    585,
                    252
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_315_order_2",
                  "label": "para",
                  "text": "The purpose of a grammar is to give an explicit description of a language. But the way\nin which we think of a grammar is closely intertwined with what we consider to be a\nlanguage. Is it a large but finite set of observed utterances and written texts? Is it some-\nthing more abstract like the implicit knowledge that competent speakers have about\ngrammatical sentences? Or is it some combination of the two? We won’t take a stand\non this issue, but instead will introduce the main approaches.",
                  "level": -1,
                  "page": 315,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    259,
                    585,
                    358
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_315_order_3",
                  "label": "para",
                  "text": "In this chapter, we will adopt the formal framework of “ generative grammar, ” in which\na “ language ” is considered to be nothing more than an enormous collection of all\ngrammatical sentences, and a grammar is a formal notation that can be used for “ gen-\nerating ” the members of this set. Grammars use recursive productions of the form\n$\\ensuremath{\\mathsf{S}}\\xspace\\rightarrow \\ensuremath{\\mathsf{S}}\\xspace$ and $\\ensuremath{\\mathsf{S}}\\xspace$ , as we will explore in Section 8.3 . In Chapter 10 we will extend this, to\nautomatically build up the meaning of a sentence out of the meanings of its parts.",
                  "level": -1,
                  "page": 315,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    367,
                    585,
                    465
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_313_order_1",
          "label": "para",
          "text": "Earlier chapters focused on words: how to identify them, analyze their structure, assign\nthem to lexical categories, and access their meanings. We have also seen how to identify\npatterns in word sequences or n-grams. However, these methods only scratch the sur-\nface of the complex constraints that govern sentences. We need a way to deal with the\nambiguity that natural language is famous for. We also need to be able to cope with\nthe fact that there are an unlimited number of possible sentences, and we can only write\nfinite programs to analyze their structures and discover their meanings.",
          "level": -1,
          "page": 313,
          "reading_order": 1,
          "bbox": [
            97,
            286,
            585,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_313_order_2",
          "label": "para",
          "text": "The goal of this chapter is to answer the following questions",
          "level": -1,
          "page": 313,
          "reading_order": 2,
          "bbox": [
            100,
            412,
            441,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_313_order_3",
          "label": "list_group",
          "text": "1. How can we use a formal grammar to describe the structure of an unlimited set of\nsentences?\n2. How do we represent the structure of sentences using syntax trees?",
          "level": -1,
          "page": 313,
          "reading_order": 3,
          "bbox": [
            100,
            430,
            586,
            465
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "1. How can we use a formal grammar to describe the structure of an unlimited set of\nsentences?",
              "bbox": [
                100,
                430,
                586,
                465
              ],
              "page": 313,
              "reading_order": 3
            },
            {
              "label": "list",
              "text": "2. How do we represent the structure of sentences using syntax trees?",
              "bbox": [
                100,
                474,
                503,
                492
              ],
              "page": 313,
              "reading_order": 4
            },
            {
              "label": "list",
              "text": "3. How do parsers analyze a sentence and automatically build a syntax tree?",
              "bbox": [
                100,
                492,
                540,
                510
              ],
              "page": 313,
              "reading_order": 5
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_313_order_6",
          "label": "para",
          "text": "Along the way, we will cover the fundamentals of English syntax, and see that there\nare systematic aspects of meaning that are much easier to capture once we have iden-\ntified the structure of sentences.",
          "level": -1,
          "page": 313,
          "reading_order": 6,
          "bbox": [
            97,
            519,
            585,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_313_order_7",
          "label": "foot",
          "text": "291",
          "level": -1,
          "page": 313,
          "reading_order": 7,
          "bbox": [
            566,
            824,
            584,
            837
          ],
          "section_number": "291",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_315_order_4",
      "label": "sec",
      "text": "Ubiquitous Ambiguity",
      "level": 1,
      "page": 315,
      "reading_order": 4,
      "bbox": [
        100,
        480,
        244,
        501
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_317_order_4",
          "label": "sub_sec",
          "text": "8.2 What's the Use of Syntax?",
          "level": 2,
          "page": 317,
          "reading_order": 4,
          "bbox": [
            97,
            421,
            335,
            444
          ],
          "section_number": "8.2",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_317_order_5",
              "label": "sub_sub_sec",
              "text": "Beyond n-grams",
              "level": 3,
              "page": 317,
              "reading_order": 5,
              "bbox": [
                98,
                456,
                207,
                478
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_317_order_6",
                  "label": "para",
                  "text": "We gave an example in Chapter 2 of how to use the frequency information in bigrams\nto generate text that seems perfectly acceptable for small sequences of words but rapidly\ndegenerates into nonsense. Here’s another pair of examples that we created by com-\nputing the bigrams over the text of a children’s story, The Adventures of Buster\nBrown (included in the Project Gutenberg Selection Corpus):",
                  "level": -1,
                  "page": 317,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    483,
                    585,
                    567
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_317_order_7",
                  "label": "para",
                  "text": "(4) a. He roared with me the pail slip down his back\nb. The worst part and clumsy looking for whoeve",
                  "level": -1,
                  "page": 317,
                  "reading_order": 7,
                  "bbox": [
                    118,
                    582,
                    433,
                    618
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_317_order_8",
                  "label": "para",
                  "text": "You intuitively know that these sequences are “ word-salad, ” but you probably find it\nhard to pin down what's wrong with them. One benefit of studying grammar is that it\nprovides a conceptual framework and vocabulary for spelling out these intuitions. Let's\ntake a closer look at the sequence the worst part and clumsy looking . This looks like a\ncoordinate structure , where two phrases are joined by a coordinating conjunction\nsuch as and , but , or or . Here's an informal (and simplified) statement of how coordi-\nnation works syntactically:",
                  "level": -1,
                  "page": 317,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    627,
                    585,
                    743
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_317_order_9",
                  "label": "para",
                  "text": "Coordinate Structure: if $v_{1}\\!\\,$ and $v_{2}\\!\\,$ are both phrases of grammatical category X, then $v_{1}\\!\\,$\nand $v_{2}\\!\\,$ is also a phrase of category X.",
                  "level": -1,
                  "page": 317,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    752,
                    583,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_317_order_10",
                  "label": "foot",
                  "text": "8.2 What's the Use of Syntax? | 295",
                  "level": -1,
                  "page": 317,
                  "reading_order": 10,
                  "bbox": [
                    431,
                    824,
                    585,
                    842
                  ],
                  "section_number": "8.2",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_0",
                  "label": "para",
                  "text": "Here are a couple of examples. In the first, two NPs (noun phrases) have been conjoined\nto make an NP , while in the second, two APs (adjective phrases) have been conjoined to\nmake an AP .",
                  "level": -1,
                  "page": 318,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    584,
                    119
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_1",
                  "label": "para",
                  "text": "(5) a. The book's ending was (NP the worst part and the best part) for me.",
                  "level": -1,
                  "page": 318,
                  "reading_order": 1,
                  "bbox": [
                    118,
                    134,
                    557,
                    152
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_2",
                  "label": "para",
                  "text": "b. On land they are (AP slow and clumsy looking).",
                  "level": -1,
                  "page": 318,
                  "reading_order": 2,
                  "bbox": [
                    144,
                    152,
                    440,
                    172
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_3",
                  "label": "para",
                  "text": "What we can’t do is conjoin an NP and an AP, which is why the worst part and clumsy\nlooking is ungrammatical. Before we can formalize these ideas, we need to understand\nthe concept of constituent structure.",
                  "level": -1,
                  "page": 318,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    179,
                    585,
                    233
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_4",
                  "label": "para",
                  "text": "Constituent structure is based on the observation that words combine with other words\nto form units. The evidence that a sequence of words forms such a unit is given by\nsubstitutability—that is, a sequence of words in a well-formed sentence can be replaced\nby a shorter sequence without rendering the sentence ill-formed. To clarify this idea,\nconsider the following sentence:",
                  "level": -1,
                  "page": 318,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    241,
                    585,
                    323
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_5",
                  "label": "para",
                  "text": "(6) The little bear saw the fine fat trout in the brook",
                  "level": -1,
                  "page": 318,
                  "reading_order": 5,
                  "bbox": [
                    118,
                    331,
                    418,
                    350
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_6",
                  "label": "para",
                  "text": "The fact that we can substitute He for The little bear indicates that the latter sequence\nis a unit. By contrast, we cannot replace little bear saw in the same way. (We use an\nasterisk at the start of a sentence to indicate that it is ungrammatical.)",
                  "level": -1,
                  "page": 318,
                  "reading_order": 6,
                  "bbox": [
                    100,
                    367,
                    585,
                    414
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_7",
                  "label": "para",
                  "text": "(7) a. He saw the fine fat trout in the brook.",
                  "level": -1,
                  "page": 318,
                  "reading_order": 7,
                  "bbox": [
                    118,
                    429,
                    386,
                    441
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_8",
                  "label": "para",
                  "text": "b.\n*The he the fine fat trout in the brook.",
                  "level": -1,
                  "page": 318,
                  "reading_order": 8,
                  "bbox": [
                    150,
                    448,
                    386,
                    465
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_9",
                  "label": "para",
                  "text": "In Figure 8-1, we systematically substitute longer sequences by shorter ones in a way\nwhich preserves grammaticality. Each sequence that forms a unit can in fact be replaced\nby a single word, and we end up with just two elements.",
                  "level": -1,
                  "page": 318,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    474,
                    585,
                    528
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_318_order_10",
                  "label": "table",
                  "text": "Figure 8-1. Substitution of word sequences: Working from the top row, we can replace particular\nsequences of words (e.g., the brook) with individual words (e.g., it); repeating this process, we arrive\nat a grammatical two-word sentence. [TABLE: <table><tr><td>the</td><td>little bear</td><td>saw</td><td>the</td><td>fine fat</td><td>trout</td><td>in</td><td>the</td><td>brook</td></tr><tr><td rowspan=\"4\">the</td><td>ear</td><td>saw</td><td>the</td><td colspan=\"2\">trout</td><td>in</td><td colspan=\"2\">it</td></tr><tr><td>He</td><td>saw</td><td colspan=\"4\">it</td><td colspan=\"2\">there</td></tr><tr><td>He</td><td colspan=\"5\">ran</td><td colspan=\"2\">there</td></tr><tr><td>He</td><td colspan=\"8\">ran</td></tr></table>]",
                  "level": -1,
                  "page": 318,
                  "reading_order": 10,
                  "bbox": [
                    100,
                    555,
                    583,
                    716
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "tab",
                      "text": "<table><tr><td>the</td><td>little bear</td><td>saw</td><td>the</td><td>fine fat</td><td>trout</td><td>in</td><td>the</td><td>brook</td></tr><tr><td rowspan=\"4\">the</td><td>ear</td><td>saw</td><td>the</td><td colspan=\"2\">trout</td><td>in</td><td colspan=\"2\">it</td></tr><tr><td>He</td><td>saw</td><td colspan=\"4\">it</td><td colspan=\"2\">there</td></tr><tr><td>He</td><td colspan=\"5\">ran</td><td colspan=\"2\">there</td></tr><tr><td>He</td><td colspan=\"8\">ran</td></tr></table>",
                      "bbox": [
                        100,
                        555,
                        583,
                        716
                      ],
                      "page": 318,
                      "reading_order": 10
                    },
                    {
                      "label": "cap",
                      "text": "Figure 8-1. Substitution of word sequences: Working from the top row, we can replace particular\nsequences of words (e.g., the brook) with individual words (e.g., it); repeating this process, we arrive\nat a grammatical two-word sentence.",
                      "bbox": [
                        97,
                        725,
                        585,
                        771
                      ],
                      "page": 318,
                      "reading_order": 11
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_318_order_12",
                  "label": "foot",
                  "text": "296 | Chapter 8: Analyzing Sentence Structure",
                  "level": -1,
                  "page": 318,
                  "reading_order": 12,
                  "bbox": [
                    97,
                    824,
                    297,
                    842
                  ],
                  "section_number": "296",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_319_order_0",
                  "label": "figure",
                  "text": "Figure 8-2. Substitution of word sequences plus grammatical categories: This diagram reproduces\nFigure 8-1 along with grammatical categories corresponding to noun phrases ( NP ), verb phrases\n( VP ), prepositional phrases ( PP ), and nominals ( Nom ). [IMAGE: ![Figure](figures/NLTK_page_319_figure_000.png)]",
                  "level": -1,
                  "page": 319,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    583,
                    241
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_319_figure_000.png)",
                      "bbox": [
                        100,
                        71,
                        583,
                        241
                      ],
                      "page": 319,
                      "reading_order": 0
                    },
                    {
                      "label": "cap",
                      "text": "Figure 8-2. Substitution of word sequences plus grammatical categories: This diagram reproduces\nFigure 8-1 along with grammatical categories corresponding to noun phrases ( NP ), verb phrases\n( VP ), prepositional phrases ( PP ), and nominals ( Nom ).",
                      "bbox": [
                        97,
                        241,
                        584,
                        289
                      ],
                      "page": 319,
                      "reading_order": 1
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_319_order_2",
                  "label": "para",
                  "text": "In Figure 8-2 , we have added grammatical category labels to the words we saw in the\nearlier figure. The labels NP , VP , and PP stand for noun phrase , verb phrase , and\nprepositional phrase , respectively.",
                  "level": -1,
                  "page": 319,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    295,
                    585,
                    349
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_319_order_3",
                  "label": "para",
                  "text": "If we now strip out the words apart from the topmost row, add an S node, and flip the\nfigure over, we end up with a standard phrase structure tree, shown in (8) . Each node\nin this tree (including the words) is called a constituent. The immediate constitu-\nents of S are NP and VP.",
                  "level": -1,
                  "page": 319,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    356,
                    585,
                    421
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_319_order_4",
                  "label": "figure",
                  "text": "As we saw in Section 8.1 , sentences can have arbitrary length. Conse-\nquently, phrase structure trees can have arbitrary depth . The cascaded\nchunk parsers we saw in Section 7.4 can only produce structures of\nbounded depth, so chunking methods aren't applicable here. [IMAGE: ![Figure](figures/NLTK_page_319_figure_004.png)]",
                  "level": -1,
                  "page": 319,
                  "reading_order": 4,
                  "bbox": [
                    118,
                    430,
                    512,
                    654
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_319_figure_004.png)",
                      "bbox": [
                        118,
                        430,
                        512,
                        654
                      ],
                      "page": 319,
                      "reading_order": 4
                    },
                    {
                      "label": "cap",
                      "text": "As we saw in Section 8.1 , sentences can have arbitrary length. Conse-\nquently, phrase structure trees can have arbitrary depth . The cascaded\nchunk parsers we saw in Section 7.4 can only produce structures of\nbounded depth, so chunking methods aren't applicable here.",
                      "bbox": [
                        171,
                        680,
                        530,
                        743
                      ],
                      "page": 319,
                      "reading_order": 5
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_319_order_6",
                  "label": "foot",
                  "text": "8.2 What's the Use of Syntax? | 297",
                  "level": -1,
                  "page": 319,
                  "reading_order": 6,
                  "bbox": [
                    431,
                    824,
                    585,
                    842
                  ],
                  "section_number": "8.2",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_320_order_0",
                  "label": "para",
                  "text": "As we will see in the next section, a grammar specifies how the sentence can be subdi-\nvided into its immediate constituents, and how these can be further subdivided until\nwe reach the level of individual words.",
                  "level": -1,
                  "page": 320,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        },
        {
          "id": "page_320_order_1",
          "label": "sub_sec",
          "text": "8.3 Context-Free Grammar",
          "level": 2,
          "page": 320,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            315,
            170
          ],
          "section_number": "8.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_320_order_2",
              "label": "sub_sub_sec",
              "text": "A Simple Grammar",
              "level": 3,
              "page": 320,
              "reading_order": 2,
              "bbox": [
                97,
                179,
                225,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_320_order_3",
                  "label": "para",
                  "text": "Let’s start off by looking at a simple context-free grammar (CFG). By convention,\nthe lefthand side of the first production is the start-symbol of the grammar, typically\n5, and all well-formed trees must have this symbol as their root label. In NLTK, context­\nfree grammars are defined in the nltk.grammar module. In Example 8­1 we define a\ngrammar and show how to parse a simple sentence admitted by the grammar.",
                  "level": -1,
                  "page": 320,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    206,
                    585,
                    295
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_320_order_4",
                  "label": "para",
                  "text": "Example 8-1. A simple context-free grammar.",
                  "level": -1,
                  "page": 320,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    304,
                    324,
                    322
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_320_order_7",
                  "label": "para",
                  "text": "The grammar in Example 8-1 contains productions involving various syntactic cate-\ngories, as laid out in Table 8-1 . The recursive descent parser used here can also be\ninspected via a graphical interface, as illustrated in Figure 8-3 ; we discuss this parser\nin more detail in Section 8.4 .",
                  "level": -1,
                  "page": 320,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    546,
                    585,
                    609
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_320_order_8",
                  "label": "table",
                  "text": "Table 8-1. Syntactic categorie [TABLE: <table><tr><td>Symbol</td><td>Meaning</td><td>Example</td></tr><tr><td>S</td><td>sentence</td><td>the man walked</td></tr><tr><td>NP</td><td>noun phrase</td><td>a dog</td></tr><tr><td>VP</td><td>verb phrase</td><td>saw a park</td></tr><tr><td>PP</td><td>prepositional phrase</td><td>with a telescope</td></tr><tr><td>Det</td><td>determiner</td><td>the</td></tr><tr><td>N</td><td>noun</td><td>dog</td></tr></table>]",
                  "level": -1,
                  "page": 320,
                  "reading_order": 8,
                  "bbox": [
                    100,
                    645,
                    306,
                    781
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "tab",
                      "text": "<table><tr><td>Symbol</td><td>Meaning</td><td>Example</td></tr><tr><td>S</td><td>sentence</td><td>the man walked</td></tr><tr><td>NP</td><td>noun phrase</td><td>a dog</td></tr><tr><td>VP</td><td>verb phrase</td><td>saw a park</td></tr><tr><td>PP</td><td>prepositional phrase</td><td>with a telescope</td></tr><tr><td>Det</td><td>determiner</td><td>the</td></tr><tr><td>N</td><td>noun</td><td>dog</td></tr></table>",
                      "bbox": [
                        100,
                        645,
                        306,
                        781
                      ],
                      "page": 320,
                      "reading_order": 8
                    },
                    {
                      "label": "cap",
                      "text": "Table 8-1. Syntactic categorie",
                      "bbox": [
                        99,
                        625,
                        243,
                        638
                      ],
                      "page": 320,
                      "reading_order": 9
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_320_order_10",
                  "label": "foot",
                  "text": "298 | Chapter 8: Analyzing Sentence Structure",
                  "level": -1,
                  "page": 320,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    297,
                    842
                  ],
                  "section_number": "298",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_321_order_0",
                  "label": "tab",
                  "text": "<table><tr><td>Symbol</td><td>Meaning</td><td>Example</td></tr><tr><td>V</td><td>ver</td><td>walked</td></tr><tr><td>P</td><td>preposition</td><td>n</td></tr></table>",
                  "level": -1,
                  "page": 321,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    315,
                    134
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_321_order_1",
                  "label": "para",
                  "text": "A production like VP -> V NP | V NP PP has a disjunction on the righthand side, shown\nby the |, and is an abbreviation for the two productions VP -> V NP and VP -> V NP PP.",
                  "level": -1,
                  "page": 321,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    152,
                    584,
                    188
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_321_order_2",
                  "label": "para",
                  "text": "If we parse the sentence The dog saw a man in the park using the grammar shown in\nExample 8-1, we end up with two trees, similar to those we saw for (3):",
                  "level": -1,
                  "page": 321,
                  "reading_order": 2,
                  "bbox": [
                    98,
                    194,
                    584,
                    225
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_321_order_3",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_321_figure_003.png)",
                  "level": -1,
                  "page": 321,
                  "reading_order": 3,
                  "bbox": [
                    118,
                    241,
                    458,
                    672
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_321_order_4",
                  "label": "para",
                  "text": "Since our grammar licenses two trees for this sentence, the sentence is said to be struc-\nturally ambiguous . The ambiguity in question is called a prepositional phrase at-\ntachment ambiguity , as we saw earlier in this chapter. As you may recall, it is an\nambiguity about attachment since the PP in the park needs to be attached to one of two\nplaces in the tree: either as a child of VP or else as a child of NP . When the PP is attached\nto VP , the intended interpretation is that the seeing event happened in the park.",
                  "level": -1,
                  "page": 321,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    689,
                    584,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_321_order_5",
                  "label": "foot",
                  "text": "8.3 Context-Free Grammar | 299",
                  "level": -1,
                  "page": 321,
                  "reading_order": 5,
                  "bbox": [
                    440,
                    824,
                    585,
                    842
                  ],
                  "section_number": "8.3",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_322_order_0",
                  "label": "figure",
                  "text": "Figure 8-3. Recursive descent parser demo: This tool allows you to watch the operation of a recursive\ndescent parser as it grows the parse tree and matches it against the input words. [IMAGE: ![Figure](figures/NLTK_page_322_figure_000.png)]",
                  "level": -1,
                  "page": 322,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    583,
                    456
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_322_figure_000.png)",
                      "bbox": [
                        100,
                        71,
                        583,
                        456
                      ],
                      "page": 322,
                      "reading_order": 0
                    },
                    {
                      "label": "cap",
                      "text": "Figure 8-3. Recursive descent parser demo: This tool allows you to watch the operation of a recursive\ndescent parser as it grows the parse tree and matches it against the input words.",
                      "bbox": [
                        97,
                        465,
                        585,
                        494
                      ],
                      "page": 322,
                      "reading_order": 1
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_322_order_2",
                  "label": "para",
                  "text": "However, if the PP is attached to NP, then it was the man who was in the park, and the\nagent of the seeing (the dog) might have been sitting on the balcony of an apartment\noverlooking the park.",
                  "level": -1,
                  "page": 322,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    501,
                    585,
                    555
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_315_order_5",
          "label": "para",
          "text": "A well-known example of ambiguity is shown in (2), from the Groucho Marx movie,\nAnimal Crackers (1930):",
          "level": -1,
          "page": 315,
          "reading_order": 5,
          "bbox": [
            97,
            508,
            584,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_315_order_6",
          "label": "para",
          "text": "(2) While hunting in Africa, I shot an elephant in my pajamas. How an elephant\ngot into my pajamas I'll never know.",
          "level": -1,
          "page": 315,
          "reading_order": 6,
          "bbox": [
            118,
            553,
            583,
            583
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_315_order_7",
          "label": "para",
          "text": "Let’\ns take a closer look at the ambiguity in the phrase: I shot an elephant in my paja-\nmas. First we need to define a simple grammar:",
          "level": -1,
          "page": 315,
          "reading_order": 7,
          "bbox": [
            97,
            598,
            584,
            629
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_315_order_9",
          "label": "foot",
          "text": "8.1 Some Grammatical Dilemmas | 293",
          "level": -1,
          "page": 315,
          "reading_order": 9,
          "bbox": [
            413,
            824,
            584,
            842
          ],
          "section_number": "8.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_316_order_0",
          "label": "para",
          "text": "This grammar permits the sentence to be analyzed in two ways, depending on whether\nthe prepositional phrase in my pajamas describes the elephant or the shooting event.",
          "level": -1,
          "page": 316,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_316_order_2",
          "label": "para",
          "text": "The program produces two bracketed structures, which we can depict as trees, as\nshown in (3):",
          "level": -1,
          "page": 316,
          "reading_order": 2,
          "bbox": [
            97,
            313,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_316_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_316_figure_003.png)",
          "level": -1,
          "page": 316,
          "reading_order": 3,
          "bbox": [
            126,
            358,
            449,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_316_order_4",
          "label": "foot",
          "text": "294 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 316,
          "reading_order": 4,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "294",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_317_order_0",
          "label": "para",
          "text": "Notice that there’s no ambiguity concerning the meaning of any of the words; e.g., the\nword shot doesn’t refer to the act of using a gun in the first sentence and using a camera\nin the second sentence.",
          "level": -1,
          "page": 317,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_317_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_317_figure_001.png)",
          "level": -1,
          "page": 317,
          "reading_order": 1,
          "bbox": [
            118,
            134,
            164,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_317_order_2",
          "label": "para",
          "text": "Your Turn: Consider the following sentences and see if you can think\nof two quite different interpretations: Fighting animals could be danger-\nous. Visiting relatives can be tiresome . Is ambiguity of the individual\nwords to blame? If not, what is the cause of the ambiguity?",
          "level": -1,
          "page": 317,
          "reading_order": 2,
          "bbox": [
            171,
            151,
            530,
            208
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_317_order_3",
          "label": "para",
          "text": "This chapter presents grammars and parsing, as the formal and computational methods\nfor investigating and modeling the linguistic phenomena we have been discussing. As\nwe shall see, patterns of well-formedness and ill-formedness in a sequence of words\ncan be understood with respect to the phrase structure and dependencies. We can\ndevelop formal models of these structures using grammars and parsers. As before, a\nkey motivation is natural language understanding . How much more of the meaning of\na text can we access when we can reliably recognize the linguistic structures it contains?\nHaving read in a text, can a program “ understand ” it enough to be able to answer simple\nquestions about “ what happened ” or “ who did what to whom ” ? Also as before, we will\ndevelop simple programs to process annotated corpora and perform useful tasks.",
          "level": -1,
          "page": 317,
          "reading_order": 3,
          "bbox": [
            97,
            231,
            586,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_322_order_3",
      "label": "sec",
      "text": "Writing Your Own Grammars",
      "level": 1,
      "page": 322,
      "reading_order": 3,
      "bbox": [
        97,
        564,
        288,
        587
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_322_order_4",
          "label": "para",
          "text": "If you are interested in experimenting with writing CFGs, you will find it helpful to\ncreate and edit your grammar in a text file, say, mygrammar.cfg. You can then load it\ninto NLTK and parse with it as follows:",
          "level": -1,
          "page": 322,
          "reading_order": 4,
          "bbox": [
            97,
            591,
            585,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_322_order_6",
          "label": "para",
          "text": "Make sure that you put a .cfg suffix on the filename, and that there are no spaces in the\nstring 'file:mygrammar.cfg'. If the command print tree produces no output, this is\nprobably because your sentence sent is not admitted by your grammar. In this case,\ncall the parser with tracing set to be on: rd_parser = nltk.RecursiveDescent",
          "level": -1,
          "page": 322,
          "reading_order": 6,
          "bbox": [
            97,
            725,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_322_order_7",
          "label": "foot",
          "text": "300 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 322,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "300",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_323_order_0",
          "label": "para",
          "text": "Parser(grammar1, trace=2). You can also check what productions are currently in the\ngrammar with the command for p in grammar1.productions(): print p.",
          "level": -1,
          "page": 323,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_323_order_1",
          "label": "para",
          "text": "When you write CFGs for parsing in NLTK, you cannot combine grammatical cate-\ngories with lexical items on the righthand side of the same production. Thus, a pro-\nduction such as PP -> 'of' NP is disallowed. In addition, you are not permitted to place\nmultiword lexical items on the righthand side of a production. So rather than writing\nNP -> 'New York' , you have to resort to something like NP -> 'New_York' instead.",
          "level": -1,
          "page": 323,
          "reading_order": 1,
          "bbox": [
            97,
            115,
            585,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_323_order_2",
      "label": "sec",
      "text": "Recursion in Syntactic Structure",
      "level": 1,
      "page": 323,
      "reading_order": 2,
      "bbox": [
        98,
        206,
        307,
        232
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_324_order_2",
          "label": "sub_sec",
          "text": "8.4 Parsing with Context-Free Grammar",
          "level": 2,
          "page": 324,
          "reading_order": 2,
          "bbox": [
            97,
            663,
            419,
            686
          ],
          "section_number": "8.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_324_order_3",
              "label": "para",
              "text": "A parser processes input sentences according to the productions of a grammar, and\nbuilds one or more constituent structures that conform to the grammar. A grammar is\na declarative specification of well-formedness—it is actually just a string, not a pro-\ngram. A parser is a procedural interpretation of the grammar. It searches through the\nspace of trees licensed by a grammar to find one that has the required sentence along\nits fringe.",
              "level": -1,
              "page": 324,
              "reading_order": 3,
              "bbox": [
                97,
                689,
                585,
                792
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_324_order_4",
              "label": "foot",
              "text": "302 | Chapter 8: Analyzing Sentence Structure",
              "level": -1,
              "page": 324,
              "reading_order": 4,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "302",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_325_order_0",
              "label": "para",
              "text": "A parser permits a grammar to be evaluated against a collection of test sentences, help-\ning linguists to discover mistakes in their grammatical analysis. A parser can serve as a\nmodel of psycholinguistic processing, helping to explain the difficulties that humans\nhave with processing certain syntactic constructions. Many natural language applica-\ntions involve parsing at some point; for example, we would expect the natural language\nquestions submitted to a question-answering system to undergo parsing as an initial\nstep.",
              "level": -1,
              "page": 325,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                188
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_325_order_1",
              "label": "para",
              "text": "In this section, we see two simple parsing algorithms, a top-down method called re-\ncursive descent parsing, and a bottom-up method called shift-reduce parsing. We also\nsee some more sophisticated algorithms, a top-down method with bottom-up filtering\ncalled left-corner parsing, and a dynamic programming technique called chart parsing.",
              "level": -1,
              "page": 325,
              "reading_order": 1,
              "bbox": [
                97,
                197,
                585,
                262
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_323_order_3",
          "label": "para",
          "text": "A grammar is said to be recursive if a category occurring on the lefthand side of a\nproduction also appears on the righthand side of a production, as illustrated in Exam-\nple 8-2. The production Nom -> Adj Nom (where Nom is the category of nominals) involves\ndirect recursion on the category Nom, whereas indirect recursion on S arises from the\ncombination of two productions, namely S -> NP VP and VP -> V S.",
          "level": -1,
          "page": 323,
          "reading_order": 3,
          "bbox": [
            97,
            232,
            585,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_323_order_4",
          "label": "para",
          "text": "Example 8-2. A recursive context-free grammar.",
          "level": -1,
          "page": 323,
          "reading_order": 4,
          "bbox": [
            97,
            331,
            335,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_323_order_6",
          "label": "para",
          "text": "To see how recursion arises from this grammar, consider the following trees. (10a)\nnvolves nested nominal phrases, while (10b) contains nested sentences.",
          "level": -1,
          "page": 323,
          "reading_order": 6,
          "bbox": [
            100,
            537,
            584,
            568
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_323_order_7",
          "label": "foot",
          "text": "8.3 Context-Free Grammar | 301",
          "level": -1,
          "page": 323,
          "reading_order": 7,
          "bbox": [
            440,
            824,
            584,
            842
          ],
          "section_number": "8.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_324_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_324_figure_000.png)",
          "level": -1,
          "page": 324,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            566,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_324_order_1",
          "label": "para",
          "text": "We've only illustrated two levels of recursion here, but there's no upper limit on the\ndepth. You can experiment with parsing sentences that involve more deeply nested\nstructures. Beware that the RecursiveDescentParser is unable to handle left-\nrecursive productions of the form X -> X Y; we will return to this in Section 8.4 .",
          "level": -1,
          "page": 324,
          "reading_order": 1,
          "bbox": [
            97,
            573,
            585,
            637
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_325_order_2",
      "label": "sec",
      "text": "Recursive Descent Parsing",
      "level": 1,
      "page": 325,
      "reading_order": 2,
      "bbox": [
        98,
        277,
        270,
        296
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_325_order_3",
          "label": "para",
          "text": "The simplest kind of parser interprets a grammar as a specification of how to break a\nhigh-level goal into several lower-level subgoals. The top-level goal is to find an S . The\nS → NP VP production permits the parser to replace this goal with two subgoals: find an\nNP , then find a VP . Each of these subgoals can be replaced in turn by sub-subgoals, using\nproductions that have NP and VP on their lefthand side. Eventually, this expansion\nprocess leads to subgoals such as: find the word telescope . Such subgoals can be directly\ncompared against the input sequence, and succeed if the next word is matched. If there\nis no match, the parser must back up and try a different alternative.",
          "level": -1,
          "page": 325,
          "reading_order": 3,
          "bbox": [
            97,
            304,
            585,
            435
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_325_order_4",
          "label": "para",
          "text": "The recursive descent parser builds a parse tree during this process. With the initial\ngoal (find an S), the S root node is created. As the process recursively expands its goals\nusing the productions of the grammar, the parse tree is extended downwards (hence\nthe name recursive descent ). We can see this in action using the graphical demonstration\nnltk.app.rdparser() . Six stages of the execution of this parser are shown in Figure 8-4 .",
          "level": -1,
          "page": 325,
          "reading_order": 4,
          "bbox": [
            97,
            439,
            585,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_325_order_5",
          "label": "para",
          "text": "During this process, the parser is often forced to choose between several possible pro-\nductions. For example, in going from step 3 to step 4, it tries to find productions with\nN on the lefthand side. The first of these is N → man. When this does not work it\nbacktracks, and tries other N productions in order, until it gets to N → dog, which\nmatches the next word in the input sentence. Much later, as shown in step 5, it finds\na complete parse. This is a tree that covers the entire sentence, without any dangling\nedges. Once a parse has been found, we can get the parser to look for additional parses.\nAgain it will backtrack and explore other choices of production in case any of them\nresult in a parse.",
          "level": -1,
          "page": 325,
          "reading_order": 5,
          "bbox": [
            97,
            528,
            585,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_325_order_6",
          "label": "para",
          "text": "NLTK provides a recursive descent parser:",
          "level": -1,
          "page": 325,
          "reading_order": 6,
          "bbox": [
            98,
            689,
            342,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_325_order_8",
          "label": "foot",
          "text": "8.4 Parsing with Context-Free Grammar | 303",
          "level": -1,
          "page": 325,
          "reading_order": 8,
          "bbox": [
            386,
            824,
            584,
            842
          ],
          "section_number": "8.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_326_order_0",
          "label": "figure",
          "text": "Figure 8-4. Six stages of a recursive descent parser: The parser begins with a tree consisting of the\nnode S; at each stage it consults the grammar to find a production that can be used to enlarge the tree;\nwhen a lexical production is encountered, its word is compared against the input; after a complete\nparse has been found, the parser backtracks to look for more parses. [IMAGE: ![Figure](figures/NLTK_page_326_figure_000.png)]",
          "level": -1,
          "page": 326,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_326_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                313
              ],
              "page": 326,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 8-4. Six stages of a recursive descent parser: The parser begins with a tree consisting of the\nnode S; at each stage it consults the grammar to find a production that can be used to enlarge the tree;\nwhen a lexical production is encountered, its word is compared against the input; after a complete\nparse has been found, the parser backtracks to look for more parses.",
              "bbox": [
                97,
                320,
                585,
                377
              ],
              "page": 326,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_326_order_2",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_326_figure_002.png)",
          "level": -1,
          "page": 326,
          "reading_order": 2,
          "bbox": [
            118,
            385,
            171,
            440
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_326_order_3",
          "label": "para",
          "text": "RecursiveDescentParser() takes an optional parameter trace. If trace\nis greater than zero, then the parser will report the steps that it takes as\nit parses a text.",
          "level": -1,
          "page": 326,
          "reading_order": 3,
          "bbox": [
            171,
            394,
            530,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_326_order_4",
          "label": "para",
          "text": "Recursive descent parsing has three key shortcomings. First, left-recursive productions\nlike NP -> NP PP send it into an infinite loop. Second, the parser wastes a lot of time\nconsidering words and structures that do not correspond to the input sentence. Third,\nthe backtracking process may discard parsed constituents that will need to be rebuilt\nagain later. For example, backtracking over VP -> V NP will discard the subtree created\nfor the NP. If the parser then proceeds with VP -> V NP PP, then the NP subtree must be\ncreated all over again.",
          "level": -1,
          "page": 326,
          "reading_order": 4,
          "bbox": [
            97,
            465,
            585,
            583
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_326_order_5",
          "label": "para",
          "text": "Recursive descent parsing is a kind of top-down parsing. Top-down parsers use a\ngrammar to predict what the input will be, before inspecting the input! However, since\nthe input is available to the parser all along, it would be more sensible to consider the\ninput sentence from the very beginning. This approach is called bottom-up parsing,\nand we will see an example in the next section.",
          "level": -1,
          "page": 326,
          "reading_order": 5,
          "bbox": [
            97,
            591,
            585,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_326_order_6",
      "label": "sec",
      "text": "Shift-Reduce Parsing",
      "level": 1,
      "page": 326,
      "reading_order": 6,
      "bbox": [
        97,
        689,
        236,
        708
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_326_order_7",
          "label": "para",
          "text": "A simple kind of bottom-up parser is the shift-reduce parser . In common with all\nbottom-up parsers, a shift-reduce parser tries to find sequences of words and phrases\nthat correspond to the righthand side of a grammar production, and replace them with\nthe lefthand side, until the whole sentence is reduced to an S.",
          "level": -1,
          "page": 326,
          "reading_order": 7,
          "bbox": [
            97,
            716,
            585,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_326_order_8",
          "label": "foot",
          "text": "304 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 326,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "304",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_327_order_0",
          "label": "para",
          "text": "The shift-reduce parser repeatedly pushes the next input word onto a stack ( Sec-\ntion 4.1 ); this is the shift operation. If the top n items on the stack match the n items\non the righthand side of some production, then they are all popped off the stack, and\nthe item on the lefthand side of the production is pushed onto the stack. This replace-\nment of the top n items with a single item is the reduce operation. The operation may\nbe applied only to the top of the stack; reducing items lower in the stack must be done\nbefore later items are pushed onto the stack. The parser finishes when all the input is\nconsumed and there is only one item remaining on the stack, a parse tree with an 5\nnode as its root. The shift-reduce parser builds a parse tree during the above process.\nEach time it pops n items off the stack, it combines them into a partial parse tree, and\npushes this back onto the stack. We can see the shift-reduce parsing algorithm in action\nusing the graphical demonstration nltk.app.srparser() . Six stages of the execution of\nthis parser are shown in Figure 8 - 5 .",
          "level": -1,
          "page": 327,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            586,
            288
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_327_order_1",
          "label": "figure",
          "text": "Figure 8-5. Six stages of a shift-reduce parser: The parser begins by shifting the first input word onto\nits stack; once the top items on the stack match the righthand side of a grammar production, they can\nbe replaced with the lefthand side of that production; the parser succeeds once all input is consumed\nand one S item remains on the stack. [IMAGE: ![Figure](figures/NLTK_page_327_figure_001.png)]",
          "level": -1,
          "page": 327,
          "reading_order": 1,
          "bbox": [
            100,
            313,
            583,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_327_figure_001.png)",
              "bbox": [
                100,
                313,
                583,
                618
              ],
              "page": 327,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Figure 8-5. Six stages of a shift-reduce parser: The parser begins by shifting the first input word onto\nits stack; once the top items on the stack match the righthand side of a grammar production, they can\nbe replaced with the lefthand side of that production; the parser succeeds once all input is consumed\nand one S item remains on the stack.",
              "bbox": [
                97,
                618,
                585,
                680
              ],
              "page": 327,
              "reading_order": 2
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_327_order_3",
          "label": "para",
          "text": "NLTK provides ShiftReduceParser(), a simple implementation of a shift-reduce parser.\nThis parser does not implement any backtracking, so it is not guaranteed to find a parse\nfor a text, even if one exists. Furthermore, it will only find at most one parse, even if\nmore parses exist. We can provide an optional trace parameter that controls how ver-\nbosely the parser reports the steps that it takes as it parses a text:",
          "level": -1,
          "page": 327,
          "reading_order": 3,
          "bbox": [
            97,
            716,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_327_order_4",
          "label": "foot",
          "text": "8.4 Parsing with Context-Free Grammar | 305",
          "level": -1,
          "page": 327,
          "reading_order": 4,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "8.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_328_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_328_figure_001.png)",
          "level": -1,
          "page": 328,
          "reading_order": 1,
          "bbox": [
            118,
            143,
            171,
            198
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_328_order_2",
          "label": "para",
          "text": "Your Turn: Run this parser in tracing mode to see the sequence of shift\nand reduce operations, using sr_parse = nltk.ShiftReduceParser(gram\nmar1, trace=2).",
          "level": -1,
          "page": 328,
          "reading_order": 2,
          "bbox": [
            171,
            152,
            530,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_328_order_3",
          "label": "para",
          "text": "A shift-reduce parser can reach a dead end and fail to find any parse, even if the input\nsentence is well-formed according to the grammar. When this happens, no input re-\nmains, and the stack contains items that cannot be reduced to an 5. The problem arises\nbecause there are choices made earlier that cannot be undone by the parser (although\nusers of the graphical demonstration can undo their choices). There are two kinds of\nchoices to be made by the parser: (a) which reduction to do when more than one is\npossible and (b) whether to shift or reduce when either action is possible.",
          "level": -1,
          "page": 328,
          "reading_order": 3,
          "bbox": [
            97,
            224,
            586,
            340
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_328_order_4",
          "label": "para",
          "text": "A shift-reduce parser may be extended to implement policies for resolving such con-\nflicts. For example, it may address shift-reduce conflicts by shifting only when no re-\nductions are possible, and it may address reduce-reduce conflicts by favoring the re-\nduction operation that removes the most items from the stack. (A generalization of the\nshift-reduce parser, a “ lookahead LR parser, ” is commonly used in programming lan-\nguage compilers.)",
          "level": -1,
          "page": 328,
          "reading_order": 4,
          "bbox": [
            97,
            349,
            585,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_328_order_5",
          "label": "para",
          "text": "The advantages of shift-reduce parsers over recursive descent parsers is that they only\nbuild structure that corresponds to the words in the input. Furthermore, they only build\neach substructure once; e.g., NP(Det(the), N(man)) is only built and pushed onto the\nstack a single time, regardless of whether it will later be used by the VP -> V NP PP\nreduction or the NP -> NP PP reduction.",
          "level": -1,
          "page": 328,
          "reading_order": 5,
          "bbox": [
            97,
            455,
            585,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_328_order_6",
      "label": "sec",
      "text": "The Left-Corner Parser",
      "level": 1,
      "page": 328,
      "reading_order": 6,
      "bbox": [
        97,
        546,
        246,
        566
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_328_order_7",
          "label": "para",
          "text": "One of the problems with the recursive descent parser is that it goes into an infinite\nloop when it encounters a left-recursive production. This is because it applies the\ngrammar productions blindly, without considering the actual input sentence. A left-\ncorner parser is a hybrid between the bottom-up and top-down approaches we have\nseen.",
          "level": -1,
          "page": 328,
          "reading_order": 7,
          "bbox": [
            97,
            573,
            585,
            656
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_328_order_8",
          "label": "para",
          "text": "A left-corner parser is a top-down parser with bottom-up filtering. Unlike an ordinary\nrecursive descent parser, it does not get trapped in left-recursive productions. Before\nstarting its work, a left-corner parser preprocesses the context-free grammar to build a\ntable where each row contains two cells, the first holding a non-terminal, and the sec-\nond holding the collection of possible left corners of that non-terminal. Table 8 - 2 il-\nlustrates this for the grammar from grammar .",
          "level": -1,
          "page": 328,
          "reading_order": 8,
          "bbox": [
            97,
            663,
            585,
            765
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_328_order_9",
          "label": "foot",
          "text": "306 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 328,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "306",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_329_order_0",
          "label": "table",
          "text": "Table 8-2. Left corners in grammar2 [TABLE: <table><tr><td>Category</td><td>Left corners (pre-terminals)</td></tr><tr><td>S</td><td>NP</td></tr><tr><td>NP</td><td>Det, PropN</td></tr><tr><td>VP</td><td>V</td></tr><tr><td>PP</td><td>P</td></tr></table>]",
          "level": -1,
          "page": 329,
          "reading_order": 0,
          "bbox": [
            100,
            98,
            270,
            189
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Category</td><td>Left corners (pre-terminals)</td></tr><tr><td>S</td><td>NP</td></tr><tr><td>NP</td><td>Det, PropN</td></tr><tr><td>VP</td><td>V</td></tr><tr><td>PP</td><td>P</td></tr></table>",
              "bbox": [
                100,
                98,
                270,
                189
              ],
              "page": 329,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Table 8-2. Left corners in grammar2",
              "bbox": [
                99,
                71,
                279,
                89
              ],
              "page": 329,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_329_order_2",
          "label": "para",
          "text": "Each time a production is considered by the parser, it checks that the next input word\nis compatible with at least one of the pre-terminal categories in the left-corner table.",
          "level": -1,
          "page": 329,
          "reading_order": 2,
          "bbox": [
            97,
            213,
            583,
            244
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_329_order_3",
      "label": "sec",
      "text": "Well-Formed Substring Tables",
      "level": 1,
      "page": 329,
      "reading_order": 3,
      "bbox": [
        97,
        259,
        297,
        279
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_332_order_5",
          "label": "sub_sec",
          "text": "8.5 Dependencies and Dependency Grammar",
          "level": 2,
          "page": 332,
          "reading_order": 5,
          "bbox": [
            97,
            591,
            459,
            619
          ],
          "section_number": "8.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_332_order_6",
              "label": "para",
              "text": "Phrase structure grammar is concerned with how words and sequences of words com-\nbine to form constituents. A distinct and complementary approach, dependency\ngrammar , focuses instead on how words relate to other words. Dependency is a binary\nasymmetric relation that holds between a head and its dependents . The head of a\nsentence is usually taken to be the tensed verb, and every other word is either dependent\non the sentence head or connects to it through a path of dependencies.",
              "level": -1,
              "page": 332,
              "reading_order": 6,
              "bbox": [
                97,
                627,
                585,
                725
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_332_order_7",
              "label": "para",
              "text": "A dependency representation is a labeled directed graph, where the nodes are the lexical\nitems and the labeled arcs represent dependency relations from heads to dependents.\nFigure 8 - 8 illustrates a dependency graph, where arrows point from heads to their\ndependents.",
              "level": -1,
              "page": 332,
              "reading_order": 7,
              "bbox": [
                97,
                734,
                585,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_332_order_8",
              "label": "foot",
              "text": "310 | Chapter 8: Analyzing Sentence Structure",
              "level": -1,
              "page": 332,
              "reading_order": 8,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "310",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_333_order_0",
              "label": "figure",
              "text": "Figure 8-8. Dependency structure: Arrows point from heads to their dependents; labels indicate the\ngrammatical function of the dependent as subject, object, or modifier. [IMAGE: ![Figure](figures/NLTK_page_333_figure_000.png)]",
              "level": -1,
              "page": 333,
              "reading_order": 0,
              "bbox": [
                100,
                80,
                583,
                188
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_333_figure_000.png)",
                  "bbox": [
                    100,
                    80,
                    583,
                    188
                  ],
                  "page": 333,
                  "reading_order": 0
                },
                {
                  "label": "cap",
                  "text": "Figure 8-8. Dependency structure: Arrows point from heads to their dependents; labels indicate the\ngrammatical function of the dependent as subject, object, or modifier.",
                  "bbox": [
                    96,
                    188,
                    585,
                    224
                  ],
                  "page": 333,
                  "reading_order": 1
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_333_order_2",
              "label": "para",
              "text": "The arcs in Figure 8-8 are labeled with the grammatical function that holds between a\ndependent and its head. For example, I is the SBJ (subject) of shot (which is the head\nof the whole sentence), and in is an NMOD (noun modifier of elephant ). In contrast to\nphrase structure grammar, therefore, dependency grammars can be used to directly\nexpress grammatical functions as a type of dependency.",
              "level": -1,
              "page": 333,
              "reading_order": 2,
              "bbox": [
                97,
                230,
                585,
                313
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_333_order_3",
              "label": "para",
              "text": "Here’s one way of encoding a dependency grammar in NLTK—note that it only cap-\ntures bare dependency information without specifying the type of dependency:",
              "level": -1,
              "page": 333,
              "reading_order": 3,
              "bbox": [
                97,
                320,
                584,
                351
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_333_order_5",
              "label": "para",
              "text": "A dependency graph is projective if, when all the words are written in linear order, the\nedges can be drawn above the words without crossing. This is equivalent to saying that\na word and all its descendants (dependents and dependents of its dependents, etc.)\nform a contiguous sequence of words within the sentence. Figure 8-8 is projective, and\nwe can parse many sentences in English using a projective dependency parser. The next\nexample shows how groucho_dep_grammar provides an alternative approach to captur-\ning the attachment ambiguity that we examined earlier with phrase structure grammar.",
              "level": -1,
              "page": 333,
              "reading_order": 5,
              "bbox": [
                97,
                563,
                585,
                680
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_333_order_7",
              "label": "foot",
              "text": "8.5 Dependencies and Dependency Grammar | 311",
              "level": -1,
              "page": 333,
              "reading_order": 7,
              "bbox": [
                368,
                824,
                584,
                842
              ],
              "section_number": "8.5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_334_order_0",
              "label": "para",
              "text": "These bracketed dependency structures can also be displayed as trees, where dep-\nendents are shown as children of their heads.",
              "level": -1,
              "page": 334,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_334_order_1",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_334_figure_001.png)",
              "level": -1,
              "page": 334,
              "reading_order": 1,
              "bbox": [
                109,
                116,
                306,
                333
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_334_order_2",
              "label": "para",
              "text": "In languages with more flexible word order than English, non-projective dependencies\nare more frequent.",
              "level": -1,
              "page": 334,
              "reading_order": 2,
              "bbox": [
                97,
                349,
                585,
                385
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_334_order_3",
              "label": "para",
              "text": "Various criteria have been proposed for deciding what is the head $H$ and what is the\ndependent $D$ in a construction C. Some of the most important are the following:",
              "level": -1,
              "page": 334,
              "reading_order": 3,
              "bbox": [
                97,
                392,
                585,
                422
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_334_order_4",
              "label": "list_group",
              "text": "1. H determines the distribution class of C; or alternatively, the external syntactic\nproperties of C are due to H.\n2. H determines the semantic type of C.",
              "level": -1,
              "page": 334,
              "reading_order": 4,
              "bbox": [
                100,
                430,
                584,
                463
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "1. H determines the distribution class of C; or alternatively, the external syntactic\nproperties of C are due to H.",
                  "bbox": [
                    100,
                    430,
                    584,
                    463
                  ],
                  "page": 334,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "2. H determines the semantic type of C.",
                  "bbox": [
                    105,
                    465,
                    333,
                    484
                  ],
                  "page": 334,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "$H$ is obligatory while $D$ may be optional",
                  "bbox": [
                    108,
                    491,
                    350,
                    505
                  ],
                  "page": 334,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "4. H selects D and determines whether it is obligatory or optional.",
                  "bbox": [
                    105,
                    510,
                    485,
                    528
                  ],
                  "page": 334,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "5. The morphological form of D is determined by H (e.g., agreement or case\ngovernment).",
                  "bbox": [
                    105,
                    528,
                    585,
                    564
                  ],
                  "page": 334,
                  "reading_order": 8
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_334_order_9",
              "label": "para",
              "text": "When we say in a phrase structure grammar that the immediate constituents of a PP\nare P and NP , we are implicitly appealing to the head/dependent distinction. A prepo-\nsitional phrase is a phrase whose head is a preposition; moreover, the NP is a dependent\nof P . The same distinction carries over to the other types of phrase that we have dis-\ncussed. The key point to note here is that although phrase structure grammars seem\nvery different from dependency grammars, they implicitly embody a recognition of\ndependency relations. Although CFGs are not intended to directly capture dependen-\ncies, more recent linguistic frameworks have increasingly adopted formalisms which\ncombine aspects of both approaches.",
              "level": -1,
              "page": 334,
              "reading_order": 9,
              "bbox": [
                97,
                573,
                586,
                720
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_329_order_4",
          "label": "para",
          "text": "The simple parsers discussed in the previous sections suffer from limitations in both\ncompleteness and efficiency. In order to remedy these, we will apply the algorithm\ndesign technique of dynamic programming to the parsing problem. As we saw in\nSection 4.7 , dynamic programming stores intermediate results and reuses them when\nappropriate, achieving significant efficiency gains. This technique can be applied to\nsyntactic parsing, allowing us to store partial solutions to the parsing task and then\nlook them up as necessary in order to efficiently arrive at a complete solution. This\napproach to parsing is known as chart parsing . We introduce the main idea in this\nsection; see the online materials available for this chapter for more implementation\ndetails.",
          "level": -1,
          "page": 329,
          "reading_order": 4,
          "bbox": [
            97,
            286,
            585,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_329_order_5",
          "label": "para",
          "text": "Dynamic programming allows us to build the PP in my pajamas just once. The first time\nwe build it we save it in a table, then we look it up when we need to use it as a sub-\nconstituent of either the object NP or the higher VP. This table is known as a well-formed\nsubstring table, or WFST for short. (The term “substring” refers to a contiguous se-\nquence of words within a sentence.) We will show how to construct the WFST bottom-\nup so as to systematically record what syntactic constituents have been found.",
          "level": -1,
          "page": 329,
          "reading_order": 5,
          "bbox": [
            97,
            456,
            585,
            557
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_329_order_6",
          "label": "para",
          "text": "Let's set our input to be the sentence in (2). The numerically specified spans of the\nWFST are reminiscent of Python's slice notation (Section 3.2 ). Another way to think\nabout the data structure is shown in Figure 8-6 , a data structure known as a chart .",
          "level": -1,
          "page": 329,
          "reading_order": 6,
          "bbox": [
            97,
            564,
            585,
            613
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_329_order_7",
          "label": "figure",
          "text": "Figure 8-6. The chart data structure: Words are the edge labels of a linear graph structure. [IMAGE: ![Figure](figures/NLTK_page_329_figure_007.png)]",
          "level": -1,
          "page": 329,
          "reading_order": 7,
          "bbox": [
            100,
            627,
            583,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_329_figure_007.png)",
              "bbox": [
                100,
                627,
                583,
                689
              ],
              "page": 329,
              "reading_order": 7
            },
            {
              "label": "cap",
              "text": "Figure 8-6. The chart data structure: Words are the edge labels of a linear graph structure.",
              "bbox": [
                97,
                689,
                539,
                707
              ],
              "page": 329,
              "reading_order": 8
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_329_order_9",
          "label": "para",
          "text": "In a WFST, we record the position of the words by filling in cells in a triangular matrix:\nthe vertical axis will denote the start position of a substring, while the horizontal axis\nwill denote the end position (thus shot will appear in the cell with coordinates (1, 2)).\nTo simplify this presentation, we will assume each word has a unique lexical category,",
          "level": -1,
          "page": 329,
          "reading_order": 9,
          "bbox": [
            97,
            734,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_329_order_10",
          "label": "foot",
          "text": "8.4 Parsing with Context-Free Grammar | 307",
          "level": -1,
          "page": 329,
          "reading_order": 10,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "8.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_330_order_0",
          "label": "para",
          "text": "and we will store this (not the word) in the matrix. So cell (1, 2) will contain the entry\n$\\mathbb{V}$ . More generally, if our input string is a1a2 ... an, and our grammar contains a pro-\nduction of the form A→ai, then we add A to the cell (i-1, i).",
          "level": -1,
          "page": 330,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_330_order_1",
          "label": "para",
          "text": "So, for every word in text, we can look up in our grammar what category it belongs to.",
          "level": -1,
          "page": 330,
          "reading_order": 1,
          "bbox": [
            97,
            132,
            584,
            146
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_330_order_3",
          "label": "para",
          "text": "For our WFST, we create an (n-1) × (n-1) matrix as a list of lists in Python, and initialize\nit with the lexical categories of each token in the init_wfst() function in Exam-\nple 8-3 . We also define a utility function display() to pretty-print the WFST for us. As\nexpected, there is a V in cell (1, 2).",
          "level": -1,
          "page": 330,
          "reading_order": 3,
          "bbox": [
            97,
            188,
            585,
            253
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_330_order_4",
          "label": "para",
          "text": "Example 8-3. Acceptor using well-formed substring table.",
          "level": -1,
          "page": 330,
          "reading_order": 4,
          "bbox": [
            97,
            267,
            378,
            280
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_330_order_9",
          "label": "foot",
          "text": "308 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 330,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "308",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_1",
          "label": "para",
          "text": "Returning to our tabular representation, given that we have Det in cell (2, 3) for the\nword an, and N in cell (3, 4) for the word elephant, what should we put into cell (2, 4)\nfor an elephant? We need to find a production of the form A → Det N. Consulting the\ngrammar, we know that we can enter NP in cell (0, 2).",
          "level": -1,
          "page": 331,
          "reading_order": 1,
          "bbox": [
            97,
            250,
            585,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_2",
          "label": "para",
          "text": "More generally, we can enter $A$ in $(i,j)$ if there is a production $A\\to B~C$ , and we find\nnon-terminal $B$ in $(i,k)$ and $C$ in $(k,j)$ . The program in Example 8-3 uses this rule to\ncomplete the WFST. By setting trace to True when calling the function\ncomplete_wfst() , we see tracing output that shows the WFST being constructed:",
          "level": -1,
          "page": 331,
          "reading_order": 2,
          "bbox": [
            97,
            322,
            584,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_4",
          "label": "para",
          "text": "For example, this says that since we found Det at wfst[0][1] and N at wfst[1][2], we\ncan add NP to wfst[0][2].",
          "level": -1,
          "page": 331,
          "reading_order": 4,
          "bbox": [
            97,
            510,
            584,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_5",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_331_figure_005.png)",
          "level": -1,
          "page": 331,
          "reading_order": 5,
          "bbox": [
            118,
            563,
            164,
            619
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_6",
          "label": "para",
          "text": "To help us easily retrieve productions by their righthand sides, we create\nan index for the grammar. This is an example of a space-time trade-off:\nwe do a reverse lookup on the grammar, instead of having to check\nthrough entire list of productions each time we want to look up via the\nrighthand side.",
          "level": -1,
          "page": 331,
          "reading_order": 6,
          "bbox": [
            171,
            573,
            530,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_7",
          "label": "para",
          "text": "We conclude that there is a parse for the whole input string once we have constructed\nan 5 node in cell (0, 7), showing that we have found a sentence that covers the whole\ninput. The final state of the WFST is depicted in Figure 8-7.",
          "level": -1,
          "page": 331,
          "reading_order": 7,
          "bbox": [
            97,
            663,
            585,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_8",
          "label": "para",
          "text": "Notice that we have not used any built-in parsing functions here. We’ve implemented\na complete primitive chart parser from the ground up!",
          "level": -1,
          "page": 331,
          "reading_order": 8,
          "bbox": [
            97,
            725,
            585,
            756
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_9",
          "label": "para",
          "text": "WFSTs have several shortcomings. First, as you can see, the WFST is not itself a parse\ntree, so the technique is strictly speaking recognizing that a sentence is admitted by a",
          "level": -1,
          "page": 331,
          "reading_order": 9,
          "bbox": [
            97,
            761,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_331_order_10",
          "label": "foot",
          "text": "8.4 Parsing with Context-Free Grammar | 309",
          "level": -1,
          "page": 331,
          "reading_order": 10,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "8.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_332_order_0",
          "label": "figure",
          "text": "Figure 8-7. The chart data structure: Non-terminals are represented as extra edges in the char [IMAGE: ![Figure](figures/NLTK_page_332_figure_000.png)]",
          "level": -1,
          "page": 332,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_332_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                277
              ],
              "page": 332,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 8-7. The chart data structure: Non-terminals are represented as extra edges in the char",
              "bbox": [
                97,
                277,
                557,
                295
              ],
              "page": 332,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_332_order_2",
          "label": "para",
          "text": "grammar, rather than parsing it. Second, it requires every non-lexical grammar pro-\nduction to be binary . Although it is possible to convert an arbitrary CFG into this form,\nwe would prefer to use an approach without such a requirement. Third, as a bottom-\nup approach it is potentially wasteful, being able to propose constituents in locations\nthat would not be licensed by the grammar.",
          "level": -1,
          "page": 332,
          "reading_order": 2,
          "bbox": [
            97,
            304,
            585,
            386
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_332_order_3",
          "label": "para",
          "text": "Finally, the WFST did not represent the structural ambiguity in the sentence (i.e., the\ntwo verb phrase readings). The VP in cell (2,8) was actually entered twice, once for a V\nNP reading, and once for a VP PP reading. These are different hypotheses, and the second\noverwrote the first (as it happens, this didn't matter since the lefthand side was the\nsame). Chart parsers use a slightly richer data structure and some interesting algorithms\nto solve these problems (see Section 8.8 ).",
          "level": -1,
          "page": 332,
          "reading_order": 3,
          "bbox": [
            97,
            394,
            585,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_332_order_4",
          "label": "para",
          "text": "Your\nTurn:\nTry\nout\nthe\ninteractive\nchart\nparser\napplication\nnltk.app.chartparser().",
          "level": -1,
          "page": 332,
          "reading_order": 4,
          "bbox": [
            171,
            519,
            530,
            548
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_334_order_10",
      "label": "sec",
      "text": "Valency and the Lexicon",
      "level": 1,
      "page": 334,
      "reading_order": 10,
      "bbox": [
        97,
        734,
        261,
        755
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_334_order_11",
          "label": "para",
          "text": "Let us take a closer look at verbs and their dependents. The grammar in Example 8-2\ncorrectly generates examples like (12).",
          "level": -1,
          "page": 334,
          "reading_order": 11,
          "bbox": [
            97,
            761,
            584,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_334_order_12",
          "label": "foot",
          "text": "312 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 334,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "312",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_0",
          "label": "para",
          "text": "(12) a. The squirrel was frightened",
          "level": -1,
          "page": 335,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            324,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_1",
          "label": "para",
          "text": "b. Chatterer saw the bear",
          "level": -1,
          "page": 335,
          "reading_order": 1,
          "bbox": [
            151,
            96,
            297,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_2",
          "label": "para",
          "text": "c. Chatterer thought Buster was angry.",
          "level": -1,
          "page": 335,
          "reading_order": 2,
          "bbox": [
            151,
            116,
            377,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_3",
          "label": "para",
          "text": "d. Joe put the fish on the log",
          "level": -1,
          "page": 335,
          "reading_order": 3,
          "bbox": [
            144,
            134,
            315,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_4",
          "label": "para",
          "text": "These possibilities correspond to the productions in Table 8-3",
          "level": -1,
          "page": 335,
          "reading_order": 4,
          "bbox": [
            100,
            161,
            451,
            180
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_5",
          "label": "table",
          "text": "Table 8-3. VP productions and their lexical heads [TABLE: <table><tr><td>Production</td><td>Lexical head</td></tr><tr><td>VP -&gt; V Adj</td><td>was</td></tr><tr><td>VP -&gt; V NP</td><td>saw</td></tr><tr><td>VP -&gt; V S</td><td>thought</td></tr><tr><td>VP -&gt; V NP PP</td><td>put</td></tr></table>]",
          "level": -1,
          "page": 335,
          "reading_order": 5,
          "bbox": [
            100,
            215,
            252,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Production</td><td>Lexical head</td></tr><tr><td>VP -&gt; V Adj</td><td>was</td></tr><tr><td>VP -&gt; V NP</td><td>saw</td></tr><tr><td>VP -&gt; V S</td><td>thought</td></tr><tr><td>VP -&gt; V NP PP</td><td>put</td></tr></table>",
              "bbox": [
                100,
                215,
                252,
                313
              ],
              "page": 335,
              "reading_order": 5
            },
            {
              "label": "cap",
              "text": "Table 8-3. VP productions and their lexical heads",
              "bbox": [
                99,
                194,
                342,
                207
              ],
              "page": 335,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_335_order_7",
          "label": "para",
          "text": "That is, was can occur with a following Adj , saw can occur with a following NP ,\nthought can occur with a following S , and put can occur with a following NP and PP . The\ndependents Adj , NP , S , and PP are often called complements of the respective verbs,\nand there are strong constraints on what verbs can occur with what complements. By\ncontrast with (12) , the word sequences in (13) are ill-formed:",
          "level": -1,
          "page": 335,
          "reading_order": 7,
          "bbox": [
            97,
            331,
            585,
            413
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_8",
          "label": "para",
          "text": "(13) a. *The squirrel was Buster was angry.",
          "level": -1,
          "page": 335,
          "reading_order": 8,
          "bbox": [
            109,
            428,
            369,
            442
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_9",
          "label": "para",
          "text": "b.\n*Chatterer saw frightened.",
          "level": -1,
          "page": 335,
          "reading_order": 9,
          "bbox": [
            150,
            448,
            316,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_10",
          "label": "para",
          "text": "c. *Chatterer thought the bear",
          "level": -1,
          "page": 335,
          "reading_order": 10,
          "bbox": [
            151,
            465,
            324,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_11",
          "label": "para",
          "text": "d.\n*Joe put on the log",
          "level": -1,
          "page": 335,
          "reading_order": 11,
          "bbox": [
            149,
            490,
            273,
            504
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_12",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_335_figure_012.png)",
          "level": -1,
          "page": 335,
          "reading_order": 12,
          "bbox": [
            118,
            519,
            171,
            575
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_13",
          "label": "para",
          "text": "With a little imagination, it is possible to invent contexts in which un-\nusual combinations of verbs and complements are interpretable. How-\never, we assume that the examples in (13) are to be interpreted in neutral\ncontexts.",
          "level": -1,
          "page": 335,
          "reading_order": 13,
          "bbox": [
            171,
            528,
            530,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_14",
          "label": "para",
          "text": "In the tradition of dependency grammar, the verbs in Table 8-3 are said to have different\nvalencies . Valency restrictions are not just applicable to verbs, but also to the other\nclasses of heads.",
          "level": -1,
          "page": 335,
          "reading_order": 14,
          "bbox": [
            97,
            609,
            585,
            657
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_15",
          "label": "para",
          "text": "Within frameworks based on phrase structure grammar, various techniques have been\nproposed for excluding the ungrammatical examples in (13) . In a CFG, we need some\nway of constraining grammar productions which expand VP so that verbs co-occur\nonly with their correct complements. We can do this by dividing the class of verbs into\n“ subcategories, ” each of which is associated with a different set of complements. For\nexample, transitive verbs such as chased and saw require a following NP object com-\nplement; that is, they are subcategorized for NP direct objects. If we introduce a new",
          "level": -1,
          "page": 335,
          "reading_order": 15,
          "bbox": [
            97,
            669,
            585,
            783
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_335_order_16",
          "label": "foot",
          "text": "8.5 Dependencies and Dependency Grammar | 313",
          "level": -1,
          "page": 335,
          "reading_order": 16,
          "bbox": [
            368,
            824,
            584,
            842
          ],
          "section_number": "8.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_0",
          "label": "para",
          "text": "category label for transitive verbs, namely TV (for transitive verb), then we can use it in\nthe following productions:",
          "level": -1,
          "page": 336,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_1",
          "label": "tab",
          "text": "<table><tr><td>VP</td><td>-&gt; TV NP</td></tr></table>",
          "level": -1,
          "page": 336,
          "reading_order": 1,
          "bbox": [
            121,
            107,
            243,
            136
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_2",
          "label": "para",
          "text": "Now *Joe thought the bear is excluded since we haven’t listed thought as a TV, but\nChatterer saw the bear is still allowed. Table 8-4 provides more examples of labels for\nverb subcategories.",
          "level": -1,
          "page": 336,
          "reading_order": 2,
          "bbox": [
            97,
            143,
            585,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_3",
          "label": "table",
          "text": "Table 8-4. Verb subcategories [TABLE: <table><tr><td>Symbol</td><td>Meaning</td><td>Example</td></tr><tr><td>IV</td><td>Intransitive ver</td><td>arked</td></tr><tr><td>TV</td><td>Transitive ver</td><td>saw a man</td></tr><tr><td>DatV</td><td>Dative ver</td><td>gave a dog to a man</td></tr><tr><td>SV</td><td>Sentential ver</td><td>said that a dog barked</td></tr></table>]",
          "level": -1,
          "page": 336,
          "reading_order": 3,
          "bbox": [
            100,
            232,
            324,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Symbol</td><td>Meaning</td><td>Example</td></tr><tr><td>IV</td><td>Intransitive ver</td><td>arked</td></tr><tr><td>TV</td><td>Transitive ver</td><td>saw a man</td></tr><tr><td>DatV</td><td>Dative ver</td><td>gave a dog to a man</td></tr><tr><td>SV</td><td>Sentential ver</td><td>said that a dog barked</td></tr></table>",
              "bbox": [
                100,
                232,
                324,
                331
              ],
              "page": 336,
              "reading_order": 3
            },
            {
              "label": "cap",
              "text": "Table 8-4. Verb subcategories",
              "bbox": [
                99,
                206,
                243,
                224
              ],
              "page": 336,
              "reading_order": 4
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_336_order_5",
          "label": "para",
          "text": "Valency is a property of lexical items, and we will discuss it further in Chapter 9.",
          "level": -1,
          "page": 336,
          "reading_order": 5,
          "bbox": [
            98,
            340,
            557,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_6",
          "label": "para",
          "text": "Complements are often contrasted with modifiers (or adjuncts), although both are\nkinds of dependents. Prepositional phrases, adjectives, and adverbs typically function\nas modifiers. Unlike complements, modifiers are optional, can often be iterated, and\nare not selected for by heads in the same way as complements. For example, the adverb\nreally can be added as a modifier to all the sentences in (14) :",
          "level": -1,
          "page": 336,
          "reading_order": 6,
          "bbox": [
            97,
            367,
            585,
            450
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_7",
          "label": "para",
          "text": "(14) a. The squirrel really was frightened.",
          "level": -1,
          "page": 336,
          "reading_order": 7,
          "bbox": [
            109,
            465,
            361,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_8",
          "label": "para",
          "text": "b. Chatterer really saw the bear.",
          "level": -1,
          "page": 336,
          "reading_order": 8,
          "bbox": [
            144,
            483,
            333,
            502
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_9",
          "label": "para",
          "text": "c. Chatterer really thought Buster was angry.",
          "level": -1,
          "page": 336,
          "reading_order": 9,
          "bbox": [
            144,
            502,
            413,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_10",
          "label": "para",
          "text": "d. Joe really put the fish on the log.",
          "level": -1,
          "page": 336,
          "reading_order": 10,
          "bbox": [
            144,
            528,
            352,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_11",
          "label": "para",
          "text": "The structural ambiguity of PP attachment, which we have illustrated in both phrase\nstructure and dependency grammars, corresponds semantically to an ambiguity in the\nscope of the modifier.",
          "level": -1,
          "page": 336,
          "reading_order": 11,
          "bbox": [
            97,
            555,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_336_order_12",
      "label": "sec",
      "text": "Scaling Up",
      "level": 1,
      "page": 336,
      "reading_order": 12,
      "bbox": [
        97,
        618,
        166,
        640
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_337_order_2",
          "label": "sub_sec",
          "text": "8.6 Grammar Development",
          "level": 2,
          "page": 337,
          "reading_order": 2,
          "bbox": [
            97,
            250,
            318,
            278
          ],
          "section_number": "8.6",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_337_order_4",
              "label": "sub_sub_sec",
              "text": "Treebanks and Grammars",
              "level": 3,
              "page": 337,
              "reading_order": 4,
              "bbox": [
                100,
                383,
                270,
                403
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_337_order_5",
                  "label": "para",
                  "text": "The corpus module defines the treebank corpus reader, which contains a 10% sample\nof the Penn Treebank Corpus.",
                  "level": -1,
                  "page": 337,
                  "reading_order": 5,
                  "bbox": [
                    100,
                    410,
                    585,
                    441
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_337_order_7",
                  "label": "para",
                  "text": "We can use this data to help develop a grammar. For example, the program in Exam-\nple 8-4 uses a simple filter to find verbs that take sentential complements. Assuming\nwe already have a production of the form VP -> SV S, this information enables us to\nidentify particular verbs that would be included in the expansion of SV.",
                  "level": -1,
                  "page": 337,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    705,
                    585,
                    770
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_337_order_8",
                  "label": "foot",
                  "text": "8.6 Grammar Development | 315",
                  "level": -1,
                  "page": 337,
                  "reading_order": 8,
                  "bbox": [
                    440,
                    824,
                    585,
                    842
                  ],
                  "section_number": "8.6",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_338_order_0",
                  "label": "cap",
                  "text": "Example 8-4. Searching a treebank to find sentential complements",
                  "level": -1,
                  "page": 338,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    422,
                    89
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_338_order_2",
                  "label": "para",
                  "text": "The PP Attachment Corpus, nltk.corpus.ppattach, is another source of information\nabout the valency of particular verbs. Here we illustrate a technique for mining this\ncorpus. It finds pairs of prepositional phrases where the preposition and noun are fixed,\nbut where the choice of verb determines whether the prepositional phrase is attached\nto the VP or to the NP.",
                  "level": -1,
                  "page": 338,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    215,
                    585,
                    304
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_338_order_4",
                  "label": "para",
                  "text": "Among the output lines of this program we find offer-from-group N: ['rejected'] V:\n['received'], which indicates that received expects a separate PP complement attached\nto the VP, while rejected does not. As before, we can use this information to help con-\nstruct the grammar.",
                  "level": -1,
                  "page": 338,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    430,
                    584,
                    501
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_338_order_5",
                  "label": "para",
                  "text": "The NLTK corpus collection includes data from the PE08 Cross-Framework and Cross\nDomain Parser Evaluation Shared Task. A collection of larger grammars has been pre-\npared for the purpose of comparing different parsers, which can be obtained by down-\nloading the large_grammars package (e.g., python -m nltk.downloader large_grammars).",
                  "level": -1,
                  "page": 338,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    510,
                    585,
                    574
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_338_order_6",
                  "label": "para",
                  "text": "The NLTK corpus collection also includes a sample from the Sinica Treebank Corpus,\nconsisting of 10,000 parsed sentences drawn from the Academia Sinica Balanced Corpus\nof Modern Chinese. Let’s load and display one of the trees in this corpus.",
                  "level": -1,
                  "page": 338,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    582,
                    585,
                    630
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_338_order_7",
                  "label": "para",
                  "text": ">> nltk.corpus.sinica_treebank.parsed_sents()[3450].draw()",
                  "level": -1,
                  "page": 338,
                  "reading_order": 7,
                  "bbox": [
                    126,
                    636,
                    440,
                    654
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_338_order_8",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_338_figure_008.png)",
                  "level": -1,
                  "page": 338,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    663,
                    467,
                    779
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_338_order_9",
                  "label": "foot",
                  "text": "316 | Chapter 8: Analyzing Sentence Structure",
                  "level": -1,
                  "page": 338,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    824,
                    297,
                    842
                  ],
                  "section_number": "316",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_337_order_3",
              "label": "para",
              "text": "Parsing builds trees over sentences, according to a phrase structure grammar. Now, all\nthe examples we gave earlier only involved toy grammars containing a handful of pro-\nductions. What happens if we try to scale up this approach to deal with realistic corpora\nof language? In this section, we will see how to access treebanks, and look at the chal-\nlenge of developing broad-coverage grammars.",
              "level": -1,
              "page": 337,
              "reading_order": 3,
              "bbox": [
                97,
                286,
                585,
                367
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_336_order_13",
          "label": "para",
          "text": "So far, we have only considered “toy grammars,” small grammars that illustrate the key\naspects of parsing. But there is an obvious question as to whether the approach can be\nscaled up to cover large corpora of natural languages. How hard would it be to construct\nsuch a set of productions by hand? In general, the answer is: very hard. Even if we allow\nourselves to use various formal devices that give much more succinct representations\nof grammar productions, it is still extremely difficult to keep control of the complex\ninteractions between the many productions required to cover the major constructions\nof a language. In other words, it is hard to modularize grammars so that one portion\ncan be developed independently of the other parts. This in turn means that it is difficult",
          "level": -1,
          "page": 336,
          "reading_order": 13,
          "bbox": [
            97,
            645,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_336_order_14",
          "label": "foot",
          "text": "314 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 336,
          "reading_order": 14,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "314",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_337_order_0",
          "label": "para",
          "text": "to distribute the task of grammar writing across a team of linguists. Another difficulty\nis that as the grammar expands to cover a wider and wider range of constructions, there\nis a corresponding increase in the number of analyses that are admitted for any one\nsentence. In other words, ambiguity increases with coverage.",
          "level": -1,
          "page": 337,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_337_order_1",
          "label": "para",
          "text": "Despite these problems, some large collaborative projects have achieved interesting and\nimpressive results in developing rule-based grammars for several languages. Examples\nare the Lexical Functional Grammar (LFG) Pargram project, the Head-Driven Phrase\nStructure Grammar (HPSG) LinGO Matrix framework, and the Lexicalized Tree Ad-\njoining Grammar XTAG Project.",
          "level": -1,
          "page": 337,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            585,
            228
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_339_order_0",
      "label": "sec",
      "text": "Pernicious Ambiguity",
      "level": 1,
      "page": 339,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        243,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_339_order_1",
          "label": "para",
          "text": "Unfortunately, as the coverage of the grammar increases and the length of the input\nsentences grows, the number of parse trees grows rapidly. In fact, it grows at an astro-\nnomical rate.",
          "level": -1,
          "page": 339,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_339_order_2",
          "label": "para",
          "text": "Let’\ns explore this issue with the help of a simple example. The word fish is both a noun\nand a verb. We can make up the sentence fish fish fish, meaning fish like to fish for other\nfish. (Try this with police if you prefer something more sensible.) Here is a toy grammar\nfor the\n“fish”\nsentences.",
          "level": -1,
          "page": 339,
          "reading_order": 2,
          "bbox": [
            96,
            160,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_339_order_4",
          "label": "para",
          "text": "Now we can try parsing a longer sentence, fish fish fish fish fish , which among other\nthings, means “fish that other fish fish are in the habit of fishing fish themselves.” We\nuse the NLTK chart parser, which is presented earlier in this chapter. This sentence has\ntwo readings.",
          "level": -1,
          "page": 339,
          "reading_order": 4,
          "bbox": [
            97,
            331,
            585,
            396
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_339_order_6",
          "label": "para",
          "text": "As the length of this sentence goes up (3, 5, 7, ...) we get the following numbers of parse\ntrees: 1; 2; 5; 14; 42; 132; 429; 1,430; 4,862; 16,796; 58,786; 208,012; .... (These are\nthe Catalan numbers , which we saw in an exercise in Chapter 4 .) The last of these is\nfor a sentence of length 23, the average length of sentences in the WSJ section of Penn\nTreebank. For a sentence of length 50 there would be over $10^{12}$ parses, and this is only\nhalf the length of the Piglet sentence ( Section 8.1 ), which young children process ef-\nfortlessly. No practical NLP system could construct millions of trees for a sentence and\nchoose the appropriate one in the context. It's clear that humans don't do this either!",
          "level": -1,
          "page": 339,
          "reading_order": 6,
          "bbox": [
            97,
            491,
            585,
            621
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_339_order_7",
          "label": "para",
          "text": "Note that the problem is not with our choice of example. (Church & Patil, 1982) point\nout that the syntactic ambiguity of PP attachment in sentences like (15) also grows in\nproportion to the Catalan numbers.",
          "level": -1,
          "page": 339,
          "reading_order": 7,
          "bbox": [
            97,
            627,
            585,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_339_order_8",
          "label": "para",
          "text": "(15) Put the block in the box on the table",
          "level": -1,
          "page": 339,
          "reading_order": 8,
          "bbox": [
            109,
            689,
            350,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_339_order_9",
          "label": "para",
          "text": "So much for structural ambiguity; what about lexical ambiguity? As soon as we try to\nconstruct a broad-coverage grammar, we are forced to make lexical entries highly am-\nbiguous for their part-of-speech. In a toy grammar, $a$ is only a determiner, dog is only\na noun, and runs is only a verb. However, in a broad-coverage grammar, $a$ is also a",
          "level": -1,
          "page": 339,
          "reading_order": 9,
          "bbox": [
            97,
            716,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_339_order_10",
          "label": "foot",
          "text": "8.6 Grammar Development | 317",
          "level": -1,
          "page": 339,
          "reading_order": 10,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "8.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_340_order_0",
          "label": "para",
          "text": "noun (e.g., part a ), dog is also a verb (meaning to follow closely), and runs is also a noun\n(e.g., ski runs ). In fact, all words can be referred to by name: e.g., the verb `ate' is spelled\nwith three letters ; in speech we do not need to supply quotation marks. Furthermore,\nit is possible to verb most nouns. Thus a parser for a broad-coverage grammar will be\noverwhelmed with ambiguity. Even complete gibberish will often have a reading, e.g.,\nthe a are of I . As (Abney, 1996) has pointed out, this is not word salad but a grammatical\nnoun phrase, in which are is a noun meaning a hundredth of a hectare (or 100 sq m),\nand a and I are nouns designating coordinates, as shown in Figure 8 - 9 .",
          "level": -1,
          "page": 340,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_340_order_1",
          "label": "figure",
          "text": "Figure 8-9. The a are of I: A schematic drawing of 27 paddocks, each being one are in size, and each\nidentified using coordinates; the top-left cell is the a are of column A (after Abney). [IMAGE: ![Figure](figures/NLTK_page_340_figure_001.png)]",
          "level": -1,
          "page": 340,
          "reading_order": 1,
          "bbox": [
            100,
            232,
            583,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_340_figure_001.png)",
              "bbox": [
                100,
                232,
                583,
                385
              ],
              "page": 340,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Figure 8-9. The a are of I: A schematic drawing of 27 paddocks, each being one are in size, and each\nidentified using coordinates; the top-left cell is the a are of column A (after Abney).",
              "bbox": [
                97,
                393,
                585,
                421
              ],
              "page": 340,
              "reading_order": 2
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_340_order_3",
          "label": "para",
          "text": "Even though this phrase is unlikely, it is still grammatical, and a broad-coverage parser\nshould be able to construct a parse tree for it. Similarly, sentences that seem to be\nunambiguous, such as John saw Mary , turn out to have other readings we would not\nhave anticipated (as Abney explains). This ambiguity is unavoidable, and leads to hor-\nrendous inefficiency in parsing seemingly innocuous sentences. The solution to these\nproblems is provided by probabilistic parsing , which allows us to rank the parses of an\nambiguous sentence on the basis of evidence from corpora.",
          "level": -1,
          "page": 340,
          "reading_order": 3,
          "bbox": [
            97,
            456,
            585,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_340_order_4",
      "label": "sec",
      "text": "Weighted Grammar",
      "level": 1,
      "page": 340,
      "reading_order": 4,
      "bbox": [
        99,
        589,
        234,
        609
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_343_order_2",
          "label": "sub_sec",
          "text": "8.7 Summary",
          "level": 2,
          "page": 343,
          "reading_order": 2,
          "bbox": [
            97,
            196,
            207,
            219
          ],
          "section_number": "8.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_343_order_3",
              "label": "list_group",
              "text": "Sentences have internal organization that can be represented using a tree. Notable\nfeatures of constituent structure are: recursion, heads, complements, and\nmodifiers.\nA grammar is a compact characterization of a potentially infinite set of sentences;\nwe say that a tree is well-formed according to a grammar, or that a grammar licenses\na tree.",
              "level": -1,
              "page": 343,
              "reading_order": 3,
              "bbox": [
                122,
                224,
                585,
                277
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "Sentences have internal organization that can be represented using a tree. Notable\nfeatures of constituent structure are: recursion, heads, complements, and\nmodifiers.",
                  "bbox": [
                    122,
                    224,
                    585,
                    277
                  ],
                  "page": 343,
                  "reading_order": 3
                },
                {
                  "label": "list",
                  "text": "A grammar is a compact characterization of a potentially infinite set of sentences;\nwe say that a tree is well-formed according to a grammar, or that a grammar licenses\na tree.",
                  "bbox": [
                    122,
                    277,
                    585,
                    331
                  ],
                  "page": 343,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "A grammar is a formal model for describing whether a given phrase can be assigned\na particular constituent or dependency structure.",
                  "bbox": [
                    122,
                    331,
                    585,
                    367
                  ],
                  "page": 343,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "Given a set of syntactic categories, a context-free grammar uses a set of productions\nto say how a phrase of some category $A$ can be analyzed into a sequence of smaller\nparts $\\alpha _{1}...\\;\\alpha _{n}$.",
                  "bbox": [
                    122,
                    374,
                    585,
                    421
                  ],
                  "page": 343,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "A dependency grammar uses productions to specify what the dependents are of a\ngiven lexical head.",
                  "bbox": [
                    121,
                    427,
                    584,
                    458
                  ],
                  "page": 343,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "Syntactic ambiguity arises when one sentence has more than one syntactic analysis\n(e.g., prepositional phrase attachment ambiguity).",
                  "bbox": [
                    122,
                    465,
                    585,
                    495
                  ],
                  "page": 343,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "A parser is a procedure for finding one or more trees corresponding to a grammat-\nically well-formed sentence.",
                  "bbox": [
                    122,
                    501,
                    584,
                    529
                  ],
                  "page": 343,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "A simple top-down parser is the recursive descent parser, which recursively ex-\npands the start symbol (usually 5) with the help of the grammar productions, and\ntries to match the input sentence. This parser cannot handle left-recursive pro-\nductions (e.g., productions such as NP -> NP PP ). It is inefficient in the way it blindly\nexpands categories without checking whether they are compatible with the input\nstring, and in repeatedly expanding the same non-terminals and discarding the\nresults.",
                  "bbox": [
                    122,
                    537,
                    585,
                    654
                  ],
                  "page": 343,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "A simple bottom-up parser is the shift-reduce parser, which shifts input onto a\nstack and tries to match the items at the top of the stack with the righthand side\nof grammar productions. This parser is not guaranteed to find a valid parse for the\ninput, even if one exists, and builds substructures without checking whether it is\nglobally consistent with the grammar.",
                  "bbox": [
                    122,
                    654,
                    585,
                    739
                  ],
                  "page": 343,
                  "reading_order": 11
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_343_order_12",
              "label": "foot",
              "text": "8.7 Summary|321",
              "level": -1,
              "page": 343,
              "reading_order": 12,
              "bbox": [
                494,
                824,
                584,
                842
              ],
              "section_number": "8.7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_344_order_0",
          "label": "sub_sec",
          "text": "8.8 Further Reading",
          "level": 2,
          "page": 344,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            261,
            100
          ],
          "section_number": "8.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_344_order_1",
              "label": "para",
              "text": "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web. For more examples of parsing with NLTK, please\nsee the Parsing HOWTO at http://www.nltk.org/howto.",
              "level": -1,
              "page": 344,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                161
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_344_order_2",
              "label": "para",
              "text": "There are many introductory books on syntax. (O’Grady et al., 2004) is a general in-\ntroduction to linguistics, while (Radford, 1988) provides a gentle introduction to trans-\nformational grammar, and can be recommended for its coverage of transformational\napproaches to unbounded dependency constructions. The most widely used term in\nlinguistics for formal grammar is generative grammar , though it has nothing to do\nwith generation (Chomsky, 1965) .",
              "level": -1,
              "page": 344,
              "reading_order": 2,
              "bbox": [
                97,
                161,
                585,
                262
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_344_order_3",
              "label": "para",
              "text": "(Burton-Roberts, 1997) is a practically oriented textbook on how to analyze constitu-\nency in English, with extensive exemplification and exercises. (Huddleston & Pullum,\n2002) provides an up-to-date and comprehensive analysis of syntactic phenomena in\nEnglish.",
              "level": -1,
              "page": 344,
              "reading_order": 3,
              "bbox": [
                97,
                268,
                584,
                335
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_344_order_4",
              "label": "para",
              "text": "Chapter 12 of (Jurafsky & Martin, 2008) covers formal grammars of English; Sections\n13.1–3 cover simple parsing algorithms and techniques for dealing with ambiguity;\nChapter 14 covers statistical parsing; and Chapter 16 covers the Chomsky hierarchy\nand the formal complexity of natural language. (Levin, 1993) has categorized English\nverbs into fine-grained classes, according to their syntactic properties.",
              "level": -1,
              "page": 344,
              "reading_order": 4,
              "bbox": [
                97,
                340,
                585,
                430
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_344_order_5",
              "label": "para",
              "text": "There are several ongoing efforts to build large-scale rule-based grammars, e.g., the\nLFG Pargram project ( http://www2.parc.com/istl/groups/nltt/pargram/ ), the HPSG Lin­\nGO Matrix framework ( http://www.delph­in.net/matrix/ ), and the XTAG Project ( http:\n//www.cis.upenn.edu/~xtag/ ).",
              "level": -1,
              "page": 344,
              "reading_order": 5,
              "bbox": [
                97,
                430,
                585,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_344_order_6",
          "label": "sub_sec",
          "text": "8.9 Exercises",
          "level": 2,
          "page": 344,
          "reading_order": 6,
          "bbox": [
            97,
            519,
            207,
            546
          ],
          "section_number": "8.9",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_344_order_7",
              "label": "list_group",
              "text": "1. ◦ Can you come up with grammatical sentences that probably have never been\nuttered before? (Take turns with a partner.) What does this tell you about human\nlanguage?\n2. ◦ Recall Strunk and White’s prohibition against using a sentence-initial however\nto mean “although.” Do a web search for however used at the start of the sentence.\nHow widely used is this construction?",
              "level": -1,
              "page": 344,
              "reading_order": 7,
              "bbox": [
                100,
                555,
                584,
                609
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "1. ◦ Can you come up with grammatical sentences that probably have never been\nuttered before? (Take turns with a partner.) What does this tell you about human\nlanguage?",
                  "bbox": [
                    100,
                    555,
                    584,
                    609
                  ],
                  "page": 344,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "2. ◦ Recall Strunk and White’s prohibition against using a sentence-initial however\nto mean “although.” Do a web search for however used at the start of the sentence.\nHow widely used is this construction?",
                  "bbox": [
                    100,
                    609,
                    585,
                    663
                  ],
                  "page": 344,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "3. ◦ Consider the sentence Kim arrived or Dana left and everyone cheered. Write down\nthe parenthesized forms to show the relative scope of and and or. Generate tree\nstructures corresponding to both of these interpretations.",
                  "bbox": [
                    100,
                    663,
                    585,
                    716
                  ],
                  "page": 344,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "4. ◦ The Tree class implements a variety of other useful methods. See the Tree help\ndocumentation for more details (i.e., import the Tree class and then type\nhelp(Tree)).",
                  "bbox": [
                    100,
                    716,
                    585,
                    770
                  ],
                  "page": 344,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "5. ◦ In this exercise you will manually construct some parse trees.",
                  "bbox": [
                    100,
                    770,
                    485,
                    788
                  ],
                  "page": 344,
                  "reading_order": 11
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_344_order_12",
              "label": "foot",
              "text": "322 | Chapter 8: Analyzing Sentence Structure",
              "level": -1,
              "page": 344,
              "reading_order": 12,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "322",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_0",
              "label": "para",
              "text": "a. Write code to produce two trees, one for each reading of the phrase old men\nand women.",
              "level": -1,
              "page": 345,
              "reading_order": 0,
              "bbox": [
                126,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_1",
              "label": "para",
              "text": "b. Encode any of the trees presented in this chapter as a labeled bracketing, and\nuse nltk.Tree() to check that it is well-formed. Now use draw() to display the\ntree.",
              "level": -1,
              "page": 345,
              "reading_order": 1,
              "bbox": [
                126,
                107,
                585,
                161
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_2",
              "label": "para",
              "text": "c. As in (a), draw a tree for The woman saw a man last Thursday.",
              "level": -1,
              "page": 345,
              "reading_order": 2,
              "bbox": [
                126,
                161,
                503,
                180
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_3",
              "label": "para",
              "text": "6. ◦ Write a recursive function to traverse a tree and return the depth of the tree, such\nthat a tree with a single node would have depth zero. (Hint: the depth of a subtree\nis the maximum depth of its children, plus one.)",
              "level": -1,
              "page": 345,
              "reading_order": 3,
              "bbox": [
                100,
                187,
                585,
                234
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_4",
              "label": "para",
              "text": "7. ◦ Analyze the A.A. Milne sentence about Piglet, by underlining all of the sentences\nit contains then replacing these with S (e.g., the first sentence becomes S when S).\nDraw a tree structure for this “compressed” sentence. What are the main syntactic\nconstructions used for building such a long sentence?",
              "level": -1,
              "page": 345,
              "reading_order": 4,
              "bbox": [
                100,
                240,
                585,
                304
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_5",
              "label": "para",
              "text": "8. ◦ In the recursive descent parser demo, experiment with changing the sentence to\nbe parsed by selecting Edit Text in the Edit menu.",
              "level": -1,
              "page": 345,
              "reading_order": 5,
              "bbox": [
                100,
                311,
                585,
                341
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_6",
              "label": "para",
              "text": "9. ◦ Can the grammar in grammar1 (Example 8-1) be used to describe sentences that\nare more than 20 words in length?",
              "level": -1,
              "page": 345,
              "reading_order": 6,
              "bbox": [
                100,
                348,
                585,
                379
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_7",
              "label": "para",
              "text": "$^{\\circ}$ Use the graphical chart-parser interface to experiment with different rule invo-\ncation strategies. Come up with your own strategy that you can execute manually\nusing the graphical interface. Describe the steps, and report any efficiency im-\nprovements it has (e.g., in terms of the size of the resulting chart). Do these im-\nprovements depend on the structure of the grammar? What do you think of the\nprospects for significant performance boosts from cleverer rule invocation\nstrategies?",
              "level": -1,
              "page": 345,
              "reading_order": 7,
              "bbox": [
                100,
                385,
                585,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_8",
              "label": "para",
              "text": "11. ◦ With pen and paper, manually trace the execution of a recursive descent parser\nand a shift-reduce parser, for a CFG you have already seen, or one of your own\ndevising.",
              "level": -1,
              "page": 345,
              "reading_order": 8,
              "bbox": [
                100,
                501,
                585,
                555
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_9",
              "label": "para",
              "text": "12. ◦ We have seen that a chart parser adds but never removes edges from a chart.\nWhy?",
              "level": -1,
              "page": 345,
              "reading_order": 9,
              "bbox": [
                100,
                555,
                584,
                591
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_10",
              "label": "para",
              "text": "13. ◦ Consider the sequence of words: Buffalo buffalo Buffalo buffalo buffalo buffalo\nBuffalo buffalo. This is a grammatically correct sentence, as explained at http://en\n.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buf\nfalo. Consider the tree diagram presented on this Wikipedia page, and write down\na suitable grammar. Normalize case to lowercase, to simulate the problem that a\nlistener has when hearing this sentence. Can you find other parses for this sentence?\nHow does the number of parse trees grow as the sentence gets longer? (More ex-\namples of these sentences can be found at http://en.wikipedia.org/wiki/List_of_ho\nmophonous_phrases.)",
              "level": -1,
              "page": 345,
              "reading_order": 10,
              "bbox": [
                100,
                591,
                585,
                743
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_11",
              "label": "para",
              "text": "14. o You can modify the grammar in the recursive descent parser demo by selecting\nEdit Grammar in the Edit menu. Change the first expansion production, namely",
              "level": -1,
              "page": 345,
              "reading_order": 11,
              "bbox": [
                100,
                750,
                585,
                780
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_345_order_12",
              "label": "foot",
              "text": "8.9 Exercises | 323",
              "level": -1,
              "page": 345,
              "reading_order": 12,
              "bbox": [
                494,
                824,
                584,
                842
              ],
              "section_number": "8.9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_0",
              "label": "para",
              "text": "NP -> Det N PP, to NP -> NP PP. Using the Step button, try to build a parse tree.\nWhat happens?",
              "level": -1,
              "page": 346,
              "reading_order": 0,
              "bbox": [
                122,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_1",
              "label": "para",
              "text": "15. o Extend the grammar in grammar2 with productions that expand prepositions as\nintransitive, transitive, and requiring a PP complement. Based on these produc-\ntions, use the method of the preceding exercise to draw a tree for the sentence Lee\nran away home.",
              "level": -1,
              "page": 346,
              "reading_order": 1,
              "bbox": [
                100,
                107,
                585,
                179
              ],
              "section_number": "15",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_2",
              "label": "para",
              "text": "16. o Pick some common verbs and complete the following tasks",
              "level": -1,
              "page": 346,
              "reading_order": 2,
              "bbox": [
                100,
                179,
                472,
                197
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_3",
              "label": "para",
              "text": "a. Write a program to find those verbs in the PP Attachment Corpus nltk.cor\npus.ppattach . Find any cases where the same verb exhibits two different at-\ntachments, but where the first noun, or second noun, or preposition stays\nunchanged (as we saw in our discussion of syntactic ambiguity in Section 8.2 ).",
              "level": -1,
              "page": 346,
              "reading_order": 3,
              "bbox": [
                126,
                197,
                585,
                268
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_4",
              "label": "para",
              "text": "b. Devise CFG grammar productions to cover some of these cases.",
              "level": -1,
              "page": 346,
              "reading_order": 4,
              "bbox": [
                126,
                268,
                512,
                288
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_5",
              "label": "para",
              "text": "17. o Write a program to compare the efficiency of a top-down chart parser compared\nwith a recursive descent parser (Section 8.4). Use the same grammar and input\nsentences for both. Compare their performance using the timeit module (see Sec-\ntion 4.7 for an example of how to do this).",
              "level": -1,
              "page": 346,
              "reading_order": 5,
              "bbox": [
                100,
                288,
                585,
                358
              ],
              "section_number": "17",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_6",
              "label": "para",
              "text": "18. • Compare the performance of the top-down, bottom-up, and left-corner parsers\nusing the same grammar and three grammatical test sentences. Use timeit to log\nthe amount of time each parser takes on the same sentence. Write a function that\nruns all three parsers on all three sentences, and prints a 3-by-3 grid of times, as\nwell as row and column totals. Discuss your findings.",
              "level": -1,
              "page": 346,
              "reading_order": 6,
              "bbox": [
                100,
                358,
                585,
                448
              ],
              "section_number": "18",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_7",
              "label": "para",
              "text": "19. o Read up on “garden path” sentences. How might the computational work of a\nparser relate to the difficulty humans have with processing these sentences? (See\nhttp://en.wikipedia.org/wiki/Garden_path_sentence.)",
              "level": -1,
              "page": 346,
              "reading_order": 7,
              "bbox": [
                100,
                448,
                585,
                501
              ],
              "section_number": "19",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_8",
              "label": "para",
              "text": "20. o To compare multiple trees in a single window, we can use the draw_trees()\nmethod. Define some trees and try it out:",
              "level": -1,
              "page": 346,
              "reading_order": 8,
              "bbox": [
                98,
                501,
                584,
                537
              ],
              "section_number": "20",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_10",
              "label": "para",
              "text": "21. • Using tree positions, list the subjects of the first 100 sentences in the Penn tree-\nbank; to make the results easier to view, limit the extracted subjects to subtrees\nwhose height is at most 2.",
              "level": -1,
              "page": 346,
              "reading_order": 10,
              "bbox": [
                98,
                573,
                585,
                627
              ],
              "section_number": "21",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_11",
              "label": "para",
              "text": "22. ● Inspect the PP Attachment Corpus and try to suggest some factors that influence\nPP attachment.",
              "level": -1,
              "page": 346,
              "reading_order": 11,
              "bbox": [
                98,
                627,
                585,
                663
              ],
              "section_number": "22",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_12",
              "label": "para",
              "text": "23. n Section 8.2, we claimed that there are linguistic regularities that cannot be\ndescribed simply in terms of n-grams. Consider the following sentence, particularly\nthe position of the phrase in his turn. Does this illustrate a problem for an approach\nbased on n-grams?",
              "level": -1,
              "page": 346,
              "reading_order": 12,
              "bbox": [
                98,
                663,
                585,
                734
              ],
              "section_number": "23",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_13",
              "label": "para",
              "text": "What was more, the in his turn somewhat youngish Nikolay Parfenovich also turned\nout to be the only person in the entire world to acquire a sincere liking to our “dis-\ncriminated-against” public procurator. (Dostoevsky: The Brothers Karamazov)",
              "level": -1,
              "page": 346,
              "reading_order": 13,
              "bbox": [
                122,
                734,
                585,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_346_order_14",
              "label": "foot",
              "text": "324 | Chapter 8: Analyzing Sentence Structure",
              "level": -1,
              "page": 346,
              "reading_order": 14,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "324",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_0",
              "label": "para",
              "text": "24. o Write a recursive function that produces a nested bracketing for a tree, leaving\nout the leaf nodes and displaying the non-terminal labels after their subtrees. So\nthe example in Section 8.6 about Pierre Vinken would produce: [[[NNP NNP]NP ,\n[ADJP [CD NNS]NP JJ]ADJP ,]NP-SBJ MD [VB [DT NN]NP [IN [DT JJ NN]NP]PP-CLR\n[NNP CD]NP-TMP]VP .]S. Consecutive categories should be separated by space.",
              "level": -1,
              "page": 347,
              "reading_order": 0,
              "bbox": [
                98,
                71,
                585,
                155
              ],
              "section_number": "24",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_1",
              "label": "para",
              "text": "25. o Download several electronic books from Project Gutenberg. Write a program to\nscan these texts for any extremely long sentences. What is the longest sentence you\ncan find? What syntactic construction(s) are responsible for such long sentences?",
              "level": -1,
              "page": 347,
              "reading_order": 1,
              "bbox": [
                98,
                161,
                585,
                209
              ],
              "section_number": "25",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_2",
              "label": "para",
              "text": "26. o Modify the functions init_wfst() and complete_wfst() so that the contents of\neach cell in the WFST is a set of non-terminal symbols rather than a single non-\nterminal.",
              "level": -1,
              "page": 347,
              "reading_order": 2,
              "bbox": [
                98,
                215,
                584,
                260
              ],
              "section_number": "26",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_3",
              "label": "para",
              "text": "27. o Consider the algorithm in Example 8-3. Can you explain why parsing context-\nfree grammar is proportional to n3, where n is the length of the input sentence?",
              "level": -1,
              "page": 347,
              "reading_order": 3,
              "bbox": [
                98,
                268,
                584,
                304
              ],
              "section_number": "27",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_4",
              "label": "para",
              "text": "28. o Process each tree of the Penn Treebank Corpus sample nltk.corpus.treebank\nand extract the productions with the help of Tree.productions(). Discard the pro-\nductions that occur only once. Productions with the same lefthand side and similar\nrighthand sides can be collapsed, resulting in an equivalent but more compact set\nof rules. Write code to output a compact grammar.",
              "level": -1,
              "page": 347,
              "reading_order": 4,
              "bbox": [
                98,
                304,
                585,
                387
              ],
              "section_number": "28",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_5",
              "label": "para",
              "text": "29.\n• One common way of defining the subject of a sentence S in English is as the noun\nphrase that is the child of S and the sibling of VP. Write a function that takes the tree\nfor a sentence and returns the subtree corresponding to the subject of the sentence.\nWhat should it do if the root node of the tree passed to this function is not S, or if\nit lacks a subject?",
              "level": -1,
              "page": 347,
              "reading_order": 5,
              "bbox": [
                98,
                394,
                586,
                474
              ],
              "section_number": "29",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_6",
              "label": "para",
              "text": "30. • Write a function that takes a grammar (such as the one defined in Exam-\nple 8-1) and returns a random sentence generated by the grammar. (Use gram\nmar.start() to find the start symbol of the grammar;grammar.productions(lhs) to\nget the list of productions from the grammar that have the specified lefthand side;\nand production.rhs() to get the righthand side of a production.)",
              "level": -1,
              "page": 347,
              "reading_order": 6,
              "bbox": [
                98,
                474,
                585,
                564
              ],
              "section_number": "30",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_7",
              "label": "para",
              "text": "31.\n• Implement a version of the shift-reduce parser using backtracking, so that it finds\nall possible parses for a sentence, what might be called a “recursive ascent parser.”\nConsult the Wikipedia entry for backtracking at http://en.wikipedia.org/wiki/Back\ntracking.",
              "level": -1,
              "page": 347,
              "reading_order": 7,
              "bbox": [
                98,
                564,
                585,
                631
              ],
              "section_number": "31",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_8",
              "label": "para",
              "text": "32. • As we saw in Chapter 7, it is possible to collapse chunks down to their chunk\nlabel. When we do this for sentences involving the word gave, we find patterns\nsuch as the following:",
              "level": -1,
              "page": 347,
              "reading_order": 8,
              "bbox": [
                98,
                636,
                585,
                685
              ],
              "section_number": "32",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_347_order_10",
              "label": "foot",
              "text": "8.9 Exercises | 325",
              "level": -1,
              "page": 347,
              "reading_order": 10,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "8.9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_348_order_0",
              "label": "para",
              "text": "a. Use this method to study the complementation patterns of a verb of interest,\nand write suitable grammar productions. (This task is sometimes called lexical\nacquisition.)",
              "level": -1,
              "page": 348,
              "reading_order": 0,
              "bbox": [
                126,
                71,
                584,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_348_order_1",
              "label": "para",
              "text": "b. Identify some English verbs that are near-synonyms, such as the dumped/filled/\nloaded example from (64) in Chapter 9 . Use the chunking method to study the\ncomplementation patterns of these verbs. Create a grammar to cover these\ncases. Can the verbs be freely substituted for each other, or are there con-\nstraints? Discuss your findings.",
              "level": -1,
              "page": 348,
              "reading_order": 1,
              "bbox": [
                126,
                125,
                585,
                209
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_348_order_2",
              "label": "para",
              "text": "33. • Develop a left-corner parser based on the recursive descent parser, and inheriting\nfrom ParseI.",
              "level": -1,
              "page": 348,
              "reading_order": 2,
              "bbox": [
                98,
                215,
                585,
                243
              ],
              "section_number": "33",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_348_order_3",
              "label": "para",
              "text": "34. • Extend NLTK's shift-reduce parser to incorporate backtracking, so that it is\nguaranteed to find all parses that exist (i.e., it is complete).",
              "level": -1,
              "page": 348,
              "reading_order": 3,
              "bbox": [
                98,
                250,
                585,
                286
              ],
              "section_number": "34",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_348_order_4",
              "label": "para",
              "text": "35. • Modify the functions init_wfst() and complete_wfst() so that when a non-\nterminal symbol is added to a cell in the WFST, it includes a record of the cells\nfrom which it was derived. Implement a function that will convert a WFST in this\nform to a parse tree.",
              "level": -1,
              "page": 348,
              "reading_order": 4,
              "bbox": [
                98,
                286,
                585,
                354
              ],
              "section_number": "35",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_348_order_5",
              "label": "foot",
              "text": "326 | Chapter 8: Analyzing Sentence Structure",
              "level": -1,
              "page": 348,
              "reading_order": 5,
              "bbox": [
                97,
                824,
                297,
                842
              ],
              "section_number": "326",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_340_order_5",
          "label": "para",
          "text": "As we have just seen, dealing with ambiguity is a key challenge in developing broad-\ncoverage parsers. Chart parsers improve the efficiency of computing multiple parses of\nthe same sentences, but they are still overwhelmed by the sheer number of possible\nparses. Weighted grammars and probabilistic parsing algorithms have provided an ef-\nfective solution to these problems.",
          "level": -1,
          "page": 340,
          "reading_order": 5,
          "bbox": [
            97,
            617,
            586,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_340_order_6",
          "label": "para",
          "text": "Before looking at these, we need to understand why the notion of grammaticality could\nbe gradient. Considering the verb give. This verb requires both a direct object (the thing\nbeing given) and an indirect object (the recipient). These complements can be given in\neither order, as illustrated in (16). In the “ prepositional dative ” form in (16a), the direct\nobject appears first, followed by a prepositional phrase containing the indirect object.",
          "level": -1,
          "page": 340,
          "reading_order": 6,
          "bbox": [
            97,
            698,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_340_order_7",
          "label": "foot",
          "text": "318 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 340,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "318",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_341_order_0",
          "label": "para",
          "text": "(16) a. Kim gave a bone to the dog.",
          "level": -1,
          "page": 341,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            325,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_341_order_1",
          "label": "para",
          "text": "b. Kim gave the dog a bone",
          "level": -1,
          "page": 341,
          "reading_order": 1,
          "bbox": [
            151,
            96,
            307,
            110
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_341_order_2",
          "label": "para",
          "text": "In the “ double object ” form in (16b), the indirect object appears first, followed by the\ndirect object. In this case, either order is acceptable. However, if the indirect object is\na pronoun, there is a strong preference for the double object construction:",
          "level": -1,
          "page": 341,
          "reading_order": 2,
          "bbox": [
            97,
            124,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_341_order_3",
          "label": "para",
          "text": "(17) a. Kim gives the heebie-jeebies to me (prepositional dative)",
          "level": -1,
          "page": 341,
          "reading_order": 3,
          "bbox": [
            109,
            186,
            485,
            200
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_341_order_4",
          "label": "para",
          "text": "b. Kim gives me the heebie-jeebies (double object).",
          "level": -1,
          "page": 341,
          "reading_order": 4,
          "bbox": [
            150,
            206,
            440,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_341_order_5",
          "label": "para",
          "text": "Using the Penn Treebank sample, we can examine all instances of prepositional dative\nand double object constructions involving give, as shown in Example 8-5.",
          "level": -1,
          "page": 341,
          "reading_order": 5,
          "bbox": [
            97,
            232,
            585,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_341_order_6",
          "label": "para",
          "text": "Example 8-5. Usage of give and gave in the Penn Treebank sample",
          "level": -1,
          "page": 341,
          "reading_order": 6,
          "bbox": [
            97,
            277,
            422,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_341_order_9",
          "label": "foot",
          "text": "8.6 Grammar Development | 319",
          "level": -1,
          "page": 341,
          "reading_order": 9,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "8.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_342_order_0",
          "label": "para",
          "text": "We can observe a strong tendency for the shortest complement to appear first. How-\never, this does not account for a form like give NP: federal judges / NP: a raise,\nwhere animacy may play a role. In fact, there turns out to be a large number of\ncontributing factors, as surveyed by (Bresnan & Hay, 2008) . Such preferences can be\nrepresented in a weighted grammar.",
          "level": -1,
          "page": 342,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            586,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_342_order_1",
          "label": "para",
          "text": "A probabilistic context-free grammar (or PCFG) is a context-free grammar that as-\nsociates a probability with each of its productions. It generates the same set of parses\nfor a text that the corresponding context-free grammar does, and assigns a probability\nto each parse. The probability of a parse generated by a PCFG is simply the product of\nthe probabilities of the productions used to generate it.",
          "level": -1,
          "page": 342,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            586,
            245
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_342_order_2",
          "label": "para",
          "text": "The simplest way to define a PCFG is to load it from a specially formatted string con-\nsisting of a sequence of weighted productions, where weights appear in brackets, as\nshown in Example 8-6.",
          "level": -1,
          "page": 342,
          "reading_order": 2,
          "bbox": [
            97,
            250,
            585,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_342_order_3",
          "label": "para",
          "text": "Example 8-6. Defining a probabilistic context-free grammar (PCFG).",
          "level": -1,
          "page": 342,
          "reading_order": 3,
          "bbox": [
            97,
            313,
            440,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_342_order_6",
          "label": "para",
          "text": "It is sometimes convenient to combine multiple productions into a single line, e.g.,\nVP -> TV NP [0.4] | IV [0.3] | DatV NP NP [0.3]. In order to ensure that the trees\ngenerated by the grammar form a probability distribution, PCFG grammars impose the\nconstraint that all productions with a given lefthand side must have probabilities that\nsum to one. The grammar in Example 8-6 obeys this constraint: for S, there is only one\nproduction, with a probability of 1.0; for VP, 0.4+0.3+0.3=1.0; and for NP, 0.8+0.2=1.0.\nThe parse tree returned by parse() includes probabilities:",
          "level": -1,
          "page": 342,
          "reading_order": 6,
          "bbox": [
            97,
            645,
            585,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_342_order_7",
          "label": "foot",
          "text": "320 | Chapter 8: Analyzing Sentence Structure",
          "level": -1,
          "page": 342,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            297,
            842
          ],
          "section_number": "320",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_343_order_1",
          "label": "para",
          "text": "Now that parse trees are assigned probabilities, it no longer matters that there may be\na huge number of possible parses for a given sentence. A parser will be responsible for\nfinding the most likely parses.",
          "level": -1,
          "page": 343,
          "reading_order": 1,
          "bbox": [
            97,
            122,
            585,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_349_order_0",
      "label": "sec",
      "text": "CHAPTER 9\n\nBuilding Feature-Based Grammars",
      "level": 1,
      "page": 349,
      "reading_order": 0,
      "bbox": [
        144,
        78,
        584,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_349_order_7",
          "label": "sub_sec",
          "text": "9.1 Grammatical Features",
          "level": 2,
          "page": 349,
          "reading_order": 7,
          "bbox": [
            97,
            591,
            306,
            611
          ],
          "section_number": "9.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_349_order_8",
              "label": "para",
              "text": "In Chapter 6 , we described how to build classifiers that rely on detecting features of\ntext. Such features may be quite simple, such as extracting the last letter of a word, or\nmore complex, such as a part-of-speech tag that has itself been predicted by the clas-\nsifier. In this chapter, we will investigate the role of features in building rule-based\ngrammars. In contrast to feature extractors, which record features that have been au-\ntomatically detected, we are now going to declare the features of words and phrases.\nWe start off with a very simple example, using dictionaries to store features and their\nvalues.",
              "level": -1,
              "page": 349,
              "reading_order": 8,
              "bbox": [
                97,
                618,
                586,
                752
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_349_order_10",
              "label": "foot",
              "text": "327",
              "level": -1,
              "page": 349,
              "reading_order": 10,
              "bbox": [
                566,
                824,
                585,
                837
              ],
              "section_number": "327",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_350_order_0",
              "label": "para",
              "text": "The objects kim and chase both have a couple of shared features, CAT (grammatical\ncategory) and ORTH (orthography, i.e., spelling). In addition, each has a more semanti-\ncally oriented feature: kim['REF'] is intended to give the referent of kim, while\nchase['REL'] gives the relation expressed by chase. In the context of rule-based gram-\nmars, such pairings of features and values are known as feature structures, and we\nwill shortly see alternative notations for them.",
              "level": -1,
              "page": 350,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                170
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_350_order_1",
              "label": "para",
              "text": "Feature structures contain various kinds of information about grammatical entities.\nThe information need not be exhaustive, and we might want to add further properties.\nFor example, in the case of a verb, it is often useful to know what “semantic role” is\nplayed by the arguments of the verb. In the case of chase, the subject plays the role of\n“agent,” whereas the object has the role of “patient.” Let’s add this information, using\n'sbj' (subject) and 'obj' (object) as placeholders which will get filled once the verb\ncombines with its grammatical arguments:",
              "level": -1,
              "page": 350,
              "reading_order": 1,
              "bbox": [
                97,
                179,
                586,
                295
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_350_order_3",
              "label": "para",
              "text": "If we now process a sentence Kim chased Lee, we want to “bind” the verb’s agent role\nto the subject and the patient role to the object. We do this by linking to the REF feature\nof the relevant NP. In the following example, we make the simple-minded assumption\nthat the NPs immediately to the left and right of the verb are the subject and object,\nrespectively. We also add a feature structure for Lee to complete the example.",
              "level": -1,
              "page": 350,
              "reading_order": 3,
              "bbox": [
                97,
                338,
                585,
                421
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_350_order_5",
              "label": "para",
              "text": "The same approach could be adopted for a different verb—say, surprise —though in\nthis case, the subject would play the role of “source” (SRC), and the object plays the role\nof “experiencer” (EXP):",
              "level": -1,
              "page": 350,
              "reading_order": 5,
              "bbox": [
                97,
                644,
                585,
                691
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_350_order_7",
              "label": "para",
              "text": "Feature structures are pretty powerful, but the way in which we have manipulated them\nis extremely ad hoc . Our next task in this chapter is to show how the framework of\ncontext-free grammar and parsing can be expanded to accommodate feature structures,\nso that we can build analyses like this in a more generic and principled way. We will",
              "level": -1,
              "page": 350,
              "reading_order": 7,
              "bbox": [
                97,
                734,
                584,
                798
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_350_order_8",
              "label": "foot",
              "text": "328 | Chapter 9: Building Feature-Based Grammars",
              "level": -1,
              "page": 350,
              "reading_order": 8,
              "bbox": [
                97,
                824,
                316,
                842
              ],
              "section_number": "328",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_351_order_0",
              "label": "para",
              "text": "start off by looking at the phenomenon of syntactic agreement; we will show how\nagreement constraints can be expressed elegantly using features, and illustrate their use\nin a simple grammar.",
              "level": -1,
              "page": 351,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_351_order_1",
              "label": "para",
              "text": "Since feature structures are a general data structure for representing information of any\nkind, we will briefly look at them from a more formal point of view, and illustrate the\nsupport for feature structures offered by NLTK. In the final part of the chapter, we\ndemonstrate that the additional expressiveness of features opens up a wide spectrum\nof possibilities for describing sophisticated aspects of linguistic structure.",
              "level": -1,
              "page": 351,
              "reading_order": 1,
              "bbox": [
                97,
                132,
                585,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_349_order_1",
          "label": "para",
          "text": "Natural languages have an extensive range of grammatical constructions which are hard\nto handle with the simple methods described in Chapter 8. In order to gain more flex-\nibility, we change our treatment of grammatical categories like S, NP, and V. In place of\natomic labels, we decompose them into structures like dictionaries, where features can\ntake on a range of values.",
          "level": -1,
          "page": 349,
          "reading_order": 1,
          "bbox": [
            97,
            304,
            586,
            386
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_349_order_2",
          "label": "para",
          "text": "The goal of this chapter is to answer the following questions",
          "level": -1,
          "page": 349,
          "reading_order": 2,
          "bbox": [
            100,
            394,
            441,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_349_order_3",
          "label": "para",
          "text": "1. How can we extend the framework of context-free grammars with features so as\nto gain more fine-grained control over grammatical categories and productions?",
          "level": -1,
          "page": 349,
          "reading_order": 3,
          "bbox": [
            100,
            420,
            585,
            451
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_349_order_4",
          "label": "para",
          "text": "2. What are the main formal properties of feature structures, and how do we use them\ncomputationally?",
          "level": -1,
          "page": 349,
          "reading_order": 4,
          "bbox": [
            100,
            456,
            584,
            492
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_349_order_5",
          "label": "para",
          "text": "3. What kinds of linguistic patterns and grammatical constructions can we now cap-\nture with feature-based grammars?",
          "level": -1,
          "page": 349,
          "reading_order": 5,
          "bbox": [
            100,
            492,
            584,
            528
          ],
          "section_number": "3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_349_order_6",
          "label": "para",
          "text": "Along the way, we will cover more topics in English syntax, including phenomena such\nas agreement, subcategorization, and unbounded dependency constructions.",
          "level": -1,
          "page": 349,
          "reading_order": 6,
          "bbox": [
            97,
            536,
            584,
            566
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_351_order_2",
      "label": "sec",
      "text": "Syntactic Agreement",
      "level": 1,
      "page": 351,
      "reading_order": 2,
      "bbox": [
        97,
        224,
        243,
        250
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_351_order_3",
          "label": "para",
          "text": "The following examples show pairs of word sequences, the first of which is grammatical\nand the second not. (We use an asterisk at the start of a word sequence to signal that\nit is ungrammatical.)",
          "level": -1,
          "page": 351,
          "reading_order": 3,
          "bbox": [
            100,
            250,
            584,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_4",
          "label": "para",
          "text": "1) a. this dog",
          "level": -1,
          "page": 351,
          "reading_order": 4,
          "bbox": [
            125,
            313,
            212,
            331
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_5",
          "label": "para",
          "text": "b.\n*these dog",
          "level": -1,
          "page": 351,
          "reading_order": 5,
          "bbox": [
            151,
            338,
            225,
            352
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_6",
          "label": "para",
          "text": "(2) a. these dogs",
          "level": -1,
          "page": 351,
          "reading_order": 6,
          "bbox": [
            118,
            366,
            226,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_7",
          "label": "para",
          "text": "b. *this dogs",
          "level": -1,
          "page": 351,
          "reading_order": 7,
          "bbox": [
            150,
            385,
            225,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_8",
          "label": "para",
          "text": "In English, nouns are usually marked as being singular or plural. The form of the de-\nmonstrative also varies: this (singular) and these (plural). Examples (1) and (2) show\nthat there are constraints on the use of demonstratives and nouns within a noun phrase:\neither both are singular or both are plural. A similar constraint holds between subjects\nand predicates:",
          "level": -1,
          "page": 351,
          "reading_order": 8,
          "bbox": [
            97,
            412,
            585,
            496
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_9",
          "label": "para",
          "text": "(3) a. the dog runs",
          "level": -1,
          "page": 351,
          "reading_order": 9,
          "bbox": [
            118,
            510,
            238,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_10",
          "label": "para",
          "text": "b. *the dog run",
          "level": -1,
          "page": 351,
          "reading_order": 10,
          "bbox": [
            150,
            528,
            237,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_11",
          "label": "para",
          "text": "(4) a. the dogs run",
          "level": -1,
          "page": 351,
          "reading_order": 11,
          "bbox": [
            118,
            555,
            243,
            574
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_12",
          "label": "para",
          "text": "b.\n*the dogs runs",
          "level": -1,
          "page": 351,
          "reading_order": 12,
          "bbox": [
            151,
            581,
            248,
            595
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_13",
          "label": "para",
          "text": "Here we can see that morphological properties of the verb co-vary with syntactic prop-\nerties of the subject noun phrase. This co-variance is called agreement . If we look\nfurther at verb agreement in English, we will see that present tense verbs typically have\ntwo inflected forms: one for third person singular, and another for every other combi-\nnation of person and number, as shown in Table 9 -1.",
          "level": -1,
          "page": 351,
          "reading_order": 13,
          "bbox": [
            97,
            609,
            585,
            690
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_351_order_14",
          "label": "foot",
          "text": "9.1 Grammatical Features | 329",
          "level": -1,
          "page": 351,
          "reading_order": 14,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "9.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_0",
          "label": "table",
          "text": "Table 9-1. Agreement paradigm for English regular verbs [TABLE: <table><tr><td></td><td>Singular</td><td>Plural</td></tr><tr><td>1st person</td><td>I run</td><td>we run</td></tr><tr><td>2nd person</td><td>you run</td><td>your run</td></tr><tr><td>3rd person</td><td>he/she/it runs</td><td>they run</td></tr></table>]",
          "level": -1,
          "page": 352,
          "reading_order": 0,
          "bbox": [
            100,
            89,
            270,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td></td><td>Singular</td><td>Plural</td></tr><tr><td>1st person</td><td>I run</td><td>we run</td></tr><tr><td>2nd person</td><td>you run</td><td>your run</td></tr><tr><td>3rd person</td><td>he/she/it runs</td><td>they run</td></tr></table>",
              "bbox": [
                100,
                89,
                270,
                170
              ],
              "page": 352,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Table 9-1. Agreement paradigm for English regular verbs",
              "bbox": [
                99,
                71,
                378,
                89
              ],
              "page": 352,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_352_order_2",
          "label": "para",
          "text": "We can make the role of morphological properties a bit more explicit, as illustrated in\n(5) and (6). These representations indicate that the verb agrees with its subject in person\nand number. (We use 3 as an abbreviation for 3rd person, SG for singular, and PL for\nplural.)",
          "level": -1,
          "page": 352,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_3",
          "label": "para",
          "text": "5) the dog run-s\ndog.3.SG run-3.SG",
          "level": -1,
          "page": 352,
          "reading_order": 3,
          "bbox": [
            124,
            268,
            234,
            298
          ],
          "section_number": "5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_4",
          "label": "para",
          "text": "6) the dog-s run\ndog.3.PL run-3.PL",
          "level": -1,
          "page": 352,
          "reading_order": 4,
          "bbox": [
            123,
            313,
            234,
            340
          ],
          "section_number": "6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_5",
          "label": "para",
          "text": "Let’s see what happens when we encode these agreement constraints in a context-free\ngrammar. We will begin with the simple CFG in (7).",
          "level": -1,
          "page": 352,
          "reading_order": 5,
          "bbox": [
            97,
            349,
            585,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_6",
          "label": "para",
          "text": "7) S\n-> NP VP\nNP N\nVP -> V",
          "level": -1,
          "page": 352,
          "reading_order": 6,
          "bbox": [
            125,
            401,
            218,
            439
          ],
          "section_number": "7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_8",
          "label": "para",
          "text": "Grammar (7) allows us to generate the sentence this dog runs; however, what we really\nwant to do is also generate these dogs run while blocking unwanted sequences like *this\ndogs run and *these dog runs. The most straightforward approach is to add new non-\nterminals and productions to the grammar:",
          "level": -1,
          "page": 352,
          "reading_order": 8,
          "bbox": [
            97,
            501,
            585,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_9",
          "label": "para",
          "text": "8) S -> NP_SG VP_SG\nS -> NP_PL VP_PL\nNP_SG -> Det_SG N_SG\nNP_PL -> Det_PL N_PL\nVP_SG -> V_SG\nVP_PL -> V_PL",
          "level": -1,
          "page": 352,
          "reading_order": 9,
          "bbox": [
            124,
            582,
            252,
            663
          ],
          "section_number": "8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_11",
          "label": "para",
          "text": "In place of a single production expanding S, we now have two productions, one covering\nthe sentences involving singular subject NPs and VPs, the other covering sentences with",
          "level": -1,
          "page": 352,
          "reading_order": 11,
          "bbox": [
            97,
            768,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_352_order_12",
          "label": "foot",
          "text": "330 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 352,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "330",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_353_order_0",
          "label": "para",
          "text": "plural subject NPs and VPs . In fact, every production in (7) has two counterparts in\n(8) . With a small grammar, this is not really such a problem, although it is aesthetically\nunappealing. However, with a larger grammar that covers a reasonable subset of Eng-\nlish constructions, the prospect of doubling the grammar size is very unattractive. Let's\nsuppose now that we used the same approach to deal with first, second, and third\nperson agreement, for both singular and plural. This would lead to the original grammar\nbeing multiplied by a factor of 6, which we definitely want to avoid. Can we do better\nthan this? In the next section, we will show that capturing number and person agree-\nment need not come at the cost of “blowing up” the number of productions.",
          "level": -1,
          "page": 353,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_353_order_1",
      "label": "sec",
      "text": "Using Attributes and Constraints",
      "level": 1,
      "page": 353,
      "reading_order": 1,
      "bbox": [
        98,
        232,
        315,
        256
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_353_order_2",
          "label": "para",
          "text": "We spoke informally of linguistic categories having properties, for example, that a noun\nhas the property of being plural. Let’s make this explicit:",
          "level": -1,
          "page": 353,
          "reading_order": 2,
          "bbox": [
            97,
            259,
            584,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_353_order_3",
          "label": "para",
          "text": "(9) N[NUM=pl]",
          "level": -1,
          "page": 353,
          "reading_order": 3,
          "bbox": [
            118,
            311,
            189,
            323
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_353_order_4",
          "label": "para",
          "text": "In (9) , we have introduced some new notation which says that the category N has a\n(grammatical) feature called NUM (short for “ number ” ) and that the value of this feature\nis p1 (short for “ plural ” ). We can add similar annotations to other categories, and use\nthem in lexical entries:",
          "level": -1,
          "page": 353,
          "reading_order": 4,
          "bbox": [
            97,
            339,
            585,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_353_order_6",
          "label": "para",
          "text": "Does this help at all? So far, it looks just like a slightly more verbose alternative to what\nwas specified in (8). Things become more interesting when we allow variables over\nfeature values, and use these to state constraints:",
          "level": -1,
          "page": 353,
          "reading_order": 6,
          "bbox": [
            97,
            519,
            585,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_353_order_8",
          "label": "para",
          "text": "We are using ?n as a variable over values of NUM; it can be instantiated either to sg or\npl, within a given production. We can read the first production as saying that whatever\nvalue NP takes for the feature NUM, VP must take the same value.",
          "level": -1,
          "page": 353,
          "reading_order": 8,
          "bbox": [
            97,
            636,
            585,
            684
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_353_order_9",
          "label": "para",
          "text": "In order to understand how these feature constraints work, it’\ns helpful to think about\nhow one would go about building a tree. Lexical productions will admit the following\nlocal trees (trees of depth one):",
          "level": -1,
          "page": 353,
          "reading_order": 9,
          "bbox": [
            97,
            696,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_353_order_10",
          "label": "foot",
          "text": "9.1 Grammatical Features | 331",
          "level": -1,
          "page": 353,
          "reading_order": 10,
          "bbox": [
            440,
            824,
            584,
            842
          ],
          "section_number": "9.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_354_order_0",
          "label": "para",
          "text": "(12) a. Det[NUM=sg]\n|\nthis",
          "level": -1,
          "page": 354,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            243,
            120
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_354_order_1",
          "label": "para",
          "text": "b. Det[NUM=pl]\n|\nthese",
          "level": -1,
          "page": 354,
          "reading_order": 1,
          "bbox": [
            151,
            133,
            243,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_354_order_2",
          "label": "para",
          "text": "(13) a. N[NUM=sg]\n|\ndog",
          "level": -1,
          "page": 354,
          "reading_order": 2,
          "bbox": [
            109,
            197,
            234,
            251
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_354_order_3",
          "label": "para",
          "text": "b. N[NUM=pl]\n___\ndogs",
          "level": -1,
          "page": 354,
          "reading_order": 3,
          "bbox": [
            150,
            259,
            234,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_354_order_4",
          "label": "para",
          "text": "Now NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n] says that whatever the NUM values of N and\nDet are, they have to be the same. Consequently, this production will permit (12a) and\n(13a) to be combined into an NP , as shown in (14a) , and it will also allow (12b) and\n(13b) to be combined, as in (14b) . By contrast, (15a) and (15b) are prohibited because\nthe roots of their subtrees differ in their values for the NUM feature; this incompatibility\nof values is indicated informally with a FAIL value at the top node.",
          "level": -1,
          "page": 354,
          "reading_order": 4,
          "bbox": [
            97,
            330,
            585,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_354_order_5",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_354_figure_005.png)",
          "level": -1,
          "page": 354,
          "reading_order": 5,
          "bbox": [
            109,
            439,
            328,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_354_order_6",
          "label": "foot",
          "text": "332 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 354,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "332",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_0",
          "label": "para",
          "text": "(15) a. NP[NUM=FAIL]\nDet[NUM=sg] N[NUM=pl]",
          "level": -1,
          "page": 355,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            326,
            156
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_1",
          "label": "para",
          "text": "b. NP[NUM=FAIL]\nDet[NUM=pI] N[NUM=sg]",
          "level": -1,
          "page": 355,
          "reading_order": 1,
          "bbox": [
            150,
            161,
            326,
            245
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_2",
          "label": "para",
          "text": "Production VP[NUM=?n] -> V[NUM=?n] says that the NUM value of the head verb has to be\nthe same as the NUM value of the VP parent. Combined with the production for expanding\nS, we derive the consequence that if the NUM value of the subject head noun is pl , then\nso is the NUM value of the VP 's head verb.",
          "level": -1,
          "page": 355,
          "reading_order": 2,
          "bbox": [
            97,
            259,
            585,
            324
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_355_figure_003.png)",
          "level": -1,
          "page": 355,
          "reading_order": 3,
          "bbox": [
            109,
            340,
            386,
            456
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_4",
          "label": "para",
          "text": "Grammar (10) illustrated lexical productions for determiners like this and these , which\nrequire a singular or plural head noun respectively. However, other determiners in\nEnglish are not choosy about the grammatical number of the noun they combine with.\nOne way of describing this would be to add two lexical entries to the grammar, one\neach for the singular and plural versions of a determiner such as the :",
          "level": -1,
          "page": 355,
          "reading_order": 4,
          "bbox": [
            97,
            472,
            585,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_6",
          "label": "para",
          "text": "However, a more elegant solution is to leave the NUM value underspecified and let it\nagree in number with whatever noun it combines with. Assigning a variable value to\nNUM is one way of achieving this result:",
          "level": -1,
          "page": 355,
          "reading_order": 6,
          "bbox": [
            97,
            591,
            585,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_8",
          "label": "para",
          "text": "But in fact we can be even more economical, and just omit any specification for NUM in\nsuch productions. We only need to explicitly enter a variable value when this constrains\nanother value elsewhere in the same production.",
          "level": -1,
          "page": 355,
          "reading_order": 8,
          "bbox": [
            97,
            672,
            585,
            720
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_9",
          "label": "para",
          "text": "The grammar in Example 9-1 illustrates most of the ideas we have introduced so far\nin this chapter, plus a couple of new ones.",
          "level": -1,
          "page": 355,
          "reading_order": 9,
          "bbox": [
            97,
            725,
            584,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_355_order_10",
          "label": "foot",
          "text": "9.1 Grammatical Features | 333",
          "level": -1,
          "page": 355,
          "reading_order": 10,
          "bbox": [
            440,
            824,
            584,
            842
          ],
          "section_number": "9.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_356_order_0",
          "label": "cap",
          "text": "Example 9-1. Example feature-based gramma",
          "level": -1,
          "page": 356,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            324,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_356_order_2",
          "label": "para",
          "text": "Notice that a syntactic category can have more than one feature: for example,\nV[TENSE=pres, NUM=pl]. In general, we can add as many features as we like.",
          "level": -1,
          "page": 356,
          "reading_order": 2,
          "bbox": [
            97,
            500,
            584,
            530
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_356_order_3",
          "label": "para",
          "text": "A final detail about Example 9-1 is the statement %start S. This “directive” tells the\nparser to take S as the start symbol for the grammar.",
          "level": -1,
          "page": 356,
          "reading_order": 3,
          "bbox": [
            97,
            537,
            585,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_356_order_4",
          "label": "para",
          "text": "In general, when we are trying to develop even a very small grammar, it is convenient\nto put the productions in a file where they can be edited, tested, and revised. We have\nsaved Example 9-1 as a file named feat0.fcfg in the NLTK data distribution. You can\nmake your own copy of this for further experimentation using nltk.data.load().",
          "level": -1,
          "page": 356,
          "reading_order": 4,
          "bbox": [
            97,
            580,
            585,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_356_order_5",
          "label": "para",
          "text": "Feature-based grammars are parsed in NLTK using an Earley chart parser (see Sec-\ntion 9.5 for more information about this) and Example 9-2 illustrates how this is carried\nout. After tokenizing the input, we import the load_parser function O , which takes a\ngrammar filename as input and returns a chart parser cp O . Calling the parser's\nnbest_parse() method will return a list trees of parse trees; trees will be empty if the\ngrammar fails to parse the input and otherwise will contain one or more parse trees,\ndepending on whether the input is syntactically ambiguous.",
          "level": -1,
          "page": 356,
          "reading_order": 5,
          "bbox": [
            97,
            653,
            585,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_356_order_6",
          "label": "foot",
          "text": "334 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 356,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "334",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_357_order_0",
          "label": "cap",
          "text": "Example 9-2. Trace of feature-based chart parser.",
          "level": -1,
          "page": 357,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            342,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_357_order_2",
          "label": "para",
          "text": "The details of the parsing procedure are not that important for present purposes. How-\never, there is an implementation issue which bears on our earlier discussion of grammar\nsize. One possible approach to parsing productions containing feature constraints is to\ncompile out all admissible values of the features in question so that we end up with a\nlarge, fully specified CFG along the lines of (8) . By contrast, the parser process illus-\ntrated in the previous examples works directly with the underspecified productions\ngiven by the grammar. Feature values “ flow upwards ” from lexical entries, and variable\nvalues are then associated with those values via bindings (i.e., dictionaries) such as\n{?n: 'sg', ?t: 'pres'} . As the parser assembles information about the nodes of the\ntree it is building, these variable bindings are used to instantiate values in these nodes;\nthus the underspecified VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] NP[] becomes\ninstantiated as VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] by\nlooking up the values of ?n and ?t in the bindings.",
          "level": -1,
          "page": 357,
          "reading_order": 2,
          "bbox": [
            97,
            331,
            585,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_357_order_3",
          "label": "para",
          "text": "Finally, we can inspect the resulting parse trees (in this case, a single one).",
          "level": -1,
          "page": 357,
          "reading_order": 3,
          "bbox": [
            99,
            553,
            521,
            567
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_357_order_5",
      "label": "sec",
      "text": "Terminology",
      "level": 1,
      "page": 357,
      "reading_order": 5,
      "bbox": [
        97,
        689,
        182,
        712
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_359_order_4",
          "label": "sub_sec",
          "text": "9.2 Processing Feature Structures",
          "level": 2,
          "page": 359,
          "reading_order": 4,
          "bbox": [
            97,
            376,
            368,
            403
          ],
          "section_number": "9.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_359_order_5",
              "label": "para",
              "text": "In this section, we will show how feature structures can be constructed and manipulated\nin NLTK. We will also discuss the fundamental operation of unification, which allows\nus to combine the information contained in two different feature structures.",
              "level": -1,
              "page": 359,
              "reading_order": 5,
              "bbox": [
                97,
                411,
                585,
                456
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_359_order_6",
              "label": "para",
              "text": "Feature structures in NLTK are declared with the FeatStruct() constructor. Atomic\nfeature values can be strings or integers.",
              "level": -1,
              "page": 359,
              "reading_order": 6,
              "bbox": [
                98,
                465,
                584,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_359_order_8",
              "label": "para",
              "text": "A feature structure is actually just a kind of dictionary, and so we access its values by\nindexing in the usual way. We can use our familiar syntax to assign values to features:",
              "level": -1,
              "page": 359,
              "reading_order": 8,
              "bbox": [
                97,
                564,
                585,
                600
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_359_order_10",
              "label": "para",
              "text": "We can also define feature structures that have complex values, as discussed earlier.",
              "level": -1,
              "page": 359,
              "reading_order": 10,
              "bbox": [
                98,
                663,
                575,
                682
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_359_order_12",
              "label": "foot",
              "text": "9.2 Processing Feature Structures | 337",
              "level": -1,
              "page": 359,
              "reading_order": 12,
              "bbox": [
                413,
                824,
                585,
                842
              ],
              "section_number": "9.2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_360_order_1",
              "label": "para",
              "text": "An alternative method of specifying feature structures is to use a bracketed string con-\nsisting of feature-value pairs in the format feature=value, where values may themselves\nbe feature structures:",
              "level": -1,
              "page": 360,
              "reading_order": 1,
              "bbox": [
                97,
                170,
                585,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_360_order_3",
              "label": "para",
              "text": "Feature structures are not inherently tied to linguistic objects; they are general-purpose\nstructures for representing knowledge. For example, we could encode information\nabout a person in a feature structure:",
              "level": -1,
              "page": 360,
              "reading_order": 3,
              "bbox": [
                97,
                313,
                585,
                367
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_360_order_5",
              "label": "para",
              "text": "In the next couple of pages, we are going to use examples like this to explore standard\noperations over feature structures. This will briefly divert us from processing natural\nlanguage, but we need to lay the groundwork before we can get back to talking about\ngrammars. Hang on tight!",
              "level": -1,
              "page": 360,
              "reading_order": 5,
              "bbox": [
                97,
                430,
                584,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_360_order_6",
              "label": "para",
              "text": "It is often helpful to view feature structures as graphs, more specifically, as directed\nacyclic graphs (DAGs). (21) is equivalent to the preceding AVM.",
              "level": -1,
              "page": 360,
              "reading_order": 6,
              "bbox": [
                97,
                501,
                584,
                537
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_360_order_7",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_360_figure_007.png)",
              "level": -1,
              "page": 360,
              "reading_order": 7,
              "bbox": [
                109,
                546,
                324,
                725
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_360_order_8",
              "label": "para",
              "text": "The feature names appear as labels on the directed arcs, and feature values appear as\nlabels on the nodes that are pointed to by the arcs.",
              "level": -1,
              "page": 360,
              "reading_order": 8,
              "bbox": [
                100,
                734,
                585,
                770
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_360_order_9",
              "label": "para",
              "text": "Just as before, feature values can be complex",
              "level": -1,
              "page": 360,
              "reading_order": 9,
              "bbox": [
                99,
                778,
                353,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_360_order_10",
              "label": "foot",
              "text": "338 | Chapter 9: Building Feature-Based Grammars",
              "level": -1,
              "page": 360,
              "reading_order": 10,
              "bbox": [
                97,
                824,
                316,
                842
              ],
              "section_number": "338",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_361_order_0",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_361_figure_000.png)",
              "level": -1,
              "page": 361,
              "reading_order": 0,
              "bbox": [
                109,
                71,
                386,
                349
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_361_order_1",
              "label": "para",
              "text": "When we look at such graphs, it is natural to think in terms of paths through the graph.\nA feature path is a sequence of arcs that can be followed from the root node. We will\nrepresent paths as tuples of arc labels. Thus, ('ADDRESS', 'STREET') is a feature path\nwhose value in (22) is the node labeled 'rue Pascal'.",
              "level": -1,
              "page": 361,
              "reading_order": 1,
              "bbox": [
                97,
                358,
                585,
                430
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_361_order_2",
              "label": "para",
              "text": "Now let’s consider a situation where Lee has a spouse named Kim, and Kim’s address\nis the same as Lee’s. We might represent this as (23).",
              "level": -1,
              "page": 361,
              "reading_order": 2,
              "bbox": [
                97,
                430,
                585,
                467
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_361_order_3",
              "label": "figure",
              "text": "However, rather than repeating the address information in the feature structure, we\ncan “ share ” the same sub-graph between different arcs: [IMAGE: ![Figure](figures/NLTK_page_361_figure_003.png)]",
              "level": -1,
              "page": 361,
              "reading_order": 3,
              "bbox": [
                109,
                474,
                333,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_361_figure_003.png)",
                  "bbox": [
                    109,
                    474,
                    333,
                    609
                  ],
                  "page": 361,
                  "reading_order": 3
                },
                {
                  "label": "cap",
                  "text": "However, rather than repeating the address information in the feature structure, we\ncan “ share ” the same sub-graph between different arcs:",
                  "bbox": [
                    97,
                    624,
                    585,
                    655
                  ],
                  "page": 361,
                  "reading_order": 4
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_361_order_5",
              "label": "foot",
              "text": "9.2 Processing Feature Structures | 339",
              "level": -1,
              "page": 361,
              "reading_order": 5,
              "bbox": [
                413,
                824,
                585,
                842
              ],
              "section_number": "9.2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_362_order_0",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_362_figure_000.png)",
              "level": -1,
              "page": 362,
              "reading_order": 0,
              "bbox": [
                109,
                71,
                449,
                343
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_362_order_1",
              "label": "para",
              "text": "In other words, the value of the path ('ADDRESS') in (24) is identical to the value of the\npath ('SPOUSE', 'ADDRESS'). DAGs such as (24) are said to involve structure shar-\ning or reentrancy. When two paths have the same value, they are said to be\nequivalent.",
              "level": -1,
              "page": 362,
              "reading_order": 1,
              "bbox": [
                97,
                358,
                585,
                427
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_362_order_2",
              "label": "para",
              "text": "In order to indicate reentrancy in our matrix-style representations, we will prefix the\nfirst occurrence of a shared feature structure with an integer in parentheses, such as\n(1) . Any later reference to that structure will use the notation ->(1) , as shown here.",
              "level": -1,
              "page": 362,
              "reading_order": 2,
              "bbox": [
                98,
                436,
                585,
                484
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_362_order_4",
              "label": "para",
              "text": "The bracketed integer is sometimes called a tag or a coindex. The choice of integer is\nnot significant. There can be any number of tags within a single feature structure.",
              "level": -1,
              "page": 362,
              "reading_order": 4,
              "bbox": [
                100,
                618,
                585,
                649
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_362_order_6",
              "label": "foot",
              "text": "340 | Chapter 9: Building Feature-Based Grammars",
              "level": -1,
              "page": 362,
              "reading_order": 6,
              "bbox": [
                97,
                824,
                316,
                842
              ],
              "section_number": "340",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_357_order_6",
          "label": "para",
          "text": "So far, we have only seen feature values like sg and pl. These simple values are usually\ncalled atomic—that is, they can’t be decomposed into subparts. A special case of\natomic values are Boolean values, that is, values that just specify whether a property\nis true or false. For example, we might want to distinguish auxiliary verbs such as can,",
          "level": -1,
          "page": 357,
          "reading_order": 6,
          "bbox": [
            97,
            716,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_357_order_7",
          "label": "foot",
          "text": "9.1 Grammatical Features | 335",
          "level": -1,
          "page": 357,
          "reading_order": 7,
          "bbox": [
            440,
            824,
            585,
            842
          ],
          "section_number": "9.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_358_order_0",
          "label": "para",
          "text": "may , will , and do with the Boolean feature AUX . Then the production V[TENSE=pres,\naux=+] -> 'can' means that can receives the value pres for TENSE and + or true for\nAUX . There is a widely adopted convention that abbreviates the representation of Boo-\nlean features f; instead of aux=+ or aux=- , we use +aux and -aux respectively. These are\njust abbreviations, however, and the parser interprets them as though + and - are like\nany other atomic value. (17) shows some representative productions:",
          "level": -1,
          "page": 358,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_358_order_2",
          "label": "para",
          "text": "We have spoken of attaching “ feature annotations ” to syntactic categories. A more\nradical approach represents the whole category—that is, the non-terminal symbol plus\nthe annotation—as a bundle of features. For example, N[NUM=sg] contains part-of-\nspeech information which can be represented as POS=N . An alternative notation for this\ncategory, therefore, is [POS=N, NUM=sg] .",
          "level": -1,
          "page": 358,
          "reading_order": 2,
          "bbox": [
            97,
            267,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_358_order_3",
          "label": "para",
          "text": "In addition to atomic-valued features, features may take values that are themselves\nfeature structures. For example, we can group together agreement features (e.g., per-\nson, number, and gender) as a distinguished part of a category, serving as the value of\nAGR . In this case, we say that AGR has a complex value. (18) depicts the structure, in a\nformat known as an attribute value matrix (AVM).",
          "level": -1,
          "page": 358,
          "reading_order": 3,
          "bbox": [
            97,
            356,
            586,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_358_order_5",
          "label": "para",
          "text": "In passing, we should point out that there are alternative approaches for displaying\nAVMs; Figure 9-1 shows an example. Although feature structures rendered in the style\nof (18) are less visually pleasing, we will stick with this format, since it corresponds to\nthe output we will be getting from NLTK.",
          "level": -1,
          "page": 358,
          "reading_order": 5,
          "bbox": [
            97,
            528,
            585,
            595
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_358_order_6",
          "label": "figure",
          "text": "Figure 9-1. Rendering a feature structure as an attribute value matrix. [IMAGE: ![Figure](figures/NLTK_page_358_figure_006.png)]",
          "level": -1,
          "page": 358,
          "reading_order": 6,
          "bbox": [
            100,
            618,
            583,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_358_figure_006.png)",
              "bbox": [
                100,
                618,
                583,
                761
              ],
              "page": 358,
              "reading_order": 6
            },
            {
              "label": "cap",
              "text": "Figure 9-1. Rendering a feature structure as an attribute value matrix.",
              "bbox": [
                97,
                770,
                440,
                788
              ],
              "page": 358,
              "reading_order": 7
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_358_order_8",
          "label": "foot",
          "text": "336 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 358,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "336",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_359_order_0",
          "label": "para",
          "text": "On the topic of representation, we also note that feature structures, like dictionaries,\nassign no particular significance to the order of features. So (18) is equivalent to:",
          "level": -1,
          "page": 359,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_359_order_1",
          "label": "para",
          "text": "(19) [AGR = [NUM = pl\n]]\n[PER = 3\n]]\n[GND = fem ]]\n[POS = N\n]",
          "level": -1,
          "page": 359,
          "reading_order": 1,
          "bbox": [
            109,
            116,
            252,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_359_order_2",
          "label": "para",
          "text": "Once we have the possibility of using features like AGR, we can refactor a grammar like\nExample 9-1 so that agreement features are bundled together. A tiny grammar illus-\ntrating this idea is shown in (20).",
          "level": -1,
          "page": 359,
          "reading_order": 2,
          "bbox": [
            97,
            197,
            585,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_363_order_0",
      "label": "sec",
      "text": "Subsumption and Unification",
      "level": 1,
      "page": 363,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        297,
        95
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_366_order_6",
          "label": "sub_sec",
          "text": "9.3 Extending a Feature-Based Grammar",
          "level": 2,
          "page": 366,
          "reading_order": 6,
          "bbox": [
            97,
            645,
            426,
            668
          ],
          "section_number": "9.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_366_order_8",
              "label": "sub_sub_sec",
              "text": "Subcategorization",
              "level": 3,
              "page": 366,
              "reading_order": 8,
              "bbox": [
                97,
                723,
                218,
                743
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_366_order_9",
                  "label": "para",
                  "text": "In Chapter 8, we augmented our category labels to represent different kinds of verbs,\nand used the labels IV and TV for intransitive and transitive verbs respectively. This\nallowed us to write productions like the following:",
                  "level": -1,
                  "page": 366,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    743,
                    585,
                    798
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_366_order_10",
                  "label": "foot",
                  "text": "344 | Chapter 9: Building Feature-Based Grammars",
                  "level": -1,
                  "page": 366,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    316,
                    842
                  ],
                  "section_number": "344",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_367_order_0",
                  "label": "para",
                  "text": "(29) VP -> IV\nVP -> TV NP",
                  "level": -1,
                  "page": 367,
                  "reading_order": 0,
                  "bbox": [
                    109,
                    71,
                    207,
                    99
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_367_order_1",
                  "label": "para",
                  "text": "Although we know that IV and TV are two kinds of V , they are just atomic non-terminal\nsymbols in a CFG and are as distinct from each other as any other pair of symbols. This\nnotation doesn't let us say anything about verbs in general; e.g., we cannot say “ All\nlexical items of category V can be marked for tense, ” since walk , say, is an item of\ncategory IV , not V . So, can we replace category labels such as TV and IV by V along with\na feature that tells us whether the verb combines with a following NP object or whether\nit can occur without any complement?",
                  "level": -1,
                  "page": 367,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    116,
                    586,
                    229
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_367_order_2",
                  "label": "para",
                  "text": "A simple approach, originally developed for a grammar framework called Generalized\nPhrase Structure Grammar (GPSG), tries to solve this problem by allowing lexical cat-\negories to bear a SUBCAT feature, which tells us what subcategorization class the item\nbelongs to. In contrast to the integer values for SUBCAT used by GPSG, the example here\nadopts more mnemonic values, namely intrans, trans, and clause:",
                  "level": -1,
                  "page": 367,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    239,
                    585,
                    322
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_367_order_3",
                  "label": "para",
                  "text": "(30) VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\nVP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\nVP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBAr",
                  "level": -1,
                  "page": 367,
                  "reading_order": 3,
                  "bbox": [
                    109,
                    331,
                    485,
                    376
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_367_order_7",
                  "label": "para",
                  "text": "When we see a lexical category like V[SUBCAT=trans] , we can interpret the SUBCAT spec-\nification as a pointer to a production in which V[SUBCAT=trans] is introduced as the\nhead child in a VP production. By convention, there is a correspondence between the\nvalues of SUBCAT and the productions that introduce lexical heads. On this approach,\nSUBCAT can appear only on lexical categories; it makes no sense, for example, to specify\na SUBCAT value on VP . As required, walk and like both belong to the category V . Never-\ntheless, walk will occur only in VPs expanded by a production with the feature\nSUBCAT=intrans on the righthand side, as opposed to like , which requires a\nSUBCAT=trans .",
                  "level": -1,
                  "page": 367,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    544,
                    585,
                    689
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_367_order_8",
                  "label": "para",
                  "text": "In our third class of verbs in (30), we have specified a category SBar. This is a label for\nsubordinate clauses, such as the complement of claim in the example You claim that\nyou like children. We require two further productions to analyze such sentences:",
                  "level": -1,
                  "page": 367,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    698,
                    585,
                    747
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_367_order_9",
                  "label": "para",
                  "text": "(31) SBar -> Comp S\nComp -> 'that'",
                  "level": -1,
                  "page": 367,
                  "reading_order": 9,
                  "bbox": [
                    109,
                    761,
                    218,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_367_order_10",
                  "label": "foot",
                  "text": "9.3 Extending a Feature-Based Grammar | 345",
                  "level": -1,
                  "page": 367,
                  "reading_order": 10,
                  "bbox": [
                    386,
                    824,
                    585,
                    842
                  ],
                  "section_number": "9.3",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_368_order_0",
                  "label": "para",
                  "text": "The resulting structure is the following.",
                  "level": -1,
                  "page": 368,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    324,
                    89
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_368_order_1",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_368_figure_001.png)",
                  "level": -1,
                  "page": 368,
                  "reading_order": 1,
                  "bbox": [
                    109,
                    98,
                    583,
                    304
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_368_order_2",
                  "label": "para",
                  "text": "An alternative treatment of subcategorization, due originally to a framework known as\ncategorical grammar, is represented in feature-based frameworks such as PATR and\nHead-driven Phrase Structure Grammar. Rather than using SUBCAT values as a way of\nindexing productions, the SUBCAT value directly encodes the valency of a head (the list\nof arguments that it can combine with). For example, a verb like put that takes NP and\nPP complements ( put the book on the table ) might be represented as (33) :",
                  "level": -1,
                  "page": 368,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    322,
                    586,
                    421
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_366_order_7",
              "label": "para",
              "text": "In this section, we return to feature-based grammar and explore a variety of linguistic\nissues, and demonstrate the benefits of incorporating features into the grammar.",
              "level": -1,
              "page": 366,
              "reading_order": 7,
              "bbox": [
                97,
                672,
                584,
                707
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_368_order_3",
          "label": "sub_sec",
          "text": "(33) V[SUBCAT=<NP, NP, PP>]",
          "level": 2,
          "page": 368,
          "reading_order": 3,
          "bbox": [
            109,
            430,
            261,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_368_order_4",
              "label": "para",
              "text": "This says that the verb can combine with three arguments. The leftmost element in the\nlist is the subject NP , while everything else—an NP followed by a PP in this case—com-\nprises the subcategorized-for complements. When a verb like put is combined with\nappropriate complements, the requirements which are specified in the SUBCAT are dis-\ncharged, and only a subject NP is needed. This category, which corresponds to what is\ntraditionally thought of as VP , might be represented as follows:",
              "level": -1,
              "page": 368,
              "reading_order": 4,
              "bbox": [
                97,
                456,
                585,
                564
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_368_order_5",
          "label": "sub_sec",
          "text": "(34) V[SUBCAT=<NP>]",
          "level": 2,
          "page": 368,
          "reading_order": 5,
          "bbox": [
            109,
            573,
            216,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_368_order_6",
              "label": "para",
              "text": "Finally, a sentence is a kind of verbal category that has no requirements for further\narguments, and hence has a SUBCAT whose value is the empty list. The tree (35) shows\nhow these category assignments combine in a parse of Kim put the book on the table.",
              "level": -1,
              "page": 368,
              "reading_order": 6,
              "bbox": [
                97,
                600,
                585,
                654
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_368_order_7",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_368_figure_007.png)",
              "level": -1,
              "page": 368,
              "reading_order": 7,
              "bbox": [
                109,
                663,
                494,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_368_order_8",
              "label": "foot",
              "text": "346 | Chapter 9: Building Feature-Based Grammars",
              "level": -1,
              "page": 368,
              "reading_order": 8,
              "bbox": [
                97,
                824,
                316,
                842
              ],
              "section_number": "346",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_363_order_1",
          "label": "para",
          "text": "It is standard to think of feature structures as providing partial information about\nsome object, in the sense that we can order feature structures according to how general\nthey are. For example, (25a) is more general (less specific) than (25b), which in turn is\nmore general than (25c).",
          "level": -1,
          "page": 363,
          "reading_order": 1,
          "bbox": [
            97,
            103,
            585,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_363_order_2",
          "label": "para",
          "text": "(25) a.[NUMBER =74]",
          "level": -1,
          "page": 363,
          "reading_order": 2,
          "bbox": [
            109,
            179,
            235,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_363_order_3",
          "label": "para",
          "text": "b. [NUMBER = 74\n[STREET = 'rue Pascal']",
          "level": -1,
          "page": 363,
          "reading_order": 3,
          "bbox": [
            150,
            197,
            297,
            225
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_363_order_4",
          "label": "para",
          "text": "c. [NUMBER = 74\n[STREET = 'rue Pascal']\n[CITY = 'Paris'",
          "level": -1,
          "page": 363,
          "reading_order": 4,
          "bbox": [
            151,
            232,
            297,
            269
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_363_order_5",
          "label": "para",
          "text": "This ordering is called subsumption ; a more general feature structure subsumes a less\ngeneral one. If $\\ensuremath{FS_0}\\xspace$ subsumes $\\ensuremath{FS_1}\\xspace$ (formally, we write $\\ensuremath{FS_0}\\xspace \\sqsubseteq \\ensuremath{FS_1}\\xspace$ ), then $\\ensuremath{FS_1}\\xspace$ must have\nall the paths and path equivalences of $\\ensuremath{FS_0}\\xspace$ , and may have additional paths and equiv-\nalences as well. Thus, (23) subsumes (24) since the latter has additional path equiva-\nlences. It should be obvious that subsumption provides only a partial ordering on fea-\nture structures, since some feature structures are incommensurable. For example,\n(26) neither subsumes nor is subsumed by (25a) .",
          "level": -1,
          "page": 363,
          "reading_order": 5,
          "bbox": [
            97,
            277,
            585,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_363_order_6",
          "label": "para",
          "text": "(26) [TELNO = 01 27 86 42 96]",
          "level": -1,
          "page": 363,
          "reading_order": 6,
          "bbox": [
            109,
            412,
            270,
            425
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_363_order_7",
          "label": "para",
          "text": "So we have seen that some feature structures are more specific than others. How do we\ngo about specializing a given feature structure? For example, we might decide that\naddresses should consist of not just a street number and a street name, but also a city.\nThat is, we might want to merge graph (27a) with (27b) to yield (27c).",
          "level": -1,
          "page": 363,
          "reading_order": 7,
          "bbox": [
            97,
            439,
            585,
            505
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_363_order_8",
          "label": "foot",
          "text": "9.2 Processing Feature Structures | 341",
          "level": -1,
          "page": 363,
          "reading_order": 8,
          "bbox": [
            413,
            824,
            584,
            842
          ],
          "section_number": "9.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_364_order_0",
          "label": "figure",
          "text": "Merging information from two feature structures is called unification and is supported\nby the unify() method. [IMAGE: ![Figure](figures/NLTK_page_364_figure_000.png)]",
          "level": -1,
          "page": 364,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            531,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_364_figure_000.png)",
              "bbox": [
                109,
                71,
                531,
                609
              ],
              "page": 364,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Merging information from two feature structures is called unification and is supported\nby the unify() method.",
              "bbox": [
                97,
                627,
                584,
                658
              ],
              "page": 364,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_364_order_3",
          "label": "foot",
          "text": "342 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 364,
          "reading_order": 3,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "342",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_365_order_0",
          "label": "para",
          "text": "Unification is formally defined as a binary operation: $FS_0\\sqcup FS_1$ . Unification is sym-\nmetric, so $FS_0\\sqcup FS_1=FS_1\\sqcup FS_0$ . The same is true in Python:",
          "level": -1,
          "page": 365,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_365_order_2",
          "label": "para",
          "text": "If we unify two feature structures that stand in the subsumption relationship, then the\nresult of unification is the most specific of the two:",
          "level": -1,
          "page": 365,
          "reading_order": 2,
          "bbox": [
            98,
            170,
            585,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_365_order_3",
          "label": "para",
          "text": "(28) If $F S_{0} \\sqsubseteq F S_{1}$, then $F S_{0} \\sqcup F S_{1}=F S_{1}$",
          "level": -1,
          "page": 365,
          "reading_order": 3,
          "bbox": [
            109,
            215,
            333,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_365_order_4",
          "label": "para",
          "text": "For example, the result of unifying (25b) with (25c) is (25c).",
          "level": -1,
          "page": 365,
          "reading_order": 4,
          "bbox": [
            98,
            249,
            440,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_365_order_5",
          "label": "para",
          "text": "Unification between $FS_0$ and $FS_1$ will fail if the two feature structures share a path π\nwhere the value of π in $FS_0$ is a distinct atom from the value of π in $FS_1$ . This is imple-\nmented by setting the result of unification to be None.",
          "level": -1,
          "page": 365,
          "reading_order": 5,
          "bbox": [
            97,
            268,
            585,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_365_order_7",
          "label": "para",
          "text": "Now, if we look at how unification interacts with structure-sharing, things become\nreally interesting. First, let's define (23) in Python:",
          "level": -1,
          "page": 365,
          "reading_order": 7,
          "bbox": [
            98,
            402,
            585,
            433
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_365_order_9",
          "label": "para",
          "text": "What happens when we augment Kim's address with a specification for CITY? Notice\nthat fs1 needs to include the whole path from the root of the feature structure down\nto CITY.",
          "level": -1,
          "page": 365,
          "reading_order": 9,
          "bbox": [
            97,
            654,
            584,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_365_order_11",
          "label": "foot",
          "text": "9.2 Processing Feature Structures | 343",
          "level": -1,
          "page": 365,
          "reading_order": 11,
          "bbox": [
            413,
            824,
            584,
            842
          ],
          "section_number": "9.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_366_order_1",
          "label": "para",
          "text": "By contrast, the result is very different if fs1 is unified with the structure sharing version\nfs2 (also shown earlier as the graph (24)):",
          "level": -1,
          "page": 366,
          "reading_order": 1,
          "bbox": [
            98,
            187,
            584,
            218
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_366_order_3",
          "label": "para",
          "text": "Rather than just updating what was in effect Kim's “ copy ” of Lee's address, we have\nnow updated both their addresses at the same time. More generally, if a unification\ninvolves specializing the value of some path π , that unification simultaneously spe-\ncializes the value of any path that is equivalent to π .",
          "level": -1,
          "page": 366,
          "reading_order": 3,
          "bbox": [
            97,
            376,
            585,
            442
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_366_order_4",
          "label": "para",
          "text": "As we have already seen, structure sharing can also be stated using variables such as\n?x.",
          "level": -1,
          "page": 366,
          "reading_order": 4,
          "bbox": [
            97,
            448,
            585,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_369_order_0",
      "label": "sec",
      "text": "Heads Revisited",
      "level": 1,
      "page": 369,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        207,
        91
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_369_order_1",
          "label": "para",
          "text": "We noted in the previous section that by factoring subcategorization information out\nof the main category label, we could express more generalizations about properties of\nverbs. Another property of this kind is the following: expressions of category V are heads\nof phrases of category VP . Similarly, Ns are heads of NPs, As (i.e., adjectives) are heads of\nAPs, and Ps (i.e., prepositions) are heads of PPs. Not all phrases have heads—for exam-\nple, it is standard to say that coordinate phrases (e.g., the book and the bell ) lack heads.\nNevertheless, we would like our grammar formalism to express the parent/head-child\nrelation where it holds. At present, V and VP are just atomic symbols, and we need to\nfind a way to relate them using features (as we did earlier to relate IV and TV ).",
          "level": -1,
          "page": 369,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            586,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_369_order_2",
          "label": "para",
          "text": "X-bar syntax addresses this issue by abstracting out the notion of phrasal level . It is\nusual to recognize three such levels. If N represents the lexical level, then N' represents\nthe next level up, corresponding to the more traditional category Nom , and N\" represents\nthe phrasal level, corresponding to the category NP . (36a) illustrates a representative\nstructure, while (36b) is the more conventional counterpart.",
          "level": -1,
          "page": 369,
          "reading_order": 2,
          "bbox": [
            97,
            259,
            585,
            340
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_369_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_369_figure_003.png)",
          "level": -1,
          "page": 369,
          "reading_order": 3,
          "bbox": [
            109,
            349,
            324,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_369_order_4",
          "label": "para",
          "text": "The head of the structure (36a) is N, and N' and N\" are called (phrasal) projections of\nN. N\" is the maximal projection , and N is sometimes called the zero projection . One\nof the central claims of X-bar syntax is that all constituents share a structural similarity.\nUsing X as a variable over N, V, A, and P , we say that directly subcategorized comple-\nments of a lexical head X are always placed as siblings of the head, whereas adjuncts are\nplaced as siblings of the intermediate category, X'\n. Thus, the configuration of the two\nP\" adjuncts in (37) contrasts with that of the complement P\" in (36a) .",
          "level": -1,
          "page": 369,
          "reading_order": 4,
          "bbox": [
            97,
            618,
            586,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_369_order_5",
          "label": "foot",
          "text": "9.3 Extending a Feature-Based Grammar | 347",
          "level": -1,
          "page": 369,
          "reading_order": 5,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "9.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_370_order_0",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_370_figure_000.png)",
          "level": -1,
          "page": 370,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            404,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_370_order_1",
          "label": "para",
          "text": "The productions in (38) illustrate how bar levels can be encoded using feature struc-\ntures. The nested structure in (37) is achieved by two applications of the recursive rule\nexpanding N[BAR=1] .",
          "level": -1,
          "page": 370,
          "reading_order": 1,
          "bbox": [
            97,
            277,
            585,
            325
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_370_order_3",
      "label": "sec",
      "text": "Auxiliary Verbs and Inversion",
      "level": 1,
      "page": 370,
      "reading_order": 3,
      "bbox": [
        97,
        403,
        290,
        430
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_370_order_4",
          "label": "para",
          "text": "Inverted clauses—where the order of subject and verb is switched—occur in English\ninterrogatives and also after “negative” adverbs:",
          "level": -1,
          "page": 370,
          "reading_order": 4,
          "bbox": [
            97,
            430,
            584,
            466
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_370_order_5",
          "label": "list_group",
          "text": "(39) a. Do you like children?\nb. Can Jody walk?\n(40) a. Rarely do you see Kim.\nb. Never have I seen this dog.",
          "level": -1,
          "page": 370,
          "reading_order": 5,
          "bbox": [
            109,
            474,
            288,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "(39) a. Do you like children?\nb. Can Jody walk?",
              "bbox": [
                109,
                474,
                288,
                519
              ],
              "page": 370,
              "reading_order": 5
            },
            {
              "label": "list",
              "text": "(40) a. Rarely do you see Kim.\nb. Never have I seen this dog.",
              "bbox": [
                109,
                528,
                324,
                565
              ],
              "page": 370,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_370_order_7",
          "label": "para",
          "text": "However, we cannot place just any verb in pre-subject position:",
          "level": -1,
          "page": 370,
          "reading_order": 7,
          "bbox": [
            98,
            573,
            460,
            594
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_370_order_8",
          "label": "list_group",
          "text": "(41) a.\n*Like you children\nb.\n*Walks Jody?\n(42) a. *Rarely see you Kim.\nb. *Never saw I this dog",
          "level": -1,
          "page": 370,
          "reading_order": 8,
          "bbox": [
            109,
            608,
            270,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "(41) a.\n*Like you children\nb.\n*Walks Jody?",
              "bbox": [
                109,
                608,
                270,
                645
              ],
              "page": 370,
              "reading_order": 8
            },
            {
              "label": "list",
              "text": "(42) a. *Rarely see you Kim.\nb. *Never saw I this dog",
              "bbox": [
                109,
                654,
                288,
                692
              ],
              "page": 370,
              "reading_order": 9
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_370_order_10",
          "label": "para",
          "text": "Verbs that can be positioned initially in inverted clauses belong to the class known as\nauxiliaries, and as well as do, can, and have include be, will, and shall. One way of\ncapturing such structures is with the following production:",
          "level": -1,
          "page": 370,
          "reading_order": 10,
          "bbox": [
            97,
            707,
            586,
            754
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_370_order_11",
          "label": "list",
          "text": "(43) S[+INV] -> V[+AUX] NP VP",
          "level": -1,
          "page": 370,
          "reading_order": 11,
          "bbox": [
            109,
            770,
            272,
            782
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_370_order_12",
          "label": "foot",
          "text": "348 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 370,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "348",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_0",
          "label": "para",
          "text": "That is, a clause marked as [+inv] consists of an auxiliary verb followed by a VP. (In a\nmore detailed grammar, we would need to place some constraints on the form of the\nVP, depending on the choice of auxiliary.) (44) illustrates the structure of an inverted\nclause:",
          "level": -1,
          "page": 371,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            136
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_371_figure_001.png)",
          "level": -1,
          "page": 371,
          "reading_order": 1,
          "bbox": [
            109,
            152,
            451,
            261
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_371_order_2",
      "label": "sec",
      "text": "Unbounded Dependency Constructions",
      "level": 1,
      "page": 371,
      "reading_order": 2,
      "bbox": [
        100,
        283,
        355,
        304
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_371_order_3",
          "label": "para",
          "text": "Consider the following contrasts:",
          "level": -1,
          "page": 371,
          "reading_order": 3,
          "bbox": [
            99,
            310,
            288,
            324
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_4",
          "label": "para",
          "text": "(45) a. You like Jody",
          "level": -1,
          "page": 371,
          "reading_order": 4,
          "bbox": [
            109,
            339,
            244,
            351
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_5",
          "label": "para",
          "text": "b. *You like",
          "level": -1,
          "page": 371,
          "reading_order": 5,
          "bbox": [
            150,
            358,
            218,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_6",
          "label": "para",
          "text": "(46) a. You put the card into the slot",
          "level": -1,
          "page": 371,
          "reading_order": 6,
          "bbox": [
            109,
            385,
            334,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_7",
          "label": "para",
          "text": "b. *You put into the slot.",
          "level": -1,
          "page": 371,
          "reading_order": 7,
          "bbox": [
            150,
            403,
            297,
            423
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_8",
          "label": "para",
          "text": "- You put the card.",
          "level": -1,
          "page": 371,
          "reading_order": 8,
          "bbox": [
            153,
            430,
            270,
            444
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_9",
          "label": "para",
          "text": "d. *You put",
          "level": -1,
          "page": 371,
          "reading_order": 9,
          "bbox": [
            149,
            448,
            217,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_10",
          "label": "para",
          "text": "The verb like requires an NP complement, while put requires both a following NP and\nPP . (45) and (46) show that these complements are obligatory: omitting them leads to\nungrammaticality. Yet there are contexts in which obligatory complements can be\nomitted, as (47) and (48) illustrate.",
          "level": -1,
          "page": 371,
          "reading_order": 10,
          "bbox": [
            97,
            474,
            585,
            541
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_11",
          "label": "para",
          "text": "(47) a. Kim knows who you like.",
          "level": -1,
          "page": 371,
          "reading_order": 11,
          "bbox": [
            109,
            555,
            315,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_12",
          "label": "para",
          "text": "b. This music, you really like.",
          "level": -1,
          "page": 371,
          "reading_order": 12,
          "bbox": [
            150,
            573,
            324,
            592
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_13",
          "label": "para",
          "text": "(48) a. Which card do you put into the slot?",
          "level": -1,
          "page": 371,
          "reading_order": 13,
          "bbox": [
            109,
            607,
            377,
            621
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_14",
          "label": "para",
          "text": "b. Which slot do you put the card into?",
          "level": -1,
          "page": 371,
          "reading_order": 14,
          "bbox": [
            150,
            627,
            377,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_15",
          "label": "para",
          "text": "That is, an obligatory complement can be omitted if there is an appropriate filler in\nthe sentence, such as the question word who in (47a) , the preposed topic this music in\n(47b) , or the wh phrases which card/slot in (48) . It is common to say that sentences like\nthose in (47) and (48) contain gaps where the obligatory complements have been\nomitted, and these gaps are sometimes made explicit using an underscore:",
          "level": -1,
          "page": 371,
          "reading_order": 15,
          "bbox": [
            97,
            654,
            585,
            737
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_16",
          "label": "para",
          "text": "(49) a. Which card do you put__ into the slot?",
          "level": -1,
          "page": 371,
          "reading_order": 16,
          "bbox": [
            109,
            752,
            395,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_17",
          "label": "para",
          "text": "b. Which slot do you put the card into __?",
          "level": -1,
          "page": 371,
          "reading_order": 17,
          "bbox": [
            150,
            770,
            395,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_371_order_18",
          "label": "foot",
          "text": "9.3 Extending a Feature-Based Grammar | 349",
          "level": -1,
          "page": 371,
          "reading_order": 18,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "9.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_0",
          "label": "para",
          "text": "So, a gap can occur if it is licensed by a filler. Conversely, fillers can occur only if there\nis an appropriate gap elsewhere in the sentence, as shown by the following examples:",
          "level": -1,
          "page": 372,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_1",
          "label": "para",
          "text": "(50) a.*Kim knows who you like Jody.",
          "level": -1,
          "page": 372,
          "reading_order": 1,
          "bbox": [
            109,
            116,
            350,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_2",
          "label": "para",
          "text": "b.\n*This music, you really like hip-hop.",
          "level": -1,
          "page": 372,
          "reading_order": 2,
          "bbox": [
            144,
            141,
            377,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_3",
          "label": "para",
          "text": "(51) a.*Which card do you put this into the slot?",
          "level": -1,
          "page": 372,
          "reading_order": 3,
          "bbox": [
            109,
            161,
            405,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_4",
          "label": "para",
          "text": "b. *Which slot do you put the card into this one?",
          "level": -1,
          "page": 372,
          "reading_order": 4,
          "bbox": [
            144,
            188,
            431,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_5",
          "label": "para",
          "text": "The mutual co-occurrence between filler and gap is sometimes termed a “ dependency. ”\nOne issue of considerable importance in theoretical linguistics has been the nature of\nthe material that can intervene between a filler and the gap that it licenses; in particular,\ncan we simply list a finite set of sequences that separate the two? The answer is no:\nthere is no upper bound on the distance between filler and gap. This fact can be easily\nillustrated with constructions involving sentential complements, as shown in (52).",
          "level": -1,
          "page": 372,
          "reading_order": 5,
          "bbox": [
            97,
            215,
            585,
            316
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_6",
          "label": "para",
          "text": "(52) a. Who do you like_?",
          "level": -1,
          "page": 372,
          "reading_order": 6,
          "bbox": [
            109,
            331,
            288,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_7",
          "label": "para",
          "text": "b. Who do you claim that you like __?",
          "level": -1,
          "page": 372,
          "reading_order": 7,
          "bbox": [
            144,
            349,
            371,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_8",
          "label": "para",
          "text": "c. Who do you claim that Jody says that you like __",
          "level": -1,
          "page": 372,
          "reading_order": 8,
          "bbox": [
            144,
            367,
            449,
            386
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_9",
          "label": "para",
          "text": "Since we can have indefinitely deep recursion of sentential complements, the gap can\nbe embedded indefinitely far inside the whole sentence. This constellation of properties\nleads to the notion of an unbounded dependency construction , that is, a filler-gap\ndependency where there is no upper bound on the distance between filler and gap.",
          "level": -1,
          "page": 372,
          "reading_order": 9,
          "bbox": [
            97,
            401,
            585,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_10",
          "label": "para",
          "text": "A variety of mechanisms have been suggested for handling unbounded dependencies\nin formal grammars; here we illustrate the approach due to Generalized Phrase Struc-\nture Grammar that involves slash categories . A slash category has the form Y/XP ; we\ninterpret this as a phrase of category Y that is missing a subconstituent of category XP .\nFor example, S/NP is an S that is missing an NP . The use of slash categories is illustrated\nin (53) .",
          "level": -1,
          "page": 372,
          "reading_order": 10,
          "bbox": [
            97,
            465,
            585,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_11",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_372_figure_011.png)",
          "level": -1,
          "page": 372,
          "reading_order": 11,
          "bbox": [
            109,
            582,
            532,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_12",
          "label": "para",
          "text": "The top part of the tree introduces the filler who (treated as an expression of category\nNP[+wh]) together with a corresponding gap-containing constituent S/NP. The gap",
          "level": -1,
          "page": 372,
          "reading_order": 12,
          "bbox": [
            97,
            761,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_372_order_13",
          "label": "foot",
          "text": "350 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 372,
          "reading_order": 13,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "350",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_373_order_0",
          "label": "para",
          "text": "information is then “percolated” down the tree via the VP/NP category, until it reaches\nthe category NP/NP. At this point, the dependency is discharged by realizing the gap\ninformation as the empty string, immediately dominated by NP/NP.",
          "level": -1,
          "page": 373,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_373_order_1",
          "label": "para",
          "text": "Do we need to think of slash categories as a completely new kind of object? Fortunately,\nwe can accommodate them within our existing feature-based framework, by treating\nslash as a feature and the category to its right as a value; that is, S/NP is reducible to\nS[SLASH=NP]. In practice, this is also how the parser interprets slash categories.",
          "level": -1,
          "page": 373,
          "reading_order": 1,
          "bbox": [
            97,
            132,
            585,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_373_order_2",
          "label": "para",
          "text": "The grammar shown in Example 9-3 illustrates the main principles of slash categories,\nand also includes productions for inverted clauses. To simplify presentation, we have\nomitted any specification of tense on the verbs.",
          "level": -1,
          "page": 373,
          "reading_order": 2,
          "bbox": [
            100,
            205,
            585,
            252
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_373_order_3",
          "label": "para",
          "text": "Example 9-3. Grammar with productions for inverted clauses and long-distance dependencies,\nmaking use of slash categories.",
          "level": -1,
          "page": 373,
          "reading_order": 3,
          "bbox": [
            97,
            266,
            584,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_373_order_5",
          "label": "para",
          "text": "The grammar in Example 9-3 contains one “gap-introduction” production, namely S[-\nINV] -> NP S/NP. In order to percolate the slash feature correctly, we need to add slashes\nwith variable values to both sides of the arrow in productions that expand S, VP, and\nNP. For example, VP/?x -> V SBar/?x is the slashed version of VP -> V SBar and says",
          "level": -1,
          "page": 373,
          "reading_order": 5,
          "bbox": [
            97,
            732,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_373_order_6",
          "label": "foot",
          "text": "9.3 Extending a Feature-Based Grammar | 351",
          "level": -1,
          "page": 373,
          "reading_order": 6,
          "bbox": [
            386,
            824,
            584,
            842
          ],
          "section_number": "9.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_374_order_0",
          "label": "para",
          "text": "that a slash value can be specified on the VP parent of a constituent if the same value is\nalso specified on the SBar child. Finally, NP/NP -> allows the slash information on NP to\nbe discharged as the empty string. Using the grammar in Example 9-3, we can parse\nthe sequence who do you claim that you like:",
          "level": -1,
          "page": 374,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_374_order_2",
          "label": "para",
          "text": "A more readable version of this tree is shown in (54)",
          "level": -1,
          "page": 374,
          "reading_order": 2,
          "bbox": [
            97,
            376,
            395,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_374_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_374_figure_003.png)",
          "level": -1,
          "page": 374,
          "reading_order": 3,
          "bbox": [
            109,
            403,
            587,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_374_order_4",
          "label": "para",
          "text": "The grammar in Example 9-3 will also allow us to parse sentences without gaps:",
          "level": -1,
          "page": 374,
          "reading_order": 4,
          "bbox": [
            100,
            609,
            557,
            628
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_374_order_6",
          "label": "foot",
          "text": "352 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 374,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "352",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_375_order_0",
          "label": "para",
          "text": "In addition, it admits inverted sentences that do not involve wh constructions",
          "level": -1,
          "page": 375,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            540,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_375_order_2",
      "label": "sec",
      "text": "Case and Gender in German",
      "level": 1,
      "page": 375,
      "reading_order": 2,
      "bbox": [
        97,
        224,
        279,
        250
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_378_order_3",
          "label": "sub_sec",
          "text": "9.4 Summary",
          "level": 2,
          "page": 378,
          "reading_order": 3,
          "bbox": [
            97,
            259,
            207,
            282
          ],
          "section_number": "9.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_378_order_4",
              "label": "list_group",
              "text": "The traditional categories of context-free grammar are atomic symbols. An impor-\ntant motivation for feature structures is to capture fine-grained distinctions that\nwould otherwise require a massive multiplication of atomic categories.\nBy using variables over feature values, we can express constraints in grammar pro-\nductions that allow the realization of different feature specifications to be inter-\ndependent.",
              "level": -1,
              "page": 378,
              "reading_order": 4,
              "bbox": [
                121,
                286,
                585,
                340
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "The traditional categories of context-free grammar are atomic symbols. An impor-\ntant motivation for feature structures is to capture fine-grained distinctions that\nwould otherwise require a massive multiplication of atomic categories.",
                  "bbox": [
                    121,
                    286,
                    585,
                    340
                  ],
                  "page": 378,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "By using variables over feature values, we can express constraints in grammar pro-\nductions that allow the realization of different feature specifications to be inter-\ndependent.",
                  "bbox": [
                    126,
                    340,
                    585,
                    394
                  ],
                  "page": 378,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "Typically we specify fixed values of features at the lexical level and constrain the\nvalues of features in phrases to unify with the corresponding values in their\nchildren.",
                  "bbox": [
                    121,
                    394,
                    585,
                    448
                  ],
                  "page": 378,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "Feature values are either atomic or complex. A particular subcase of atomic value\nis the Boolean value, represented by convention as [+/- feat].",
                  "bbox": [
                    125,
                    454,
                    585,
                    484
                  ],
                  "page": 378,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "Two features can share a value (either atomic or complex). Structures with shared\nvalues are said to be re-entrant. Shared values are represented by numerical indexes\n(or tags) in AVMs.",
                  "bbox": [
                    121,
                    491,
                    585,
                    538
                  ],
                  "page": 378,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "A path in a feature structure is a tuple of features corresponding to the labels on a\nsequence of arcs from the root of the graph representation.",
                  "bbox": [
                    122,
                    545,
                    584,
                    575
                  ],
                  "page": 378,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "Two paths are equivalent if they share a value.",
                  "bbox": [
                    125,
                    582,
                    386,
                    596
                  ],
                  "page": 378,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "Feature structures are partially ordered by subsumption. $FS_{0}$ subsumes $FS_{1}$ when\n$FS_{0}$ is more general (less informative) than $FS_{1}$.",
                  "bbox": [
                    126,
                    600,
                    584,
                    636
                  ],
                  "page": 378,
                  "reading_order": 11
                },
                {
                  "label": "list",
                  "text": "The unification of two structures $FS_{0}$ and $FS_{1}$, if successful, is the feature structure\n$FS_{2}$ that contains the combined information of both $FS_{0}$ and $FS_{1}$.",
                  "bbox": [
                    121,
                    636,
                    585,
                    672
                  ],
                  "page": 378,
                  "reading_order": 12
                },
                {
                  "label": "list",
                  "text": "If unification specializes a path π in FS, then it also specializes every path π' equiv-\nalent to π.",
                  "bbox": [
                    122,
                    672,
                    584,
                    707
                  ],
                  "page": 378,
                  "reading_order": 13
                },
                {
                  "label": "list",
                  "text": "We can use feature structures to build succinct analyses of a wide variety of lin-\nguistic phenomena, including verb subcategorization, inversion constructions,\nunbounded dependency constructions, and case government.",
                  "bbox": [
                    121,
                    714,
                    584,
                    762
                  ],
                  "page": 378,
                  "reading_order": 14
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_378_order_15",
              "label": "foot",
              "text": "356 | Chapter 9: Building Feature-Based Grammars",
              "level": -1,
              "page": 378,
              "reading_order": 15,
              "bbox": [
                97,
                824,
                316,
                842
              ],
              "section_number": "356",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_379_order_0",
          "label": "sub_sec",
          "text": "9.5 Further Reading",
          "level": 2,
          "page": 379,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            261,
            100
          ],
          "section_number": "9.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_379_order_1",
              "label": "para",
              "text": "Please consult http://www.nltk.org/ for further materials on this chapter, including\nHOWTOs feature structures, feature grammars, Earley parsing, and grammar test\nsuites.",
              "level": -1,
              "page": 379,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                153
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_379_order_2",
              "label": "para",
              "text": "For an excellent introduction to the phenomenon of agreement, see (Corbett, 2006).",
              "level": -1,
              "page": 379,
              "reading_order": 2,
              "bbox": [
                98,
                161,
                583,
                179
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_379_order_3",
              "label": "para",
              "text": "The earliest use of features in theoretical linguistics was designed to capture phono-\nlogical properties of phonemes. For example, a sound like /b/ might be decomposed\ninto the structure [ +labial, +voice ] . An important motivation was to capture gener-\nalizations across classes of segments, for example, that /n/ gets realized as /m/ preceding\nany +labial consonant. Within Chomskyan grammar, it was standard to use atomic\nfeatures for phenomena such as agreement, and also to capture generalizations across\nsyntactic categories, by analogy with phonology. A radical expansion of the use of\nfeatures in theoretical syntax was advocated by Generalized Phrase Structure Grammar\n(GPSG; [Gazdar et al., 1985] ), particularly in the use of features with complex values.",
              "level": -1,
              "page": 379,
              "reading_order": 3,
              "bbox": [
                97,
                188,
                585,
                335
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_379_order_4",
              "label": "para",
              "text": "Coming more from the perspective of computational linguistics, (Kay, 1985) proposed\nthat functional aspects of language could be captured by unification of attribute-value\nstructures, and a similar approach was elaborated by (Grosz & Stickel, 1983) within\nthe PATR-II formalism. Early work in Lexical-Functional grammar (LFG; [Kaplan &\nBresnan, 1982] ) introduced the notion of an f-structure that was primarily intended\nto represent the grammatical relations and predicate-argument structure associated\nwith a constituent structure parse. (Shieber, 1986) provides an excellent introduction\nto this phase of research into feature-based grammars.",
              "level": -1,
              "page": 379,
              "reading_order": 4,
              "bbox": [
                97,
                340,
                585,
                475
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_379_order_5",
              "label": "para",
              "text": "One conceptual difficulty with algebraic approaches to feature structures arose when\nresearchers attempted to model negation. An alternative perspective, pioneered by\n(Kasper & Rounds, 1986) and (Johnson, 1988) , argues that grammars involve descrip-\ntions of feature structures rather than the structures themselves. These descriptions are\ncombined using logical operations such as conjunction, and negation is just the usual\nlogical operation over feature descriptions. This description-oriented perspective was\nintegral to LFG from the outset (Kaplan, 1989) , and was also adopted by later versions\nof Head-Driven Phrase Structure Grammar (HPSG; [Sag & Wasow, 1999] ). A com-\nprehensive bibliography of HPSG literature can be found at http://www.cl.uni-bremen\n.de/HPSG-Bib/ .",
              "level": -1,
              "page": 379,
              "reading_order": 5,
              "bbox": [
                97,
                483,
                585,
                645
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_379_order_6",
              "label": "para",
              "text": "Feature structures, as presented in this chapter, are unable to capture important con-\nstraints on linguistic information. For example, there is no way of saying that the only\npermissible values for NUM are sg and pl , while a specification such as [NUM=masc] is\nanomalous. Similarly, we cannot say that the complex value of AGR must contain spec-\nifications for the features PER , NUM , and GND , but cannot contain a specification such as\n[SUBCAT=trans] . Typed feature structures were developed to remedy this deficiency.\nA good early review of work on typed feature structures is (Emele & Zajac, 1990) . A\nmore comprehensive examination of the formal foundations can be found in",
              "level": -1,
              "page": 379,
              "reading_order": 6,
              "bbox": [
                97,
                654,
                585,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_379_order_7",
              "label": "foot",
              "text": "9.5 Further Reading | 357",
              "level": -1,
              "page": 379,
              "reading_order": 7,
              "bbox": [
                467,
                824,
                585,
                842
              ],
              "section_number": "9.5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_0",
              "label": "para",
              "text": "(Carpenter, 1992), while (Copestake, 2002) focuses on implementing an HPSG-orien-\nted approach to typed feature structures.",
              "level": -1,
              "page": 380,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_1",
              "label": "para",
              "text": "There is a copious literature on the analysis of German within feature-based grammar\nframeworks. (Nerbonnæ, Netter & Pollard, 1994) is a good starting point for the HPSG\nliterature on this topic, while (Müller, 2002) gives a very extensive and detailed analysis\nof German syntax in HPSG.",
              "level": -1,
              "page": 380,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                179
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_2",
              "label": "para",
              "text": "Chapter 15 of (Jurafsky & Martin, 2008) discusses feature structures, the unification\nalgorithm, and the integration of unification into parsing algorithms.",
              "level": -1,
              "page": 380,
              "reading_order": 2,
              "bbox": [
                97,
                188,
                584,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_380_order_3",
          "label": "sub_sec",
          "text": "9.6 Exercises",
          "level": 2,
          "page": 380,
          "reading_order": 3,
          "bbox": [
            97,
            241,
            201,
            263
          ],
          "section_number": "9.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_380_order_4",
              "label": "para",
              "text": "1. ◦ What constraints are required to correctly parse word sequences like I am hap-\npy and she is happy but not *you is happy or *they am happy? Implement two sol-\nutions for the present tense paradigm of the verb be in English, first taking Gram-\nmar (8) as your starting point, and then taking Grammar (20) as the starting point.",
              "level": -1,
              "page": 380,
              "reading_order": 4,
              "bbox": [
                100,
                277,
                585,
                342
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_5",
              "label": "para",
              "text": "2. ◦ Develop a variant of grammar in Example 9-1 that uses a feature COUNT to make\nthe distinctions shown here:",
              "level": -1,
              "page": 380,
              "reading_order": 5,
              "bbox": [
                100,
                342,
                585,
                376
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_6",
              "label": "para",
              "text": "(56) a. The boy sings",
              "level": -1,
              "page": 380,
              "reading_order": 6,
              "bbox": [
                109,
                394,
                245,
                412
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_7",
              "label": "para",
              "text": "b. *Boy sings",
              "level": -1,
              "page": 380,
              "reading_order": 7,
              "bbox": [
                144,
                412,
                225,
                430
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_8",
              "label": "para",
              "text": "(57) a. The boys sing",
              "level": -1,
              "page": 380,
              "reading_order": 8,
              "bbox": [
                109,
                439,
                245,
                457
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_9",
              "label": "para",
              "text": "b. Boys sing",
              "level": -1,
              "page": 380,
              "reading_order": 9,
              "bbox": [
                144,
                457,
                220,
                478
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_10",
              "label": "para",
              "text": "(58) a. The water is precious.",
              "level": -1,
              "page": 380,
              "reading_order": 10,
              "bbox": [
                109,
                492,
                297,
                510
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_11",
              "label": "para",
              "text": "b. Water is precious.",
              "level": -1,
              "page": 380,
              "reading_order": 11,
              "bbox": [
                144,
                510,
                270,
                528
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_12",
              "label": "para",
              "text": "3. ◦ Write a function subsumes() that holds of two feature structures fs1 and fs2 just\nin case fs1 subsumes fs2.",
              "level": -1,
              "page": 380,
              "reading_order": 12,
              "bbox": [
                100,
                528,
                585,
                564
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_13",
              "label": "para",
              "text": "4. ◦ Modify the grammar illustrated in (30) to incorporate a BAR feature for dealing\nwith phrasal projections.",
              "level": -1,
              "page": 380,
              "reading_order": 13,
              "bbox": [
                100,
                564,
                585,
                602
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_14",
              "label": "para",
              "text": "5. ◦ Modify the German grammar in Example 9-4 to incorporate the treatment of\nsubcategorization presented in Section 9.3 .",
              "level": -1,
              "page": 380,
              "reading_order": 14,
              "bbox": [
                100,
                602,
                584,
                639
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_15",
              "label": "para",
              "text": "6. • Develop a feature-based grammar that will correctly describe the following\nSpanish noun phrases:",
              "level": -1,
              "page": 380,
              "reading_order": 15,
              "bbox": [
                100,
                645,
                585,
                676
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_16",
              "label": "para",
              "text": "(59) un cuadro hermos-o\nINDEF.SG.MASC picture beautiful-SG.MASC\n‘a beautiful picture’",
              "level": -1,
              "page": 380,
              "reading_order": 16,
              "bbox": [
                109,
                689,
                315,
                734
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_17",
              "label": "para",
              "text": "(60) un-os cuadro-s hermos-os\nINDEF-PL.MASC picture-PL beautiful-PL.MASC",
              "level": -1,
              "page": 380,
              "reading_order": 17,
              "bbox": [
                109,
                743,
                324,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_380_order_18",
              "label": "foot",
              "text": "358 | Chapter 9: Building Feature-Based Grammars",
              "level": -1,
              "page": 380,
              "reading_order": 18,
              "bbox": [
                97,
                824,
                316,
                842
              ],
              "section_number": "358",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_0",
              "label": "para",
              "text": "(61) un-a cortina hermos-a\nINDEF-SG.FEM curtain beautiful-SG.FEM\n‘a beautiful curtain’",
              "level": -1,
              "page": 381,
              "reading_order": 0,
              "bbox": [
                109,
                71,
                300,
                116
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_1",
              "label": "para",
              "text": "(62) un-as cortina-s hermos-as\nINDEF-PL.FEM curtain beautiful-PL.FEM\n'beautiful curtains'",
              "level": -1,
              "page": 381,
              "reading_order": 1,
              "bbox": [
                109,
                132,
                306,
                170
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_2",
              "label": "para",
              "text": "7. o Develop a wrapper for the earley_parser so that a trace is only printed if the\ninput sequence fails to parse.",
              "level": -1,
              "page": 381,
              "reading_order": 2,
              "bbox": [
                100,
                179,
                585,
                210
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_3",
              "label": "para",
              "text": "8. o Consider the feature structures shown in Example 9-5",
              "level": -1,
              "page": 381,
              "reading_order": 3,
              "bbox": [
                100,
                215,
                441,
                232
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_4",
              "label": "para",
              "text": "Example 9-5. Exploring feature structure",
              "level": -1,
              "page": 381,
              "reading_order": 4,
              "bbox": [
                126,
                241,
                324,
                259
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_6",
              "label": "para",
              "text": "Work out on paper what the result is of the following unifications. (Hint: you might\nfind it useful to draw the graph structures.)",
              "level": -1,
              "page": 381,
              "reading_order": 6,
              "bbox": [
                118,
                410,
                585,
                441
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_7",
              "label": "para",
              "text": "a, fs1 and fs2",
              "level": -1,
              "page": 381,
              "reading_order": 7,
              "bbox": [
                126,
                447,
                211,
                458
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_8",
              "label": "para",
              "text": "b. fs1 and fs3",
              "level": -1,
              "page": 381,
              "reading_order": 8,
              "bbox": [
                126,
                465,
                211,
                483
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_9",
              "label": "para",
              "text": "c. fs4 and fs5",
              "level": -1,
              "page": 381,
              "reading_order": 9,
              "bbox": [
                126,
                488,
                211,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_10",
              "label": "para",
              "text": "d. fs5 and fs6",
              "level": -1,
              "page": 381,
              "reading_order": 10,
              "bbox": [
                126,
                509,
                211,
                521
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_11",
              "label": "para",
              "text": "e. fs5 and fs7",
              "level": -1,
              "page": 381,
              "reading_order": 11,
              "bbox": [
                126,
                528,
                211,
                546
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_12",
              "label": "para",
              "text": "f. fs8 and fs9",
              "level": -1,
              "page": 381,
              "reading_order": 12,
              "bbox": [
                126,
                546,
                211,
                564
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_13",
              "label": "para",
              "text": "g. fs8 and fs10",
              "level": -1,
              "page": 381,
              "reading_order": 13,
              "bbox": [
                126,
                571,
                217,
                585
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_14",
              "label": "para",
              "text": "Check your answers using NLTK.",
              "level": -1,
              "page": 381,
              "reading_order": 14,
              "bbox": [
                122,
                591,
                315,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_15",
              "label": "para",
              "text": "9. o List two feature structures that subsume [A=?x, B=?x]",
              "level": -1,
              "page": 381,
              "reading_order": 15,
              "bbox": [
                100,
                609,
                440,
                627
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_16",
              "label": "para",
              "text": "10.\n• Ignoring structure sharing, give an informal algorithm for unifying two feature\nstructures.",
              "level": -1,
              "page": 381,
              "reading_order": 16,
              "bbox": [
                100,
                633,
                585,
                663
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_17",
              "label": "para",
              "text": "11. o Extend the German grammar in Example 9-4 so that it can handle so-called verb-\nsecond structures like the following:",
              "level": -1,
              "page": 381,
              "reading_order": 17,
              "bbox": [
                100,
                670,
                584,
                701
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_18",
              "label": "para",
              "text": "(63) Heute sieht der Hund die Katze",
              "level": -1,
              "page": 381,
              "reading_order": 18,
              "bbox": [
                109,
                716,
                324,
                728
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_19",
              "label": "para",
              "text": "12. o Seemingly synonymous verbs have slightly different syntactic properties (Levin,\n1993). Consider the following patterns of grammaticality for the verbs loaded ,\nfilled , and dumped . Can you write grammar productions to handle such data?",
              "level": -1,
              "page": 381,
              "reading_order": 19,
              "bbox": [
                100,
                734,
                584,
                783
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_381_order_20",
              "label": "foot",
              "text": "9.6 Exercises | 359",
              "level": -1,
              "page": 381,
              "reading_order": 20,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "9.6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_0",
              "label": "para",
              "text": "(64) a. The farmer loaded the cart with sand",
              "level": -1,
              "page": 382,
              "reading_order": 0,
              "bbox": [
                109,
                71,
                377,
                89
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_1",
              "label": "para",
              "text": "b. The farmer loaded sand into the car",
              "level": -1,
              "page": 382,
              "reading_order": 1,
              "bbox": [
                150,
                89,
                369,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_2",
              "label": "para",
              "text": "c. The farmer filled the cart with sand",
              "level": -1,
              "page": 382,
              "reading_order": 2,
              "bbox": [
                151,
                107,
                368,
                134
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_3",
              "label": "para",
              "text": "d. *The farmer filled sand into the cart",
              "level": -1,
              "page": 382,
              "reading_order": 3,
              "bbox": [
                149,
                134,
                368,
                152
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_4",
              "label": "para",
              "text": "e. *The farmer dumped the cart with sand",
              "level": -1,
              "page": 382,
              "reading_order": 4,
              "bbox": [
                151,
                152,
                388,
                170
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_5",
              "label": "para",
              "text": "f. The farmer dumped sand into the cart",
              "level": -1,
              "page": 382,
              "reading_order": 5,
              "bbox": [
                153,
                170,
                386,
                192
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_6",
              "label": "para",
              "text": "13. • Morphological paradigms are rarely completely regular, in the sense of every cell\nin the matrix having a different realization. For example, the present tense conju-\ngation of the lexeme walk has only two distinct forms: walks for the third-person\nsingular, and walk for all other combinations of person and number. A successful\nanalysis should not require redundantly specifying that five out of the six possible\nmorphological combinations have the same realization. Propose and implement a\nmethod for dealing with this.",
              "level": -1,
              "page": 382,
              "reading_order": 6,
              "bbox": [
                100,
                197,
                585,
                313
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_7",
              "label": "para",
              "text": "14. • So-called head features are shared between the parent node and head child. For\nexample, TENSE is a head feature that is shared between a VP and its head V child.\nSee (Gazdar et al., 1985) for more details. Most of the features we have looked at\nare head features—exceptions are SUBCAT and SLASH . Since the sharing of head fea-\ntures is predictable, it should not need to be stated explicitly in the grammar\nproductions. Develop an approach that automatically accounts for this regular\nbehavior of head features.",
              "level": -1,
              "page": 382,
              "reading_order": 7,
              "bbox": [
                100,
                313,
                585,
                430
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_8",
              "label": "para",
              "text": "15. • Extend NLTK's treatment of feature structures to allow unification into list-\nvalued features, and use this to implement an HPSG-style analysis of subcategori-\nzation, whereby the SUBCAT of a head category is the concatenation of its\ncomplements' categories with the SUBCAT value of its immediate parent.",
              "level": -1,
              "page": 382,
              "reading_order": 8,
              "bbox": [
                100,
                439,
                585,
                503
              ],
              "section_number": "15",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_9",
              "label": "para",
              "text": "16. • Extend NLTK's treatment of feature structures to allow productions with un-\nderspecified categories, such as S[-INV] -> ?x S/?x.",
              "level": -1,
              "page": 382,
              "reading_order": 9,
              "bbox": [
                100,
                509,
                584,
                540
              ],
              "section_number": "16",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_10",
              "label": "para",
              "text": "17.\n• Extend NLTK’s treatment of feature structures to allow typed feature structures.",
              "level": -1,
              "page": 382,
              "reading_order": 10,
              "bbox": [
                100,
                546,
                584,
                564
              ],
              "section_number": "17",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_11",
              "label": "para",
              "text": "18. • Pick some grammatical constructions described in (Huddleston & Pullum,\n2002), and develop a feature-based grammar to account for them.",
              "level": -1,
              "page": 382,
              "reading_order": 11,
              "bbox": [
                100,
                564,
                584,
                600
              ],
              "section_number": "18",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_382_order_12",
              "label": "foot",
              "text": "360 | Chapter 9: Building Feature-Based Grammars",
              "level": -1,
              "page": 382,
              "reading_order": 12,
              "bbox": [
                97,
                824,
                316,
                842
              ],
              "section_number": "360",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_375_order_3",
          "label": "para",
          "text": "Compared with English, German has a relatively rich morphology for agreement. For\nexample, the definite article in German varies with case, gender, and number, as shown\nin Table 9-2.",
          "level": -1,
          "page": 375,
          "reading_order": 3,
          "bbox": [
            97,
            250,
            585,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_375_order_4",
          "label": "table",
          "text": "Table 9-2. Morphological paradigm for the German definite article [TABLE: <table><tr><td>Case</td><td>Masculine</td><td>Feminine</td><td>Neutral</td><td>Plural</td></tr><tr><td>Nominative</td><td>der</td><td>die</td><td>das</td><td>die</td></tr><tr><td>Genitive</td><td>des</td><td>der</td><td>des</td><td>der</td></tr><tr><td>Dative</td><td>dem</td><td>der</td><td>dem</td><td>den</td></tr><tr><td>Accusative</td><td>den</td><td>die</td><td>das</td><td>die</td></tr></table>]",
          "level": -1,
          "page": 375,
          "reading_order": 4,
          "bbox": [
            100,
            340,
            350,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Case</td><td>Masculine</td><td>Feminine</td><td>Neutral</td><td>Plural</td></tr><tr><td>Nominative</td><td>der</td><td>die</td><td>das</td><td>die</td></tr><tr><td>Genitive</td><td>des</td><td>der</td><td>des</td><td>der</td></tr><tr><td>Dative</td><td>dem</td><td>der</td><td>dem</td><td>den</td></tr><tr><td>Accusative</td><td>den</td><td>die</td><td>das</td><td>die</td></tr></table>",
              "bbox": [
                100,
                340,
                350,
                439
              ],
              "page": 375,
              "reading_order": 4
            },
            {
              "label": "cap",
              "text": "Table 9-2. Morphological paradigm for the German definite article",
              "bbox": [
                99,
                313,
                423,
                331
              ],
              "page": 375,
              "reading_order": 5
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_375_order_6",
          "label": "para",
          "text": "Subjects in German take the nominative case, and most verbs govern their objects in\nthe accusative case. However, there are exceptions, such as helfen , that govern the\ndative case:",
          "level": -1,
          "page": 375,
          "reading_order": 6,
          "bbox": [
            97,
            448,
            585,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_375_order_7",
          "label": "para",
          "text": "(55) a. Die\nKatze\nsieht\nden\nHund\nthe.NOM.FEM.SG cat.3.FEM.SG see.3.SG the.ACC.MASC.SG dog.3.MASC.SG\n‘the cat sees the dog’",
          "level": -1,
          "page": 375,
          "reading_order": 7,
          "bbox": [
            109,
            519,
            458,
            559
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_375_order_8",
          "label": "para",
          "text": "b. *Die\nKatze\nsieht\ndem\nHund\nthe.NOM.FEM.SG cat.3.FEM.SG see.3.SG the.DAT.MASC.SG dog.3.MASC.SG\nc. Die\nKatze\nhilft\ndem\nHund\nthe.NOM.FEM.SG cat.3.FEM.SG help.3.SG the.DAT.MASC.SG dog.3.MASC.SG\n‘the cat helps the dog’\nd. *Die\nKatze\nhilft\nden\nHund\nthe.NOM.FEM.SG cat.3.FEM.SG help.3.SG the.ACC.MASC.SG dog.3.MASC.SG",
          "level": -1,
          "page": 375,
          "reading_order": 8,
          "bbox": [
            149,
            564,
            462,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_375_order_9",
          "label": "para",
          "text": "The grammar in Example 9-4 illustrates the interaction of agreement (comprising per-\nson, number, and gender) with case.",
          "level": -1,
          "page": 375,
          "reading_order": 9,
          "bbox": [
            97,
            689,
            584,
            719
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_375_order_10",
          "label": "foot",
          "text": "9.3 Extending a Feature-Based Grammar | 353",
          "level": -1,
          "page": 375,
          "reading_order": 10,
          "bbox": [
            386,
            824,
            584,
            842
          ],
          "section_number": "9.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_376_order_0",
          "label": "para",
          "text": "Example 9-4. Example feature-based gramma",
          "level": -1,
          "page": 376,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            324,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_376_order_2",
          "label": "foot",
          "text": "354 | Chapter 9: Building Feature-Based Grammars",
          "level": -1,
          "page": 376,
          "reading_order": 2,
          "bbox": [
            97,
            824,
            316,
            842
          ],
          "section_number": "354",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_377_order_1",
          "label": "para",
          "text": "As you can see, the feature objcase is used to specify the case that a verb governs on its\nobject. The next example illustrates the parse tree for a sentence containing a verb that\ngoverns the dative case:",
          "level": -1,
          "page": 377,
          "reading_order": 1,
          "bbox": [
            97,
            152,
            585,
            199
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_377_order_3",
          "label": "para",
          "text": "In developing grammars, excluding ungrammatical word sequences is often as chal-\nlenging as parsing grammatical ones. In order to get an idea where and why a sequence\nfails to parse, setting the trace parameter of the load_parser() method can be crucial.\nConsider the following parse failure:",
          "level": -1,
          "page": 377,
          "reading_order": 3,
          "bbox": [
            97,
            373,
            585,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_377_order_5",
          "label": "foot",
          "text": "9.3 Extending a Feature-Based Grammar | 355",
          "level": -1,
          "page": 377,
          "reading_order": 5,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "9.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_378_order_0",
          "label": "para",
          "text": "The last two Scanner lines in the trace show that den is recognized as admitting two\npossible categories: Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] and\nDet[AGR=[NUM='pl', PER=3], CASE='dat'] . We know from the grammar in Exam-\nple 9-4 that Katze has category N[AGR=[GND=fem, NUM=sg, PER=3]] . Thus there is no\nbinding for the variable ?a in production:",
          "level": -1,
          "page": 378,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_378_order_1",
          "label": "para",
          "text": "NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=? a] N[CASE=?c, AGR=?a]",
          "level": -1,
          "page": 378,
          "reading_order": 1,
          "bbox": [
            122,
            161,
            467,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_378_order_2",
          "label": "para",
          "text": "that will satisfy these constraints, since the AGR value of Katze will not unify with either\nof the AGR values of den , that is, with either [GND='masc', NUM='sg', PER=3] or\n[NUM='pl', PER=3] .",
          "level": -1,
          "page": 378,
          "reading_order": 2,
          "bbox": [
            97,
            186,
            585,
            233
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_383_order_0",
      "label": "sec",
      "text": "CHAPTER 10\n\nAnalyzing the Meaning of Sentences",
      "level": 1,
      "page": 383,
      "reading_order": 0,
      "bbox": [
        118,
        78,
        584,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_383_order_6",
          "label": "sub_sec",
          "text": "10.1 Natural Language Understanding",
          "level": 2,
          "page": 383,
          "reading_order": 6,
          "bbox": [
            98,
            553,
            413,
            576
          ],
          "section_number": "10.1",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_383_order_7",
              "label": "sub_sub_sec",
              "text": "Querying a Database",
              "level": 3,
              "page": 383,
              "reading_order": 7,
              "bbox": [
                97,
                591,
                235,
                610
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_383_order_8",
                  "label": "para",
                  "text": "Suppose we have a program that lets us type in a natural language question and gives\nus back the right answer:",
                  "level": -1,
                  "page": 383,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    618,
                    585,
                    654
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_383_order_9",
                  "label": "para",
                  "text": "(1) a. Which country is Athens in\nb. Greece.",
                  "level": -1,
                  "page": 383,
                  "reading_order": 9,
                  "bbox": [
                    118,
                    663,
                    324,
                    698
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_383_order_10",
                  "label": "para",
                  "text": "How hard is it to write such a program? And can we just use the same techniques that\nwe’ve encountered so far in this book, or does it involve something new? In this section,\nwe will show that solving the task in a restricted domain is pretty straightforward. But\nwe will also see that to address the problem in a more general way, we have to open up\na whole new box of ideas and techniques, involving the representation of meaning.",
                  "level": -1,
                  "page": 383,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    707,
                    585,
                    797
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_383_order_11",
                  "label": "foot",
                  "text": "361",
                  "level": -1,
                  "page": 383,
                  "reading_order": 11,
                  "bbox": [
                    566,
                    824,
                    584,
                    842
                  ],
                  "section_number": "361",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_0",
                  "label": "para",
                  "text": "So let’s start off by assuming that we have data about cities and countries in a structured\nform. To be concrete, we will use a database table whose first few rows are shown in\nTable 10-1.",
                  "level": -1,
                  "page": 384,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    119
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_1",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_384_figure_001.png)",
                  "level": -1,
                  "page": 384,
                  "reading_order": 1,
                  "bbox": [
                    118,
                    134,
                    171,
                    197
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_2",
                  "label": "para",
                  "text": "The data illustrated in Table 10 - 1 is drawn from the Chat-80 system\n(Warren & Pereira, 1982) . Population figures are given in thousands,\nbut note that the data used in these examples dates back at least to the\n1980s, and was already somewhat out of date at the point when (Warren\n& Pereira, 1982) was published.",
                  "level": -1,
                  "page": 384,
                  "reading_order": 2,
                  "bbox": [
                    171,
                    151,
                    530,
                    224
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_3",
                  "label": "table",
                  "text": "Table 10-1. city_table: A table of cities, countries, and populations [TABLE: <table><tr><td>City</td><td>Country</td><td>Population</td></tr><tr><td>athens</td><td>greece</td><td>1368</td></tr><tr><td>angkok</td><td>thailand</td><td>1178</td></tr><tr><td>arcelona</td><td>Spain</td><td>1280</td></tr><tr><td>erlin</td><td>east_germany</td><td>3481</td></tr><tr><td>irmingham</td><td>united_kingdom</td><td>1112</td></tr></table>]",
                  "level": -1,
                  "page": 384,
                  "reading_order": 3,
                  "bbox": [
                    100,
                    268,
                    297,
                    385
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "tab",
                      "text": "<table><tr><td>City</td><td>Country</td><td>Population</td></tr><tr><td>athens</td><td>greece</td><td>1368</td></tr><tr><td>angkok</td><td>thailand</td><td>1178</td></tr><tr><td>arcelona</td><td>Spain</td><td>1280</td></tr><tr><td>erlin</td><td>east_germany</td><td>3481</td></tr><tr><td>irmingham</td><td>united_kingdom</td><td>1112</td></tr></table>",
                      "bbox": [
                        100,
                        268,
                        297,
                        385
                      ],
                      "page": 384,
                      "reading_order": 3
                    },
                    {
                      "label": "cap",
                      "text": "Table 10-1. city_table: A table of cities, countries, and populations",
                      "bbox": [
                        99,
                        241,
                        422,
                        259
                      ],
                      "page": 384,
                      "reading_order": 4
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_384_order_5",
                  "label": "para",
                  "text": "The obvious way to retrieve answers from this tabular data involves writing queries in\na database query language such as SQL.",
                  "level": -1,
                  "page": 384,
                  "reading_order": 5,
                  "bbox": [
                    100,
                    403,
                    584,
                    439
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_6",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_384_figure_006.png)",
                  "level": -1,
                  "page": 384,
                  "reading_order": 6,
                  "bbox": [
                    109,
                    448,
                    171,
                    510
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_7",
                  "label": "para",
                  "text": "SQL (Structured Query Language) is a language designed for retrieving\nand managing data in relational databases. If you want to find out more\nabout_SQL, http://www.w3schools.com/sql/ is a convenient online\nreference.",
                  "level": -1,
                  "page": 384,
                  "reading_order": 7,
                  "bbox": [
                    171,
                    463,
                    530,
                    519
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_8",
                  "label": "para",
                  "text": "For example, executing the query (2) will pull out the value 'greece':",
                  "level": -1,
                  "page": 384,
                  "reading_order": 8,
                  "bbox": [
                    98,
                    537,
                    494,
                    558
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_9",
                  "label": "para",
                  "text": "(2) SELECT Country FROM city_table WHERE City = 'athens'",
                  "level": -1,
                  "page": 384,
                  "reading_order": 9,
                  "bbox": [
                    118,
                    573,
                    467,
                    591
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_10",
                  "label": "para",
                  "text": "This specifies a result set consisting of all values for the column Country in data rows\nwhere the value of the City column is 'athens'.",
                  "level": -1,
                  "page": 384,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    600,
                    585,
                    636
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_11",
                  "label": "para",
                  "text": "How can we get the same effect using English as our input to the query system? The\nfeature-based grammar formalism described in Chapter 9 makes it easy to translate\nfrom English to SQL. The grammar sql0.fcfg illustrates how to assemble a meaning\nrepresentation for a sentence in tandem with parsing the sentence. Each phrase struc-\nture rule is supplemented with a recipe for constructing a value for the feature SEM . You\ncan see that these recipes are extremely simple; in each case, we use the string concat-\nenation operation + to splice the values for the child constituents to make a value for\nthe parent constituent.",
                  "level": -1,
                  "page": 384,
                  "reading_order": 11,
                  "bbox": [
                    97,
                    636,
                    585,
                    771
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_384_order_12",
                  "label": "foot",
                  "text": "362 | Chapter 10: Analyzing the Meaning of Sentences",
                  "level": -1,
                  "page": 384,
                  "reading_order": 12,
                  "bbox": [
                    97,
                    824,
                    328,
                    842
                  ],
                  "section_number": "362",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_385_order_1",
                  "label": "para",
                  "text": "This allows us to parse a query into SQL:",
                  "level": -1,
                  "page": 385,
                  "reading_order": 1,
                  "bbox": [
                    100,
                    277,
                    333,
                    295
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_385_order_3",
                  "label": "para",
                  "text": "Your Turn: Run the parser with maximum tracing on, i.e., cp =\nload_parser('grammars/book_grammars/sql0.fcfg', trace=3) , and ex-\namine how the values of SEM are built up as complete edges are added\nto the chart.",
                  "level": -1,
                  "page": 385,
                  "reading_order": 3,
                  "bbox": [
                    171,
                    430,
                    530,
                    486
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_385_order_4",
                  "label": "para",
                  "text": "Finally, we execute the query over the database city.db and retrieve some results",
                  "level": -1,
                  "page": 385,
                  "reading_order": 4,
                  "bbox": [
                    98,
                    510,
                    559,
                    528
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_385_order_6",
                  "label": "para",
                  "text": "Since each row r is a one-element tuple, we print out the member of the tuple rather\nthan the tuple itself 0.",
                  "level": -1,
                  "page": 385,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    591,
                    585,
                    627
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_385_order_7",
                  "label": "para",
                  "text": "To summarize, we have defined a task where the computer returns useful data in re-\nsponse to a natural language query, and we implemented this by translating a small\nsubset of English into SQL. We can say that our NLTK code already “ understands ”\nSQL, given that Python is able to execute SQL queries against a database, and by ex-\ntension it also “ understands ” queries such as What cities are located in China . This\nparallels being able to translate from Dutch into English as an example of natural lan-\nguage understanding. Suppose that you are a native speaker of English, and have started\nto learn Dutch. Your teacher asks if you understand what (3) means:",
                  "level": -1,
                  "page": 385,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    636,
                    585,
                    770
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_385_order_8",
                  "label": "list",
                  "text": "3) Margrietje houdt van Brunoke.",
                  "level": -1,
                  "page": 385,
                  "reading_order": 8,
                  "bbox": [
                    126,
                    779,
                    324,
                    797
                  ],
                  "section_number": "3",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_385_order_9",
                  "label": "foot",
                  "text": "10.1 Natural Language Understanding | 363",
                  "level": -1,
                  "page": 385,
                  "reading_order": 9,
                  "bbox": [
                    395,
                    824,
                    584,
                    842
                  ],
                  "section_number": "10.1",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_0",
                  "label": "para",
                  "text": "If you know the meanings of the individual words in (3), and know how these meanings\nare combined to make up the meaning of the whole sentence, you might say that (3)\nmeans the same as Margrietje loves Brunoke.",
                  "level": -1,
                  "page": 386,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_1",
                  "label": "para",
                  "text": "An observer—let’\ns call her Olga—might well take this as evidence that you do grasp\nthe meaning of (3). But this would depend on Olga herself understanding English. If\nshe doesn’t, then your translation from Dutch to English is not going to convince her\nof your ability to understand Dutch. We will return to this issue shortly.",
                  "level": -1,
                  "page": 386,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    132,
                    585,
                    197
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_2",
                  "label": "para",
                  "text": "The grammar sql0.fcfg , together with the NLTK Earley parser, is instrumental in car-\nrying out the translation from English to SQL. How adequate is this grammar? You saw\nthat the SQL translation for the whole sentence was built up from the translations of\nthe components. However, there does not seem to be a lot of justification for these\ncomponent meaning representations. For example, if we look at the analysis of the\nnoun phrase Which cities , the determiner and noun correspond respectively to the SQL\nfragments SELECT and City FROM city_table . But neither of these has a well-defined\nmeaning in isolation from the other.",
                  "level": -1,
                  "page": 386,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    205,
                    585,
                    335
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_3",
                  "label": "para",
                  "text": "There is another criticism we can level at the grammar: we have “hard-wired” an em­\nbarrassing amount of detail about the database into it. We need to know the name of\nthe relevant table (e.g., city_table) and the names of the fields. But our database could\nhave contained exactly the same rows of data yet used a different table name and dif-\nferent field names, in which case the SQL queries would not be executable. Equally,\nwe could have stored our data in a different format, such as XML, in which case re­\ntrieving the same results would require us to translate our English queries into an XML\nquery language rather than SQL. These considerations suggest that we should be trans­\nlating English into something that is more abstract and generic than SQL.",
                  "level": -1,
                  "page": 386,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    340,
                    586,
                    492
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_4",
                  "label": "para",
                  "text": "In order to sharpen the point, let’\ns consider another English query and its translation:",
                  "level": -1,
                  "page": 386,
                  "reading_order": 4,
                  "bbox": [
                    98,
                    500,
                    584,
                    514
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_5",
                  "label": "para",
                  "text": "(4) a. What cities are in China and have populations above 1,000,000",
                  "level": -1,
                  "page": 386,
                  "reading_order": 5,
                  "bbox": [
                    118,
                    528,
                    530,
                    546
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_6",
                  "label": "para",
                  "text": "b. SELECT City FROM city_table WHERE Country = 'china' AND Population >\n1000",
                  "level": -1,
                  "page": 386,
                  "reading_order": 6,
                  "bbox": [
                    150,
                    546,
                    584,
                    577
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_7",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_386_figure_007.png)",
                  "level": -1,
                  "page": 386,
                  "reading_order": 7,
                  "bbox": [
                    109,
                    591,
                    171,
                    654
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_8",
                  "label": "para",
                  "text": "Your Turn: Extend the grammar sql0.fcfg so that it will translate (4a)\ninto (4b) , and check the values returned by the query. Remember that\nfigures in the Chat-80 database are given in thousands, hence 1000 in\n(4b) represents one million inhabitants.",
                  "level": -1,
                  "page": 386,
                  "reading_order": 8,
                  "bbox": [
                    171,
                    608,
                    530,
                    665
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_9",
                  "label": "para",
                  "text": "You will probably find it easiest to first extend the grammar to handle\nqueries like What cities have populations above 1,000,000 before tack-\nling conjunction. After you have had a go at this task, you can compare\nyour solution to grammars/book_grammars/sql1.fcfg in the NLTK data\ndistribution.",
                  "level": -1,
                  "page": 386,
                  "reading_order": 9,
                  "bbox": [
                    171,
                    672,
                    530,
                    743
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_386_order_10",
                  "label": "foot",
                  "text": "364 | Chapter 10: Analyzing the Meaning of Sentences",
                  "level": -1,
                  "page": 386,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    328,
                    842
                  ],
                  "section_number": "364",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_387_order_0",
                  "label": "para",
                  "text": "Observe that the and conjunction in (4a) is translated into an AND in the SQL counter-\npart, (4b) . The latter tells us to select results from rows where two conditions are true\ntogether: the value of the Country column is 'china' and the value of the Population\ncolumn is greater than 1000 . This interpretation for and involves a new idea: it talks\nabout what is true in some particular situation , and tells us that Cond1 AND Cond2 is true\nin situation s if and only if condition Cond1 is true in s and condition Cond2 is true in s .\nAlthough this doesn’t account for the full range of meanings of and in English, it has\nthe nice property that it is independent of any query language. In fact, we have given\nit the standard interpretation from classical logic. In the following sections, we will\nexplore an approach in which sentences of natural language are translated into logic\ninstead of an executable query language such as SQL. One advantage of logical for-\nmalisms is that they are more abstract and therefore more generic. If we wanted to,\nonce we had our translation into logic, we could then translate it into various other\nspecial-purpose languages. In fact, most serious attempts to query databases via natural\nlanguage have used this methodology.",
                  "level": -1,
                  "page": 387,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    322
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_383_order_1",
          "label": "para",
          "text": "We have seen how useful it is to harness the power of a computer to process text on a\nlarge scale. However, now that we have the machinery of parsers and feature-based\ngrammars, can we do anything similarly useful by analyzing the meaning of sentences?\nThe goal of this chapter is to answer the following questions:",
          "level": -1,
          "page": 383,
          "reading_order": 1,
          "bbox": [
            97,
            304,
            585,
            369
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_383_order_2",
          "label": "para",
          "text": "1. How can we represent natural language meaning so that a computer can process\nthese representations?",
          "level": -1,
          "page": 383,
          "reading_order": 2,
          "bbox": [
            100,
            376,
            585,
            412
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_383_order_3",
          "label": "para",
          "text": "2. How can we associate meaning representations with an unlimited set of sentences?",
          "level": -1,
          "page": 383,
          "reading_order": 3,
          "bbox": [
            100,
            412,
            585,
            431
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_383_order_4",
          "label": "para",
          "text": "3. How can we use programs that connect the meaning representations of sentences\nto stores of knowledge?",
          "level": -1,
          "page": 383,
          "reading_order": 4,
          "bbox": [
            100,
            438,
            585,
            468
          ],
          "section_number": "3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_383_order_5",
          "label": "para",
          "text": "Along the way we will learn some formal techniques in the field of logical semantics,\nand see how these can be used for interrogating databases that store facts about the\nworld.",
          "level": -1,
          "page": 383,
          "reading_order": 5,
          "bbox": [
            97,
            474,
            585,
            523
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_387_order_1",
      "label": "sec",
      "text": "Natural Language, Semantics, and Logic",
      "level": 1,
      "page": 387,
      "reading_order": 1,
      "bbox": [
        98,
        331,
        364,
        358
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_390_order_1",
          "label": "sub_sec",
          "text": "10.2 Propositional Logic",
          "level": 2,
          "page": 390,
          "reading_order": 1,
          "bbox": [
            98,
            143,
            297,
            172
          ],
          "section_number": "10.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_390_order_2",
              "label": "para",
              "text": "A logical language is designed to make reasoning formally explicit. As a result, it can\ncapture aspects of natural language which determine whether a set of sentences is con-\nsistent. As part of this approach, we need to develop logical representations of a sen-\ntence $\\varphi$ that formally capture the truth-conditions of $\\varphi$ . We'll start off with a simple\nexample:",
              "level": -1,
              "page": 390,
              "reading_order": 2,
              "bbox": [
                97,
                179,
                585,
                261
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_390_order_3",
              "label": "para",
              "text": "(8) [Klaus chased Evi] and [Evi ran away]",
              "level": -1,
              "page": 390,
              "reading_order": 3,
              "bbox": [
                118,
                268,
                359,
                289
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_390_order_4",
              "label": "para",
              "text": "Let's replace the two sub-sentences in (8) by $\\varphi$ and $\\psi$ respectively, and put & for the\nlogical operator corresponding to the English word and : $\\varphi$ & $\\psi$ . This structure is the\nlogical form of (8) .",
              "level": -1,
              "page": 390,
              "reading_order": 4,
              "bbox": [
                97,
                304,
                585,
                351
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_390_order_5",
              "label": "para",
              "text": "Propositional logic allows us to represent just those parts of linguistic structure that\ncorrespond to certain sentential connectives. We have just looked at and . Other such\nconnectives are not , or , and if..., then.... In the formalization of propositional logic, the\ncounterparts of such connectives are sometimes called Boolean operators . The basic\nexpressions of propositional logic are propositional symbols , often written as P , Q ,\nR , etc. There are varying conventions for representing Boolean operators. Since we will\nbe focusing on ways of exploring logic within NLTK, we will stick to the following\nASCII versions of the operators:",
              "level": -1,
              "page": 390,
              "reading_order": 5,
              "bbox": [
                97,
                358,
                585,
                492
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_390_order_6",
              "label": "tab",
              "text": "<table><tr><td>>>> nltk.boolean_ops() negation</td><td>-&gt;/underline</td></tr><tr><td>conjunction</td><td>&gt;/underline</td></tr><tr><td>disjunction</td><td>|</td></tr><tr><td>implication</td><td>~&gt;/underline</td></tr></table>",
              "level": -1,
              "page": 390,
              "reading_order": 6,
              "bbox": [
                122,
                492,
                245,
                576
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_390_order_7",
              "label": "para",
              "text": "From the propositional symbols and the Boolean operators we can build an infinite set\nof well-formed formulas (or just formulas, for short) of propositional logic. First,\nevery propositional letter is a formula. Then if $\\varphi$ is a formula, so is -φ. And if $\\varphi$ and\nψ are formulas, then so are (φ & ψ), (φ | ψ), (φ -> ψ), and(φ <-> ψ).",
              "level": -1,
              "page": 390,
              "reading_order": 7,
              "bbox": [
                97,
                582,
                585,
                654
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_390_order_8",
              "label": "para",
              "text": "Table 10-2 specifies the truth-conditions for formulas containing these operators. As\nbefore we use $\\varphi$ and $\\psi$ as variables over sentences, and abbreviate if and only if as iff .",
              "level": -1,
              "page": 390,
              "reading_order": 8,
              "bbox": [
                100,
                654,
                585,
                690
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_390_order_9",
              "label": "table",
              "text": "Table 10-2. Truth conditions for the Boolean operators in propositional logit [TABLE: <table><tr><td rowspan=\"2\">Boolean operator negation ( it is not the case that... )</td><td colspan=\"3\">Truth conditions</td></tr><tr><td>-φ is true in s</td><td>iff</td><td>φ is false in s</td></tr><tr><td>conjunction ( and )</td><td>(φ&amp;ψ) is true in s</td><td>iff</td><td>φ is true in s and ψ is true in s</td></tr></table>]",
              "level": -1,
              "page": 390,
              "reading_order": 9,
              "bbox": [
                100,
                725,
                539,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "tab",
                  "text": "<table><tr><td rowspan=\"2\">Boolean operator negation ( it is not the case that... )</td><td colspan=\"3\">Truth conditions</td></tr><tr><td>-φ is true in s</td><td>iff</td><td>φ is false in s</td></tr><tr><td>conjunction ( and )</td><td>(φ&amp;ψ) is true in s</td><td>iff</td><td>φ is true in s and ψ is true in s</td></tr></table>",
                  "bbox": [
                    100,
                    725,
                    539,
                    788
                  ],
                  "page": 390,
                  "reading_order": 9
                },
                {
                  "label": "cap",
                  "text": "Table 10-2. Truth conditions for the Boolean operators in propositional logit",
                  "bbox": [
                    99,
                    698,
                    468,
                    717
                  ],
                  "page": 390,
                  "reading_order": 10
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_390_order_11",
              "label": "foot",
              "text": "368 | Chapter 10: Analyzing the Meaning of Sentences",
              "level": -1,
              "page": 390,
              "reading_order": 11,
              "bbox": [
                97,
                824,
                328,
                842
              ],
              "section_number": "368",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_0",
              "label": "tab",
              "text": "<table><tr><td>Boolean operator</td><td colspan=\"3\">Truth conditions</td></tr><tr><td>disjunction ( ar )</td><td>(φ|ψ) is true in s</td><td>iff</td><td>φis true in s or φis true in s</td></tr><tr><td>mplication ( if..., then... )</td><td>(φ-&gt;ψ) is true in s</td><td>iff</td><td>φis false in s or φis true in s</td></tr><tr><td>equivalence ( if and only if )</td><td>(φ&lt;-&gt;ψ) is true in s</td><td>iff</td><td>φand φare both true in s or both false in s</td></tr></table>",
              "level": -1,
              "page": 391,
              "reading_order": 0,
              "bbox": [
                100,
                79,
                530,
                152
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_1",
              "label": "para",
              "text": "These rules are generally straightforward, though the truth conditions for implication\ndepart in many cases from our usual intuitions about the conditional in English. A\nformula of the form (P -> Q) is false only when P is true and Q is false. If P is false (say,\nP corresponds to The moon is made of green cheese) and Q is true (say, Q corresponds to\nTwo plus two equals four), then P -> Q will come out true.",
              "level": -1,
              "page": 391,
              "reading_order": 1,
              "bbox": [
                97,
                170,
                584,
                255
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_2",
              "label": "para",
              "text": "NLTK's LogicParser() parses logical expressions into various subclasses of Expression:",
              "level": -1,
              "page": 391,
              "reading_order": 2,
              "bbox": [
                98,
                259,
                584,
                278
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_4",
              "label": "para",
              "text": "From a computational perspective, logics give us an important tool for performing\ninference. Suppose you state that Freedonia is not to the north of Sylvania, and you\ngive as your reasons that Sylvania is to the north of Freedonia. In this case, you have\nproduced an argument . The sentence Sylvania is to the north of Freedonia is the\nassumption of the argument, while Freedonia is not to the north of Sylvania is the\nconclusion . The step of moving from one or more assumptions to a conclusion is called\ninference . Informally, it is common to write arguments in a format where the conclu-\nsion is preceded by therefore .",
              "level": -1,
              "page": 391,
              "reading_order": 4,
              "bbox": [
                97,
                412,
                585,
                542
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_5",
              "label": "para",
              "text": "(9) Sylvania is to the north of Freedonia",
              "level": -1,
              "page": 391,
              "reading_order": 5,
              "bbox": [
                118,
                555,
                350,
                573
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_6",
              "label": "para",
              "text": "Therefore, Freedonia is not to the north of Sylvania.",
              "level": -1,
              "page": 391,
              "reading_order": 6,
              "bbox": [
                144,
                581,
                440,
                592
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_7",
              "label": "para",
              "text": "An argument is valid if there is no possible situation in which its premises are all true\nand its conclusion is not true.",
              "level": -1,
              "page": 391,
              "reading_order": 7,
              "bbox": [
                97,
                609,
                585,
                637
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_8",
              "label": "para",
              "text": "Now, the validity of (9) crucially depends on the meaning of the phrase to the north\nof, in particular, the fact that it is an asymmetric relation:",
              "level": -1,
              "page": 391,
              "reading_order": 8,
              "bbox": [
                97,
                645,
                584,
                680
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_9",
              "label": "para",
              "text": "10) if x is to the north of y then y is not to the north of x",
              "level": -1,
              "page": 391,
              "reading_order": 9,
              "bbox": [
                118,
                689,
                440,
                707
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_10",
              "label": "para",
              "text": "Unfortunately, we can't express such rules in propositional logic: the smallest elements\nwe have to play with are atomic propositions, and we cannot “ look inside ” these to\ntalk about relations between individuals $x$ and $y$ . The best we can do in this case is\ncapture a particular case of the asymmetry. Let's use the propositional symbol SnF to",
              "level": -1,
              "page": 391,
              "reading_order": 10,
              "bbox": [
                97,
                723,
                585,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_391_order_11",
              "label": "foot",
              "text": "10.2 Propositional Logic | 369",
              "level": -1,
              "page": 391,
              "reading_order": 11,
              "bbox": [
                449,
                824,
                585,
                842
              ],
              "section_number": "10.2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_392_order_0",
              "label": "para",
              "text": "stand for Sylvania is to the north of Freedonia and FnS for Freedomia is to the north of\nSylvania . To say that Freedonia is not to the north of Sylvania , we write -FnS . That is,\nwe treat not as equivalent to the phrase it is not the case that ..., and translate this as the\none-place Boolean operator -. Replacing x and y in (10) by Sylvania and Freedonia\nrespectively gives us an implication that can be written as:",
              "level": -1,
              "page": 392,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                155
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_392_order_1",
          "label": "sub_sec",
          "text": "(11) SnF -> -FnS",
          "level": 2,
          "page": 392,
          "reading_order": 1,
          "bbox": [
            109,
            170,
            210,
            182
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_392_order_2",
              "label": "para",
              "text": "How about giving a version of the complete argument? We will replace the first sentence\nof (9) by two formulas of propositional logic: SnF , and also the implication in (11) ,\nwhich expresses (rather poorly) our background knowledge of the meaning of to the\nnorth of . We'll write [A1, ..., An] / C to represent the argument that conclusion C\nfollows from assumptions [A1, ..., An] . This leads to the following as a representation\nof argument (9) :",
              "level": -1,
              "page": 392,
              "reading_order": 2,
              "bbox": [
                97,
                197,
                585,
                296
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_392_order_3",
          "label": "sub_sec",
          "text": "(12)[SnF, SnF -> -FnS]/ -FnS",
          "level": 2,
          "page": 392,
          "reading_order": 3,
          "bbox": [
            109,
            304,
            297,
            324
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_392_order_4",
              "label": "para",
              "text": "This is a valid argument: if SnF and SnF -> -FnS are both true in a situation s, then\n-FnS must also be true in s. By contrast, if FnS were true, this would conflict with our\nunderstanding that two objects cannot both be to the north of each other in any possible\nsituation. Equivalently, the list [SnF, SnF -> -FnS, FnS] is inconsistent—these sen-\ntences cannot all be true together.",
              "level": -1,
              "page": 392,
              "reading_order": 4,
              "bbox": [
                97,
                339,
                585,
                421
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_392_order_5",
              "label": "para",
              "text": "Arguments can be tested for “ syntactic validity ” by using a proof system. We will say\na little bit more about this later on in Section 10.3 . Logical proofs can be carried out\nwith NLTK's inference module, for example, via an interface to the third-party theo-\nrem prover Prover9. The inputs to the inference mechanism first have to be parsed into\nlogical expressions by LogicParser() .",
              "level": -1,
              "page": 392,
              "reading_order": 5,
              "bbox": [
                97,
                429,
                585,
                510
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_392_order_7",
              "label": "para",
              "text": "Here's another way of seeing why the conclusion follows. SnF -> -FnS is semantically\nequivalent to -SnF | -FnS , where $\\mid$ is the two-place operator corresponding to or . In\ngeneral, $\\varphi \\mid \\psi$ is true in a situation s if either $\\varphi$ is true in s or $\\varphi$ is true in s . Now, suppose\nboth SnF and -SnF | -FnS are true in situation s . If SnF is true, then -SnF cannot also be\ntrue; a fundamental assumption of classical logic is that a sentence cannot be both true\nand false in a situation. Consequently, -FnS must be true.",
              "level": -1,
              "page": 392,
              "reading_order": 7,
              "bbox": [
                97,
                618,
                585,
                716
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_392_order_8",
              "label": "para",
              "text": "Recall that we interpret sentences of a logical language relative to a model, which is a\nvery simplified version of the world. A model for propositional logic needs to assign\nthe values True or False to every possible formula. We do this inductively: first, every\npropositional symbol is assigned a value, and then we compute the value of complex",
              "level": -1,
              "page": 392,
              "reading_order": 8,
              "bbox": [
                97,
                724,
                585,
                788
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_392_order_9",
              "label": "foot",
              "text": "370 | Chapter 10: Analyzing the Meaning of Sentences",
              "level": -1,
              "page": 392,
              "reading_order": 9,
              "bbox": [
                97,
                824,
                328,
                842
              ],
              "section_number": "370",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_0",
              "label": "para",
              "text": "formulas by consulting the meanings of the Boolean operators (i.e., Table 10-2) and\napplying them to the values of the formula’s components. A Valuation is a mapping\nfrom basic symbols of the logic to their values. Here’s an example:",
              "level": -1,
              "page": 393,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_1",
              "label": "para",
              "text": ">>> val = nltk.Valuation([('P', True), ('Q', True), ('R', False)])",
              "level": -1,
              "page": 393,
              "reading_order": 1,
              "bbox": [
                118,
                125,
                478,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_2",
              "label": "para",
              "text": "We initialize a Valuation with a list of pairs, each of which consists of a semantic symbol\nand a semantic value. The resulting object is essentially just a dictionary that maps\nlogical symbols (treated as strings) to appropriate values.",
              "level": -1,
              "page": 393,
              "reading_order": 2,
              "bbox": [
                97,
                152,
                585,
                200
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_4",
              "label": "para",
              "text": "As we will see later, our models need to be somewhat more complicated in order to\nhandle the more complex logical forms discussed in the next section; for the time being,\njust ignore the dom and g parameters in the following declarations.",
              "level": -1,
              "page": 393,
              "reading_order": 4,
              "bbox": [
                97,
                241,
                584,
                290
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_6",
              "label": "para",
              "text": "Now let's initialize a model m that uses val:",
              "level": -1,
              "page": 393,
              "reading_order": 6,
              "bbox": [
                98,
                331,
                342,
                349
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_7",
              "label": "para",
              "text": ">> m = nltk.Model(dom, val)",
              "level": -1,
              "page": 393,
              "reading_order": 7,
              "bbox": [
                126,
                356,
                272,
                367
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_8",
              "label": "para",
              "text": "Every model comes with an evaluate() method, which will determine the semantic\nvalue of logical expressions, such as formulas of propositional logic; of course, these\nvalues depend on the initial truth values we assigned to propositional symbols such as P ,\nQ , and R .",
              "level": -1,
              "page": 393,
              "reading_order": 8,
              "bbox": [
                97,
                376,
                585,
                440
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_10",
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_393_figure_010.png)",
              "level": -1,
              "page": 393,
              "reading_order": 10,
              "bbox": [
                118,
                571,
                164,
                627
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_11",
              "label": "para",
              "text": "Your Turn: Experiment with evaluating different formulas of proposi-\ntional logic. Does the model give the values that you expected?",
              "level": -1,
              "page": 393,
              "reading_order": 11,
              "bbox": [
                171,
                581,
                530,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_12",
              "label": "para",
              "text": "Up until now, we have been translating our English sentences into propositional logic.\nBecause we are confined to representing atomic sentences with letters such as P and\nQ , we cannot dig into their internal structure. In effect, we are saying that there is no\nsemantic benefit in dividing atomic sentences into subjects, objects, and predicates.\nHowever, this seems wrong: if we want to formalize arguments such as (9) , we have to\nbe able to “ look inside ” basic sentences. As a result, we will move beyond propositional\nlogic to something more expressive, namely first-order logic. This is what we turn to\nin the next section.",
              "level": -1,
              "page": 393,
              "reading_order": 12,
              "bbox": [
                97,
                654,
                584,
                782
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_393_order_13",
              "label": "foot",
              "text": "10.2 Propositional Logic | 371",
              "level": -1,
              "page": 393,
              "reading_order": 13,
              "bbox": [
                449,
                824,
                584,
                842
              ],
              "section_number": "10.2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_394_order_0",
          "label": "sub_sec",
          "text": "10.3 First-Order Logic",
          "level": 2,
          "page": 394,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            272,
            100
          ],
          "section_number": "10.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_394_order_3",
              "label": "sub_sub_sec",
              "text": "Syntax",
              "level": 3,
              "page": 394,
              "reading_order": 3,
              "bbox": [
                97,
                259,
                144,
                281
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_394_order_4",
                  "label": "para",
                  "text": "First-order logic keeps all the Boolean operators of propositional logic, but it adds some\nimportant new mechanisms. To start with, propositions are analyzed into predicates\nand arguments, which takes us a step closer to the structure of natural languages. The\nstandard construction rules for first-order logic recognize terms such as individual\nvariables and individual constants, and predicates that take differing numbers of ar-\nguments . For example, Angus walks might be formalized as walk ( angus ) and Angus\nsees Bertie as see ( angus , bertie ). We will call walk a unary predicate , and see a binary\npredicate . The symbols used as predicates do not have intrinsic meaning, although it\nis hard to remember this. Returning to one of our earlier examples, there is no logical\ndifference between (13a) and (13b) .",
                  "level": -1,
                  "page": 394,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    286,
                    585,
                    450
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_394_order_5",
                  "label": "list_group",
                  "text": "(13) a. love(margrietje, brunoke)\nb. houden_van(margrietje, brunoke",
                  "level": -1,
                  "page": 394,
                  "reading_order": 5,
                  "bbox": [
                    109,
                    465,
                    308,
                    483
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "list",
                      "text": "(13) a. love(margrietje, brunoke)",
                      "bbox": [
                        109,
                        465,
                        308,
                        483
                      ],
                      "page": 394,
                      "reading_order": 5
                    },
                    {
                      "label": "list",
                      "text": "b. houden_van(margrietje, brunoke",
                      "bbox": [
                        144,
                        483,
                        350,
                        502
                      ],
                      "page": 394,
                      "reading_order": 6
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_394_order_7",
                  "label": "para",
                  "text": "By itself, first-order logic has nothing substantive to say about lexical semantics — the\nmeaning of individual words — although some theories of lexical semantics can be en-\ncoded in first-order logic. Whether an atomic predication like see( angus , bertie ) is true\nor false in a situation is not a matter of logic, but depends on the particular valuation\nthat we have chosen for the constants see , angus , and bertie . For this reason, such\nexpressions are called non-logical constants . By contrast, logical constants (such\nas the Boolean operators) always receive the same interpretation in every model for\nfirst-order logic.",
                  "level": -1,
                  "page": 394,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    510,
                    585,
                    646
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_394_order_8",
                  "label": "para",
                  "text": "We should mention here that one binary predicate has special status, namely equality,\nas in formulas such as angus = aj . Equality is regarded as a logical constant, since for\nindividual terms $t_1$ and $t_2$ , the formula $t_1 = t_2$ is true if and only if $t_1$ and $t_2$ refer to one\nand the same entity.",
                  "level": -1,
                  "page": 394,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    654,
                    585,
                    716
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_394_order_9",
                  "label": "para",
                  "text": "It is often helpful to inspect the syntactic structure of expressions of first-order logic,\nand the usual way of doing this is to assign types to expressions. Following the tradition\nof Montague grammar, we will use two basic types: e is the type of entities, while t is\nthe type of formulas, i.e., expressions that have truth values. Given these two basic",
                  "level": -1,
                  "page": 394,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    725,
                    585,
                    797
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_394_order_10",
                  "label": "foot",
                  "text": "372 | Chapter 10: Analyzing the Meaning of Sentences",
                  "level": -1,
                  "page": 394,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    328,
                    842
                  ],
                  "section_number": "372",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_395_order_0",
                  "label": "para",
                  "text": "types, we can form complex types for function expressions. That is, given any types\n$\\sigma $ and $\\tau $ , $\\langle \\sigma ,\\tau \\rangle $ is a complex type corresponding to functions from ' $\\sigma $ things' to 't things'.\nFor example, $\\langle e,t \\rangle $ is the type of expressions from entities to truth values, namely unary\npredicates. The LogicParser can be invoked so that it carries out type checking.",
                  "level": -1,
                  "page": 395,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_395_order_2",
                  "label": "para",
                  "text": "Why do we see <e,?> at the end of this example? Although the type-checker will try to\ninfer as many types as possible, in this case it has not managed to fully specify the type\nof walk , since its result type is unknown. Although we are intending walk to receive type\n<e, t> , as far as the type-checker knows, in this context it could be of some other type,\nsuch as <e, e> or <e, <e, t> . To help the type-checker, we need to specify a signa-\nture , implemented as a dictionary that explicitly associates types with non-logical con-\nstants:",
                  "level": -1,
                  "page": 395,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    286,
                    585,
                    403
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_395_order_4",
                  "label": "para",
                  "text": "A binary predicate has type $\\langle e, \\: \\langle e, \\: t \\rangle \\rangle$ . Although this is the type of something which\ncombines first with an argument of type $e$ to make a unary predicate, we represent\nbinary predicates as combining directly with their two arguments. For example, the\npredicate see in the translation of Angus sees Cyril will combine with its arguments to\ngive the result see(angus, cyril).",
                  "level": -1,
                  "page": 395,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    465,
                    585,
                    549
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_395_order_5",
                  "label": "para",
                  "text": "In first-order logic, arguments of predicates can also be individual variables such as $x$ ,\n$y$ , and $z$ . In NLTK, we adopt the convention that variables of type $e$ are all lowercase.\nIndividual variables are similar to personal pronouns like he , she , and it , in that we need\nto know about the context of use in order to figure out their denotation. One way of\ninterpreting the pronoun in (14) is by pointing to a relevant individual in the local\ncontext.",
                  "level": -1,
                  "page": 395,
                  "reading_order": 5,
                  "bbox": [
                    96,
                    555,
                    584,
                    654
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_395_order_6",
                  "label": "para",
                  "text": "14) He disappeared.",
                  "level": -1,
                  "page": 395,
                  "reading_order": 6,
                  "bbox": [
                    118,
                    670,
                    234,
                    684
                  ],
                  "section_number": "14",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_395_order_7",
                  "label": "para",
                  "text": "Another way is to supply a textual antecedent for the pronoun he , for example, by\nuttering (15a) prior to (14) . Here, we say that he is coreferential with the noun phrase\nCyril . In such a context, (14) is semantically equivalent to (15b) .",
                  "level": -1,
                  "page": 395,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    698,
                    585,
                    746
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_395_order_8",
                  "label": "list_group",
                  "text": "(15) a. Cyril is Angus's dog.\nb. Cyril disappeared.",
                  "level": -1,
                  "page": 395,
                  "reading_order": 8,
                  "bbox": [
                    109,
                    761,
                    288,
                    779
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "list",
                      "text": "(15) a. Cyril is Angus's dog.",
                      "bbox": [
                        109,
                        761,
                        288,
                        779
                      ],
                      "page": 395,
                      "reading_order": 8
                    },
                    {
                      "label": "list",
                      "text": "b. Cyril disappeared.",
                      "bbox": [
                        150,
                        779,
                        270,
                        797
                      ],
                      "page": 395,
                      "reading_order": 9
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_395_order_10",
                  "label": "foot",
                  "text": "10.3 First-Order Logic | 373",
                  "level": -1,
                  "page": 395,
                  "reading_order": 10,
                  "bbox": [
                    464,
                    824,
                    584,
                    842
                  ],
                  "section_number": "10.3",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_0",
                  "label": "para",
                  "text": "Consider by contrast the occurrence of he in (16a) . In this case, it is bound by the\nindefinite NP a dog , and this is a different relationship than coreference. If we replace\nthe pronoun he by a dog , the result (16b) is not semantically equivalent to (16a) .",
                  "level": -1,
                  "page": 396,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_1",
                  "label": "para",
                  "text": "(16) a. Angus had a dog but he disappeared.",
                  "level": -1,
                  "page": 396,
                  "reading_order": 1,
                  "bbox": [
                    109,
                    134,
                    378,
                    152
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_2",
                  "label": "para",
                  "text": "b. Angus had a dog but a dog disappeared.",
                  "level": -1,
                  "page": 396,
                  "reading_order": 2,
                  "bbox": [
                    150,
                    152,
                    395,
                    172
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_3",
                  "label": "para",
                  "text": "Corresponding to (17a), we can construct an open formula (17b) with two occurrences\nof the variable x. (We ignore tense to simplify exposition.)",
                  "level": -1,
                  "page": 396,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    186,
                    585,
                    217
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_4",
                  "label": "para",
                  "text": "(17) a. He is a dog and he disappeared.",
                  "level": -1,
                  "page": 396,
                  "reading_order": 4,
                  "bbox": [
                    109,
                    232,
                    350,
                    250
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_5",
                  "label": "para",
                  "text": "b. $dog(x)\\,{\\normalfont \\textsf{\\&}}\\,$ disappear(x)",
                  "level": -1,
                  "page": 396,
                  "reading_order": 5,
                  "bbox": [
                    150,
                    250,
                    297,
                    268
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_6",
                  "label": "para",
                  "text": "By placing an existential quantifier $\\exists x$ (“for some $x$ ”) in front of (17b) , we can\nbind these variables, as in (18a) , which means (18b) or, more idiomatically, (18c) .",
                  "level": -1,
                  "page": 396,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    277,
                    584,
                    313
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_7",
                  "label": "para",
                  "text": "(18) a. $\\exists x .(\\operatorname{dog}(\\mathrm{x}) \\& \\operatorname{disappear}(x))$",
                  "level": -1,
                  "page": 396,
                  "reading_order": 7,
                  "bbox": [
                    109,
                    322,
                    324,
                    340
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_8",
                  "label": "para",
                  "text": "b. At least one entity is a dog and disappeared",
                  "level": -1,
                  "page": 396,
                  "reading_order": 8,
                  "bbox": [
                    150,
                    340,
                    414,
                    361
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_9",
                  "label": "para",
                  "text": "c. A dog disappeared",
                  "level": -1,
                  "page": 396,
                  "reading_order": 9,
                  "bbox": [
                    151,
                    367,
                    273,
                    385
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_10",
                  "label": "para",
                  "text": "Here is the NLTK counterpart of (18a):",
                  "level": -1,
                  "page": 396,
                  "reading_order": 10,
                  "bbox": [
                    98,
                    394,
                    324,
                    412
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_11",
                  "label": "para",
                  "text": "(19) exists x.(dog(x) & disappear(x))",
                  "level": -1,
                  "page": 396,
                  "reading_order": 11,
                  "bbox": [
                    109,
                    421,
                    342,
                    439
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_12",
                  "label": "para",
                  "text": "In addition to the existential quantifier, first-order logic offers us the universal quan-\ntifier $\\forall x$ (“for all $x$”), illustrated in (20).",
                  "level": -1,
                  "page": 396,
                  "reading_order": 12,
                  "bbox": [
                    97,
                    448,
                    585,
                    483
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_13",
                  "label": "para",
                  "text": "( 2 0 ) \\quad \\mathrm { a . ~ } \\forall x . ( d o g ( x ) \\rightarrow d i s a p p e a r ( x ) )",
                  "level": -1,
                  "page": 396,
                  "reading_order": 13,
                  "bbox": [
                    109,
                    492,
                    324,
                    519
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_14",
                  "label": "para",
                  "text": "b. Everything has the property that if it is a dog, it disappears.",
                  "level": -1,
                  "page": 396,
                  "reading_order": 14,
                  "bbox": [
                    150,
                    519,
                    503,
                    537
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_15",
                  "label": "para",
                  "text": "c. Every dog disappeared.",
                  "level": -1,
                  "page": 396,
                  "reading_order": 15,
                  "bbox": [
                    151,
                    537,
                    299,
                    555
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_16",
                  "label": "para",
                  "text": "Here is the NLTK counterpart of (20a):",
                  "level": -1,
                  "page": 396,
                  "reading_order": 16,
                  "bbox": [
                    98,
                    564,
                    324,
                    583
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_17",
                  "label": "para",
                  "text": "(21) all x.(dog(x) -> disappear(x))",
                  "level": -1,
                  "page": 396,
                  "reading_order": 17,
                  "bbox": [
                    109,
                    591,
                    327,
                    612
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_18",
                  "label": "para",
                  "text": "Although (20a) is the standard first-order logic translation of (20c), the truth conditions\naren't necessarily what you expect. The formula says that if some x is a dog, then x\ndisappears—but it doesn’t say that there are any dogs. So in a situation where there are\nno dogs, (20a) will still come out true. (Remember that (P -> Q) is true when P is false.)\nNow you might argue that every dog disappeared does presuppose the existence of dogs,\nand that the logic formalization is simply wrong. But it is possible to find other examples\nthat lack such a presupposition. For instance, we might explain that the value of the\nPython expression astring.replace('ate', '8') is the result of replacing every occur-\nrence of 'ate' in astring by '8', even though there may in fact be no such occurrences\n(Table 3-2).",
                  "level": -1,
                  "page": 396,
                  "reading_order": 18,
                  "bbox": [
                    97,
                    627,
                    585,
                    789
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_396_order_19",
                  "label": "foot",
                  "text": "374 | Chapter 10: Analyzing the Meaning of Sentences",
                  "level": -1,
                  "page": 396,
                  "reading_order": 19,
                  "bbox": [
                    97,
                    824,
                    333,
                    842
                  ],
                  "section_number": "374",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_397_order_0",
                  "label": "para",
                  "text": "We have seen a number of examples where variables are bound by quantifiers. What\nhappens in formulas such as the following?",
                  "level": -1,
                  "page": 397,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    584,
                    107
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_397_order_1",
                  "label": "para",
                  "text": "(exists x. dog(x)) -> bark(x))",
                  "level": -1,
                  "page": 397,
                  "reading_order": 1,
                  "bbox": [
                    126,
                    114,
                    288,
                    126
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_397_order_2",
                  "label": "para",
                  "text": "The scope of the exists x quantifier is dog(x), so the occurrence of x in bark(x) is\nunbound. Consequently it can become bound by some other quantifier, for example,\nall_x in the next formula:",
                  "level": -1,
                  "page": 397,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    134,
                    585,
                    180
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_397_order_3",
                  "label": "para",
                  "text": "all x.((exists x. dog(x)) -> bark(x))",
                  "level": -1,
                  "page": 397,
                  "reading_order": 3,
                  "bbox": [
                    122,
                    188,
                    324,
                    206
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_397_order_4",
                  "label": "para",
                  "text": "In general, an occurrence of a variable x in a formula φ is free in φ if that occurrence\ndoesn’t fall within the scope of all x or some x in φ. Conversely, if x is free in formula\nφ, then it is bound in all x.φ and exists x.φ. If all variable occurrences in a formula\nare bound, the formula is said to be closed.",
                  "level": -1,
                  "page": 397,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    214,
                    585,
                    277
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_397_order_5",
                  "label": "para",
                  "text": "We mentioned before that the parse() method of NLTK's LogicParser returns objects\nof class Expression. Each instance expr of this class comes with a method free(), which\nreturns the set of variables that are free in expr.",
                  "level": -1,
                  "page": 397,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    286,
                    585,
                    334
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_394_order_1",
              "label": "para",
              "text": "In the remainder of this chapter, we will represent the meaning of natural language\nexpressions by translating them into first-order logic. Not all of natural language se-\nmantics can be expressed in first-order logic. But it is a good choice for computational\nsemantics because it is expressive enough to represent many aspects of semantics, and\non the other hand, there are excellent systems available off the shelf for carrying out\nautomated inference in first-order logic.",
              "level": -1,
              "page": 394,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_394_order_2",
              "label": "para",
              "text": "Our next step will be to describe how formulas of first-order logic are constructed, and\nthen how such formulas can be evaluated in a model.",
              "level": -1,
              "page": 394,
              "reading_order": 2,
              "bbox": [
                97,
                215,
                585,
                250
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_387_order_2",
          "label": "para",
          "text": "We started out trying to capture the meaning of (1a) by translating it into a query in\nanother language, SQL, which the computer could interpret and execute. But this still\nbegged the question whether the translation was correct. Stepping back from database\nquery, we noted that the meaning of and seems to depend on being able to specify when\nstatements are true or not in a particular situation. Instead of translating a sentence S\nfrom one language to another, we try to say what S is about by relating it to a situation\nin the world. Let’s pursue this further. Imagine there is a situation s where there are\ntwo entities, Margrietje and her favorite doll, Brunoke. In addition, there is a relation\nholding between the two entities, which we will call the love relation. If you understand\nthe meaning of (3), then you know that it is true in situation s. In part, you know this\nbecause you know that Margrietje refers to Margrietje, Brunoke refers to Brunoke, and\nhoudt van refers to the love relation.",
          "level": -1,
          "page": 387,
          "reading_order": 2,
          "bbox": [
            97,
            358,
            585,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_387_order_3",
          "label": "para",
          "text": "We have introduced two fundamental notions in semantics. The first is that declarative\nsentences are true or false in certain situations. The second is that definite noun phrases\nand proper nouns refer to things in the world. So (3) is true in a situation where Mar-\ngrietje loves the doll Brunoke, here illustrated in Figure 10-1.",
          "level": -1,
          "page": 387,
          "reading_order": 3,
          "bbox": [
            97,
            564,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_387_order_4",
          "label": "para",
          "text": "Once we have adopted the notion of truth in a situation, we have a powerful tool for\nreasoning. In particular, we can look at sets of sentences, and ask whether they could\nbe true together in some situation. For example, the sentences in (5) can be both true,\nwhereas those in (6) and (7) cannot be. In other words, the sentences in (5) are con-\nsistent , whereas those in (6) and (7) are inconsistent .",
          "level": -1,
          "page": 387,
          "reading_order": 4,
          "bbox": [
            97,
            636,
            585,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_387_order_5",
          "label": "list_group",
          "text": "(5) a. Sylvania is to the north of Freedonia.\nb. Freedonia is a republic",
          "level": -1,
          "page": 387,
          "reading_order": 5,
          "bbox": [
            118,
            734,
            377,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "(5) a. Sylvania is to the north of Freedonia.",
              "bbox": [
                118,
                734,
                377,
                752
              ],
              "page": 387,
              "reading_order": 5
            },
            {
              "label": "list",
              "text": "b. Freedonia is a republic",
              "bbox": [
                144,
                759,
                297,
                773
              ],
              "page": 387,
              "reading_order": 6
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_387_order_7",
          "label": "foot",
          "text": "10.1 Natural Language Understanding | 365",
          "level": -1,
          "page": 387,
          "reading_order": 7,
          "bbox": [
            395,
            824,
            585,
            842
          ],
          "section_number": "10.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_388_order_0",
          "label": "figure",
          "text": "Figure 10-1. Depiction of a situation in which Margrietje loves Brunoke. [IMAGE: ![Figure](figures/NLTK_page_388_figure_000.png)]",
          "level": -1,
          "page": 388,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_388_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                367
              ],
              "page": 388,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 10-1. Depiction of a situation in which Margrietje loves Brunoke.",
              "bbox": [
                97,
                376,
                450,
                394
              ],
              "page": 388,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_388_order_2",
          "label": "para",
          "text": "(6) a. The capital of Freedonia has a population of 9,000.\nb. No city in Freedonia has a population of 9,000.",
          "level": -1,
          "page": 388,
          "reading_order": 2,
          "bbox": [
            118,
            394,
            458,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_388_order_3",
          "label": "para",
          "text": "(7) a. Sylvania is to the north of Freedonia.\nb. Freedonia is to the north of Sylvania.",
          "level": -1,
          "page": 388,
          "reading_order": 3,
          "bbox": [
            118,
            448,
            377,
            485
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_388_order_4",
          "label": "para",
          "text": "We have chosen sentences about fictional countries (featured in the Marx Brothers'\n1933 movie Duck Soup ) to emphasize that your ability to reason about these examples\ndoes not depend on what is true or false in the actual world. If you know the meaning\nof the word no , and also know that the capital of a country is a city in that country,\nthen you should be able to conclude that the two sentences in (6) are inconsistent,\nregardless of where Freedonia is or what the population of its capital is. That is, there's\nno possible situation in which both sentences could be true. Similarly, if you know that\nthe relation expressed by to the north of is asymmetric, then you should be able to\nconclude that the two sentences in (7) are inconsistent.",
          "level": -1,
          "page": 388,
          "reading_order": 4,
          "bbox": [
            97,
            500,
            585,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_388_order_5",
          "label": "para",
          "text": "Broadly speaking, logic-based approaches to natural language semantics focus on those\naspects of natural language that guide our judgments of consistency and inconsistency.\nThe syntax of a logical language is designed to make these features formally explicit.\nAs a result, determining properties like consistency can often be reduced to symbolic\nmanipulation, that is, to a task that can be carried out by a computer. In order to pursue\nthis approach, we first want to develop a technique for representing a possible situation.\nWe do this in terms of something that logicians call a “model.”",
          "level": -1,
          "page": 388,
          "reading_order": 5,
          "bbox": [
            97,
            654,
            585,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_388_order_6",
          "label": "foot",
          "text": "366 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 388,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "366",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_389_order_0",
          "label": "para",
          "text": "A model for a set W of sentences is a formal representation of a situation in which all\nthe sentences in W are true. The usual way of representing models involves set theory.\nThe domain D of discourse (all the entities we currently care about) is a set of individ-\nuals, while relations are treated as sets built up from D. Let's look at a concrete example.\nOur domain D will consist of three children, Stefan, Klaus, and Evi, represented re-\nspectively as s, k, and e. We write this as D = {s, k, e}. The expression boy denotes\nthe set consisting of Stefan and Klaus, the expression girl denotes the set consisting of\nEvi, and the expression is running denotes the set consisting of Stefan and Evi. Fig-\nure 10-2 is a graphical rendering of the model.",
          "level": -1,
          "page": 389,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            586,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_389_order_1",
          "label": "figure",
          "text": "Figure 10-2. Diagram of a model containing a domain D and subsets of D corresponding to the\npredicates boy, girl, and is running. [IMAGE: ![Figure](figures/NLTK_page_389_figure_001.png)]",
          "level": -1,
          "page": 389,
          "reading_order": 1,
          "bbox": [
            100,
            232,
            583,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_389_figure_001.png)",
              "bbox": [
                100,
                232,
                583,
                474
              ],
              "page": 389,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Figure 10-2. Diagram of a model containing a domain D and subsets of D corresponding to the\npredicates boy, girl, and is running.",
              "bbox": [
                97,
                483,
                585,
                512
              ],
              "page": 389,
              "reading_order": 2
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_389_order_3",
          "label": "para",
          "text": "Later in this chapter we will use models to help evaluate the truth or falsity of English\nsentences, and in this way to illustrate some methods for representing meaning. How-\never, before going into more detail, let's put the discussion into a broader perspective,\nand link back to a topic that we briefly raised in Section 1.5 . Can a computer understand\nthe meaning of a sentence? And how could we tell if it did? This is similar to asking\n“ Can a computer think? ” Alan Turing famously proposed to answer this by examining\nthe ability of a computer to hold sensible conversations with a human (Turing, 1950).\nSuppose you are having a chat session with a person and a computer, but you are not\ntold at the outset which is which. If you cannot identify which of your partners is the\ncomputer after chatting with each of them, then the computer has successfully imitated\na human. If a computer succeeds in passing itself off as human in this “ imitation game ”\n(or “ Turing Test ” as it is popularly known), then according to Turing, we should be\nprepared to say that the computer can think and can be said to be intelligent. So Turing\nside-stepped the question of somehow examining the internal states of a computer by\ninstead using its behavior as evidence of intelligence. By the same reasoning, we have\nassumed that in order to say that a computer understands English, it just needs to",
          "level": -1,
          "page": 389,
          "reading_order": 3,
          "bbox": [
            97,
            536,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_389_order_4",
          "label": "foot",
          "text": "10.1 Natural Language Understanding | 367",
          "level": -1,
          "page": 389,
          "reading_order": 4,
          "bbox": [
            395,
            824,
            585,
            842
          ],
          "section_number": "10.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_390_order_0",
          "label": "para",
          "text": "behave as though it did. What is important here is not so much the specifics of Turing’s\nimitation game, but rather the proposal to judge a capacity for natural language un-\nderstanding in terms of observable behavior.",
          "level": -1,
          "page": 390,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_397_order_7",
      "label": "sec",
      "text": "First-Order Theorem Proving",
      "level": 1,
      "page": 397,
      "reading_order": 7,
      "bbox": [
        100,
        526,
        288,
        546
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_397_order_8",
          "label": "para",
          "text": "Recall the constraint on to the north of, which we proposed earlier as (10):",
          "level": -1,
          "page": 397,
          "reading_order": 8,
          "bbox": [
            99,
            553,
            521,
            567
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_397_order_9",
          "label": "para",
          "text": "(22) if x is to the north of y then y is not to the north of x",
          "level": -1,
          "page": 397,
          "reading_order": 9,
          "bbox": [
            109,
            582,
            440,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_397_order_10",
          "label": "para",
          "text": "We observed that propositional logic is not expressive enough to represent generali-\nzations about binary predicates, and as a result we did not properly capture the argu-\nment Sylvania is to the north of Freedonia. Therefore, Freedonia is not to the north of\nSylvania.",
          "level": -1,
          "page": 397,
          "reading_order": 10,
          "bbox": [
            97,
            609,
            584,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_397_order_11",
          "label": "para",
          "text": "You have no doubt realized that first-order logic, by contrast, is ideal for formalizing\nsuch rules:",
          "level": -1,
          "page": 397,
          "reading_order": 11,
          "bbox": [
            97,
            680,
            585,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_397_order_12",
          "label": "para",
          "text": "all x. all y.(north_of(x, y) -> -north_of(y, x))",
          "level": -1,
          "page": 397,
          "reading_order": 12,
          "bbox": [
            122,
            716,
            386,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_397_order_13",
          "label": "para",
          "text": "Even better, we can perform automated inference to show the validity of the argument.",
          "level": -1,
          "page": 397,
          "reading_order": 13,
          "bbox": [
            98,
            743,
            584,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_397_order_14",
          "label": "foot",
          "text": "10.3 First-Order Logic | 375",
          "level": -1,
          "page": 397,
          "reading_order": 14,
          "bbox": [
            464,
            824,
            585,
            842
          ],
          "section_number": "10.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_398_order_0",
          "label": "para",
          "text": "The general case in theorem proving is to determine whether a formula that we want\nto prove (a proof goal) can be derived by a finite sequence of inference steps from a\nlist of assumed formulas. We write this as A ⊢ g, where A is a (possibly empty) list of\nassumptions, and g is a proof goal. We will illustrate this with NLTK's interface to the\ntheorem prover Prover9. First, we parse the required proof goal ⊘ and the two as-\nsumptions ⊘⊘. Then we create a Prover9 instance ⊘, and call its prove() method on\nthe goal, given the list of assumptions ⊘.",
          "level": -1,
          "page": 398,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            586,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_398_order_2",
          "label": "para",
          "text": "Happily, the theorem prover agrees with us that the argument is valid. By contrast, it\nconcludes that it is not possible to infer north_of(f, s) from our assumptions:",
          "level": -1,
          "page": 398,
          "reading_order": 2,
          "bbox": [
            97,
            284,
            585,
            315
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_398_order_4",
      "label": "sec",
      "text": "Summarizing the Language of First-Order Logic",
      "level": 1,
      "page": 398,
      "reading_order": 4,
      "bbox": [
        97,
        376,
        413,
        395
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_398_order_5",
          "label": "para",
          "text": "We’ll take this opportunity to restate our earlier syntactic rules for propositional logic\nand add the formation rules for quantifiers; together, these give us the syntax of first-\norder logic. In addition, we make explicit the types of the expressions involved. We’ll\nadopt the convention that $\\langle e^n,t\\rangle$ is the type of a predicate that combines with $n$ argu-\nments of type $e$ to yield an expression of type $t$ . In this case, we say that $n$ is the arity\nof the predicate.",
          "level": -1,
          "page": 398,
          "reading_order": 5,
          "bbox": [
            97,
            403,
            585,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_398_order_6",
          "label": "list_group",
          "text": "1. If $P$ is a predicate of type $\\langle e^{n}, t\\rangle$, and $\\alpha_{1}, \\ldots \\alpha_{n}$ are terms of type $e$, then\n$P\\left(\\alpha_{1}, \\ldots \\alpha_{n}\\right)$ is of type $t$.\n2. If $\\alpha$ and $\\beta$ are both of type $e$, then $(\\alpha=\\beta)$ and $(\\alpha !=\\beta)$ are of type $t$.",
          "level": -1,
          "page": 398,
          "reading_order": 6,
          "bbox": [
            100,
            510,
            584,
            546
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "1. If $P$ is a predicate of type $\\langle e^{n}, t\\rangle$, and $\\alpha_{1}, \\ldots \\alpha_{n}$ are terms of type $e$, then\n$P\\left(\\alpha_{1}, \\ldots \\alpha_{n}\\right)$ is of type $t$.",
              "bbox": [
                100,
                510,
                584,
                546
              ],
              "page": 398,
              "reading_order": 6
            },
            {
              "label": "list",
              "text": "2. If $\\alpha$ and $\\beta$ are both of type $e$, then $(\\alpha=\\beta)$ and $(\\alpha !=\\beta)$ are of type $t$.",
              "bbox": [
                100,
                546,
                503,
                564
              ],
              "page": 398,
              "reading_order": 7
            },
            {
              "label": "list",
              "text": "3. If $\\varphi$ is of type t, then so is -φ.",
              "bbox": [
                100,
                564,
                288,
                583
              ],
              "page": 398,
              "reading_order": 8
            },
            {
              "label": "list",
              "text": "4. If $\\varphi $ and $\\psi $ are of type t, then so are (φ&ψ), (φ|ψ), (φ-> ψ), and (φ<-> ψ)",
              "bbox": [
                100,
                590,
                557,
                609
              ],
              "page": 398,
              "reading_order": 9
            },
            {
              "label": "list",
              "text": "5. If $\\varphi $ is of type t, and x is a variable of type e, then exists x.φ and all x.φ are of\ntype t.",
              "bbox": [
                100,
                609,
                584,
                645
              ],
              "page": 398,
              "reading_order": 10
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_398_order_11",
          "label": "para",
          "text": "Table 10-3 summarizes the new logical constants of the logic module, and two of the\nmethods of Expressions .",
          "level": -1,
          "page": 398,
          "reading_order": 11,
          "bbox": [
            100,
            652,
            585,
            682
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_398_order_12",
          "label": "foot",
          "text": "376 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 398,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "376",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_0",
          "label": "table",
          "text": "Table 10-3. Summary of new logical relations and operators required for first-order logic [TABLE: <table><tr><td>Example</td><td>Description</td></tr><tr><td>=</td><td>Equality</td></tr><tr><td>!=</td><td>Inequality</td></tr><tr><td>exists</td><td>Existential quantifier</td></tr><tr><td>all</td><td>Universal quantifier</td></tr></table>]",
          "level": -1,
          "page": 399,
          "reading_order": 0,
          "bbox": [
            100,
            89,
            243,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "tab",
              "text": "<table><tr><td>Example</td><td>Description</td></tr><tr><td>=</td><td>Equality</td></tr><tr><td>!=</td><td>Inequality</td></tr><tr><td>exists</td><td>Existential quantifier</td></tr><tr><td>all</td><td>Universal quantifier</td></tr></table>",
              "bbox": [
                100,
                89,
                243,
                197
              ],
              "page": 399,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Table 10-3. Summary of new logical relations and operators required for first-order logic",
              "bbox": [
                99,
                71,
                530,
                89
              ],
              "page": 399,
              "reading_order": 1
            }
          ],
          "is_merged": true
        }
      ]
    },
    {
      "id": "page_399_order_2",
      "label": "sec",
      "text": "Truth in Mode",
      "level": 1,
      "page": 399,
      "reading_order": 2,
      "bbox": [
        100,
        214,
        190,
        232
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_399_order_3",
          "label": "para",
          "text": "We have looked at the syntax of first-order logic, and in Section 10.4 we will examine\nthe task of translating English into first-order logic. Yet as we argued in Section 10.1 ,\nthis gets us further forward only if we can give a meaning to sentences of first-order\nlogic. In other words, we need to give a truth-conditional semantics to first-order logic.\nFrom the point of view of computational semantics, there are obvious limits to how far\none can push this approach. Although we want to talk about sentences being true or\nfalse in situations, we only have the means of representing situations in the computer\nin a symbolic manner. Despite this limitation, it is still possible to gain a clearer picture\nof truth-conditional semantics by encoding models in NLTK.",
          "level": -1,
          "page": 399,
          "reading_order": 3,
          "bbox": [
            97,
            241,
            585,
            388
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_4",
          "label": "para",
          "text": "Given a first-order logic language $L$ , a model $M$ for $L$ is a pair $\\langle D,\\ Val \\rangle$ , where $D$ is an\nnon-empty set called the domain of the model, and Val is a function called the valu-\nation function , which assigns values from $D$ to expressions of $L$ as follows:",
          "level": -1,
          "page": 399,
          "reading_order": 4,
          "bbox": [
            97,
            394,
            584,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_5",
          "label": "para",
          "text": "1. For every individual constant c in L, Val(c) is an element of D.",
          "level": -1,
          "page": 399,
          "reading_order": 5,
          "bbox": [
            100,
            455,
            476,
            469
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_6",
          "label": "para",
          "text": "2. For every predicate symbol P of arity n ≥ 0, Val(P) is a function from Dn to\n{True, False} . (If the arity of P is 0, then Val(P) is simply a truth value, and P is\nregarded as a propositional symbol.)",
          "level": -1,
          "page": 399,
          "reading_order": 6,
          "bbox": [
            100,
            474,
            585,
            523
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_7",
          "label": "para",
          "text": "According to 2, if P is of arity 2, then Val(P) will be a function f from pairs of elements\nof D to { True, False}. In the models we shall build in NLTK, we’ll adopt a more con-\nvenient alternative, in which Val(P) is a set S of pairs, defined as follows:",
          "level": -1,
          "page": 399,
          "reading_order": 7,
          "bbox": [
            97,
            528,
            585,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_8",
          "label": "para",
          "text": "(23) S = { s | f ( s ) = True }",
          "level": -1,
          "page": 399,
          "reading_order": 8,
          "bbox": [
            109,
            596,
            252,
            610
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_9",
          "label": "para",
          "text": "Such an $f$ is called the characteristic function of $S$ (as discussed in the further\nreadings).",
          "level": -1,
          "page": 399,
          "reading_order": 9,
          "bbox": [
            97,
            625,
            585,
            655
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_10",
          "label": "para",
          "text": "Relations are represented semantically in NLTK in the standard set-theoretic way: as\nsets of tuples. For example, let's suppose we have a domain of discourse consisting of\nthe individuals Bertie, Olive, and Cyril, where Bertie is a boy, Olive is a girl, and Cyril\nis a dog. For mnemonic reasons, we use b , o , and c as the corresponding labels in the\nmodel. We can declare the domain as follows:",
          "level": -1,
          "page": 399,
          "reading_order": 10,
          "bbox": [
            97,
            663,
            586,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_11",
          "label": "para",
          "text": ">> dom = set(['b', 'o', 'c']",
          "level": -1,
          "page": 399,
          "reading_order": 11,
          "bbox": [
            126,
            752,
            279,
            764
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_399_order_12",
          "label": "foot",
          "text": "10.3 First-Order Logic | 377",
          "level": -1,
          "page": 399,
          "reading_order": 12,
          "bbox": [
            464,
            824,
            585,
            842
          ],
          "section_number": "10.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_400_order_0",
          "label": "para",
          "text": "We will use the utility function parse_valuation() to convert a sequence of strings of\nthe form symbol => value into a Valuation object.",
          "level": -1,
          "page": 400,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_400_order_2",
          "label": "para",
          "text": "So according to this valuation, the value of see is a set of tuples such that Bertie sees\nOlive, Cyril sees Bertie, and Olive sees Cyril.",
          "level": -1,
          "page": 400,
          "reading_order": 2,
          "bbox": [
            97,
            383,
            585,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_400_order_3",
          "label": "para",
          "text": "Your Turn: Draw a picture of the domain dom and the sets correspond-\ning to each of the unary predicates, by analogy with the diagram shown\nin Figure 10-2.",
          "level": -1,
          "page": 400,
          "reading_order": 3,
          "bbox": [
            171,
            439,
            530,
            484
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_400_order_4",
          "label": "para",
          "text": "You may have noticed that our unary predicates (i.e, boy, girl, dog) also come out as\nsets of singleton tuples, rather than just sets of individuals. This is a convenience which\nallows us to have a uniform treatment of relations of any arity. A predication of the\nform P(τ1, ... τn), where P is of arity n, comes out true just in case the tuple of values\ncorresponding to (τ1, ... τn) belongs to the set of tuples in the value of P.",
          "level": -1,
          "page": 400,
          "reading_order": 4,
          "bbox": [
            97,
            510,
            585,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_400_order_6",
      "label": "sec",
      "text": "Individual Variables and Assignments",
      "level": 1,
      "page": 400,
      "reading_order": 6,
      "bbox": [
        100,
        670,
        350,
        689
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_400_order_7",
          "label": "para",
          "text": "In our models, the counterpart of a context of use is a variable assignment . This is a\nmapping from individual variables to entities in the domain. Assignments are created\nusing the Assignment constructor, which also takes the model's domain of discourse as\na parameter. We are not required to actually enter any bindings, but if we do, they are\nin a ( variable , value ) format similar to what we saw earlier for valuations.",
          "level": -1,
          "page": 400,
          "reading_order": 7,
          "bbox": [
            97,
            698,
            585,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_400_order_8",
          "label": "foot",
          "text": "378 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 400,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "378",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_1",
          "label": "para",
          "text": "In addition, there is a print() format for assignments which uses a notation closer to\nthat often found in logic textbooks:",
          "level": -1,
          "page": 401,
          "reading_order": 1,
          "bbox": [
            97,
            116,
            584,
            153
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_3",
          "label": "para",
          "text": "Let’s now look at how we can evaluate an atomic formula of first-order logic. First, we\ncreate a model, and then we call the evaluate() method to compute the truth value:",
          "level": -1,
          "page": 401,
          "reading_order": 3,
          "bbox": [
            97,
            196,
            585,
            227
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_5",
          "label": "para",
          "text": "What’s happening here? We are evaluating a formula which is similar to our earlier\nexample, see(olive, cyril). However, when the interpretation function encounters\nthe variable y, rather than checking for a value in val, it asks the variable assignment\ng to come up with a value:",
          "level": -1,
          "page": 401,
          "reading_order": 5,
          "bbox": [
            97,
            277,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_7",
          "label": "para",
          "text": "Since we already know that individuals o and c stand in the see relation, the value\nTrue is what we expected. In this case, we can say that assignment g satisfies the for-\nmula see(olive, y). By contrast, the following formula evaluates to False relative to\ng (check that you see why this is).",
          "level": -1,
          "page": 401,
          "reading_order": 7,
          "bbox": [
            97,
            385,
            585,
            456
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_9",
          "label": "para",
          "text": "In our approach (though not in standard first-order logic), variable assignments are\npartial. For example, g says nothing about any variables apart from x and y. The method\npurge() clears all bindings from an assignment.",
          "level": -1,
          "page": 401,
          "reading_order": 9,
          "bbox": [
            97,
            492,
            585,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_11",
          "label": "para",
          "text": "If we now try to evaluate a formula such as see(olive, y) relative to g, it is like trying\nto interpret a sentence containing a him when we don’t know what him refers to. In\nthis case, the evaluation function fails to deliver a truth value.",
          "level": -1,
          "page": 401,
          "reading_order": 11,
          "bbox": [
            97,
            600,
            585,
            646
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_13",
          "label": "para",
          "text": "Since our models already contain rules for interpreting Boolean operators, arbitrarily\ncomplex formulas can be composed and evaluated.",
          "level": -1,
          "page": 401,
          "reading_order": 13,
          "bbox": [
            97,
            689,
            585,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_15",
          "label": "para",
          "text": "The general process of determining truth or falsity of a formula in a model is called\nmodel checking.",
          "level": -1,
          "page": 401,
          "reading_order": 15,
          "bbox": [
            97,
            761,
            584,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_401_order_16",
          "label": "foot",
          "text": "10.3 First-Order Logic | 379",
          "level": -1,
          "page": 401,
          "reading_order": 16,
          "bbox": [
            464,
            824,
            585,
            842
          ],
          "section_number": "10.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_402_order_0",
      "label": "sec",
      "text": "Quantification",
      "level": 1,
      "page": 402,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        193,
        93
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_402_order_1",
          "label": "para",
          "text": "One of the crucial insights of modern logic is that the notion of variable satisfaction\ncan be used to provide an interpretation for quantified formulas. Let’s use (24) as an\nexample.",
          "level": -1,
          "page": 402,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            584,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_402_order_2",
          "label": "para",
          "text": "(24) exists x.(girl(x) & walk(x))",
          "level": -1,
          "page": 402,
          "reading_order": 2,
          "bbox": [
            109,
            161,
            315,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_402_order_3",
          "label": "para",
          "text": "When is it true? Let's think about all the individuals in our domain, i.e., in dom. We\nwant to check whether any of these individuals has the property of being a girl and\nwalking. In other words, we want to know if there is some $u$ in dom such that g[u/x]\nsatisfies the open formula (25).",
          "level": -1,
          "page": 402,
          "reading_order": 3,
          "bbox": [
            97,
            188,
            585,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_402_order_4",
          "label": "para",
          "text": "(25) girl(x) & walk(x)",
          "level": -1,
          "page": 402,
          "reading_order": 4,
          "bbox": [
            109,
            268,
            246,
            287
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_402_order_5",
          "label": "para",
          "text": "Consider the following:",
          "level": -1,
          "page": 402,
          "reading_order": 5,
          "bbox": [
            97,
            295,
            234,
            315
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_402_order_7",
          "label": "para",
          "text": "evaluate() returns True here because there is some u in dom such that (25) is satisfied\nby an assignment which binds x to u. In fact, o is such a u:",
          "level": -1,
          "page": 402,
          "reading_order": 7,
          "bbox": [
            97,
            358,
            584,
            389
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_402_order_9",
          "label": "para",
          "text": "One useful tool offered by NLTK is the satisfiers() method. This returns a set of all\nthe individuals that satisfy an open formula. The method parameters are a parsed for-\nmula, a variable, and an assignment. Here are a few examples:",
          "level": -1,
          "page": 402,
          "reading_order": 9,
          "bbox": [
            97,
            430,
            585,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_402_order_11",
          "label": "para",
          "text": "It's useful to think about why fmla2 and fmla3 receive the values they do. The truth\nconditions for -> mean that fmla2 is equivalent to -girl(x) | walk(x) , which is satisfied\nby something that either isn’t a girl or walks. Since neither b (Bertie) nor c (Cyril) are\ngirls, according to model m , they both satisfy the whole formula. And of course o satisfies\nthe formula because o satisfies both disjuncts. Now, since every member of the domain\nof discourse satisfies fmla2 , the corresponding universally quantified formula is also\ntrue.",
          "level": -1,
          "page": 402,
          "reading_order": 11,
          "bbox": [
            97,
            609,
            585,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_402_order_13",
          "label": "foot",
          "text": "380 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 402,
          "reading_order": 13,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "380",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_0",
          "label": "para",
          "text": "In other words, a universally quantified formula $\\forall x.\\varphi$ is true with respect to g just in\ncase for every u, φ is true with respect to g[u/x].",
          "level": -1,
          "page": 403,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_1",
          "label": "para",
          "text": "Your Turn: Try to figure out, first with pencil and paper, and then using\nm.evaluate() , what the truth values are for all x.(girl(x) &\nwalk(x)) and exists x.(boy(x) -> walk(x)) . Make sure you understand\nwhy they receive these values.",
          "level": -1,
          "page": 403,
          "reading_order": 1,
          "bbox": [
            171,
            134,
            530,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_2",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_403_figure_002.png)",
          "level": -1,
          "page": 403,
          "reading_order": 2,
          "bbox": [
            118,
            124,
            171,
            180
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_403_order_3",
      "label": "sec",
      "text": "Quantifier Scope Ambiguity",
      "level": 1,
      "page": 403,
      "reading_order": 3,
      "bbox": [
        97,
        215,
        281,
        235
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_403_order_4",
          "label": "para",
          "text": "What happens when we want to give a formal representation of a sentence with two\nquantifiers, such as the following?",
          "level": -1,
          "page": 403,
          "reading_order": 4,
          "bbox": [
            97,
            241,
            584,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_5",
          "label": "para",
          "text": "26) Everybody admires someone",
          "level": -1,
          "page": 403,
          "reading_order": 5,
          "bbox": [
            116,
            286,
            306,
            304
          ],
          "section_number": "26",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_6",
          "label": "para",
          "text": "There are (at least) two ways of expressing  (26) in first-order logic:",
          "level": -1,
          "page": 403,
          "reading_order": 6,
          "bbox": [
            100,
            313,
            476,
            332
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_7",
          "label": "para",
          "text": "(27) a. all x.(person(x) -> exists y.(person(y) & admire(x,y))\nb. exists y.(person(y) & all x.(person(x) -> admire(x,y))",
          "level": -1,
          "page": 403,
          "reading_order": 7,
          "bbox": [
            109,
            340,
            503,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_8",
          "label": "para",
          "text": "Can we use both of these? The answer is yes, but they have different meanings. (27b)\nis logically stronger than (27a) : it claims that there is a unique person, say, Bruce, who\nis admired by everyone. (27a) , on the other hand, just requires that for every person\n$u$ , we can find some person $u^\\prime$ whom $u$ admires; but this could be a different person\n$u^\\prime$ in each case. We distinguish between (27a) and (27b) in terms of the scope of the\nquantifiers. In the first, $\\forall$ has wider scope than $\\exists$ , whereas in (27b) , the scope ordering\nis reversed. So now we have two ways of representing the meaning of (26) , and they\nare both quite legitimate. In other words, we are claiming that (26) is ambiguous with\nrespect to quantifier scope, and the formulas in (27) give us a way to make the two\nreadings explicit. However, we are not just interested in associating two distinct rep-\nresentations with (26) ; we also want to show in detail how the two representations lead\nto different conditions for truth in a model.",
          "level": -1,
          "page": 403,
          "reading_order": 8,
          "bbox": [
            97,
            394,
            585,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_9",
          "label": "para",
          "text": "In order to examine the ambiguity more closely, let’s fix our valuation as follows",
          "level": -1,
          "page": 403,
          "reading_order": 9,
          "bbox": [
            98,
            600,
            557,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_11",
          "label": "para",
          "text": "The admire relation can be visualized using the mapping diagram shown in (28).",
          "level": -1,
          "page": 403,
          "reading_order": 11,
          "bbox": [
            100,
            761,
            557,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_403_order_12",
          "label": "foot",
          "text": "10.3 First-Order Logic | 381",
          "level": -1,
          "page": 403,
          "reading_order": 12,
          "bbox": [
            464,
            824,
            584,
            842
          ],
          "section_number": "10.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_0",
          "label": "para",
          "text": "(28",
          "level": -1,
          "page": 404,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            130,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_1",
          "label": "para",
          "text": "b",
          "level": -1,
          "page": 404,
          "reading_order": 1,
          "bbox": [
            178,
            98,
            187,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_2",
          "label": "para",
          "text": "b",
          "level": -1,
          "page": 404,
          "reading_order": 2,
          "bbox": [
            297,
            98,
            315,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_3",
          "label": "para",
          "text": "j",
          "level": -1,
          "page": 404,
          "reading_order": 3,
          "bbox": [
            178,
            152,
            185,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_4",
          "label": "para",
          "text": "j",
          "level": -1,
          "page": 404,
          "reading_order": 4,
          "bbox": [
            297,
            152,
            315,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_5",
          "label": "para",
          "text": "e",
          "level": -1,
          "page": 404,
          "reading_order": 5,
          "bbox": [
            178,
            206,
            189,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_6",
          "label": "para",
          "text": "In (28), an arrow between two individuals x and y indicates that x admires y. So j and\nb both admire b (Bruce is very vain), while e admires m and m admires e. In this model,\nformula (27a) is true but (27b) is false. One way of exploring these results is by using\nthe satisfiers() method of Model objects.",
          "level": -1,
          "page": 404,
          "reading_order": 6,
          "bbox": [
            97,
            331,
            585,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_8",
          "label": "para",
          "text": "This shows that fmla4 holds of every individual in the domain. By contrast, consider\nthe formula fmla5; this has no satisfiers for the variable y.",
          "level": -1,
          "page": 404,
          "reading_order": 8,
          "bbox": [
            97,
            492,
            585,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_10",
          "label": "para",
          "text": "That is, there is no person that is admired by everybody. Taking a different open for-\nmula, fmla6, we can verify that there is a person, namely Bruce, who is admired by both\nJulia and Bruce.",
          "level": -1,
          "page": 404,
          "reading_order": 10,
          "bbox": [
            97,
            582,
            584,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_12",
          "label": "para",
          "text": "Your Turn: Devise a new model based on m2 such that (27a) comes out\nfalse in your model; similarly, devise a new model such that (27b) comes\nout true.",
          "level": -1,
          "page": 404,
          "reading_order": 12,
          "bbox": [
            171,
            698,
            530,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_404_order_13",
          "label": "foot",
          "text": "382 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 404,
          "reading_order": 13,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "382",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_405_order_0",
      "label": "sec",
      "text": "Model Building",
      "level": 1,
      "page": 405,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        198,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_407_order_0",
          "label": "sub_sec",
          "text": "10.4 The Semantics of English Sentences",
          "level": 2,
          "page": 407,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            422,
            100
          ],
          "section_number": "10.4",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_407_order_1",
              "label": "sub_sub_sec",
              "text": "Compositional Semantics in Feature-Based Grammar",
              "level": 3,
              "page": 407,
              "reading_order": 1,
              "bbox": [
                97,
                115,
                449,
                134
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_407_order_2",
                  "label": "para",
                  "text": "At the beginning of the chapter we briefly illustrated a method of building semantic\nrepresentations on the basis of a syntactic parse, using the grammar framework devel-\noped in Chapter 9 . This time, rather than constructing an SQL query, we will build a\nlogical form. One of our guiding ideas for designing such grammars is the Principle of\nCompositionality . (Also known as Frege's Principle; see [Partee, 1995] for the for-\nmulation given.)",
                  "level": -1,
                  "page": 407,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    143,
                    586,
                    241
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_407_order_3",
                  "label": "para",
                  "text": "Principle of Compositionality: the meaning of a whole is a function of the meanings\nof the parts and of the way they are syntactically combined.",
                  "level": -1,
                  "page": 407,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    249,
                    585,
                    280
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_407_order_4",
                  "label": "para",
                  "text": "We will assume that the semantically relevant parts of a complex expression are given\nby a theory of syntactic analysis. Within this chapter, we will take it for granted that\nexpressions are parsed against a context-free grammar. However, this is not entailed\nby the Principle of Compositionality.",
                  "level": -1,
                  "page": 407,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    286,
                    585,
                    353
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_407_order_5",
                  "label": "para",
                  "text": "Our goal now is to integrate the construction of a semantic representation in a manner\nthat can be smoothly with the process of parsing. (29) illustrates a first approximation\nto the kind of analyses we would like to build.",
                  "level": -1,
                  "page": 407,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    358,
                    585,
                    412
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_407_order_6",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_407_figure_006.png)",
                  "level": -1,
                  "page": 407,
                  "reading_order": 6,
                  "bbox": [
                    142,
                    421,
                    386,
                    537
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_407_order_7",
                  "label": "para",
                  "text": "In (29), the SEM value at the root node shows a semantic representation for the whole\nsentence, while the SEM values at lower nodes show semantic representations for con-\nstituents of the sentence. Since the values of SEM have to be treated in a special manner,\nthey are distinguished from other feature values by being enclosed in angle brackets.",
                  "level": -1,
                  "page": 407,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    546,
                    585,
                    618
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_407_order_8",
                  "label": "para",
                  "text": "So far, so good, but how do we write grammar rules that will give us this kind of result?\nOur approach will be similar to that adopted for the grammar sql0.fcfg at the start of\nthis chapter, in that we will assign semantic representations to lexical nodes, and then\ncompose the semantic representations for each phrase from those of its child nodes.\nHowever, in the present case we will use function application rather than string con-\ncatenation as the mode of composition. To be more specific, suppose we have NP and\nVP constituents with appropriate values for their SEM nodes. Then the SEM value of an\nS is handled by a rule like (30). (Observe that in the case where the value of SEM is a\nvariable, we omit the angle brackets.)",
                  "level": -1,
                  "page": 407,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    618,
                    586,
                    771
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_407_order_9",
                  "label": "foot",
                  "text": "10.4 The Semantics of English Sentences | 385",
                  "level": -1,
                  "page": 407,
                  "reading_order": 9,
                  "bbox": [
                    386,
                    824,
                    585,
                    842
                  ],
                  "section_number": "10.4",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_408_order_0",
                  "label": "para",
                  "text": "(30) S[SEM=<?vp(?np)>] -> NP[SEM=?subj] VP[SEM=?vp]",
                  "level": -1,
                  "page": 408,
                  "reading_order": 0,
                  "bbox": [
                    109,
                    71,
                    431,
                    89
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_408_order_1",
                  "label": "para",
                  "text": "(30) tells us that given some SEM value ?subj for the subject NP and some SEM value ?vp\nfor the VP, the SEM value of the S parent is constructed by applying ?vp as a function\nexpression to ?np. From this, we can conclude that ?vp has to denote a function which\nhas the denotation of ?np in its domain. (30) is a nice example of building semantics\nusing the principle of compositionality.",
                  "level": -1,
                  "page": 408,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    98,
                    585,
                    188
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_408_order_2",
                  "label": "para",
                  "text": "To complete the grammar is very straightforward; all we require are the rules shown\nhere:",
                  "level": -1,
                  "page": 408,
                  "reading_order": 2,
                  "bbox": [
                    100,
                    188,
                    584,
                    224
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_408_order_4",
                  "label": "para",
                  "text": "The VP rule says that the parent’s semantics is the same as the head child’s semantics.\nThe two lexical rules provide non-logical constants to serve as the semantic values of\nCyril and barks respectively. There is an additional piece of notation in the entry for\nbarks which we will explain shortly.",
                  "level": -1,
                  "page": 408,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    277,
                    586,
                    344
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_408_order_5",
                  "label": "para",
                  "text": "Before launching into compositional semantic rules in more detail, we need to add a\nnew tool to our kit, namely the λ-calculus. This provides us with an invaluable tool for\ncombining expressions of first-order logic as we assemble a meaning representation for\nan English sentence.",
                  "level": -1,
                  "page": 408,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    349,
                    585,
                    421
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_405_order_1",
          "label": "para",
          "text": "We have been assuming that we already had a model, and wanted to check the truth\nof a sentence in the model. By contrast, model building tries to create a new model,\ngiven some set of sentences. If it succeeds, then we know that the set is consistent, since\nwe have an existence proof of the model.",
          "level": -1,
          "page": 405,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_405_order_2",
          "label": "para",
          "text": "We invoke the Mace4 model builder by creating an instance of Mace() and calling its\nbuild_model() method, in an analogous way to calling the Prover9 theorem prover. One\noption is to treat our candidate set of sentences as assumptions, while leaving the goal\nunspecified. The following interaction shows how both [a, c1] and [a, c2] are con-\nsistent lists, since Mace succeeds in building a model for each of them, whereas [c1,\nc2] is inconsistent.",
          "level": -1,
          "page": 405,
          "reading_order": 2,
          "bbox": [
            97,
            177,
            585,
            272
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_405_order_4",
          "label": "para",
          "text": "We can also use the model builder as an adjunct to the theorem prover. Let's suppose\nwe are trying to prove A - g, i.e., that g is logically derivable from assumptions A = [a1,\na2,..., an]. We can feed this same input to Mace4, and the model builder will try to\nfind a counterexample, that is, to show that g does not follow from A. So, given this\ninput, Mace4 will try to find a model for the assumptions A together with the negation\nof g, namely the list A' = [a1, a2,..., an, -g]. If g fails to follow from S, then Mace4\nmay well return with a counterexample faster than Prover9 concludes that it cannot\nfind the required proof. Conversely, if g is provable from S, Mace4 may take a long time\nunsuccessfully trying to find a countermodel, and will eventually give up.",
          "level": -1,
          "page": 405,
          "reading_order": 4,
          "bbox": [
            97,
            421,
            585,
            568
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_405_order_5",
          "label": "para",
          "text": "Let's consider a concrete scenario. Our assumptions are the list [ There is a woman that\nevery man loves , Adam is a man , Eve is a woman ]. Our conclusion is Adam loves Eve .\nCan Mace4 find a model in which the premises are true but the conclusion is false? In\nthe following code, we use MaceCommand() , which will let us inspect the model that has\nbeen built.",
          "level": -1,
          "page": 405,
          "reading_order": 5,
          "bbox": [
            97,
            573,
            585,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_405_order_7",
          "label": "foot",
          "text": "10.3 First-Order Logic | 383",
          "level": -1,
          "page": 405,
          "reading_order": 7,
          "bbox": [
            464,
            824,
            584,
            842
          ],
          "section_number": "10.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_406_order_0",
          "label": "para",
          "text": "So the answer is yes: Mace4 found a countermodel in which there is some woman other\nthan Eve that Adam loves. But let’s have a closer look at Mace4’s model, converted to\nthe format we use for valuations:",
          "level": -1,
          "page": 406,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_406_order_2",
          "label": "para",
          "text": "The general form of this valuation should be familiar to you: it contains some individual\nconstants and predicates, each with an appropriate kind of value. What might be puz-\nzling is the C1 . This is a “Skolem constant” that the model builder introduces as a\nrepresentative of the existential quantifier. That is, when the model builder encoun-\ntered the exists y part of a4 , it knew that there is some individual b in the domain\nwhich satisfies the open formula in the body of a4 . However, it doesn't know whether\nb is also the denotation of an individual constant anywhere else in its input, so it makes\nup a new name for b on the fly, namely C1 . Now, since our premises said nothing about\nthe individual constants adam and eve , the model builder has decided there is no reason\nto treat them as denoting different entities, and they both get mapped to a . Moreover,\nwe didn't specify that man and woman denote disjoint sets, so the model builder lets their\ndenotations overlap. This illustrates quite dramatically the implicit knowledge that we\nbring to bear in interpreting our scenario, but which the model builder knows nothing\nabout. So let's add a new assumption which makes the sets of men and women disjoint.\nThe model builder still produces a countermodel, but this time it is more in accord with\nour intuitions about the situation:",
          "level": -1,
          "page": 406,
          "reading_order": 2,
          "bbox": [
            97,
            231,
            585,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_406_order_4",
          "label": "para",
          "text": "On reflection, we can see that there is nothing in our premises which says that Eve is\nthe only woman in the domain of discourse, so the countermodel in fact is acceptable.\nIf we wanted to rule it out, we would have to add a further assumption such as exists\ny. all x. (woman(x) -> (x = y)) to ensure that there is only one woman in the model.",
          "level": -1,
          "page": 406,
          "reading_order": 4,
          "bbox": [
            97,
            663,
            585,
            730
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_406_order_5",
          "label": "foot",
          "text": "384 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 406,
          "reading_order": 5,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "384",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_408_order_6",
      "label": "sec",
      "text": "The λ-Calculus",
      "level": 1,
      "page": 408,
      "reading_order": 6,
      "bbox": [
        97,
        430,
        192,
        448
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_408_order_7",
          "label": "para",
          "text": "In Section 1.3, we pointed out that mathematical set notation was a helpful method of\nspecifying properties P of words that we wanted to select from a document. We illus-\ntrated this with (31), which we glossed as “the set of all w such that w is an element of\nV (the vocabulary) and w has property P”.",
          "level": -1,
          "page": 408,
          "reading_order": 7,
          "bbox": [
            97,
            456,
            586,
            524
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_408_order_8",
          "label": "para",
          "text": "(31)\\{w|w∈V&P(w)\\}",
          "level": -1,
          "page": 408,
          "reading_order": 8,
          "bbox": [
            109,
            537,
            253,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_408_order_9",
          "label": "para",
          "text": "It turns out to be extremely useful to add something to first-order logic that will achieve\nthe same effect. We do this with the λ­operator (pronounced “lambda”). The λ coun­\nterpart to (31) is (32). (Since we are not trying to do set theory here, we just treat V as\na unary predicate.)",
          "level": -1,
          "page": 408,
          "reading_order": 9,
          "bbox": [
            97,
            564,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_408_order_10",
          "label": "para",
          "text": "(32) $\\lambda w .(V(w) \\& \\mathrm{P}(w)$",
          "level": -1,
          "page": 408,
          "reading_order": 10,
          "bbox": [
            109,
            645,
            243,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_408_order_11",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_408_figure_011.png)",
          "level": -1,
          "page": 408,
          "reading_order": 11,
          "bbox": [
            109,
            672,
            171,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_408_order_12",
          "label": "para",
          "text": "$\\lambda$ expressions were originally designed by Alonzo Church to represent\ncomputable functions and to provide a foundation for mathematics and\nlogic. The theory in which $\\lambda$ expressions are studied is known as the\n$\\lambda$ -calculus. Note that the $\\lambda$ -calculus is not part of first-order logic — both\ncan be used independently of the other.",
          "level": -1,
          "page": 408,
          "reading_order": 12,
          "bbox": [
            171,
            689,
            530,
            763
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_408_order_13",
          "label": "foot",
          "text": "386 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 408,
          "reading_order": 13,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "386",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_0",
          "label": "para",
          "text": "$\\boldsymbol{\\lambda}$ is a binding operator, just as the first-order logic quantifiers are. If we have an open\nformula, such as (33a) , then we can bind the variable $x$ with the $\\boldsymbol{\\lambda}$ operator, as shown\nin (33b) . The corresponding NLTK representation is given in (33c) .",
          "level": -1,
          "page": 409,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_1",
          "label": "para",
          "text": "(33) a. $(w a l k(x) \\& c h e w \\_g u m(x))$",
          "level": -1,
          "page": 409,
          "reading_order": 1,
          "bbox": [
            109,
            134,
            315,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_2",
          "label": "para",
          "text": "b. λx.(walk(x) & chew_gum(x)",
          "level": -1,
          "page": 409,
          "reading_order": 2,
          "bbox": [
            150,
            152,
            325,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_3",
          "label": "para",
          "text": "c. \\x.(walk(x) & chew_gum(x))",
          "level": -1,
          "page": 409,
          "reading_order": 3,
          "bbox": [
            151,
            178,
            327,
            192
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_4",
          "label": "para",
          "text": "Remember that \\ is a special character in Python strings. We must either escape it (with\nanother \\), or else use “raw strings” (Section 3.4) as shown here:",
          "level": -1,
          "page": 409,
          "reading_order": 4,
          "bbox": [
            97,
            206,
            584,
            238
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_6",
          "label": "para",
          "text": "We have a special name for the result of binding the variables in an expression:\n$\\boldsymbol{\\lambda}$ -abstraction . When you first encounter $\\boldsymbol{\\lambda}$ -abstracts, it can be hard to get an intuitive\nsense of their meaning. A couple of English glosses for (33b) are: “be an x such that x\nwalks and x chews gum” or “have the property of walking and chewing gum.” It has\noften been suggested that $\\boldsymbol{\\lambda}$ -abstracts are good representations for verb phrases (or\nsubjectless clauses), particularly when these occur as arguments in their own right. This\nis illustrated in (34a) and its translation, (34b) .",
          "level": -1,
          "page": 409,
          "reading_order": 6,
          "bbox": [
            97,
            358,
            585,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_7",
          "label": "para",
          "text": "(34) a. To walk and chew gum is hard",
          "level": -1,
          "page": 409,
          "reading_order": 7,
          "bbox": [
            109,
            483,
            350,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_8",
          "label": "para",
          "text": "b. hard(\\x.(walk(x) & chew_gum(x)",
          "level": -1,
          "page": 409,
          "reading_order": 8,
          "bbox": [
            151,
            508,
            352,
            522
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_9",
          "label": "para",
          "text": "So the general picture is this: given an open formula $\\varphi $ with free variable x, abstracting\nover x yields a property expression λx.φ—the property of being an x such that φ. Here’s\na more official version of how abstracts are built:",
          "level": -1,
          "page": 409,
          "reading_order": 9,
          "bbox": [
            97,
            537,
            585,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_10",
          "label": "para",
          "text": "(35) If $\\alpha$ is of type $\\tau$, and $x$ is a variable of type $e$, then \\left\\langle x . \\alpha\\right.$ is of type $\\langle e, \\tau\\rangle$.",
          "level": -1,
          "page": 409,
          "reading_order": 10,
          "bbox": [
            109,
            599,
            539,
            613
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_11",
          "label": "para",
          "text": "(34b) illustrated a case where we say something about a property, namely that it is hard.\nBut what we usually do with properties is attribute them to individuals. And in fact, if\n$\\varphi $ is an open formula, then the abstract $\\boldsymbol {\\lambda x}.\\varphi $ can be used as a unary predicate. In (36),\n(33b) is predicated of the term gerald.",
          "level": -1,
          "page": 409,
          "reading_order": 11,
          "bbox": [
            97,
            627,
            584,
            691
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_12",
          "label": "para",
          "text": "(36) \\x.(walk(x) & chew_gum(x)) (gerald",
          "level": -1,
          "page": 409,
          "reading_order": 12,
          "bbox": [
            109,
            706,
            353,
            720
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_13",
          "label": "para",
          "text": "Now (36) says that Gerald has the property of walking and chewing gum, which has\nthe same meaning as (37) .",
          "level": -1,
          "page": 409,
          "reading_order": 13,
          "bbox": [
            97,
            734,
            585,
            765
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_14",
          "label": "para",
          "text": "(37) (walk(gerald) & chew_gum(gerald))",
          "level": -1,
          "page": 409,
          "reading_order": 14,
          "bbox": [
            109,
            779,
            350,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_409_order_15",
          "label": "foot",
          "text": "10.4 The Semantics of English Sentences | 387",
          "level": -1,
          "page": 409,
          "reading_order": 15,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "10.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_0",
          "label": "para",
          "text": "What we have done here is remove the \\x from the beginning of \\x.(walk(x) &\nchew_gum(x)) and replaced all occurrences of x in (walk(x) & chew_gum(x)) by gerald.\nWe’ll use a[β/x] as notation for the operation of replacing all free occurrences of x in\na by the expression β. So",
          "level": -1,
          "page": 410,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_1",
          "label": "para",
          "text": "walk(x) & chew_gum(x))[gerald/x]",
          "level": -1,
          "page": 410,
          "reading_order": 1,
          "bbox": [
            126,
            143,
            299,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_2",
          "label": "para",
          "text": "represents the same expression as (37) . The “ reduction ” of (36) to (37) is an extremely\nuseful operation in simplifying semantic representations, and we shall use it a lot in the\nrest of this chapter. The operation is often called $\\beta$ -reduction . In order for it to be\nsemantically justified, we want it to hold that $\\lambda x$ . $\\alpha(\\beta)$ has the same semantic value as\n$\\alpha[\\beta/x]$ . This is indeed true, subject to a slight complication that we will come to shortly.\nIn order to carry out $\\beta$ -reduction of expressions in NLTK, we can call the simplify()\nmethod",
          "level": -1,
          "page": 410,
          "reading_order": 2,
          "bbox": [
            97,
            169,
            585,
            281
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_4",
          "label": "para",
          "text": "Although we have so far only considered cases where the body of the λ-abstract is an\nopen formula, i.e., of type t, this is not a necessary restriction; the body can be any well-\nformed expression. Here's an example with two λs:",
          "level": -1,
          "page": 410,
          "reading_order": 4,
          "bbox": [
            97,
            365,
            585,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_5",
          "label": "para",
          "text": "(38)\\x.\\y.(dog(x) & own(y, x)",
          "level": -1,
          "page": 410,
          "reading_order": 5,
          "bbox": [
            109,
            427,
            297,
            441
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_6",
          "label": "para",
          "text": "Just as (33b) plays the role of a unary predicate, (38) works like a binary predicate: it\ncan be applied directly to two arguments O . The LogicParser allows nested As such as\n\\x. ∖y. to be written in the abbreviated form \\x y. O .",
          "level": -1,
          "page": 410,
          "reading_order": 6,
          "bbox": [
            96,
            456,
            585,
            503
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_8",
          "label": "para",
          "text": "All our $\\boldsymbol{\\lambda}$ -abstracts so far have involved the familiar first-order variables: x, y, and so on\n—variables of type e. But suppose we want to treat one abstract, say, \\x.walk(x), as\nthe argument of another $\\boldsymbol{\\lambda}$ -abstract? We might try this:",
          "level": -1,
          "page": 410,
          "reading_order": 8,
          "bbox": [
            97,
            572,
            585,
            620
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_9",
          "label": "para",
          "text": "y.y(angus)(\\x.walk(x))",
          "level": -1,
          "page": 410,
          "reading_order": 9,
          "bbox": [
            126,
            627,
            245,
            640
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_10",
          "label": "para",
          "text": "But since the variable y is stipulated to be of type e, \\y.y(angus) only applies to argu-\nments of type e while \\x.walk(x) is of type ⟨e, t ⟩! Instead, we need to allow abstraction\nover variables of higher type. Let's use P and Q as variables of type ⟨e, t⟩, and then we\ncan have an abstract such as \\P.P(angus). Since P is of type ⟨e, t⟩, the whole abstract is\nof type ⟨⟨e, t⟩, t⟩. Then \\P.P(angus)(\\x.walk(x)) is legal, and can be simplified via β-\nreduction to \\x.walk(x)(angus) and then again to walk(angus).",
          "level": -1,
          "page": 410,
          "reading_order": 10,
          "bbox": [
            97,
            645,
            585,
            747
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_410_order_11",
          "label": "foot",
          "text": "388 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 410,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "388",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_0",
          "label": "para",
          "text": "When carrying out $\\beta$ -reduction, some care has to be taken with variables. Consider,\nfor example, the $\\lambda$ -terms (39a) and (39b) , which differ only in the identity of a free\nvariable.",
          "level": -1,
          "page": 411,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            119
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_1",
          "label": "para",
          "text": "(39) a.\\y.see(y, x)",
          "level": -1,
          "page": 411,
          "reading_order": 1,
          "bbox": [
            109,
            134,
            243,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_2",
          "label": "para",
          "text": "b. \\y.see(y, z)",
          "level": -1,
          "page": 411,
          "reading_order": 2,
          "bbox": [
            151,
            158,
            243,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_3",
          "label": "para",
          "text": "Suppose now that we apply the λ-term \\P.exists x.P(x) to each of these terms:",
          "level": -1,
          "page": 411,
          "reading_order": 3,
          "bbox": [
            97,
            186,
            557,
            200
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_4",
          "label": "para",
          "text": "(40) a.\\P.exists x.P(x)(\\y.see(y, x))",
          "level": -1,
          "page": 411,
          "reading_order": 4,
          "bbox": [
            109,
            215,
            352,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_5",
          "label": "para",
          "text": "b. \\P.exists x.P(x)(\\y.see(y, z))",
          "level": -1,
          "page": 411,
          "reading_order": 5,
          "bbox": [
            109,
            232,
            352,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_6",
          "label": "para",
          "text": "We pointed out earlier that the results of the application should be semantically equiv-\nalent. But if we let the free variable x in (39a) fall inside the scope of the existential\nquantifier in (40a), then after reduction, the results will be different:",
          "level": -1,
          "page": 411,
          "reading_order": 6,
          "bbox": [
            97,
            259,
            584,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_7",
          "label": "para",
          "text": "(41) a. exists x.see(x, x)\nb. exists x.see(x, z)",
          "level": -1,
          "page": 411,
          "reading_order": 7,
          "bbox": [
            109,
            322,
            279,
            360
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_8",
          "label": "para",
          "text": "(41a) means there is some x that sees him/herself, whereas (41b) means that there is\nsome x that sees an unspecified individual z . What has gone wrong here? Clearly, we\nwant to forbid the kind of variable “capture” shown in (41a) .",
          "level": -1,
          "page": 411,
          "reading_order": 8,
          "bbox": [
            97,
            376,
            585,
            423
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_9",
          "label": "para",
          "text": "In order to deal with this problem, let's step back a moment. Does it matter what\nparticular name we use for the variable bound by the existential quantifier in the func-\ntion expression of (40a) ? The answer is no. In fact, given any variable-binding expres-\nsion (involving $\\forall$ , $\\exists$ , or $\\boldsymbol{\\uplambda}$ ), the name chosen for the bound variable is completely arbi-\ntrary. For example, exists x.P(x) and exists y.P(y) are equivalent; they are called\nQ-equivalents , or alphabetic variants . The process of relabeling bound variables is\nknown as Q-conversion . When we test for equality of VariableBinderExpression s in\nthe logic module (i.e., using ==), we are in fact testing for α -equivalence:",
          "level": -1,
          "page": 411,
          "reading_order": 9,
          "bbox": [
            97,
            430,
            585,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_11",
          "label": "para",
          "text": "When β-reduction is carried out on an application f(a), we check whether there are\nfree variables in a that also occur as bound variables in any subterms of f. Suppose, as\nin the example just discussed, that x is free in a, and that f contains the subterm exists\nx.P(x). In this case, we produce an alphabetic variant of exists x.P(x), say, exists\nz1.P(z1), and then carry on with the reduction. This relabeling is carried out automat-\nically by the β-reduction code in logic, and the results can be seen in the following\nexample:",
          "level": -1,
          "page": 411,
          "reading_order": 11,
          "bbox": [
            97,
            680,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_411_order_12",
          "label": "foot",
          "text": "10.4 The Semantics of English Sentences | 389",
          "level": -1,
          "page": 411,
          "reading_order": 12,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "10.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_1",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_412_figure_001.png)",
          "level": -1,
          "page": 412,
          "reading_order": 1,
          "bbox": [
            100,
            152,
            171,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_2",
          "label": "para",
          "text": "As you work through examples like these in the following sections, you\nmay find that the logical expressions which are returned have different\nvariable names; for example, you might see z14 in place of z1 in the\npreceding formula. This change in labeling is innocuous — in fact, it is\njust an illustration of alphabetic variants.",
          "level": -1,
          "page": 412,
          "reading_order": 2,
          "bbox": [
            171,
            161,
            530,
            238
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_3",
          "label": "para",
          "text": "After this excursus, let's return to the task of building logical forms for English\nsentences.",
          "level": -1,
          "page": 412,
          "reading_order": 3,
          "bbox": [
            97,
            259,
            584,
            289
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_412_order_4",
      "label": "sec",
      "text": "Quantified NPs",
      "level": 1,
      "page": 412,
      "reading_order": 4,
      "bbox": [
        97,
        304,
        198,
        325
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_413_order_0",
          "label": "sub_sec",
          "text": "(44)\\P.all x.(dog(x) -> P(x))",
          "level": 2,
          "page": 413,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            297,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_413_order_1",
              "label": "para",
              "text": "We are pretty much done now, except that we also want to carry out a further abstrac-\ntion plus application for the process of combining the semantics of the determiner $a$ ,\nnamely (45), with the semantics of dog.",
              "level": -1,
              "page": 413,
              "reading_order": 1,
              "bbox": [
                97,
                98,
                584,
                152
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_413_order_2",
          "label": "sub_sec",
          "text": "(45)\\Q P.exists x.(Q(x) & P(x))",
          "level": 2,
          "page": 413,
          "reading_order": 2,
          "bbox": [
            109,
            161,
            308,
            180
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_413_order_3",
              "label": "para",
              "text": "Applying (45) as a function expression to \\x.dog(x)yields (43), and applying that to\n\\x.bark(x) gives us \\P.exists x.(dog(x) & P(x))(\\x.bark(x)). Finally, carrying out β-\nreduction yields just what we wanted, namely (42b).",
              "level": -1,
              "page": 413,
              "reading_order": 3,
              "bbox": [
                97,
                188,
                584,
                242
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_412_order_5",
          "label": "para",
          "text": "At the start of this section, we briefly described how to build a semantic representation\nfor Cyril barks . You would be forgiven for thinking this was all too easy—surely there\nis a bit more to building compositional semantics. What about quantifiers, for instance?\nRight, this is a crucial issue. For example, we want (42a) to be given the logical form\nin (42b) . How can this be accomplished?",
          "level": -1,
          "page": 412,
          "reading_order": 5,
          "bbox": [
            97,
            331,
            585,
            415
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_6",
          "label": "para",
          "text": "(42) a. A dog barks.",
          "level": -1,
          "page": 412,
          "reading_order": 6,
          "bbox": [
            109,
            430,
            243,
            444
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_7",
          "label": "para",
          "text": "b. exists x.(dog(x) & bark(x))",
          "level": -1,
          "page": 412,
          "reading_order": 7,
          "bbox": [
            144,
            448,
            333,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_8",
          "label": "para",
          "text": "Let's make the assumption that our only operation for building complex semantic rep-\nresentations is function application. Then our problem is this: how do we give a se-\nmantic representation to the quantified NPs a dog so that it can be combined with\nbark to give the result in (42b) ? As a first step, let's make the subject's SEM value act as\nthe function expression rather than the argument. (This is sometimes called type-\nraising .) Now we are looking for a way of instantiating ?np so that\n[SEM=<?np(\\x.bark(x))>] is equivalent to [SEM=<exists x.(dog(x) & bark(x))>] .\nDoesn't this look a bit reminiscent of carrying out β -reduction in the λ -calculus? In\nother words, we want a λ -term M to replace ?np so that applying M to \\x.bark(x) yields\n(42b) . To do this, we replace the occurrence of \\x.bark(x) in (42b) by a predicate\nvariable P , and bind the variable with λ , as shown in (43) .",
          "level": -1,
          "page": 412,
          "reading_order": 8,
          "bbox": [
            97,
            474,
            585,
            657
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_9",
          "label": "para",
          "text": "(43)\\P.exists x.(dog(x)& P(x))",
          "level": -1,
          "page": 412,
          "reading_order": 9,
          "bbox": [
            109,
            672,
            308,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_10",
          "label": "para",
          "text": "We have used a different style of variable in (43)—that is,\n'P' rather than 'x' or 'y'—\nto signal that we are abstracting over a different kind of object—not an individual, but\na function expression of type ⟨ e, t ⟩ . So the type of (43) as a whole is ⟨⟨e, t⟩ , t⟩ . We will\ntake this to be the type of NPs in general. To illustrate further, a universally quantified\nNP will look like (44) .",
          "level": -1,
          "page": 412,
          "reading_order": 10,
          "bbox": [
            97,
            698,
            585,
            781
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_412_order_11",
          "label": "foot",
          "text": "390 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 412,
          "reading_order": 11,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "390",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_413_order_4",
      "label": "sec",
      "text": "Transitive Verbs",
      "level": 1,
      "page": 413,
      "reading_order": 4,
      "bbox": [
        100,
        257,
        207,
        277
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_413_order_6",
          "label": "sub_sec",
          "text": "(46) Angus chases a dog",
          "level": 2,
          "page": 413,
          "reading_order": 6,
          "bbox": [
            109,
            313,
            253,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_413_order_7",
              "label": "para",
              "text": "The output semantics that we want to build is exists x.(dog(x) & chase(angus, x)) .\nLet's look at how we can use λ -abstraction to get this result. A significant constraint\non possible solutions is to require that the semantic representation of a dog be inde-\npendent of whether the NP acts as subject or object of the sentence. In other words, we\nwant to get the formula just shown as our output while sticking to (43) as the NP se-\nmantics. A second constraint is that VPs should have a uniform type of interpretation,\nregardless of whether they consist of just an intransitive verb or a transitive verb plus\nobject. More specifically, we stipulate that VPs are always of type ⟨ e , t ⟩ . Given these\nconstraints, here’s a semantic representation for chases a dog that does the trick.",
              "level": -1,
              "page": 413,
              "reading_order": 7,
              "bbox": [
                97,
                340,
                585,
                489
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_413_order_8",
          "label": "sub_sec",
          "text": "(47) Vy.exists x.(dog(x) & chase(y, x))",
          "level": 2,
          "page": 413,
          "reading_order": 8,
          "bbox": [
            109,
            501,
            352,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_413_order_9",
              "label": "para",
              "text": "Think of (47) as the property of being a y such that for some dog x, y chases x; or more\ncolloquially, being a y who chases a dog. Our task now resolves to designing a semantic\nrepresentation for chases which can combine with (43) so as to allow (47) to be derived.\nLet's carry out the inverse of β-reduction on (47) , giving rise to (48) .",
              "level": -1,
              "page": 413,
              "reading_order": 9,
              "bbox": [
                97,
                528,
                585,
                603
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_413_order_10",
          "label": "sub_sec",
          "text": "(48) \\P.exists x.(dog(x) & P(x))(\\z.chase(y, z))",
          "level": 2,
          "page": 413,
          "reading_order": 10,
          "bbox": [
            100,
            618,
            413,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_413_order_11",
              "label": "para",
              "text": "(48) may be slightly hard to read at first; you need to see that it involves applying the\nquantified NP representation from (43) to \\z.chase(y,z). (48) is equivalent via β-\nreduction to exists x.(dog(x) & chase(y, x)).",
              "level": -1,
              "page": 413,
              "reading_order": 11,
              "bbox": [
                97,
                645,
                585,
                698
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_413_order_12",
              "label": "para",
              "text": "Now let’s replace the function expression in (48) by a variable X of the same type as an\nNP, that is, of type 〈⟨e, t⟩, t⟩.",
              "level": -1,
              "page": 413,
              "reading_order": 12,
              "bbox": [
                97,
                698,
                584,
                734
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_413_order_13",
          "label": "sub_sec",
          "text": "(49) X(\\z.chase(y, z))",
          "level": 2,
          "page": 413,
          "reading_order": 13,
          "bbox": [
            109,
            743,
            246,
            762
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_413_order_14",
              "label": "foot",
              "text": "10.4 The Semantics of English Sentences | 391",
              "level": -1,
              "page": 413,
              "reading_order": 14,
              "bbox": [
                386,
                824,
                584,
                842
              ],
              "section_number": "10.4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_414_order_0",
              "label": "para",
              "text": "The representation of a transitive verb will have to apply to an argument of the type of\nX to yield a function expression of the type of VPs, that is, of type $\\langle e,t \\rangle $. We can ensure\nthis by abstracting over both the X variable in (49) and also the subject variable y. So\nthe full solution is reached by giving chases the semantic representation shown in (50).",
              "level": -1,
              "page": 414,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                586,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_414_order_1",
          "label": "sub_sec",
          "text": "(50)\\x y.X(\\x.chase(y, x))",
          "level": 2,
          "page": 414,
          "reading_order": 1,
          "bbox": [
            109,
            152,
            279,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_414_order_2",
              "label": "para",
              "text": "If (50) is applied to (43), the result after β-reduction is equivalent to (47), which is what\nwe wanted all along:",
              "level": -1,
              "page": 414,
              "reading_order": 2,
              "bbox": [
                97,
                179,
                585,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_414_order_4",
              "label": "para",
              "text": "In order to build a semantic representation for a sentence, we also need to combine in\nthe semantics of the subject NP . If the latter is a quantified expression, such as every\ngirl , everything proceeds in the same way as we showed for a dog barks earlier on; the\nsubject is translated as a function expression which is applied to the semantic repre-\nsentation of the VP . However, we now seem to have created another problem for our-\nselves with proper names. So far, these have been treated semantically as individual\nconstants, and these cannot be applied as functions to expressions like (47) . Conse-\nquently, we need to come up with a different semantic representation for them. What\nwe do in this case is reinterpret proper names so that they too are function expressions,\nlike quantified NPs . Here is the required λ -expression for Angus :",
              "level": -1,
              "page": 414,
              "reading_order": 4,
              "bbox": [
                96,
                331,
                585,
                501
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_414_order_5",
          "label": "sub_sec",
          "text": "(51) \\P.P(angus)",
          "level": 2,
          "page": 414,
          "reading_order": 5,
          "bbox": [
            109,
            510,
            209,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_414_order_6",
              "label": "para",
              "text": "(51) denotes the characteristic function corresponding to the set of all properties which\nare true of Angus. Converting from an individual constant angus to \\P.P(angus) is an-\nother example of type-raising, briefly mentioned earlier, and allows us to replace a\nBoolean-valued application such as \\x.walk(x)(angus) with an equivalent function ap-\nplication \\P.P(angus)(\\x.walk(x)) . By β-reduction, both expressions reduce to\nwalk(angus) .",
              "level": -1,
              "page": 414,
              "reading_order": 6,
              "bbox": [
                97,
                537,
                584,
                638
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_414_order_7",
              "label": "para",
              "text": "The grammar simple-sem.fcfg contains a small set of rules for parsing and translating\nsimple examples of the kind that we have been looking at. Here’s a slightly more com-\nplicated example:",
              "level": -1,
              "page": 414,
              "reading_order": 7,
              "bbox": [
                97,
                645,
                585,
                694
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_414_order_9",
              "label": "foot",
              "text": "392 | Chapter 10: Analyzing the Meaning of Sentences",
              "level": -1,
              "page": 414,
              "reading_order": 9,
              "bbox": [
                97,
                824,
                328,
                842
              ],
              "section_number": "392",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_415_order_1",
              "label": "para",
              "text": "NLTK provides some utilities to make it easier to derive and inspect semantic inter-\npretations. The function batch_interpret() is intended for batch interpretation of a list\nof input sentences. It builds a dictionary d where for each sentence sent in the input,\nd[sent] is a list of pairs (synrep,semrep) consisting of trees and semantic representations\nfor sent. The value is a list since sent may be syntactically ambiguous; in the following\nexample, however, there is only one parse tree per sentence in the list.",
              "level": -1,
              "page": 415,
              "reading_order": 1,
              "bbox": [
                97,
                116,
                585,
                219
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_415_order_3",
              "label": "para",
              "text": "We have seen now how to convert English sentences into logical forms, and earlier we\nsaw how logical forms could be checked as true or false in a model. Putting these two\nmappings together, we can check the truth value of English sentences in a given model.\nLet's take model m as defined earlier. The utility batch_evaluate() resembles\nbatch_interpret() , except that we need to pass a model and a variable assignment as\nparameters. The output is a triple ( synrep , semrep , value ), where synrep , semrep are as\nbefore, and value is a truth value. For simplicity, the following example only processes\na single sentence.",
              "level": -1,
              "page": 415,
              "reading_order": 3,
              "bbox": [
                97,
                412,
                585,
                549
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_415_order_5",
              "label": "foot",
              "text": "10.4 The Semantics of English Sentences | 393",
              "level": -1,
              "page": 415,
              "reading_order": 5,
              "bbox": [
                386,
                824,
                584,
                842
              ],
              "section_number": "10.4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_413_order_5",
          "label": "para",
          "text": "Our next challenge is to deal with sentences containing transitive verbs, such as (46) .",
          "level": -1,
          "page": 413,
          "reading_order": 5,
          "bbox": [
            97,
            285,
            583,
            299
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_416_order_1",
      "label": "sec",
      "text": "Quantifier Ambiguity Revisited",
      "level": 1,
      "page": 416,
      "reading_order": 1,
      "bbox": [
        97,
        125,
        303,
        146
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_419_order_0",
          "label": "sub_sec",
          "text": "10.5 Discourse Semantics",
          "level": 2,
          "page": 419,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            301,
            98
          ],
          "section_number": "10.5",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_419_order_2",
              "label": "sub_sub_sec",
              "text": "Discourse Representation Theory",
              "level": 3,
              "page": 419,
              "reading_order": 2,
              "bbox": [
                100,
                222,
                316,
                241
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_419_order_3",
                  "label": "para",
                  "text": "The standard approach to quantification in first-order logic is limited to single senten-\nces. Yet there seem to be examples where the scope of a quantifier can extend over two\nor more sentences. We saw one earlier, and here's a second example, together with a\ntranslation.",
                  "level": -1,
                  "page": 419,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    249,
                    585,
                    313
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_419_order_4",
                  "label": "para",
                  "text": "(54) a. Angus owns a dog. It bit Irene.",
                  "level": -1,
                  "page": 419,
                  "reading_order": 4,
                  "bbox": [
                    109,
                    322,
                    342,
                    341
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_419_order_5",
                  "label": "para",
                  "text": "b. $\\exists x$.(dog(x) && own(Angus, x) && bite(x, Irene))",
                  "level": -1,
                  "page": 419,
                  "reading_order": 5,
                  "bbox": [
                    144,
                    348,
                    422,
                    367
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_419_order_6",
                  "label": "para",
                  "text": "That is, the NP a dog acts like a quantifier which binds the it in the second sentence.\nDiscourse Representation Theory (DRT) was developed with the specific goal of pro-\nviding a means for handling this and other semantic phenomena which seem to be\ncharacteristic of discourse. A discourse representation structure (DRS) presents the\nmeaning of discourse in terms of a list of discourse referents and a list of conditions.\nThe discourse referents are the things under discussion in the discourse, and they\ncorrespond to the individual variables of first-order logic. The DRS conditions apply\nto those discourse referents, and correspond to atomic open formulas of first-order\nlogic. Figure 10 - 4 illustrates how a DRS for the first sentence in (54a) is augmented to\nbecome a DRS for both sentences.",
                  "level": -1,
                  "page": 419,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    376,
                    585,
                    537
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_419_order_7",
                  "label": "para",
                  "text": "When the second sentence of (54a) is processed, it is interpreted in the context of what\nis already present in the lefthand side of Figure 10-4 . The pronoun it triggers the addi-\ntion of a new discourse referent, say, u , and we need to find an anaphoric\nantecedent for it—that is, we want to work out what it refers to. In DRT, the task of\nfinding the antecedent for an anaphoric pronoun involves linking it to a discourse ref-\nerent already within the current DRS, and y is the obvious choice. (We will say more\nabout anaphora resolution shortly.) This processing step gives rise to a new condition\nu = y . The remaining content contributed by the second sentence is also merged with\nthe content of the first, and this is shown on the righthand side of Figure 10-4 .",
                  "level": -1,
                  "page": 419,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    546,
                    586,
                    698
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_419_order_8",
                  "label": "para",
                  "text": "Figure 10-4 illustrates how a DRS can represent more than just a single sentence. In\nthis case, it is a two-sentence discourse, but in principle a single DRS could correspond\nto the interpretation of a whole text. We can inquire into the truth conditions of the\nrighthand DRS in Figure 10-4 . Informally, it is true in some situation $s$ if there are\nentities a , c , and i in s corresponding to the discourse referents in the DRS such that",
                  "level": -1,
                  "page": 419,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    698,
                    585,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_419_order_9",
                  "label": "foot",
                  "text": "10.5 Discourse Semantics | 397",
                  "level": -1,
                  "page": 419,
                  "reading_order": 9,
                  "bbox": [
                    449,
                    824,
                    585,
                    842
                  ],
                  "section_number": "10.5",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_420_order_0",
                  "label": "figure",
                  "text": "Figure 10-4. Building a DRS: The DRS on the lefthand side represents the result of processing the first\nsentence in the discourse, while the DRS on the righthand side shows the effect of processing the second\nsentence and integrating its content. [IMAGE: ![Figure](figures/NLTK_page_420_figure_000.png)]",
                  "level": -1,
                  "page": 420,
                  "reading_order": 0,
                  "bbox": [
                    100,
                    71,
                    583,
                    313
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_420_figure_000.png)",
                      "bbox": [
                        100,
                        71,
                        583,
                        313
                      ],
                      "page": 420,
                      "reading_order": 0
                    },
                    {
                      "label": "cap",
                      "text": "Figure 10-4. Building a DRS: The DRS on the lefthand side represents the result of processing the first\nsentence in the discourse, while the DRS on the righthand side shows the effect of processing the second\nsentence and integrating its content.",
                      "bbox": [
                        97,
                        320,
                        585,
                        362
                      ],
                      "page": 420,
                      "reading_order": 1
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_420_order_2",
                  "label": "para",
                  "text": "all the conditions are true in s; that is, a is named Angus, c is a dog, a owns c, i is named\nIrene, and c bit i.",
                  "level": -1,
                  "page": 420,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    367,
                    584,
                    403
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_420_order_3",
                  "label": "para",
                  "text": "In order to process DRSs computationally, we need to convert them into a linear format.\nHere's an example, where the DRS is a pair consisting of a list of discourse referents\nand a list of DRS conditions:",
                  "level": -1,
                  "page": 420,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    412,
                    585,
                    456
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_420_order_4",
                  "label": "para",
                  "text": "[x, y], [angus(x), dog(y), own(x,y)])",
                  "level": -1,
                  "page": 420,
                  "reading_order": 4,
                  "bbox": [
                    126,
                    465,
                    326,
                    483
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_420_order_5",
                  "label": "para",
                  "text": "The easiest way to build a DRS object in NLTK is by parsing a string representation",
                  "level": -1,
                  "page": 420,
                  "reading_order": 5,
                  "bbox": [
                    100,
                    490,
                    559,
                    504
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_420_order_7",
                  "label": "para",
                  "text": "We can use the draw() method ❶ to visualize the result, as shown in Figure 10-5 .",
                  "level": -1,
                  "page": 420,
                  "reading_order": 7,
                  "bbox": [
                    98,
                    573,
                    557,
                    587
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_420_order_8",
                  "label": "para",
                  "text": ">> drs1.draw() ♖",
                  "level": -1,
                  "page": 420,
                  "reading_order": 8,
                  "bbox": [
                    126,
                    591,
                    219,
                    609
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_420_order_9",
                  "label": "figure",
                  "text": "Figure 10-5. DRS screenshot. [IMAGE: ![Figure](figures/NLTK_page_420_figure_009.png)]",
                  "level": -1,
                  "page": 420,
                  "reading_order": 9,
                  "bbox": [
                    100,
                    618,
                    583,
                    725
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_420_figure_009.png)",
                      "bbox": [
                        100,
                        618,
                        583,
                        725
                      ],
                      "page": 420,
                      "reading_order": 9
                    },
                    {
                      "label": "cap",
                      "text": "Figure 10-5. DRS screenshot.",
                      "bbox": [
                        97,
                        734,
                        243,
                        746
                      ],
                      "page": 420,
                      "reading_order": 10
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_420_order_11",
                  "label": "para",
                  "text": "When we discussed the truth conditions of the DRSs in Figure 10 - 4 , we assumed that\nthe topmost discourse referents were interpreted as existential quantifiers, while the",
                  "level": -1,
                  "page": 420,
                  "reading_order": 11,
                  "bbox": [
                    97,
                    768,
                    585,
                    798
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_420_order_12",
                  "label": "foot",
                  "text": "398 | Chapter 10: Analyzing the Meaning of Sentences",
                  "level": -1,
                  "page": 420,
                  "reading_order": 12,
                  "bbox": [
                    97,
                    824,
                    328,
                    842
                  ],
                  "section_number": "398",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_421_order_0",
                  "label": "para",
                  "text": "conditions were interpreted as though they are conjoined. In fact, every DRS can be\ntranslated into a formula of first-order logic, and the fol() method implements this\ntranslation.",
                  "level": -1,
                  "page": 421,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    119
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_421_order_2",
                  "label": "para",
                  "text": "In addition to the functionality available for first-order logic expressions, DRT\nExpressions have a DRS-concatenation operator, represented as the + symbol. The\nconcatenation of two DRSs is a single DRS containing the merged discourse referents\nand the conditions from both arguments. DRS-concatenation automatically $\\alpha$ -converts\nbound variables to avoid name-clashes.",
                  "level": -1,
                  "page": 421,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    161,
                    585,
                    243
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_421_order_4",
                  "label": "para",
                  "text": "While all the conditions seen so far have been atomic, it is possible to embed one DRS\nwithin another, and this is how universal quantification is handled. In drs3 , there are\nno top-level discourse referents, and the sole condition is made up of two sub-DRSs,\nconnected by an implication. Again, we can use fol() to get a handle on the truth\nconditions.",
                  "level": -1,
                  "page": 421,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    322,
                    585,
                    406
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_421_order_6",
                  "label": "para",
                  "text": "We pointed out earlier that DRT is designed to allow anaphoric pronouns to be inter-\npreted by linking to existing discourse referents. DRT sets constraints on which dis-\ncourse referents are “ accessible ” as possible antecedents, but is not intended to explain\nhow a particular antecedent is chosen from the set of candidates. The module\nnltk.sem.drt_resolve_anaphora adopts a similarly conservative strategy: if the DRS\ncontains a condition of the form PRO(x) , the method resolve_anaphora() replaces this\nwith a condition of the form x = [...] , where [...] is a list of possible antecedents.",
                  "level": -1,
                  "page": 421,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    465,
                    585,
                    579
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_421_order_8",
                  "label": "para",
                  "text": "Since the algorithm for anaphora resolution has been separated into its own module,\nthis facilitates swapping in alternative procedures that try to make more intelligent\nguesses about the correct antecedent.",
                  "level": -1,
                  "page": 421,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    680,
                    585,
                    734
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_421_order_9",
                  "label": "para",
                  "text": "Our treatment of DRSs is fully compatible with the existing machinery for handling $\\lambda$ -\nabstraction, and consequently it is straightforward to build compositional semantic\nrepresentations that are based on DRT rather than first-order logic. This technique is",
                  "level": -1,
                  "page": 421,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    743,
                    585,
                    791
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_421_order_10",
                  "label": "foot",
                  "text": "10.5 Discourse Semantics | 399",
                  "level": -1,
                  "page": 421,
                  "reading_order": 10,
                  "bbox": [
                    449,
                    824,
                    585,
                    842
                  ],
                  "section_number": "10.5",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_422_order_0",
                  "label": "para",
                  "text": "illustrated in the following rule for indefinites (which is part of the grammar drt.fcfg).\nFor ease of comparison, we have added the parallel rule for indefinites from simple-\nsem.cfg.",
                  "level": -1,
                  "page": 422,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    584,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_422_order_2",
                  "label": "para",
                  "text": "To get a better idea of how the DRT rule works, look at this subtree for the NP a dog:",
                  "level": -1,
                  "page": 422,
                  "reading_order": 2,
                  "bbox": [
                    100,
                    161,
                    583,
                    180
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_422_order_4",
                  "label": "para",
                  "text": "The $\\lambda$ -abstract for the indefinite is applied as a function expression to \\x.([],\n[dog(x)]) which leads to \\Q.(([x],[]) + ([],[dog(x)]) + Q(x)); after simplification,\nwe get \\Q.(([x],[dog(x)]) + Q(x)) as the representation for the NP as a whole.",
                  "level": -1,
                  "page": 422,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    249,
                    584,
                    296
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_422_order_5",
                  "label": "para",
                  "text": "In order to parse with grammar drt.fcfg, we specify in the call to load_earley() that\nSEM values in feature structures are to be parsed using DrtParser in place of the default\nLogicParser.",
                  "level": -1,
                  "page": 422,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    304,
                    585,
                    353
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_419_order_1",
              "label": "para",
              "text": "A discourse is a sequence of sentences. Very often, the interpretation of a sentence in\na discourse depends on what preceded it. A clear example of this comes from anaphoric\npronouns, such as he, she, and it. Given a discourse such as Angus used to have a dog.\nBut he recently disappeared., you will probably interpret he as referring to Angus’s dog.\nHowever, in Angus used to have a dog. He took him for walks in New Town., you are\nmore likely to interpret he as referring to Angus himself.",
              "level": -1,
              "page": 419,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_416_order_2",
          "label": "para",
          "text": "One important limitation of the methods described earlier is that they do not deal with\nscope ambiguity. Our translation method is syntax-driven, in the sense that the se-\nmantic representation is closely coupled with the syntactic analysis, and the scope of\nthe quantifiers in the semantics therefore reflects the relative scope of the corresponding\nNPs in the syntactic parse tree. Consequently, a sentence like (26) , repeated here, will\nalways be translated as (53a) , not (53b) .",
          "level": -1,
          "page": 416,
          "reading_order": 2,
          "bbox": [
            97,
            152,
            586,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_416_order_3",
          "label": "para",
          "text": "(52) Every girl chases a dog",
          "level": -1,
          "page": 416,
          "reading_order": 3,
          "bbox": [
            109,
            266,
            271,
            280
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_416_order_4",
          "label": "para",
          "text": "(53) a. all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))\nb. exists y.(dog(y) & all x.(girl(x) -> chase(x,y)))",
          "level": -1,
          "page": 416,
          "reading_order": 4,
          "bbox": [
            109,
            295,
            470,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_416_order_5",
          "label": "para",
          "text": "There are numerous approaches to dealing with scope ambiguity, and we will look very\nbriefly at one of the simplest. To start with, let’s briefly consider the structure of scoped\nformulas. Figure 10-3 depicts the way in which the two readings of (52) differ.",
          "level": -1,
          "page": 416,
          "reading_order": 5,
          "bbox": [
            97,
            340,
            585,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_416_order_6",
          "label": "figure",
          "text": "Figure 10-3. Quantifier scopings. [IMAGE: ![Figure](figures/NLTK_page_416_figure_006.png)]",
          "level": -1,
          "page": 416,
          "reading_order": 6,
          "bbox": [
            100,
            403,
            583,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_416_figure_006.png)",
              "bbox": [
                100,
                403,
                583,
                564
              ],
              "page": 416,
              "reading_order": 6
            },
            {
              "label": "cap",
              "text": "Figure 10-3. Quantifier scopings.",
              "bbox": [
                97,
                573,
                261,
                586
              ],
              "page": 416,
              "reading_order": 7
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_416_order_8",
          "label": "para",
          "text": "Let's consider the lefthand structure first. At the top, we have the quantifier corre-\nsponding to every girl . The $\\varphiup$ can be thought of as a placeholder for whatever is inside\nthe scope of the quantifier. Moving downward, we see that we can plug in the quantifier\ncorresponding to a dog as an instantiation of $\\varphiup$ . This gives a new placeholder $\\psiup$ , rep-\nresenting the scope of a dog , and into this we can plug the “ core ” of the semantics,\nnamely the open sentence corresponding to x chases y . The structure on the righthand\nside is identical, except we have swapped round the order of the two quantifiers.",
          "level": -1,
          "page": 416,
          "reading_order": 8,
          "bbox": [
            97,
            609,
            585,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_416_order_9",
          "label": "para",
          "text": "In the method known as Cooper storage , a semantic representation is no longer an\nexpression of first-order logic, but instead a pair consisting of a “ core ” semantic rep-\nresentation plus a list of binding operators . For the moment, think of a binding op-\nerator as being identical to the semantic representation of a quantified NP such as (44) or",
          "level": -1,
          "page": 416,
          "reading_order": 9,
          "bbox": [
            97,
            734,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_416_order_10",
          "label": "foot",
          "text": "394 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 416,
          "reading_order": 10,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "394",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_0",
          "label": "para",
          "text": "(45) . Following along the lines indicated in Figure 10-3 , let's assume that we have\nconstructed a Cooper-storage-style semantic representation of sentence (52) , and let's\ntake our core to be the open formula chase(x,y) . Given a list of binding operators\ncorresponding to the two NPs in (52) , we pick a binding operator off the list, and com-\nbine it with the core.",
          "level": -1,
          "page": 417,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_1",
          "label": "para",
          "text": "\\P.exists y.(dog(y) & P(y))(\\z2.chase(z1,z2)",
          "level": -1,
          "page": 417,
          "reading_order": 1,
          "bbox": [
            122,
            161,
            359,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_2",
          "label": "para",
          "text": "Then we take the result, and apply the next binding operator from the list to it.",
          "level": -1,
          "page": 417,
          "reading_order": 2,
          "bbox": [
            100,
            186,
            548,
            200
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_3",
          "label": "para",
          "text": "\\P.all x.(girl(x) -> P(x))(\\z1.exists x.(dog(x) & chase(z1,x)))",
          "level": -1,
          "page": 417,
          "reading_order": 3,
          "bbox": [
            122,
            206,
            467,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_4",
          "label": "para",
          "text": "Once the list is empty, we have a conventional logical form for the sentence. Combining\nbinding operators with the core in this way is called S-Retrieval . If we are careful to\nallow every possible order of binding operators (for example, by taking all permutations\nof the list; see Section 4.5 ), then we will be able to generate every possible scope ordering\nof quantifiers.",
          "level": -1,
          "page": 417,
          "reading_order": 4,
          "bbox": [
            97,
            230,
            585,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_5",
          "label": "para",
          "text": "The next question to address is how we build up a core+store representation compo-\nsitionally. As before, each phrasal and lexical rule in the grammar will have a SEM feature,\nbut now there will be embedded features CORE and STORE. To illustrate the machinery,\nlet’s consider a simpler example, namely Cyril smiles. Here’s a lexical rule for the verb\nsmiles (taken from the grammar storage.fcfg), which looks pretty innocuous:",
          "level": -1,
          "page": 417,
          "reading_order": 5,
          "bbox": [
            97,
            313,
            585,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_6",
          "label": "para",
          "text": "V[SEM=[CORE=<\\x.smile(x)>, STORE=(/)]] -> 'smiles'",
          "level": -1,
          "page": 417,
          "reading_order": 6,
          "bbox": [
            126,
            403,
            396,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_7",
          "label": "para",
          "text": "The rule for the proper name Cyril is more complex.",
          "level": -1,
          "page": 417,
          "reading_order": 7,
          "bbox": [
            100,
            430,
            396,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_8",
          "label": "para",
          "text": "NP[SEM=[CORE=<@x>, STORE=(<bo(\\P.P(cyril),@x)>)]] -> 'Cyril",
          "level": -1,
          "page": 417,
          "reading_order": 8,
          "bbox": [
            122,
            448,
            441,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_9",
          "label": "para",
          "text": "The bo predicate has two subparts: the standard (type-raised) representation of a proper\nname, and the expression @x, which is called the address of the binding operator. (We’ll\nexplain the need for the address variable shortly.) @x is a metavariable, that is, a variable\nthat ranges over individual variables of the logic and, as you will see, also provides the\nvalue of core. The rule for VP just percolates up the semantics of the IV, and the inter-\nesting work is done by the S rule.",
          "level": -1,
          "page": 417,
          "reading_order": 9,
          "bbox": [
            97,
            474,
            585,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_11",
          "label": "para",
          "text": "The core value at the S node is the result of applying the VP 's core value, namely\n\\x.smile(x) , to the subject NP 's value. The latter will not be @x , but rather an instan-\ntiation of @x , say, z3 . After ${\\sf p}$ -reduction, <?vp(?subj)> will be unified with\n<smile(z3)> . Now, when @x is instantiated as part of the parsing process, it will be\ninstantiated uniformly. In particular, the occurrence of @x in the subject NP 's STORE will\nalso be mapped to z3 , yielding the element bo(\\P.P(cyril),z3) . These steps can be seen\nin the following parse tree.",
          "level": -1,
          "page": 417,
          "reading_order": 11,
          "bbox": [
            97,
            642,
            585,
            755
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_417_order_12",
          "label": "foot",
          "text": "10.4 The Semantics of English Sentences | 395",
          "level": -1,
          "page": 417,
          "reading_order": 12,
          "bbox": [
            386,
            824,
            585,
            842
          ],
          "section_number": "10.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_418_order_1",
          "label": "para",
          "text": "Let’s return to our more complex example, (52), and see what the storage style SEM\nvalue is, after parsing with grammar storage.cfg.",
          "level": -1,
          "page": 418,
          "reading_order": 1,
          "bbox": [
            97,
            134,
            585,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_418_order_3",
          "label": "para",
          "text": "It should be clearer now why the address variables are an important part of the binding\noperator. Recall that during S-retrieval, we will be taking binding operators off the\nSTORE list and applying them successively to the CORE . Suppose we start with bo(\\P.all\nx.(girl(x) -> P(x)),z1) , which we want to combine with chase(z1,z2) . The quantifier\npart of the binding operator is V.all x.(girl(x) -> P(x)) , and to combine this with\nchase(z1,z2) , the latter needs to first be turned into a λ -abstract. How do we know\nwhich variable to abstract over? This is what the address z1 tells us, i.e., that every\ngirl has the role of chaser rather than chasee.",
          "level": -1,
          "page": 418,
          "reading_order": 3,
          "bbox": [
            96,
            206,
            585,
            340
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_418_order_4",
          "label": "para",
          "text": "The module nltk.sem.cooper_storage deals with the task of turning storage-style se-\nmantic representations into standard logical forms. First, we construct a CooperStore\ninstance, and inspect its STORE and CORE.",
          "level": -1,
          "page": 418,
          "reading_order": 4,
          "bbox": [
            100,
            349,
            585,
            396
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_418_order_6",
          "label": "para",
          "text": "Finally, we call s_retrieve() and check the readings",
          "level": -1,
          "page": 418,
          "reading_order": 6,
          "bbox": [
            98,
            555,
            395,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_418_order_8",
          "label": "foot",
          "text": "396 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 418,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "396",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_422_order_7",
      "label": "sec",
      "text": "Discourse Processing",
      "level": 1,
      "page": 422,
      "reading_order": 7,
      "bbox": [
        98,
        439,
        234,
        459
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_424_order_5",
          "label": "sub_sec",
          "text": "10.6 Summary",
          "level": 2,
          "page": 424,
          "reading_order": 5,
          "bbox": [
            98,
            322,
            217,
            349
          ],
          "section_number": "10.6",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_424_order_6",
              "label": "list_group",
              "text": "First-order logic is a suitable language for representing natural language meaning\nin a computational setting since it is flexible enough to represent many useful as-\npects of natural meaning, and there are efficient theorem provers for reasoning with\nfirst-order logic. (Equally, there are a variety of phenomena in natural language\nsemantics which are believed to require more powerful logical mechanisms.)\nAs well as translating natural language sentences into first-order logic, we can state\nthe truth conditions of these sentences by examining models of first-order formu-\nlas.",
              "level": -1,
              "page": 424,
              "reading_order": 6,
              "bbox": [
                122,
                358,
                585,
                440
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "First-order logic is a suitable language for representing natural language meaning\nin a computational setting since it is flexible enough to represent many useful as-\npects of natural meaning, and there are efficient theorem provers for reasoning with\nfirst-order logic. (Equally, there are a variety of phenomena in natural language\nsemantics which are believed to require more powerful logical mechanisms.)",
                  "bbox": [
                    122,
                    358,
                    585,
                    440
                  ],
                  "page": 424,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "As well as translating natural language sentences into first-order logic, we can state\nthe truth conditions of these sentences by examining models of first-order formu-\nlas.",
                  "bbox": [
                    122,
                    446,
                    585,
                    492
                  ],
                  "page": 424,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "In order to build meaning representations compositionally, we supplement first-\norder logic with the $\\boldsymbol{\\lambda}$ -calculus.",
                  "bbox": [
                    126,
                    500,
                    584,
                    531
                  ],
                  "page": 424,
                  "reading_order": 8
                },
                {
                  "label": "list",
                  "text": "β-reduction in the λ-calculus corresponds semantically to application of a function\nto an argument. Syntactically, it involves replacing a variable bound by λ in the\nfunction expression with the expression that provides the argument in the function\napplication.",
                  "bbox": [
                    122,
                    537,
                    585,
                    601
                  ],
                  "page": 424,
                  "reading_order": 9
                },
                {
                  "label": "list",
                  "text": "A key part of constructing a model lies in building a valuation which assigns in-\nterpretations to non-logical constants. These are interpreted as either $n$ -ary predi-\ncates or as individual constants.",
                  "bbox": [
                    122,
                    608,
                    585,
                    654
                  ],
                  "page": 424,
                  "reading_order": 10
                },
                {
                  "label": "list",
                  "text": "An open expression is an expression containing one or more free variables. Open\nexpressions receive an interpretation only when their free variables receive values\nfrom a variable assignment.",
                  "bbox": [
                    122,
                    661,
                    585,
                    709
                  ],
                  "page": 424,
                  "reading_order": 11
                },
                {
                  "label": "list",
                  "text": "Quantifiers are interpreted by constructing, for a formula $\\varphi [x]$ open in variable x,\nhe set of individuals which make $\\varphi [x]$ true when an assignment g assigns them as\nthe value of x. The quantifier then places constraints on that set.",
                  "bbox": [
                    126,
                    715,
                    585,
                    762
                  ],
                  "page": 424,
                  "reading_order": 12
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_424_order_13",
              "label": "foot",
              "text": "402 | Chapter 10: Analyzing the Meaning of Sentences",
              "level": -1,
              "page": 424,
              "reading_order": 13,
              "bbox": [
                97,
                824,
                328,
                842
              ],
              "section_number": "402",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_0",
              "label": "list_group",
              "text": "A closed expression is one that has no free variables; that is, the variables are all\nbound. A closed sentence is true or false with respect to all variable assignments.\nIf two formulas differ only in the label of the variable bound by binding operator\n(i.e., λ or a quantifier) , they are said to be α-equivalents. The result of relabeling\na bound variable in a formula is called α-conversion.",
              "level": -1,
              "page": 425,
              "reading_order": 0,
              "bbox": [
                118,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "A closed expression is one that has no free variables; that is, the variables are all\nbound. A closed sentence is true or false with respect to all variable assignments.",
                  "bbox": [
                    118,
                    71,
                    584,
                    107
                  ],
                  "page": 425,
                  "reading_order": 0
                },
                {
                  "label": "list",
                  "text": "If two formulas differ only in the label of the variable bound by binding operator\n(i.e., λ or a quantifier) , they are said to be α-equivalents. The result of relabeling\na bound variable in a formula is called α-conversion.",
                  "bbox": [
                    118,
                    107,
                    585,
                    161
                  ],
                  "page": 425,
                  "reading_order": 1
                },
                {
                  "label": "list",
                  "text": "Given a formula with two nested quantifiers $Q_1$ and $Q_2$ , the outermost quantifier\n$Q_1$ is said to have wide scope (or scope over $Q_2$ ). English sentences are frequently\nambiguous with respect to the scope of the quantifiers they contain.",
                  "bbox": [
                    118,
                    161,
                    585,
                    215
                  ],
                  "page": 425,
                  "reading_order": 2
                },
                {
                  "label": "list",
                  "text": "English sentences can be associated with a semantic representation by treating\nSEM as a feature in a feature-based grammar. The SEM value of a complex expressions,\ntypically involves functional application of the SEM values of the component\nexpressions.",
                  "bbox": [
                    118,
                    215,
                    585,
                    286
                  ],
                  "page": 425,
                  "reading_order": 3
                }
              ],
              "is_merged": true
            }
          ]
        },
        {
          "id": "page_425_order_4",
          "label": "sub_sec",
          "text": "10.7 Further Reading",
          "level": 2,
          "page": 425,
          "reading_order": 4,
          "bbox": [
            98,
            304,
            270,
            333
          ],
          "section_number": "10.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_425_order_5",
              "label": "para",
              "text": "Consult http://www.nltk.org/ for further materials on this chapter and on how to install\nthe Prover9 theorem prover and Mace4 model builder. General information about these\ntwo inference tools is given by (McCune, 2008).",
              "level": -1,
              "page": 425,
              "reading_order": 5,
              "bbox": [
                97,
                340,
                585,
                389
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_6",
              "label": "para",
              "text": "For more examples of semantic analysis with NLTK, please see the semantics and logic\nHOWTOs at http://www.nltk.org/howto. Note that there are implementations of two\nother approaches to scope ambiguity, namely Hole semantics as described in (Black-\nburn & Bos, 2005), and Glue semantics, as described in (Dalrymple et al., 1999).",
              "level": -1,
              "page": 425,
              "reading_order": 6,
              "bbox": [
                97,
                394,
                584,
                465
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_7",
              "label": "para",
              "text": "There are many phenomena in natural language semantics that have not been touched\non in this chapter, most notably:",
              "level": -1,
              "page": 425,
              "reading_order": 7,
              "bbox": [
                100,
                465,
                584,
                502
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_8",
              "label": "para",
              "text": "1. Events, tense, and aspect",
              "level": -1,
              "page": 425,
              "reading_order": 8,
              "bbox": [
                100,
                510,
                270,
                528
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_9",
              "label": "para",
              "text": "2. Semantic roles",
              "level": -1,
              "page": 425,
              "reading_order": 9,
              "bbox": [
                100,
                528,
                207,
                546
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_10",
              "label": "para",
              "text": "3. Generalized quantifiers, such as most",
              "level": -1,
              "page": 425,
              "reading_order": 10,
              "bbox": [
                100,
                554,
                333,
                568
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_11",
              "label": "para",
              "text": "4. Intensional constructions involving, for example, verbs such as may and believe",
              "level": -1,
              "page": 425,
              "reading_order": 11,
              "bbox": [
                100,
                573,
                574,
                591
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_12",
              "label": "para",
              "text": "While (1) and (2) can be dealt with using first-order logic, (3) and (4) require different\nlogics. These issues are covered by many of the references in the following readings.",
              "level": -1,
              "page": 425,
              "reading_order": 12,
              "bbox": [
                97,
                599,
                585,
                630
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_13",
              "label": "para",
              "text": "A comprehensive overview of results and techniques in building natural language front-\nends to databases can be found in (Androutsopoulos, Ritchie & Thanisch, 1995).",
              "level": -1,
              "page": 425,
              "reading_order": 13,
              "bbox": [
                97,
                636,
                584,
                672
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_14",
              "label": "para",
              "text": "Any introductory book to modern logic will present propositional and first-order logic.\n(Hodges, 1977) is highly recommended as an entertaining and insightful text with many\nillustrations from natural language.",
              "level": -1,
              "page": 425,
              "reading_order": 14,
              "bbox": [
                97,
                679,
                585,
                726
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_15",
              "label": "para",
              "text": "For a wide-ranging, two-volume textbook on logic that also presents contemporary\nmaterial on the formal semantics of natural language, including Montague Grammar\nand intensional logic, see (Gamut, 1991a, 1991b) . (Kamp & Reyle, 1993) provides the",
              "level": -1,
              "page": 425,
              "reading_order": 15,
              "bbox": [
                97,
                734,
                585,
                783
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_425_order_16",
              "label": "foot",
              "text": "10.7 Further Reading | 403",
              "level": -1,
              "page": 425,
              "reading_order": 16,
              "bbox": [
                465,
                824,
                584,
                842
              ],
              "section_number": "10.7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_0",
              "label": "para",
              "text": "definitive account of Discourse Representation Theory, and covers a large and inter-\nesting fragment of natural language, including tense, aspect, and modality. Another\ncomprehensive study of the semantics of many natural language constructions is (Car-\npenter, 1997).",
              "level": -1,
              "page": 426,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                585,
                143
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_1",
              "label": "para",
              "text": "There are numerous works that introduce logical semantics within the framework of\nlinguistic theory. (Chierchia & McConnell-Ginet, 1990) is relatively agnostic about\nsyntax, while (Heim & Kratzer, 1998) and (Larson & Segal, 1995) are both more ex-\nplicitly oriented toward integrating truth-conditional semantics into a Chomskyan\nframework.",
              "level": -1,
              "page": 426,
              "reading_order": 1,
              "bbox": [
                97,
                143,
                585,
                225
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_2",
              "label": "para",
              "text": "(Blackburn & Bos, 2005) is the first textbook devoted to computational semantics, and\nprovides an excellent introduction to the area. It expands on many of the topics covered\nin this chapter, including underspecification of quantifier scope ambiguity, first-order\ninference, and discourse processing.",
              "level": -1,
              "page": 426,
              "reading_order": 2,
              "bbox": [
                97,
                232,
                585,
                304
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_3",
              "label": "para",
              "text": "To gain an overview of more advanced contemporary approaches to semantics, in-\ncluding treatments of tense and generalized quantifiers, try consulting (Lappin, 1996)\nor (van Benthem & ter Meulen, 1997) .",
              "level": -1,
              "page": 426,
              "reading_order": 3,
              "bbox": [
                97,
                311,
                584,
                358
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_426_order_4",
          "label": "sub_sec",
          "text": "10.8 Exercises",
          "level": 2,
          "page": 426,
          "reading_order": 4,
          "bbox": [
            98,
            385,
            211,
            403
          ],
          "section_number": "10.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_426_order_5",
              "label": "para",
              "text": "1. ◦ Translate the following sentences into propositional logic and verify that they\nparse with LogicParser. Provide a key that shows how the propositional variables\nin your translation correspond to expressions of English.",
              "level": -1,
              "page": 426,
              "reading_order": 5,
              "bbox": [
                100,
                412,
                585,
                465
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_6",
              "label": "para",
              "text": "a. If Angus sings, it is not the case that Bertie sulks.",
              "level": -1,
              "page": 426,
              "reading_order": 6,
              "bbox": [
                126,
                465,
                424,
                485
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_7",
              "label": "para",
              "text": "b. Cyril runs and barks.",
              "level": -1,
              "page": 426,
              "reading_order": 7,
              "bbox": [
                126,
                492,
                270,
                510
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_8",
              "label": "para",
              "text": "c. It will snow if it doesn't rain.",
              "level": -1,
              "page": 426,
              "reading_order": 8,
              "bbox": [
                126,
                510,
                315,
                528
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_9",
              "label": "para",
              "text": "d. It's not the case that Irene will be happy if Olive or Tofu comes.",
              "level": -1,
              "page": 426,
              "reading_order": 9,
              "bbox": [
                126,
                528,
                512,
                547
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_10",
              "label": "para",
              "text": "e. Pat didn’t cough or sneeze",
              "level": -1,
              "page": 426,
              "reading_order": 10,
              "bbox": [
                126,
                554,
                297,
                573
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_11",
              "label": "para",
              "text": "f. If you don't come if I call, I won't come if you call.",
              "level": -1,
              "page": 426,
              "reading_order": 11,
              "bbox": [
                126,
                573,
                440,
                591
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_12",
              "label": "para",
              "text": "2. ◦ Translate the following sentences into predicate-argument formulas of first-order\nlogic.",
              "level": -1,
              "page": 426,
              "reading_order": 12,
              "bbox": [
                100,
                591,
                585,
                627
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_13",
              "label": "para",
              "text": "a. Angus likes Cyril and Irene hates Cyril.",
              "level": -1,
              "page": 426,
              "reading_order": 13,
              "bbox": [
                126,
                627,
                369,
                646
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_14",
              "label": "para",
              "text": "b. Tofu is taller than Bertie.",
              "level": -1,
              "page": 426,
              "reading_order": 14,
              "bbox": [
                126,
                653,
                288,
                664
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_15",
              "label": "para",
              "text": "c. Bruce loves himself and Pat does too",
              "level": -1,
              "page": 426,
              "reading_order": 15,
              "bbox": [
                126,
                672,
                351,
                689
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_16",
              "label": "para",
              "text": "d. Cyril saw Bertie, but Angus didn't.",
              "level": -1,
              "page": 426,
              "reading_order": 16,
              "bbox": [
                126,
                689,
                344,
                708
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_17",
              "label": "para",
              "text": "e. Cyril is a four-legged friend",
              "level": -1,
              "page": 426,
              "reading_order": 17,
              "bbox": [
                126,
                715,
                300,
                729
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_18",
              "label": "para",
              "text": "f. Tofu and Olive are near each other.",
              "level": -1,
              "page": 426,
              "reading_order": 18,
              "bbox": [
                126,
                734,
                350,
                752
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_19",
              "label": "para",
              "text": "3. ◦ Translate the following sentences into quantified formulas of first-order logic\na. Angus likes someone and someone likes Julia.",
              "level": -1,
              "page": 426,
              "reading_order": 19,
              "bbox": [
                100,
                752,
                574,
                791
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_426_order_20",
              "label": "foot",
              "text": "404 | Chapter 10: Analyzing the Meaning of Sentences",
              "level": -1,
              "page": 426,
              "reading_order": 20,
              "bbox": [
                97,
                824,
                328,
                842
              ],
              "section_number": "404",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_0",
              "label": "para",
              "text": "b. Angus loves a dog who loves him.",
              "level": -1,
              "page": 427,
              "reading_order": 0,
              "bbox": [
                126,
                71,
                342,
                89
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_1",
              "label": "para",
              "text": "c. Nobody smiles at Pat.",
              "level": -1,
              "page": 427,
              "reading_order": 1,
              "bbox": [
                126,
                96,
                271,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_2",
              "label": "para",
              "text": "d. Somebody coughs and sneezes.",
              "level": -1,
              "page": 427,
              "reading_order": 2,
              "bbox": [
                126,
                116,
                324,
                130
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_3",
              "label": "para",
              "text": "e. Nobody coughed or sneezed.",
              "level": -1,
              "page": 427,
              "reading_order": 3,
              "bbox": [
                126,
                134,
                315,
                152
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_4",
              "label": "para",
              "text": "f. Bruce loves somebody other than Bruce.",
              "level": -1,
              "page": 427,
              "reading_order": 4,
              "bbox": [
                126,
                152,
                377,
                170
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_5",
              "label": "para",
              "text": "g. Nobody other than Matthew loves Pat.",
              "level": -1,
              "page": 427,
              "reading_order": 5,
              "bbox": [
                126,
                178,
                368,
                192
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_6",
              "label": "para",
              "text": "h. Cyril likes everyone except for Irene",
              "level": -1,
              "page": 427,
              "reading_order": 6,
              "bbox": [
                126,
                197,
                351,
                215
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_7",
              "label": "para",
              "text": "i. Exactly one person is asleep",
              "level": -1,
              "page": 427,
              "reading_order": 7,
              "bbox": [
                126,
                215,
                306,
                233
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_8",
              "label": "para",
              "text": "4. ◦ Translate the following verb phrases using λ-abstracts and quantified formulas\nof first-order logic.",
              "level": -1,
              "page": 427,
              "reading_order": 8,
              "bbox": [
                100,
                240,
                585,
                271
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_9",
              "label": "para",
              "text": "a. feed Cyril and give a capuccino to Angus",
              "level": -1,
              "page": 427,
              "reading_order": 9,
              "bbox": [
                126,
                277,
                379,
                291
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_10",
              "label": "para",
              "text": "b. be given 'War and Peace' by Pa",
              "level": -1,
              "page": 427,
              "reading_order": 10,
              "bbox": [
                126,
                295,
                324,
                313
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_11",
              "label": "para",
              "text": "c. be loved by everyone",
              "level": -1,
              "page": 427,
              "reading_order": 11,
              "bbox": [
                126,
                313,
                270,
                331
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_12",
              "label": "para",
              "text": "d, be loved or detested by everyone",
              "level": -1,
              "page": 427,
              "reading_order": 12,
              "bbox": [
                126,
                339,
                333,
                350
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_13",
              "label": "para",
              "text": "e. be loved by everyone and detested by no-one",
              "level": -1,
              "page": 427,
              "reading_order": 13,
              "bbox": [
                126,
                358,
                404,
                376
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_14",
              "label": "para",
              "text": "5. ◦ Consider the following statements:",
              "level": -1,
              "page": 427,
              "reading_order": 14,
              "bbox": [
                105,
                376,
                333,
                395
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_16",
              "label": "para",
              "text": "Clearly something is missing here, namely a declaration of the value of e1 . In order\nfor ApplicationExpression(e1, e2) to be β-convertible to exists y.love(pat, y) ,\ne1 must be a λ-abstract which can take pat as an argument. Your task is to construct\nsuch an abstract, bind it to e1 , and satisfy yourself that these statements are all\nsatisfied (up to alphabetic variance). In addition, provide an informal English\ntranslation of e3.simplify() .",
              "level": -1,
              "page": 427,
              "reading_order": 16,
              "bbox": [
                118,
                474,
                585,
                575
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_17",
              "label": "para",
              "text": "Now carry on doing this same task for the further cases of e3.simplify() shown\nhere:",
              "level": -1,
              "page": 427,
              "reading_order": 17,
              "bbox": [
                118,
                582,
                584,
                610
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_19",
              "label": "para",
              "text": "6. ◦ As in the preceding exercise, find a λ-abstract e1 that yields results equivalent to\nthose shown here:",
              "level": -1,
              "page": 427,
              "reading_order": 19,
              "bbox": [
                105,
                716,
                584,
                752
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_427_order_21",
              "label": "foot",
              "text": "10.8 Exercises | 405",
              "level": -1,
              "page": 427,
              "reading_order": 21,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "10.8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_428_order_3",
              "label": "para",
              "text": "7. ◦ As in the preceding exercise, find a λ-abstract e1 that yields results equivalent to\nthose shown here:",
              "level": -1,
              "page": 428,
              "reading_order": 3,
              "bbox": [
                100,
                224,
                584,
                253
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_428_order_5",
              "label": "para",
              "text": "8. o Develop a method for translating English sentences into formulas with binary\ngeneralized quantifiers . In such an approach, given a generalized quantifier Q , a\nquantified formula is of the form O(A, B) , where both A and B are expressions of\ntype ⟨ e , t ⟩ . Then, for example, all(A, B) is true iff A denotes a subset of what B\ndenotes.",
              "level": -1,
              "page": 428,
              "reading_order": 5,
              "bbox": [
                105,
                439,
                586,
                520
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_428_order_6",
              "label": "para",
              "text": "9. a Extend the approach in the preceding exercise so that the truth conditions for\nquantifiers such as most and exactly three can be computed in a model.",
              "level": -1,
              "page": 428,
              "reading_order": 6,
              "bbox": [
                105,
                528,
                585,
                560
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_428_order_7",
              "label": "para",
              "text": "10. o Modify the sem.evaluate code so that it will give a helpful error message if an\nexpression is not in the domain of a model's valuation function.",
              "level": -1,
              "page": 428,
              "reading_order": 7,
              "bbox": [
                100,
                564,
                584,
                600
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_428_order_8",
              "label": "para",
              "text": "11. • Select three or four contiguous sentences from a book for children. A possible\nsource of examples are the collections of stories in nltk.corpus.gutenberg: bryant-\nstories.txt, burgess-busterbrown.txt, and edgeworth-parents.txt. Develop a\ngrammar that will allow your sentences to be translated into first-order logic, and\nbuild a model that will allow those translations to be checked for truth or falsity.",
              "level": -1,
              "page": 428,
              "reading_order": 8,
              "bbox": [
                100,
                600,
                585,
                689
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_428_order_9",
              "label": "para",
              "text": "12. • Carry out the preceding exercise, but use DRT as the meaning representation",
              "level": -1,
              "page": 428,
              "reading_order": 9,
              "bbox": [
                100,
                689,
                574,
                707
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_428_order_10",
              "label": "para",
              "text": "13. • Taking (Warren & Pereira, 1982) as a starting point, develop a technique for\nconverting a natural language query into a form that can be evaluated more effi-\nciently in a model. For example, given a query of the form (P(x) & Q(x)), convert\nit to (Q(x) & P(x)) if the extension of Q is smaller than the extension of P.",
              "level": -1,
              "page": 428,
              "reading_order": 10,
              "bbox": [
                100,
                707,
                585,
                779
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_428_order_11",
              "label": "foot",
              "text": "406 | Chapter 10: Analyzing the Meaning of Sentences",
              "level": -1,
              "page": 428,
              "reading_order": 11,
              "bbox": [
                97,
                824,
                328,
                842
              ],
              "section_number": "406",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_422_order_8",
          "label": "para",
          "text": "When we interpret a sentence, we use a rich context for interpretation, determined in\npart by the preceding context and in part by our background assumptions. DRT pro-\nvides a theory of how the meaning of a sentence is integrated into a representation of\nthe prior discourse, but two things have been glaringly absent from the processing\napproach just discussed. First, there has been no attempt to incorporate any kind of\ninference; and second, we have only processed individual sentences. These omissions\nare redressed by the module nltk.inference.discourse.",
          "level": -1,
          "page": 422,
          "reading_order": 8,
          "bbox": [
            97,
            465,
            586,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_422_order_9",
          "label": "para",
          "text": "Whereas a discourse is a sequence $s_1,...\\,s_n$ of sentences, a discourse thread is a sequence\n$s_1$ - $r_i,...\\,s_n$ - $r_j$ of readings, one for each sentence in the discourse. The module processes\nsentences incrementally, keeping track of all possible threads when there is ambiguity.\nFor simplicity, the following example ignores scope ambiguity:",
          "level": -1,
          "page": 422,
          "reading_order": 9,
          "bbox": [
            97,
            591,
            585,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_422_order_11",
          "label": "para",
          "text": "When a new sentence is added to the current discourse, setting the parameter\nconsistchk=True causes consistency to be checked by invoking the model checker for\neach thread, i.e., each sequence of admissible readings. In this case, the user has the\noption of retracting the sentence in question.",
          "level": -1,
          "page": 422,
          "reading_order": 11,
          "bbox": [
            97,
            724,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_422_order_12",
          "label": "foot",
          "text": "400 | Chapter 10: Analyzing the Meaning of Sentences",
          "level": -1,
          "page": 422,
          "reading_order": 12,
          "bbox": [
            97,
            824,
            328,
            842
          ],
          "section_number": "400",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_423_order_1",
          "label": "para",
          "text": "In a similar manner, we use informchk=True to check whether a new sentence $\\varphi$ is\ninformative relative to the current discourse. The theorem prover treats existing sen-\ntences in the thread as assumptions and attempts to prove $\\varphi$; it is informative if no such\nproof can be found.",
          "level": -1,
          "page": 423,
          "reading_order": 1,
          "bbox": [
            97,
            197,
            585,
            264
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_423_order_3",
          "label": "para",
          "text": "It is also possible to pass in an additional set of assumptions as background knowledge\nand use these to filter out inconsistent readings; see the Discourse HOWTO at http://\nwww.nltk.org/howto for more details.",
          "level": -1,
          "page": 423,
          "reading_order": 3,
          "bbox": [
            97,
            320,
            585,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_423_order_4",
          "label": "para",
          "text": "The discourse module can accommodate semantic ambiguity and filter out readings\nthat are not admissible. The following example invokes both Glue Semantics as well\nas DRT. Since the Glue Semantics module is configured to use the wide-coverage Malt\ndependency parser, the input ( Every dog chases a boy. He runs.) needs to be tagged as\nwell as tokenized.",
          "level": -1,
          "page": 423,
          "reading_order": 4,
          "bbox": [
            97,
            376,
            585,
            456
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_423_order_7",
          "label": "para",
          "text": "The first sentence of the discourse has two possible readings, depending on the quan-\ntifier scoping. The unique reading of the second sentence represents the pronoun He\nvia the condition PRO(x) . Now let's look at the discourse threads that result:",
          "level": -1,
          "page": 423,
          "reading_order": 7,
          "bbox": [
            97,
            696,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_423_order_9",
          "label": "foot",
          "text": "10.5 Discourse Semantics | 401",
          "level": -1,
          "page": 423,
          "reading_order": 9,
          "bbox": [
            449,
            824,
            584,
            842
          ],
          "section_number": "10.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_424_order_1",
          "label": "para",
          "text": "When we examine threads d0 and d1, we see that reading s0-r0, where every dog out-\nscopes a boy, is deemed inadmissible because the pronoun in the second sentence\ncannot be resolved. By contrast, in thread d1 the pronoun (relettered to z10) has been\nbound via the equation (z10 = z6).",
          "level": -1,
          "page": 424,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            173
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_424_order_2",
          "label": "para",
          "text": "Inadmissible readings can be filtered out by passing the parameter filter=True.",
          "level": -1,
          "page": 424,
          "reading_order": 2,
          "bbox": [
            98,
            179,
            549,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_424_order_4",
          "label": "para",
          "text": "Although this little discourse is extremely limited, it should give you a feel for the kind\nof semantic processing issues that arise when we go beyond single sentences, and also\na feel for the techniques that can be deployed to address them.",
          "level": -1,
          "page": 424,
          "reading_order": 4,
          "bbox": [
            97,
            250,
            585,
            300
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_429_order_0",
      "label": "sec",
      "text": "CHAPTER 11\nManaging Linguistic Data",
      "level": 1,
      "page": 429,
      "reading_order": 0,
      "bbox": [
        252,
        71,
        583,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_429_order_6",
          "label": "sub_sec",
          "text": "11.1 Corpus Structure: A Case Study",
          "level": 2,
          "page": 429,
          "reading_order": 6,
          "bbox": [
            98,
            563,
            386,
            586
          ],
          "section_number": "11.1",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_429_order_8",
              "label": "sub_sub_sec",
              "text": "The Structure of TIMIT",
              "level": 3,
              "page": 429,
              "reading_order": 8,
              "bbox": [
                97,
                689,
                244,
                707
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_429_order_9",
                  "label": "para",
                  "text": "Like the Brown Corpus, which displays a balanced selection of text genres and sources,\nTIMIT includes a balanced selection of dialects, speakers, and materials. For each of\neight dialect regions, 50 male and female speakers having a range of ages and educa-\ntional backgrounds each read 10 carefully chosen sentences. Two sentences, read by\nall speakers, were designed to bring out dialect variation:",
                  "level": -1,
                  "page": 429,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    716,
                    586,
                    798
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_429_order_10",
                  "label": "foot",
                  "text": "407",
                  "level": -1,
                  "page": 429,
                  "reading_order": 10,
                  "bbox": [
                    566,
                    824,
                    585,
                    837
                  ],
                  "section_number": "407",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_430_order_0",
                  "label": "para",
                  "text": "(1) a. she had your dark suit in greasy wash water all year\nb. don't ask me to carry an oily rag like that",
                  "level": -1,
                  "page": 430,
                  "reading_order": 0,
                  "bbox": [
                    118,
                    71,
                    458,
                    110
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_430_order_1",
                  "label": "para",
                  "text": "The remaining sentences were chosen to be phonetically rich, involving all phones\n(sounds) and a comprehensive range of diphones (phone bigrams). Additionally, the\ndesign strikes a balance between multiple speakers saying the same sentence in order\nto permit comparison across speakers, and having a large range of sentences covered\nby the corpus to get maximal coverage of diphones. Five of the sentences read by each\nspeaker are also read by six other speakers (for comparability). The remaining three\nsentences read by each speaker were unique to that speaker (for coverage).",
                  "level": -1,
                  "page": 430,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    124,
                    585,
                    241
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_430_order_2",
                  "label": "para",
                  "text": "NLTK includes a sample from the TIMIT Corpus. You can access its documentation\nin the usual way, using help(nltk.corpus.timit). Print nltk.corpus.timit.fileids()\nto see a list of the 160 recorded utterances in the corpus sample. Each filename has\ninternal structure, as shown in Figure 11-1.",
                  "level": -1,
                  "page": 430,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    241,
                    585,
                    313
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_430_order_3",
                  "label": "figure",
                  "text": "Figure 11-1. Structure of a TIMIT identifier: Each recording is labeled using a string made up of the\nspeaker's dialect region, gender, speaker identifier, sentence type, and sentence identifier. [IMAGE: ![Figure](figures/NLTK_page_430_figure_003.png)]",
                  "level": -1,
                  "page": 430,
                  "reading_order": 3,
                  "bbox": [
                    100,
                    322,
                    583,
                    627
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "fig",
                      "text": "![Figure](figures/NLTK_page_430_figure_003.png)",
                      "bbox": [
                        100,
                        322,
                        583,
                        627
                      ],
                      "page": 430,
                      "reading_order": 3
                    },
                    {
                      "label": "cap",
                      "text": "Figure 11-1. Structure of a TIMIT identifier: Each recording is labeled using a string made up of the\nspeaker's dialect region, gender, speaker identifier, sentence type, and sentence identifier.",
                      "bbox": [
                        97,
                        635,
                        583,
                        663
                      ],
                      "page": 430,
                      "reading_order": 4
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_430_order_5",
                  "label": "para",
                  "text": "Each item has a phonetic transcription which can be accessed using the phones() meth-\nod. We can access the corresponding word tokens in the customary way. Both access\nmethods permit an optional argument offset=True, which includes the start and end\noffsets of the corresponding span in the audio file.",
                  "level": -1,
                  "page": 430,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    689,
                    585,
                    753
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_430_order_7",
                  "label": "foot",
                  "text": "408 | Chapter11: Managing Linguistic Data",
                  "level": -1,
                  "page": 430,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    824,
                    288,
                    842
                  ],
                  "section_number": "408",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_431_order_1",
                  "label": "para",
                  "text": "In addition to this text data, TIMIT includes a lexicon that provides the canonical\npronunciation of every word, which can be compared with a particular utterance:",
                  "level": -1,
                  "page": 431,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    170,
                    584,
                    206
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_431_order_3",
                  "label": "para",
                  "text": "This gives us a sense of what a speech processing system would have to do in producing\nor recognizing speech in this particular dialect (New England). Finally, TIMIT includes\ndemographic data about the speakers, permitting fine-grained study of vocal, social,\nand gender characteristics.",
                  "level": -1,
                  "page": 431,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    286,
                    585,
                    351
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_429_order_7",
              "label": "para",
              "text": "The TIMIT Corpus was the first annotated speech database to be widely distributed,\nand it has an especially clear organization. TIMIT was developed by a consortium in-\ncluding Texas Instruments and MIT, from which it derives its name. It was designed\nto provide data for the acquisition of acoustic-phonetic knowledge and to support the\ndevelopment and evaluation of automatic speech recognition systems.",
              "level": -1,
              "page": 429,
              "reading_order": 7,
              "bbox": [
                97,
                591,
                585,
                675
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_429_order_1",
          "label": "para",
          "text": "Structured collections of annotated linguistic data are essential in most areas of NLP;\nhowever, we still face many obstacles in using them. The goal of this chapter is to answer\nthe following questions:",
          "level": -1,
          "page": 429,
          "reading_order": 1,
          "bbox": [
            97,
            277,
            585,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_429_order_2",
          "label": "para",
          "text": "1. How do we design a new language resource and ensure that its coverage, balance,\nand documentation support a wide range of uses?",
          "level": -1,
          "page": 429,
          "reading_order": 2,
          "bbox": [
            100,
            340,
            584,
            371
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_429_order_3",
          "label": "para",
          "text": "2. When existing data is in the wrong format for some analysis tool, how can we\nconvert it to a suitable format?",
          "level": -1,
          "page": 429,
          "reading_order": 3,
          "bbox": [
            100,
            376,
            585,
            405
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_429_order_4",
          "label": "para",
          "text": "3. What is a good way to document the existence of a resource we have created so\nthat others can easily find it?",
          "level": -1,
          "page": 429,
          "reading_order": 4,
          "bbox": [
            100,
            412,
            584,
            448
          ],
          "section_number": "3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_429_order_5",
          "label": "para",
          "text": "Along the way, we will study the design of existing corpora, the typical workflow for\ncreating a corpus, and the life cycle of a corpus. As in other chapters, there will be many\nexamples drawn from practical experience managing linguistic data, including data\nthat has been collected in the course of linguistic fieldwork, laboratory work, and web\ncrawling.",
          "level": -1,
          "page": 429,
          "reading_order": 5,
          "bbox": [
            97,
            456,
            585,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_431_order_5",
      "label": "sec",
      "text": "Notable Design Features",
      "level": 1,
      "page": 431,
      "reading_order": 5,
      "bbox": [
        98,
        421,
        261,
        444
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_431_order_6",
          "label": "para",
          "text": "TIMIT illustrates several key features of corpus design. First, the corpus contains two\nlayers of annotation, at the phonetic and orthographic levels. In general, a text or speech\ncorpus may be annotated at many different linguistic levels, including morphological,\nsyntactic, and discourse levels. Moreover, even at a given level there may be different\nlabeling schemes or even disagreement among annotators, such that we want to rep-\nresent multiple versions. A second property of TIMIT is its balance across multiple\ndimensions of variation, for coverage of dialect regions and diphones. The inclusion of\nspeaker demographics brings in many more independent variables that may help to\naccount for variation in the data, and which facilitate later uses of the corpus for pur-\nposes that were not envisaged when the corpus was created, such as sociolinguistics.\nA third property is that there is a sharp division between the original linguistic event\ncaptured as an audio recording and the annotations of that event. The same holds true\nof text corpora, in the sense that the original text usually has an external source, and\nis considered to be an immutable artifact. Any transformations of that artifact which\ninvolve human judgment—even something as simple as tokenization—are subject to\nlater revision; thus it is important to retain the source material in a form that is as close\nto the original as possible.",
          "level": -1,
          "page": 431,
          "reading_order": 6,
          "bbox": [
            97,
            453,
            586,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_431_order_7",
          "label": "para",
          "text": "A fourth feature of TIMIT is the hierarchical structure of the corpus. With 4 files per\nsentence, and 10 sentences for each of 500 speakers, there are 20,000 files. These are\norganized into a tree structure, shown schematically in Figure 11 - 2 . At the top level",
          "level": -1,
          "page": 431,
          "reading_order": 7,
          "bbox": [
            97,
            741,
            585,
            789
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_431_order_8",
          "label": "foot",
          "text": "1.1 Corpus Structure: A Case Study | 409",
          "level": -1,
          "page": 431,
          "reading_order": 8,
          "bbox": [
            413,
            824,
            585,
            842
          ],
          "section_number": "1.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_432_order_0",
          "label": "figure",
          "text": "Figure 11-2. Structure of the published TIMIT Corpus: The CD-ROM contains doc, train, and test\ndirectories at the top level; the train and test directories both have eight sub-directories, one per dialect\nregion; each of these contains further subdirectories, one per speaker; the contents of the directory for\nfemale speaker aks0 are listed, showing 10 wav files accompanied by a text transcription, a word-\naligned transcription, and a phonetic transcription. [IMAGE: ![Figure](figures/NLTK_page_432_figure_000.png)]",
          "level": -1,
          "page": 432,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            583,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_432_figure_000.png)",
              "bbox": [
                100,
                71,
                583,
                421
              ],
              "page": 432,
              "reading_order": 0
            },
            {
              "label": "cap",
              "text": "Figure 11-2. Structure of the published TIMIT Corpus: The CD-ROM contains doc, train, and test\ndirectories at the top level; the train and test directories both have eight sub-directories, one per dialect\nregion; each of these contains further subdirectories, one per speaker; the contents of the directory for\nfemale speaker aks0 are listed, showing 10 wav files accompanied by a text transcription, a word-\naligned transcription, and a phonetic transcription.",
              "bbox": [
                96,
                421,
                584,
                497
              ],
              "page": 432,
              "reading_order": 1
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_432_order_2",
          "label": "para",
          "text": "there is a split between training and testing sets, which gives away its intended use for\ndeveloping and evaluating statistical models.",
          "level": -1,
          "page": 432,
          "reading_order": 2,
          "bbox": [
            97,
            501,
            585,
            538
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_432_order_3",
          "label": "para",
          "text": "Finally, notice that even though TIMIT is a speech corpus, its transcriptions and asso-\nciated data are just text, and can be processed using programs just like any other text\ncorpus. Therefore, many of the computational methods described in this book are ap-\nplicable. Moreover, notice that all of the data types included in the TIMIT Corpus fall\ninto the two basic categories of lexicon and text, which we will discuss later. Even the\nspeaker demographics data is just another instance of the lexicon data type.",
          "level": -1,
          "page": 432,
          "reading_order": 3,
          "bbox": [
            97,
            546,
            585,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_432_order_4",
          "label": "para",
          "text": "This last observation is less surprising when we consider that text and record structures\nare the primary domains for the two subfields of computer science that focus on data\nmanagement, namely text retrieval and databases. A notable feature of linguistic data\nmanagement is that it usually brings both data types together, and that it can draw on\nresults and techniques from both fields.",
          "level": -1,
          "page": 432,
          "reading_order": 4,
          "bbox": [
            100,
            654,
            585,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_432_order_5",
          "label": "foot",
          "text": "410 | Chapter 11: Managing Linguistic Data",
          "level": -1,
          "page": 432,
          "reading_order": 5,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "410",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_433_order_0",
      "label": "sec",
      "text": "Fundamental Data Types",
      "level": 1,
      "page": 433,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        270,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_434_order_1",
          "label": "sub_sec",
          "text": "11.2 The Life Cycle of a Corpus",
          "level": 2,
          "page": 434,
          "reading_order": 1,
          "bbox": [
            98,
            232,
            342,
            259
          ],
          "section_number": "11.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_434_order_2",
              "label": "para",
              "text": "Corpora are not born fully formed, but involve careful preparation and input from\nmany people over an extended period. Raw data needs to be collected, cleaned up,\ndocumented, and stored in a systematic structure. Various layers of annotation might\nbe applied, some requiring specialized knowledge of the morphology or syntax of the\nlanguage. Success at this stage depends on creating an efficient workflow involving\nappropriate tools and format converters. Quality control procedures can be put in place\nto find inconsistencies in the annotations, and to ensure the highest possible level of\ninter-annotator agreement. Because of the scale and complexity of the task, large cor-\npora may take years to prepare, and involve tens or hundreds of person-years of effort.\nIn this section, we briefly review the various stages in the life cycle of a corpus.",
              "level": -1,
              "page": 434,
              "reading_order": 2,
              "bbox": [
                97,
                259,
                586,
                430
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_433_order_1",
          "label": "para",
          "text": "Despite its complexity, the TIMIT Corpus contains only two fundamental data types,\nnamely lexicons and texts. As we saw in Chapter 2, most lexical resources can be rep-\nresented using a record structure, i.e., a key plus one or more fields, as shown in\nFigure 11 - 3 . A lexical resource could be a conventional dictionary or comparative\nwordlist, as illustrated. It could also be a phrasal lexicon, where the key field is a phrase\nrather than a single word. A thesaurus also consists of record-structured data, where\nwe look up entries via non-key fields that correspond to topics. We can also construct\nspecial tabulations (known as paradigms) to illustrate contrasts and systematic varia-\ntion, as shown in Figure 11 - 3 for three verbs. TIMIT's speaker table is also a kind of\nlexicon.",
          "level": -1,
          "page": 433,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            586,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_433_order_2",
          "label": "figure",
          "text": "Figure 11-3. Basic linguistic data types—lexicons and texts: Amid their diversity, lexicons have a\nrecord structure, whereas annotated texts have a temporal organization. [IMAGE: ![Figure](figures/NLTK_page_433_figure_002.png)]",
          "level": -1,
          "page": 433,
          "reading_order": 2,
          "bbox": [
            100,
            277,
            583,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_433_figure_002.png)",
              "bbox": [
                100,
                277,
                583,
                627
              ],
              "page": 433,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Figure 11-3. Basic linguistic data types—lexicons and texts: Amid their diversity, lexicons have a\nrecord structure, whereas annotated texts have a temporal organization.",
              "bbox": [
                97,
                636,
                584,
                666
              ],
              "page": 433,
              "reading_order": 3
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_433_order_4",
          "label": "para",
          "text": "At the most abstract level, a text is a representation of a real or fictional speech event,\nand the time-course of that event carries over into the text itself. A text could be a small\nunit, such as a word or sentence, or a complete narrative or dialogue. It may come with\nannotations such as part-of-speech tags, morphological analysis, discourse structure,\nand so forth. As we saw in the IOB tagging technique (Chapter 7), it is possible to\nrepresent higher-level constituents using tags on individual words. Thus the abstraction\nof text shown in Figure 11 - 3 is sufficient.",
          "level": -1,
          "page": 433,
          "reading_order": 4,
          "bbox": [
            97,
            680,
            584,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_433_order_5",
          "label": "foot",
          "text": "1.1 Corpus Structure: A Case Study | 411",
          "level": -1,
          "page": 433,
          "reading_order": 5,
          "bbox": [
            413,
            824,
            584,
            842
          ],
          "section_number": "1.1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_434_order_0",
          "label": "para",
          "text": "Despite the complexities and idiosyncrasies of individual corpora, at base they are col-\nlections of texts together with record-structured data. The contents of a corpus are\noften biased toward one or the other of these types. For example, the Brown Corpus\ncontains 500 text files, but we still use a table to relate the files to 15 different genres.\nAt the other end of the spectrum, WordNet contains 117,659 synset records, yet it\nincorporates many example sentences (mini-texts) to illustrate word usages. TIMIT is\nan interesting midpoint on this spectrum, containing substantial free-standing material\nof both the text and lexicon types.",
          "level": -1,
          "page": 434,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_434_order_3",
      "label": "sec",
      "text": "Three Corpus Creation Scenarios",
      "level": 1,
      "page": 434,
      "reading_order": 3,
      "bbox": [
        97,
        439,
        310,
        461
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_434_order_4",
          "label": "para",
          "text": "In one type of corpus, the design unfolds over in the course of the creator's explorations.\nThis is the pattern typical of traditional “field linguistics,” in which material from elic-\nitation sessions is analyzed as it is gathered, with tomorrow's elicitation often based on\nquestions that arise in analyzing today’s. The resulting corpus is then used during sub-\nsequent years of research, and may serve as an archival resource indefinitely. Comput-\nerization is an obvious boon to work of this type, as exemplified by the popular program\nShoebox, now over two decades old and re-released as Toolbox (see Section 2.4 ). Other\nsoftware tools, even simple word processors and spreadsheets, are routinely used to\nacquire the data. In the next section, we will look at how to extract data from these\nsources.",
          "level": -1,
          "page": 434,
          "reading_order": 4,
          "bbox": [
            97,
            465,
            585,
            630
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_434_order_5",
          "label": "para",
          "text": "Another corpus creation scenario is typical of experimental research where a body of\ncarefully designed material is collected from a range of human subjects, then analyzed\nto evaluate a hypothesis or develop a technology. It has become common for such\ndatabases to be shared and reused within a laboratory or company, and often to be\npublished more widely. Corpora of this type are the basis of the “commontask” method\nof research management, which over the past two decades has become the norm in\ngovernment-funded research programs in language technology. We have already en-\ncountered many such corpora in the earlier chapters; we will see how to write Python",
          "level": -1,
          "page": 434,
          "reading_order": 5,
          "bbox": [
            97,
            636,
            586,
            772
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_434_order_6",
          "label": "foot",
          "text": "412 | Chapter 11: Managing Linguistic Data",
          "level": -1,
          "page": 434,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "412",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_435_order_0",
          "label": "para",
          "text": "programs to implement the kinds of curation tasks that are necessary before such cor-\npora are published.",
          "level": -1,
          "page": 435,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_435_order_1",
          "label": "para",
          "text": "Finally, there are efforts to gather a “ reference corpus ” for a particular language, such\nas the American National Corpus (ANC) and the British National Corpus (BNC). Here\nthe goal has been to produce a comprehensive record of the many forms, styles, and\nuses of a language. Apart from the sheer challenge of scale, there is a heavy reliance on\nautomatic annotation tools together with post-editing to fix any errors. However, we\ncan write programs to locate and repair the errors, and also to analyze the corpus for\nbalance.",
          "level": -1,
          "page": 435,
          "reading_order": 1,
          "bbox": [
            97,
            115,
            585,
            225
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_435_order_2",
      "label": "sec",
      "text": "Quality Control",
      "level": 1,
      "page": 435,
      "reading_order": 2,
      "bbox": [
        97,
        241,
        198,
        263
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_435_order_6",
          "label": "sub_sec",
          "text": "Caution!",
          "level": 2,
          "page": 435,
          "reading_order": 6,
          "bbox": [
            171,
            633,
            209,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_435_order_7",
              "label": "para",
              "text": "Care should be exercised when interpreting an inter-annotator agree-\nment score, since annotation tasks vary greatly in their difficulty. For\nexample, 90 % agreement would be a terrible score for part-of-speech\ntagging, but an exceptional score for semantic role labeling.",
              "level": -1,
              "page": 435,
              "reading_order": 7,
              "bbox": [
                171,
                652,
                530,
                709
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_435_order_8",
              "label": "para",
              "text": "The Kappa coefficient K measures agreement between two people making category\njudgments, correcting for expected chance agreement. For example, suppose an item\nis to be annotated, and four coding options are equally likely. In this case, two people\ncoding randomly would be expected to agree 25% of the time. Thus, an agreement of",
              "level": -1,
              "page": 435,
              "reading_order": 8,
              "bbox": [
                97,
                733,
                586,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_435_order_9",
              "label": "foot",
              "text": "11.2 The Life Cycle of a Corpus | 413",
              "level": -1,
              "page": 435,
              "reading_order": 9,
              "bbox": [
                430,
                824,
                584,
                842
              ],
              "section_number": "11.2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_436_order_0",
              "label": "figure",
              "text": "Figure 11-4. Three segmentations of a sequence: The small rectangles represent characters, words,\nsentences, in short, any sequence which might be divided into linguistic units; $S_{1}$ and $S_{2}$ are in close\nagreement, but both differ significantly from $S_{3}$ . [IMAGE: ![Figure](figures/NLTK_page_436_figure_000.png)]",
              "level": -1,
              "page": 436,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                583,
                197
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_436_figure_000.png)",
                  "bbox": [
                    100,
                    71,
                    583,
                    197
                  ],
                  "page": 436,
                  "reading_order": 0
                },
                {
                  "label": "cap",
                  "text": "Figure 11-4. Three segmentations of a sequence: The small rectangles represent characters, words,\nsentences, in short, any sequence which might be divided into linguistic units; $S_{1}$ and $S_{2}$ are in close\nagreement, but both differ significantly from $S_{3}$ .",
                  "bbox": [
                    97,
                    206,
                    585,
                    250
                  ],
                  "page": 436,
                  "reading_order": 1
                }
              ],
              "is_merged": true
            },
            {
              "id": "page_436_order_2",
              "label": "para",
              "text": "25% will be assigned K = 0, and better levels of agreement will be scaled accordingly.\nFor an agreement of 50%, we would get K = 0.333, as 50 is a third of the way from 25\nto 100. Many other agreement measures exist; see help(nltk.metrics.agreement) for\ndetails.",
              "level": -1,
              "page": 436,
              "reading_order": 2,
              "bbox": [
                97,
                259,
                585,
                322
              ],
              "section_number": "25",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_436_order_3",
              "label": "para",
              "text": "We can also measure the agreement between two independent segmentations of lan-\nguage input, e.g., for tokenization, sentence segmentation, and named entity recogni-\ntion. In Figure 11 - 4 we see three possible segmentations of a sequence of items which\nmight have been produced by annotators (or programs). Although none of them agree\nexactly, S $_1$ and S $_2$ are in close agreement, and we would like a suitable measure. Win-\ndowdiff is a simple algorithm for evaluating the agreement of two segmentations by\nrunning a sliding window over the data and awarding partial credit for near misses. If\nwe preprocess our tokens into a sequence of zeros and ones, to record when a token is\nfollowed by a boundary, we can represent the segmentations as strings and apply the\nwindowdiff scorer.",
              "level": -1,
              "page": 436,
              "reading_order": 3,
              "bbox": [
                97,
                331,
                585,
                493
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_436_order_5",
              "label": "para",
              "text": "In this example, the window had a size of 3. The windowdiff computation slides this\nwindow across a pair of strings. At each position it totals up the number of boundaries\nfound inside this window, for both strings, then computes the difference. These dif-\nferences are then summed. We can increase or shrink the window size to control the\nsensitivity of the measure.",
              "level": -1,
              "page": 436,
              "reading_order": 5,
              "bbox": [
                97,
                627,
                585,
                708
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_435_order_3",
          "label": "para",
          "text": "Good tools for automatic and manual preparation of data are essential. However, the\ncreation of a high-quality corpus depends just as much on such mundane things as\ndocumentation, training, and workflow. Annotation guidelines define the task and\ndocument the markup conventions. They may be regularly updated to cover difficult\ncases, along with new rules that are devised to achieve more consistent annotations.\nAnnotators need to be trained in the procedures, including methods for resolving cases\nnot covered in the guidelines. A workflow needs to be established, possibly with sup-\nporting software, to keep track of which files have been initialized, annotated, validated,\nmanually checked, and so on. There may be multiple layers of annotation, provided by\ndifferent specialists. Cases of uncertainty or disagreement may require adjudication.",
          "level": -1,
          "page": 435,
          "reading_order": 3,
          "bbox": [
            97,
            268,
            585,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_435_order_4",
          "label": "para",
          "text": "Large annotation tasks require multiple annotators, which raises the problem of\nachieving consistency. How consistently can a group of annotators perform? We can\neasily measure consistency by having a portion of the source material independently\nannotated by two people. This may reveal shortcomings in the guidelines or differing\nabilities with the annotation task. In cases where quality is paramount, the entire corpus\ncan be annotated twice, and any inconsistencies adjudicated by an expert.",
          "level": -1,
          "page": 435,
          "reading_order": 4,
          "bbox": [
            97,
            439,
            586,
            541
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_435_order_5",
          "label": "para",
          "text": "It is considered best practice to report the inter-annotator agreement that was achieved\nfor a corpus (e.g., by double-annotating 10% of the corpus). This score serves as a\nhelpful upper bound on the expected performance of any automatic system that is\ntrained on this corpus.",
          "level": -1,
          "page": 435,
          "reading_order": 5,
          "bbox": [
            97,
            546,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_436_order_6",
      "label": "sec",
      "text": "Curation Versus Evolution",
      "level": 1,
      "page": 436,
      "reading_order": 6,
      "bbox": [
        97,
        725,
        270,
        743
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_438_order_1",
          "label": "sub_sub_sec",
          "text": "Caution!",
          "level": 3,
          "page": 438,
          "reading_order": 1,
          "bbox": [
            171,
            125,
            209,
            135
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_438_order_2",
              "label": "para",
              "text": "Sometimes an updated corpus contains revisions of base material that\nhas been externally annotated. Tokens might be split or merged, and\nconstituents may have been rearranged. There may not be a one-to-one\ncorrespondence between old and new identifiers. It is better to cause\nstandoff annotations to break on such components of the new version\nthan to silently allow their identifiers to refer to incorrect locations.",
              "level": -1,
              "page": 438,
              "reading_order": 2,
              "bbox": [
                171,
                143,
                530,
                226
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_438_order_3",
          "label": "sub_sec",
          "text": "11.3 Acquiring Data",
          "level": 2,
          "page": 438,
          "reading_order": 3,
          "bbox": [
            98,
            250,
            261,
            278
          ],
          "section_number": "11.3",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_438_order_4",
              "label": "sub_sub_sec",
              "text": "Obtaining Data from the Web",
              "level": 3,
              "page": 438,
              "reading_order": 4,
              "bbox": [
                97,
                294,
                297,
                313
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_438_order_5",
                  "label": "para",
                  "text": "The Web is a rich source of data for language analysis purposes. We have already\ndiscussed methods for accessing individual files, RSS feeds, and search engine results\n(see Section 3.1 ). However, in some cases we want to obtain large quantities of web text.",
                  "level": -1,
                  "page": 438,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    321,
                    585,
                    368
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_438_order_6",
                  "label": "para",
                  "text": "The simplest approach is to obtain a published corpus of web text. The ACL Special\nInterest Group on Web as Corpus (SIGWAC) maintains a list of resources at http://\nwww.sigwac.org.uk/. The advantage of using a well-defined web corpus is that they are\ndocumented, stable, and permit reproducible experimentation.",
                  "level": -1,
                  "page": 438,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    376,
                    585,
                    441
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_438_order_7",
                  "label": "para",
                  "text": "If the desired content is localized to a particular website, there are many utilities for\ncapturing all the accessible contents of a site, such as GNU Wget ( http://www.gnu.org/\nsoftware/wget/ ). For maximal flexibility and control, a web crawler can be used, such\nas Heritrix ( http://crawler.archive.org/ ). Crawlers permit fine-grained control over\nwhere to look, which links to follow, and how to organize the results. For example, if\nwe want to compile a bilingual text collection having corresponding pairs of documents\nin each language, the crawler needs to detect the structure of the site in order to extract\nthe correspondence between the documents, and it needs to organize the downloaded\npages in such a way that the correspondence is captured. It might be tempting to write\nyour own web crawler, but there are dozens of pitfalls having to do with detecting\nMIME types, converting relative to absolute URLs, avoiding getting trapped in cyclic\nlink structures, dealing with network latencies, avoiding overloading the site or being\nbanned from accessing the site, and so on.",
                  "level": -1,
                  "page": 438,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    448,
                    586,
                    664
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            },
            {
              "id": "page_438_order_8",
              "label": "sub_sub_sec",
              "text": "Obtaining Data from Word Processor Files",
              "level": 3,
              "page": 438,
              "reading_order": 8,
              "bbox": [
                97,
                679,
                377,
                698
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_438_order_9",
                  "label": "para",
                  "text": "Word processing software is often used in the manual preparation of texts and lexicons\nin projects that have limited computational infrastructure. Such projects often provide\ntemplates for data entry, though the word processing software does not ensure that the\ndata is correctly structured. For example, each text may be required to have a title and\ndate. Similarly, each lexical entry may have certain obligatory fields. As the data grows",
                  "level": -1,
                  "page": 438,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    707,
                    585,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_438_order_10",
                  "label": "foot",
                  "text": "416 | Chapter 11: Managing Linguistic Data",
                  "level": -1,
                  "page": 438,
                  "reading_order": 10,
                  "bbox": [
                    97,
                    824,
                    288,
                    842
                  ],
                  "section_number": "416",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_439_order_0",
                  "label": "para",
                  "text": "in size and complexity, a larger proportion of time may be spent maintaining its con-\nsistency.",
                  "level": -1,
                  "page": 439,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    584,
                    107
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_439_order_1",
                  "label": "para",
                  "text": "How can we extract the content of such files so that we can manipulate it in external\nprograms? Moreover, how can we validate the content of these files to help authors\ncreate well-structured data, so that the quality of the data can be maximized in the\ncontext of the original authoring process?",
                  "level": -1,
                  "page": 439,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    115,
                    585,
                    179
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_439_order_2",
                  "label": "para",
                  "text": "Consider a dictionary in which each entry has a part-of-speech field, drawn from a set\nof 20 possibilities, displayed after the pronunciation field, and rendered in 11-point\nbold type. No conventional word processor has search or macro functions capable of\nverifying that all part-of-speech fields have been correctly entered and displayed. This\ntask requires exhaustive manual checking. If the word processor permits the document\nto be saved in a non-proprietary format, such as text, HTML, or XML, we can some-\ntimes write programs to do this checking automatically.",
                  "level": -1,
                  "page": 439,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    188,
                    585,
                    304
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_439_order_3",
                  "label": "para",
                  "text": "Consider the following fragment of a lexical entry: “sleep [sli:p] v.i. condition of body\nand mind...”. We can key in such text using MSWord, then “Save as Web Page,” then\ninspect the resulting HTML file:",
                  "level": -1,
                  "page": 439,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    304,
                    584,
                    358
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_439_order_5",
                  "label": "para",
                  "text": "Observe that the entry is represented as an HTML paragraph, using the <p> element,\nand that the part of speech appears inside a <span style='font-size:11.0pt'> element.\nThe following program defines the set of legal parts-of-speech, legal_pos. Then it ex­\ntracts all 11­point content from the dict.htm file and stores it in the set used_pos. Observe\nthat the search pattern contains a parenthesized sub­expression; only the material that\nmatches this subexpression is returned by re.findall. Finally, the program constructs\nthe set of illegal parts-of-speech as the set difference between used_pos and legal_pos:",
                  "level": -1,
                  "page": 439,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    474,
                    585,
                    593
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_439_order_7",
                  "label": "para",
                  "text": "This simple program represents the tip of the iceberg. We can develop sophisticated\ntools to check the consistency of word processor files, and report errors so that the\nmaintainer of the dictionary can correct the original file using the original word\nprocessor .",
                  "level": -1,
                  "page": 439,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    698,
                    585,
                    765
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_439_order_8",
                  "label": "foot",
                  "text": "11.3 Acquiring Data | 417",
                  "level": -1,
                  "page": 439,
                  "reading_order": 8,
                  "bbox": [
                    467,
                    824,
                    585,
                    842
                  ],
                  "section_number": "11.3",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_440_order_0",
                  "label": "para",
                  "text": "Once we know the data is correctly formatted, we can write other programs to convert\nthe data into a different format. The program in Example 11-1 strips out the HTML\nmarkup using nltk.clean_html(), extracts the words and their pronunciations, and\ngenerates output in “comma-separated value” (CSV) format.",
                  "level": -1,
                  "page": 440,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_440_order_1",
                  "label": "para",
                  "text": "Example 11-1. Converting HTML created by Microsoft Word into comma-separated values.",
                  "level": -1,
                  "page": 440,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    152,
                    549,
                    170
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": [
        {
          "id": "page_436_order_7",
          "label": "para",
          "text": "As large corpora are published, researchers are increasingly likely to base their inves-\ntigations on balanced, focused subsets that were derived from corpora produced for",
          "level": -1,
          "page": 436,
          "reading_order": 7,
          "bbox": [
            97,
            752,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_436_order_8",
          "label": "foot",
          "text": "414 | Chapter 11: Managing Linguistic Data",
          "level": -1,
          "page": 436,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "414",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_437_order_0",
          "label": "para",
          "text": "entirely different reasons. For instance, the Switchboard database, originally collected\nfor speaker identification research, has since been used as the basis for published studies\nin speech recognition, word pronunciation, disfluency, syntax, intonation, and dis-\ncourse structure. The motivations for recycling linguistic corpora include the desire to\nsave time and effort, the desire to work on material available to others for replication,\nand sometimes a desire to study more naturalistic forms of linguistic behavior than\nwould be possible otherwise. The process of choosing a subset for such a study may\ncount as a non-trivial contribution in itself.",
          "level": -1,
          "page": 437,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_437_order_1",
          "label": "para",
          "text": "In addition to selecting an appropriate subset of a corpus, this new work could involve\nreformatting a text file (e.g., converting to XML), renaming files, retokenizing the text,\nselecting a subset of the data to enrich, and so forth. Multiple research groups might\ndo this work independently, as illustrated in Figure 11-5 . At a later date, should some-\none want to combine sources of information from different versions, the task will\nprobably be extremely onerous.",
          "level": -1,
          "page": 437,
          "reading_order": 1,
          "bbox": [
            97,
            214,
            585,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_437_order_2",
          "label": "figure",
          "text": "Figure 11-5. Evolution of a corpus over time: After a corpus is published, research groups will use it\nindependently, selecting and enriching different pieces; later research that seeks to integrate separate\nannotations confronts the difficult challenge of aligning the annotations. [IMAGE: ![Figure](figures/NLTK_page_437_figure_002.png)]",
          "level": -1,
          "page": 437,
          "reading_order": 2,
          "bbox": [
            100,
            322,
            583,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_437_figure_002.png)",
              "bbox": [
                100,
                322,
                583,
                439
              ],
              "page": 437,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Figure 11-5. Evolution of a corpus over time: After a corpus is published, research groups will use it\nindependently, selecting and enriching different pieces; later research that seeks to integrate separate\nannotations confronts the difficult challenge of aligning the annotations.",
              "bbox": [
                97,
                448,
                585,
                494
              ],
              "page": 437,
              "reading_order": 3
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_437_order_4",
          "label": "para",
          "text": "The task of using derived corpora is made even more difficult by the lack of any record\nabout how the derived version was created, and which version is the most up-to-date.",
          "level": -1,
          "page": 437,
          "reading_order": 4,
          "bbox": [
            100,
            510,
            584,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_437_order_5",
          "label": "para",
          "text": "An alternative to this chaotic situation is for a corpus to be centrally curated, and for\ncommittees of experts to revise and extend it at periodic intervals, considering sub-\nmissions from third parties and publishing new releases from time to time. Print dic-\ntionaries and national corpora may be centrally curated in this way. However, for most\ncorpora this model is simply impractical.",
          "level": -1,
          "page": 437,
          "reading_order": 5,
          "bbox": [
            97,
            555,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_437_order_6",
          "label": "para",
          "text": "A middle course is for the original corpus publication to have a scheme for identifying\nany sub-part. Each sentence, tree, or lexical entry could have a globally unique identi-\nfier, and each token, node, or field (respectively) could have a relative offset. Annota-\ntions, including segmentations, could reference the source using this identifier scheme\n(a method which is known as standoff annotation ). This way, new annotations could\nbe distributed independently of the source, and multiple independent annotations of\nthe same source could be compared and updated without touching the source.",
          "level": -1,
          "page": 437,
          "reading_order": 6,
          "bbox": [
            97,
            645,
            586,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_437_order_7",
          "label": "para",
          "text": "If the corpus publication is provided in multiple versions, the version number or date\ncould be part of the identification scheme. A table of correspondences between",
          "level": -1,
          "page": 437,
          "reading_order": 7,
          "bbox": [
            97,
            768,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_437_order_8",
          "label": "foot",
          "text": "11.2 The Life Cycle of a Corpus | 415",
          "level": -1,
          "page": 437,
          "reading_order": 8,
          "bbox": [
            430,
            824,
            585,
            842
          ],
          "section_number": "11.2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_438_order_0",
          "label": "para",
          "text": "identifiers across editions of the corpus would permit any standoff annotations to be\nupdated easily.",
          "level": -1,
          "page": 438,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_440_order_4",
      "label": "sec",
      "text": "Obtaining Data from Spreadsheets and Databases",
      "level": 1,
      "page": 440,
      "reading_order": 4,
      "bbox": [
        97,
        349,
        427,
        376
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_440_order_5",
          "label": "para",
          "text": "Spreadsheets are often used for acquiring wordlists or paradigms. For example, a com-\nparative wordlist may be created using a spreadsheet, with a row for each cognate set\nand a column for each language (see nltk.corpus.swadesh and www.rosettapro\nject.org). Most spreadsheet software can export their data in CSV format. As we will\nsee later, it is easy for Python programs to access these using the csv module.",
          "level": -1,
          "page": 440,
          "reading_order": 5,
          "bbox": [
            97,
            376,
            585,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_440_order_6",
          "label": "para",
          "text": "Sometimes lexicons are stored in a full-fledged relational database. When properly\nnormalized, these databases can ensure the validity of the data. For example, we can\nrequire that all parts-of-speech come from a specified vocabulary by declaring that the\npart-of-speech field is an enumerated type or a foreign key that references a separate\npart-of-speech table. However, the relational model requires the structure of the data\n(the schema) be declared in advance, and this runs counter to the dominant approach\nto structuring linguistic data, which is highly exploratory. Fields which were assumed\nto be obligatory and unique often turn out to be optional and repeatable. A relational\ndatabase can accommodate this when it is fully known in advance; however, if it is not,\nor if just about every property turns out to be optional or repeatable, the relational\napproach is unworkable.",
          "level": -1,
          "page": 440,
          "reading_order": 6,
          "bbox": [
            91,
            465,
            585,
            650
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_440_order_7",
          "label": "para",
          "text": "Nevertheless, when our goal is simply to extract the contents from a database, it is\nenough to dump out the tables (or SQL query results) in CSV format and load them\ninto our program. Our program might perform a linguistically motivated query that\ncannot easily be expressed in SQL, e.g., select all words that appear in example sentences\nfor which no dictionary entry is provided . For this task, we would need to extract enough\ninformation from a record for it to be uniquely identified, along with the headwords\nand example sentences. Let's suppose this information was now available in a CSV file\ndict.csv :",
          "level": -1,
          "page": 440,
          "reading_order": 7,
          "bbox": [
            91,
            654,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_440_order_8",
          "label": "foot",
          "text": "418 | Chapter 11: Managing Linguistic Data",
          "level": -1,
          "page": 440,
          "reading_order": 8,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "418",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_441_order_1",
          "label": "para",
          "text": "Now we can express this query as shown here",
          "level": -1,
          "page": 441,
          "reading_order": 1,
          "bbox": [
            98,
            122,
            359,
            136
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_441_order_3",
          "label": "para",
          "text": "This information would then guide the ongoing work to enrich the lexicon, work that\nupdates the content of the relational database.",
          "level": -1,
          "page": 441,
          "reading_order": 3,
          "bbox": [
            97,
            258,
            585,
            288
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_441_order_4",
      "label": "sec",
      "text": "Converting Data Formats",
      "level": 1,
      "page": 441,
      "reading_order": 4,
      "bbox": [
        97,
        304,
        270,
        323
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_441_order_5",
          "label": "para",
          "text": "Annotated linguistic data rarely arrives in the most convenient format, and it is often\nnecessary to perform various kinds of format conversion. Converting between character\nencodings has already been discussed (see Section 3.3 ). Here we focus on the structure\nof the data.",
          "level": -1,
          "page": 441,
          "reading_order": 5,
          "bbox": [
            97,
            331,
            585,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_441_order_6",
          "label": "para",
          "text": "In the simplest case, the input and output formats are isomorphic. For instance, we\nmight be converting lexical data from Toolbox format to XML, and it is straightforward\nto transliterate the entries one at a time (Section 11.4 ). The structure of the data is\nreflected in the structure of the required program: a for loop whose body takes care of\na single entry.",
          "level": -1,
          "page": 441,
          "reading_order": 6,
          "bbox": [
            97,
            403,
            586,
            485
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_441_order_7",
          "label": "para",
          "text": "In another common case, the output is a digested form of the input, such as an inverted\nfile index. Here it is necessary to build an index structure in memory (see Example 4.8),\nthen write it to a file in the desired format. The following example constructs an index\nthat maps the words of a dictionary definition to the corresponding lexeme ❶ for each\nlexical entry ❷, having tokenized the definition text ❸, and discarded short words ❹.\nOnce the index has been constructed, we open a file and then iterate over the index\nentries, to write out the lines in the required format",
          "level": -1,
          "page": 441,
          "reading_order": 7,
          "bbox": [
            97,
            492,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_441_order_9",
          "label": "para",
          "text": "The resulting file dict.idx contains the following lines. (With a larger dictionary, we\nwould expect to find multiple lexemes listed for each index entry.)",
          "level": -1,
          "page": 441,
          "reading_order": 9,
          "bbox": [
            97,
            752,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_441_order_10",
          "label": "foot",
          "text": "11.3 Acquiring Data | 419",
          "level": -1,
          "page": 441,
          "reading_order": 10,
          "bbox": [
            467,
            824,
            585,
            842
          ],
          "section_number": "11.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_442_order_1",
          "label": "para",
          "text": "In some cases, the input and output data both consist of two or more dimensions. For\ninstance, the input might be a set of files, each containing a single column of word\nfrequency data. The required output might be a two-dimensional table in which the\noriginal columns appear as rows. In such cases we populate an internal data structure\nby filling up one column at a time, then read off the data one row at a time as we write\ndata to the output file.",
          "level": -1,
          "page": 442,
          "reading_order": 1,
          "bbox": [
            97,
            224,
            585,
            323
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_442_order_2",
          "label": "para",
          "text": "In the most vexing cases, the source and target formats have slightly different coverage\nof the domain, and information is unavoidably lost when translating between them.\nFor example, we could combine multiple Toolbox files to create a single CSV file con-\ntaining a comparative wordlist, losing all but the \\1x field of the input files. If the CSV\nfile was later modified, it would be a labor-intensive process to inject the changes into\nthe original Toolbox files. A partial solution to this “round-tripping” problem is to\nassociate explicit identifiers with each linguistic object, and to propagate the identifiers\nwith the objects.",
          "level": -1,
          "page": 442,
          "reading_order": 2,
          "bbox": [
            97,
            331,
            585,
            462
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_442_order_3",
      "label": "sec",
      "text": "Deciding Which Layers of Annotation to Include",
      "level": 1,
      "page": 442,
      "reading_order": 3,
      "bbox": [
        98,
        474,
        413,
        497
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_442_order_5",
          "label": "sub_sec",
          "text": "Word tokenization",
          "level": 2,
          "page": 442,
          "reading_order": 5,
          "bbox": [
            98,
            680,
            207,
            692
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_442_order_6",
              "label": "para",
              "text": "The orthographic form of text does not unambiguously identify its tokens. A to-\nkenized and normalized version, in addition to the conventional orthographic ver-\nsion, may be a very convenient resource.",
              "level": -1,
              "page": 442,
              "reading_order": 6,
              "bbox": [
                122,
                696,
                585,
                743
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_442_order_7",
          "label": "sub_sec",
          "text": "Sentence segmentation",
          "level": 2,
          "page": 442,
          "reading_order": 7,
          "bbox": [
            97,
            743,
            225,
            764
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_442_order_8",
              "label": "para",
              "text": "As we saw in Chapter 3, sentence segmentation can be more difficult than it seems.\nSome corpora therefore use explicit annotations to mark sentence segmentation.",
              "level": -1,
              "page": 442,
              "reading_order": 8,
              "bbox": [
                122,
                764,
                584,
                797
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_442_order_9",
              "label": "foot",
              "text": "420 | Chapter11: Managing Linguistic Data",
              "level": -1,
              "page": 442,
              "reading_order": 9,
              "bbox": [
                97,
                824,
                288,
                842
              ],
              "section_number": "420",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_443_order_0",
          "label": "sub_sec",
          "text": "Paragraph segmentation",
          "level": 2,
          "page": 443,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            234,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_443_order_1",
              "label": "para",
              "text": "paragraphs and other structural elements (headings, chapters, etc.) may be explic-\nitly annotated.",
              "level": -1,
              "page": 443,
              "reading_order": 1,
              "bbox": [
                125,
                89,
                585,
                125
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_443_order_2",
          "label": "sub_sec",
          "text": "Part-of-speech",
          "level": 2,
          "page": 443,
          "reading_order": 2,
          "bbox": [
            98,
            125,
            180,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_443_order_3",
              "label": "para",
              "text": "The syntactic category of each word in a document",
              "level": -1,
              "page": 443,
              "reading_order": 3,
              "bbox": [
                118,
                143,
                413,
                161
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_443_order_4",
          "label": "sub_sec",
          "text": "Syntactic structure",
          "level": 2,
          "page": 443,
          "reading_order": 4,
          "bbox": [
            97,
            161,
            207,
            180
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_443_order_5",
              "label": "para",
              "text": "A tree structure showing the constituent structure of a sentence.",
              "level": -1,
              "page": 443,
              "reading_order": 5,
              "bbox": [
                118,
                180,
                487,
                197
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_443_order_6",
          "label": "sub_sec",
          "text": "Shallow semantics",
          "level": 2,
          "page": 443,
          "reading_order": 6,
          "bbox": [
            97,
            197,
            199,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_443_order_7",
              "label": "para",
              "text": "Named entity and coreference annotations, and semantic role labels.",
              "level": -1,
              "page": 443,
              "reading_order": 7,
              "bbox": [
                122,
                215,
                513,
                232
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_443_order_8",
          "label": "sub_sec",
          "text": "Dialogue and discourse\nDialogue act tags a",
          "level": 2,
          "page": 443,
          "reading_order": 8,
          "bbox": [
            98,
            232,
            227,
            271
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_443_order_10",
              "label": "para",
              "text": "Unfortunately, there is not much consistency between existing corpora in how they\nrepresent their annotations. However, two general classes of annotation representation\nshould be distinguished. Inline annotation modifies the original document by insert-\ning special symbols or control sequences that carry the annotated information. For\nexample, when part-of-speech tagging a document, the string \"fly\" might be replaced\nwith the string \"fly/NN\" , to indicate that the word fly is a noun in this context. In\ncontrast, standoff annotation does not modify the original document, but instead\ncreates a new file that adds annotation information using pointers that reference the\noriginal document. For example, this new document might contain the string \"<token\nid=8 pos='NN'/>\" , to indicate that token 8 is a noun.",
              "level": -1,
              "page": 443,
              "reading_order": 10,
              "bbox": [
                97,
                277,
                585,
                448
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_442_order_4",
          "label": "para",
          "text": "Published corpora vary greatly in the richness of the information they contain. At a\nminimum, a corpus will typically contain at least a sequence of sound or orthographic\nsymbols. At the other end of the spectrum, a corpus could contain a large amount of\ninformation about the syntactic structure, morphology, prosody, and semantic content\nof every sentence, plus annotation of discourse relations or dialogue acts. These extra\nlayers of annotation may be just what someone needs for performing a particular data\nanalysis task. For example, it may be much easier to find a given linguistic pattern if\nwe can search for specific syntactic structures; and it may be easier to categorize a\nlinguistic pattern if every word has been tagged with its sense. Here are some commonly\nprovided annotation layers:",
          "level": -1,
          "page": 442,
          "reading_order": 4,
          "bbox": [
            97,
            501,
            586,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_443_order_11",
      "label": "sec",
      "text": "Standards and Tools",
      "level": 1,
      "page": 443,
      "reading_order": 11,
      "bbox": [
        97,
        456,
        234,
        476
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_443_order_12",
          "label": "para",
          "text": "For a corpus to be widely useful, it needs to be available in a widely supported format.\nHowever, the cutting edge of NLP research depends on new kinds of annotations,\nwhich by definition are not widely supported. In general, adequate tools for creation,\npublication, and use of linguistic data are not widely available. Most projects must\ndevelop their own set of tools for internal use, which is no help to others who lack the\nnecessary resources. Furthermore, we do not have adequate, generally accepted stand-\nards for expressing the structure and content of corpora. Without such standards, gen-\neral-purpose tools are impossible—though at the same time, without available tools,\nadequate standards are unlikely to be developed, used, and accepted.",
          "level": -1,
          "page": 443,
          "reading_order": 12,
          "bbox": [
            97,
            483,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_443_order_13",
          "label": "para",
          "text": "One response to this situation has been to forge ahead with developing a generic format\nthat is sufficiently expressive to capture a wide variety of annotation types (see Sec-\ntion 11.8 for examples). The challenge for NLP is to write programs that cope with the\ngenerality of such formats. For example, if the programming task involves tree data,\nand the file format permits arbitrary directed graphs, then input data must be validated\nto check for tree properties such as rootedness, connectedness, and acyclicity. If the\ninput files contain other layers of annotation, the program would need to know how\nto ignore them when the data was loaded, but not invalidate or obliterate those layers\nwhen the tree data was saved back to the file.",
          "level": -1,
          "page": 443,
          "reading_order": 13,
          "bbox": [
            97,
            636,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_443_order_14",
          "label": "foot",
          "text": "11.3 Acquiring Data | 421",
          "level": -1,
          "page": 443,
          "reading_order": 14,
          "bbox": [
            467,
            824,
            584,
            842
          ],
          "section_number": "11.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_444_order_0",
          "label": "para",
          "text": "Another response has been to write one-off scripts to manipulate corpus formats; such\nscripts litter the filespaces of many NLP researchers. NLTK’s corpus readers are a more\nsystematic approach, founded on the premise that the work of parsing a corpus format\nshould be done only once (per programming language).",
          "level": -1,
          "page": 444,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_444_order_1",
          "label": "para",
          "text": "Instead of focusing on a common format, we believe it is more promising to develop a\ncommon interface (see nltk.corpus ). Consider the case of treebanks, an important\ncorpus type for work in NLP. There are many ways to store a phrase structure tree in\na file. We can use nested parentheses, or nested XML elements, or a dependency no-\ntation with a (child-id, parent-id ) pair on each line, or an XML version of the dependency\nnotation, etc. However, in each case the logical structure is almost the same. It is much\neasier to devise a common interface that allows application programmers to write code\nto access tree data using methods such as children() , leaves() , depth() , and so forth.\nNote that this approach follows accepted practice within computer science, viz. ab-\nstract data types, object-oriented design, and the three-layer architecture ( Fig-\nure 11 - 6 ). The last of these — from the world of relational databases — allows end-user\napplications to use a common model (the “ relational model”) and a common language\n(SQL) to abstract away from the idiosyncrasies of file storage. It also allows innovations\nin filesystem technologies to occur without disturbing end-user applications. In the\nsame way, a common corpus interface insulates application programs from data\nformats.",
          "level": -1,
          "page": 444,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            585,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_444_order_2",
          "label": "figure",
          "text": "Figure 11-6. A common format versus a common interface. [IMAGE: ![Figure](figures/NLTK_page_444_figure_002.png)]",
          "level": -1,
          "page": 444,
          "reading_order": 2,
          "bbox": [
            91,
            421,
            583,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_444_figure_002.png)",
              "bbox": [
                91,
                421,
                583,
                627
              ],
              "page": 444,
              "reading_order": 2
            },
            {
              "label": "cap",
              "text": "Figure 11-6. A common format versus a common interface.",
              "bbox": [
                97,
                627,
                387,
                645
              ],
              "page": 444,
              "reading_order": 3
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_444_order_4",
          "label": "para",
          "text": "In this context, when creating a new corpus for dissemination, it is expedient to use a\nwidely used format wherever possible. When this is not possible, the corpus could be\naccompanied with software—such as an nltk.corpus module—that supports existing\ninterface methods.",
          "level": -1,
          "page": 444,
          "reading_order": 4,
          "bbox": [
            97,
            654,
            585,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_444_order_5",
      "label": "sec",
      "text": "Special Considerations When Working with Endangered Languages",
      "level": 1,
      "page": 444,
      "reading_order": 5,
      "bbox": [
        97,
        734,
        548,
        761
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_447_order_0",
          "label": "sub_sec",
          "text": "11.4 Working with XML",
          "level": 2,
          "page": 447,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            288,
            100
          ],
          "section_number": "11.4",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_447_order_2",
              "label": "sub_sub_sec",
              "text": "Using XML for Linguistic Structures",
              "level": 3,
              "page": 447,
              "reading_order": 2,
              "bbox": [
                98,
                232,
                328,
                259
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_447_order_3",
                  "label": "para",
                  "text": "Thanks to its flexibility and extensibility, XML is a natural choice for representing\ninguistic structures. Here’s an example of a simple lexical entry.",
                  "level": -1,
                  "page": 447,
                  "reading_order": 3,
                  "bbox": [
                    100,
                    259,
                    585,
                    296
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_447_order_5",
                  "label": "para",
                  "text": "It consists of a series of XML tags enclosed in angle brackets. Each opening tag, such\nas <gloss> , is matched with a closing tag, </gloss> ; together they constitute an XML\nelement . The preceding example has been laid out nicely using whitespace, but it could\nequally have been put on a single long line. Our approach to processing XML will\nusually not be sensitive to whitespace. In order for XML to be well formed , all opening\ntags must have corresponding closing tags, at the same level of nesting (i.e., the XML\ndocument must be a well-formed tree).",
                  "level": -1,
                  "page": 447,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    394,
                    585,
                    510
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_447_order_6",
                  "label": "para",
                  "text": "XML permits us to repeat elements, e.g., to add another gloss field, as we see next. We\nwill use different whitespace to underscore the point that layout does not matter.",
                  "level": -1,
                  "page": 447,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    519,
                    585,
                    555
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_447_order_7",
                  "label": "para",
                  "text": "(3) <entry><head>whale</headword><pos>noun</pos><gloss>any of the\nlarger cetacean mammals having a streamlined body and breathing\nthrough a blowhole on the head</gloss><gloss>a very large person;\nimpressive in size or qualities</gloss></entry>",
                  "level": -1,
                  "page": 447,
                  "reading_order": 7,
                  "bbox": [
                    118,
                    564,
                    494,
                    619
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_447_order_8",
                  "label": "para",
                  "text": "A further step might be to link our lexicon to some external resource, such as WordNet,\nusing external identifiers. In (4) we group the gloss and a synset identifier inside a new\nelement, which we have called “sense.”",
                  "level": -1,
                  "page": 447,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    627,
                    585,
                    680
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_447_order_10",
                  "label": "foot",
                  "text": "11.4 Working with XML | 425",
                  "level": -1,
                  "page": 447,
                  "reading_order": 10,
                  "bbox": [
                    449,
                    824,
                    585,
                    842
                  ],
                  "section_number": "11.4",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_448_order_1",
                  "label": "para",
                  "text": "Alternatively, we could have represented the synset identifier using an XML\nattribute , without the need for any nested structure, as in (5) .",
                  "level": -1,
                  "page": 448,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    152,
                    585,
                    181
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_448_order_3",
                  "label": "para",
                  "text": "This illustrates some of the flexibility of XML. If it seems somewhat arbitrary, that’s\nbecause it is! Following the rules of XML, we can invent new attribute names, and nest\nthem as deeply as we like. We can repeat elements, leave them out, and put them in a\ndifferent order each time. We can have fields whose presence depends on the value of\nsome other field; e.g., if the part of speech is verb, then the entry can have a\npast_tense element to hold the past tense of the verb, but if the part of speech is noun,\nno past_tense element is permitted. To impose some order over all this freedom, we\ncan constrain the structure of an XML file using a “schema,” which is a declaration\nakin to a context-free grammar. Tools exist for testing the validity of an XML file with\nrespect to a schema.",
                  "level": -1,
                  "page": 448,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    313,
                    586,
                    480
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_447_order_1",
              "label": "para",
              "text": "The Extensible Markup Language (XML) provides a framework for designing domain-\nspecific markup languages. It is sometimes used for representing annotated text and\nfor lexical resources. Unlike HTML with its predefined tags, XML permits us to make\nup our own tags. Unlike a database, XML permits us to create data without first spec-\nifying its structure, and it permits us to have optional and repeatable elements. In this\nsection, we briefly review some features of XML that are relevant for representing lin-\nguistic data, and show how to access data stored in XML files using Python programs.",
              "level": -1,
              "page": 447,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                224
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_444_order_6",
          "label": "para",
          "text": "The importance of language to science and the arts is matched in significance by the\ncultural treasure embodied in language. Each of the world’s ~7,000 human languages",
          "level": -1,
          "page": 444,
          "reading_order": 6,
          "bbox": [
            97,
            761,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_444_order_7",
          "label": "foot",
          "text": "422 | Chapter11: Managing Linguistic Data",
          "level": -1,
          "page": 444,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "422",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_445_order_0",
          "label": "para",
          "text": "is rich in unique respects, in its oral histories and creation legends, down to its gram-\nmatical constructions and its very words and their nuances of meaning. Threatened\nremnant cultures have words to distinguish plant subspecies according to therapeutic\nuses that are unknown to science. Languages evolve over time as they come into contact\nwith each other, and each one provides a unique window onto human pre-history. In\nmany parts of the world, small linguistic variations from one town to the next add up\nto a completely different language in the space of a half-hour drive. For its breathtaking\ncomplexity and diversity, human language is as a colorful tapestry stretching through\ntime and space.",
          "level": -1,
          "page": 445,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_445_order_1",
          "label": "para",
          "text": "However, most of the world's languages face extinction. In response to this, many\nlinguists are hard at work documenting the languages, constructing rich records of this\nimportant facet of the world's linguistic heritage. What can the field of NLP offer to\nhelp with this effort? Developing taggers, parsers, named entity recognizers, etc., is not\nan early priority, and there is usually insufficient data for developing such tools in any\ncase. Instead, the most frequently voiced need is to have better tools for collecting and\ncurating data, with a focus on texts and lexicons.",
          "level": -1,
          "page": 445,
          "reading_order": 1,
          "bbox": [
            97,
            231,
            585,
            344
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_445_order_2",
          "label": "para",
          "text": "On the face of things, it should be a straightforward matter to start collecting texts in\nan endangered language. Even if we ignore vexed issues such as who owns the texts,\nand sensitivities surrounding cultural knowledge contained in the texts, there is the\nobvious practical issue of transcription. Most languages lack a standard orthography.\nWhen a language has no literary tradition, the conventions of spelling and punctuation\nare not well established. Therefore it is common practice to create a lexicon in tandem\nwith a text collection, continually updating the lexicon as new words appear in the\ntexts. This work could be done using a text processor (for the texts) and a spreadsheet\n(for the lexicon). Better still, SIL’s free linguistic software Toolbox and Fieldworks\nprovide sophisticated support for integrated creation of texts and lexicons.",
          "level": -1,
          "page": 445,
          "reading_order": 2,
          "bbox": [
            97,
            349,
            585,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_445_order_3",
          "label": "para",
          "text": "When speakers of the language in question are trained to enter texts themselves, a\ncommon obstacle is an overriding concern for correct spelling. Having a lexicon greatly\nhelps this process, but we need to have lookup methods that do not assume someone\ncan determine the citation form of an arbitrary word. The problem may be acute for\nlanguages having a complex morphology that includes prefixes. In such cases it helps\nto tag lexical items with semantic domains, and to permit lookup by semantic domain\nor by gloss.",
          "level": -1,
          "page": 445,
          "reading_order": 3,
          "bbox": [
            97,
            519,
            585,
            640
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_445_order_4",
          "label": "para",
          "text": "Permitting lookup by pronunciation similarity is also a big help. Here's a simple dem-\nonstration of how to do this. The first step is to identify confusible letter sequences,\nand map complex versions to simpler versions. We might also notice that the relative\norder of letters within a cluster of consonants is a source of spelling errors, and so we\nnormalize the order of consonants.",
          "level": -1,
          "page": 445,
          "reading_order": 4,
          "bbox": [
            97,
            645,
            585,
            727
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_445_order_5",
          "label": "foot",
          "text": "11.3 Acquiring Data | 423",
          "level": -1,
          "page": 445,
          "reading_order": 5,
          "bbox": [
            467,
            824,
            584,
            842
          ],
          "section_number": "11.3",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_446_order_1",
          "label": "para",
          "text": "Next, we create a mapping from signatures to words, for all the words in our lexicon.\nWe can use this to get candidate corrections for a given input word (but we must first\ncompute that word’s signature).",
          "level": -1,
          "page": 446,
          "reading_order": 1,
          "bbox": [
            97,
            250,
            585,
            299
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_446_order_3",
          "label": "para",
          "text": "Finally, we should rank the results in terms of similarity with the original word. This\nis done by the function rank(). The only remaining function provides a simple interface\nto the user:",
          "level": -1,
          "page": 446,
          "reading_order": 3,
          "bbox": [
            97,
            356,
            585,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_446_order_5",
          "label": "para",
          "text": "This is just one illustration where a simple program can facilitate access to lexical data\nin a context where the writing system of a language may not be standardized, or where\nusers of the language may not have a good command of spellings. Other simple appli-\ncations of NLP in this area include building indexes to facilitate access to data, gleaning\nwordlists from texts, locating examples of word usage in constructing a lexicon, de-\ntecting prevalent or exceptional patterns in poorly understood data, and performing\nspecialized validation on data created using various linguistic software tools. We will\nreturn to the last of these in Section 11.5 .",
          "level": -1,
          "page": 446,
          "reading_order": 5,
          "bbox": [
            97,
            615,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_446_order_6",
          "label": "foot",
          "text": "424 | Chapter11: Managing Linguistic Data",
          "level": -1,
          "page": 446,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "424",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_448_order_4",
      "label": "sec",
      "text": "The Role of XML",
      "level": 1,
      "page": 448,
      "reading_order": 4,
      "bbox": [
        97,
        492,
        207,
        510
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_448_order_5",
          "label": "para",
          "text": "We can use XML to represent many kinds of linguistic information. However, the\nflexibility comes at a price. Each time we introduce a complication, such as by permit-\nting an element to be optional or repeated, we make more work for any program that\naccesses the data. We also make it more difficult to check the validity of the data, or to\ninterrogate the data using one of the XML query languages.",
          "level": -1,
          "page": 448,
          "reading_order": 5,
          "bbox": [
            97,
            519,
            585,
            603
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_448_order_6",
          "label": "para",
          "text": "Thus, using XML to represent linguistic structures does not magically solve the data\nmodeling problem. We still have to work out how to structure the data, then define\nthat structure with a schema, and then write programs to read and write the format\nand convert it to other formats. Similarly, we still need to follow some standard prin-\nciples concerning data normalization. It is wise to avoid making duplicate copies of the\nsame information, so that we don’t end up with inconsistent data when only one copy\nis changed. For example, a cross-reference that was represented as <xref>headword</\nxref> would duplicate the storage of the headword of some other lexical entry, and the\nlink would break if the copy of the string at the other location was modified. Existential\ndependencies between information types need to be modeled, so that we can’t create\nelements without a home. For example, if sense definitions cannot exist independently",
          "level": -1,
          "page": 448,
          "reading_order": 6,
          "bbox": [
            97,
            609,
            585,
            792
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_448_order_7",
          "label": "foot",
          "text": "426 | Chapter11: Managing Linguistic Data",
          "level": -1,
          "page": 448,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "426",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_449_order_0",
          "label": "para",
          "text": "of a lexical entry, the sense element can be nested inside the entry element. Many-to-\nmany relations need to be abstracted out of hierarchical structures. For example, if a\nword can have many corresponding senses, and a sense can have several corresponding\nwords, then both words and senses must be enumerated separately, as must the list of\n(word, sense) pairings. This complex structure might even be split across three separate\nXML files.",
          "level": -1,
          "page": 449,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            586,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_449_order_1",
          "label": "para",
          "text": "As we can see, although XML provides us with a convenient format accompanied by\nan extensive collection of tools, it offers no panacea.",
          "level": -1,
          "page": 449,
          "reading_order": 1,
          "bbox": [
            97,
            179,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_449_order_2",
      "label": "sec",
      "text": "The ElementTree Interface",
      "level": 1,
      "page": 449,
      "reading_order": 2,
      "bbox": [
        97,
        224,
        273,
        243
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_449_order_3",
          "label": "para",
          "text": "Python’s ElementTree module provides a convenient way to access data stored in XML\nfiles. ElementTree is part of Python’s standard library (since Python 2.5), and is also\nprovided as part of NLTK in case you are using Python 2.4.",
          "level": -1,
          "page": 449,
          "reading_order": 3,
          "bbox": [
            97,
            250,
            585,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_449_order_4",
          "label": "para",
          "text": "We will illustrate the use of ElementTree using a collection of Shakespeare plays that\nhave been formatted using XML. Let's load the XML file and inspect the raw data, first\nat the top of the file ❶ , where we see some XML headers and the name of a schema\ncalled play.dtd , followed by the root element PLAY . We pick it up again at the start of\nAct 1 ☺ . (Some blank lines have been omitted from the output.)",
          "level": -1,
          "page": 449,
          "reading_order": 4,
          "bbox": [
            97,
            312,
            586,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_449_order_6",
          "label": "para",
          "text": "We have just accessed the XML data as a string. As we can see, the string at the start\nof Act 1 contains XML tags for title, scene, stage directions, and so forth.",
          "level": -1,
          "page": 449,
          "reading_order": 6,
          "bbox": [
            97,
            600,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_449_order_7",
          "label": "para",
          "text": "The next step is to process the file contents as structured XML data, using Element\nTree . We are processing a file (a multiline string) and building a tree, so it's not sur-\nprising that the method name is parse O . The variable merchant contains an XML ele-\nment PLAY ⊘ . This element has internal structure; we can use an index to get its first\nchild, a TITLE element S . We can also see the text content of this element, the title of\nthe play O . To get a list of all the child elements, we use the getchildren() method S .",
          "level": -1,
          "page": 449,
          "reading_order": 7,
          "bbox": [
            97,
            644,
            585,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_449_order_9",
          "label": "foot",
          "text": "11.4 Working with XML | 427",
          "level": -1,
          "page": 449,
          "reading_order": 9,
          "bbox": [
            456,
            824,
            585,
            842
          ],
          "section_number": "11.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_450_order_1",
          "label": "para",
          "text": "The play consists of a title, the personae, a scene description, a subtitle, and five acts.\nEach act has a title and some scenes, and each scene consists of speeches which are\nmade up of lines, a structure with four levels of nesting. Let’s dig down into Act IV:",
          "level": -1,
          "page": 450,
          "reading_order": 1,
          "bbox": [
            97,
            197,
            585,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_450_order_3",
          "label": "fig",
          "text": "![Figure](figures/NLTK_page_450_figure_003.png)",
          "level": -1,
          "page": 450,
          "reading_order": 3,
          "bbox": [
            118,
            480,
            164,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_450_order_4",
          "label": "para",
          "text": "Your Turn: Repeat some of the methods just shown, for one of the\nother Shakespeare plays included in the corpus, such as Romeo and Ju-\nliet or Macbeth . For a list, see nltk.corpus.shakespeare.fileids() .",
          "level": -1,
          "page": 450,
          "reading_order": 4,
          "bbox": [
            171,
            490,
            530,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_450_order_5",
          "label": "para",
          "text": "Although we can access the entire tree this way, it is more convenient to search for sub-\nelements with particular names. Recall that the elements at the top level have several\ntypes. We can iterate over just the types we are interested in (such as the acts), using\nmerchant.findall('ACT') . Here’s an example of doing such tag-specific searches at ev-\nery level of nesting:",
          "level": -1,
          "page": 450,
          "reading_order": 5,
          "bbox": [
            97,
            564,
            585,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_450_order_7",
          "label": "foot",
          "text": "428 | Chapter11: Managing Linguistic Data",
          "level": -1,
          "page": 450,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "428",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_451_order_1",
          "label": "para",
          "text": "Instead of navigating each step of the way down the hierarchy, we can search for par-\nticular embedded elements. For example, let's examine the sequence of speakers. We\ncan use a frequency distribution to see who has the most to say:",
          "level": -1,
          "page": 451,
          "reading_order": 1,
          "bbox": [
            97,
            197,
            585,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_451_order_3",
          "label": "para",
          "text": "We can also look for patterns in who follows whom in the dialogues. Since there are\n23 speakers, we need to reduce the “vocabulary” to a manageable size first, using the\nmethod described in Section 5.3 .",
          "level": -1,
          "page": 451,
          "reading_order": 3,
          "bbox": [
            97,
            330,
            585,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_451_order_5",
          "label": "para",
          "text": "Ignoring the entry of 153 for exchanges between people other than the top five, the\nlargest values suggest that Othello and Portia have the most significant interactions.",
          "level": -1,
          "page": 451,
          "reading_order": 5,
          "bbox": [
            97,
            573,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_451_order_6",
      "label": "sec",
      "text": "Using ElementTree for Accessing Toolbox Data",
      "level": 1,
      "page": 451,
      "reading_order": 6,
      "bbox": [
        98,
        618,
        404,
        645
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_451_order_7",
          "label": "para",
          "text": "In Section 2.4, we saw a simple interface for accessing Toolbox data, a popular and\nwell-established format used by linguists for managing data. In this section, we discuss\na variety of techniques for manipulating Toolbox data in ways that are not supported\nby the Toolbox software. The methods we discuss could be applied to other record-\nstructured data, regardless of the actual file format.",
          "level": -1,
          "page": 451,
          "reading_order": 7,
          "bbox": [
            97,
            645,
            585,
            730
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_451_order_8",
          "label": "para",
          "text": "We can use the toolbox.xml() method to access a Toolbox file and load it into an\nElementTree object. This file contains a lexicon for the Rotokas language of Papua New\nGuinea.",
          "level": -1,
          "page": 451,
          "reading_order": 8,
          "bbox": [
            97,
            734,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_451_order_9",
          "label": "foot",
          "text": "11.4 Working with XML | 429",
          "level": -1,
          "page": 451,
          "reading_order": 9,
          "bbox": [
            456,
            824,
            585,
            842
          ],
          "section_number": "11.4",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_452_order_1",
          "label": "para",
          "text": "There are two ways to access the contents of the lexicon object: by indexes and by\npaths. Indexes use the familiar syntax; thus lexicon[3] returns entry number 3 (which\nis actually the fourth entry counting from zero) and lexicon[3][0] returns its first field:",
          "level": -1,
          "page": 452,
          "reading_order": 1,
          "bbox": [
            100,
            107,
            585,
            156
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_452_order_3",
          "label": "para",
          "text": "The second way to access the contents of the lexicon object uses paths. The lexicon is\na series of record objects, each containing a series of field objects, such as 1x and ps.\nWe can conveniently address all of the lexemes using the path record/1x. Here we use\nthe findall() function to search for any matches to the path record/1x, and we access\nthe text content of the element, normalizing it to lowercase:",
          "level": -1,
          "page": 452,
          "reading_order": 3,
          "bbox": [
            97,
            250,
            585,
            332
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_452_order_5",
          "label": "para",
          "text": "Let’s view the Toolbox data in XML format. The write() method of ElementTree ex-\npects a file object. We usually create one of these using Python’s built-in open() func-\ntion. In order to see the output displayed on the screen, we can use a special predefined\nfile object called stdout ❶ (standard output), defined in Python’s sys module.",
          "level": -1,
          "page": 452,
          "reading_order": 5,
          "bbox": [
            97,
            385,
            585,
            452
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_452_order_7",
      "label": "sec",
      "text": "Formatting Entries",
      "level": 1,
      "page": 452,
      "reading_order": 7,
      "bbox": [
        98,
        707,
        225,
        728
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_453_order_1",
          "label": "sub_sec",
          "text": "11.5 Working with Toolbox Data",
          "level": 2,
          "page": 453,
          "reading_order": 1,
          "bbox": [
            98,
            358,
            358,
            382
          ],
          "section_number": "11.5",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_453_order_5",
              "label": "sub_sub_sec",
              "text": "Adding a Field to Each Entry",
              "level": 3,
              "page": 453,
              "reading_order": 5,
              "bbox": [
                97,
                582,
                282,
                609
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_453_order_6",
                  "label": "para",
                  "text": "It is often convenient to add new fields that are derived automatically from existing\nones. Such fields often facilitate search and analysis. For instance, in Example 11-2 we\ndefine a function cv() , which maps a string of consonants and vowels to the corre-\nsponding CV sequence, e.g., kakapua would map to CVCVCW . This mapping has four\nsteps. First, the string is converted to lowercase, then we replace any non-alphabetic\ncharacters [^a-z] with an underscore. Next, we replace all vowels with V . Finally, any-\nthing that is not a V or an underscore must be a consonant, so we replace it with a C .\nNow, we can scan the lexicon and add a new cv field after every 1x field. Exam-\nple 11-2 shows what this does to a particular entry; note the last line of output, which\nshows the new cv field.",
                  "level": -1,
                  "page": 453,
                  "reading_order": 6,
                  "bbox": [
                    97,
                    609,
                    585,
                    779
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_453_order_7",
                  "label": "foot",
                  "text": "11.5 Working with Toolbox Data | 431",
                  "level": -1,
                  "page": 453,
                  "reading_order": 7,
                  "bbox": [
                    413,
                    824,
                    584,
                    842
                  ],
                  "section_number": "11.5",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_454_order_4",
                  "label": "para",
                  "text": "If a Toolbox file is being continually updated, the program in Exam-\nple 11-2 will need to be run more than once. It would be possible to\nmodify add_cv_field() to modify the contents of an existing entry.\nHowever, it is a safer practice to use such programs to create enriched\nfiles for the purpose of data analysis, without replacing the manually\ncurated source files.",
                  "level": -1,
                  "page": 454,
                  "reading_order": 4,
                  "bbox": [
                    171,
                    535,
                    530,
                    618
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_454_order_5",
                  "label": "fig",
                  "text": "![Figure](figures/NLTK_page_454_figure_005.png)",
                  "level": -1,
                  "page": 454,
                  "reading_order": 5,
                  "bbox": [
                    121,
                    519,
                    164,
                    582
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_453_order_2",
              "label": "para",
              "text": "Given the popularity of Toolbox among linguists, we will discuss some further methods\nfor working with Toolbox data. Many of the methods discussed in previous chapters,\nsuch as counting, building frequency distributions, and tabulating co-occurrences, can\nbe applied to the content of Toolbox entries. For example, we can trivially compute\nthe average number of fields for each entry:",
              "level": -1,
              "page": 453,
              "reading_order": 2,
              "bbox": [
                97,
                385,
                585,
                474
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_453_order_4",
              "label": "para",
              "text": "In this section, we will discuss two tasks that arise in the context of documentary lin-\nguistics, neither of which is supported by the Toolbox software.",
              "level": -1,
              "page": 453,
              "reading_order": 4,
              "bbox": [
                97,
                537,
                584,
                573
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_452_order_8",
          "label": "para",
          "text": "We can use the same idea we saw in the previous section to generate HTML tables\ninstead of plain text. This would be useful for publishing a Toolbox lexicon on the\nWeb. It produces HTML elements <table>, <tr>(table row), and <td>(table data).",
          "level": -1,
          "page": 452,
          "reading_order": 8,
          "bbox": [
            97,
            734,
            585,
            783
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_452_order_9",
          "label": "foot",
          "text": "430 | Chapter11: Managing Linguistic Data",
          "level": -1,
          "page": 452,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "430",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_454_order_6",
      "label": "sec",
      "text": "Validating a Toolbox Lexicon",
      "level": 1,
      "page": 454,
      "reading_order": 6,
      "bbox": [
        97,
        645,
        288,
        665
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_457_order_0",
          "label": "sub_sec",
          "text": "11.6 Describing Language Resources Using OLAC Metadata",
          "level": 2,
          "page": 457,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            566,
            100
          ],
          "section_number": "11.6",
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_457_order_2",
              "label": "sub_sub_sec",
              "text": "What Is Metadata",
              "level": 3,
              "page": 457,
              "reading_order": 2,
              "bbox": [
                97,
                170,
                216,
                188
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_457_order_3",
                  "label": "para",
                  "text": "The simplest definition of metadata is “structured data about data.” Metadata is de-\nscriptive information about an object or resource, whether it be physical or electronic.\nAlthough the term “metadata” itself is relatively new, the underlying concepts behind\nmetadata have been in use for as long as collections of information have been organized.\nLibrary catalogs represent a well-established type of metadata; they have served as col-\nlection management and resource discovery tools for decades. Metadata can be gen-\nerated either “by hand” or automatically using software.",
                  "level": -1,
                  "page": 457,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    197,
                    585,
                    313
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_457_order_4",
                  "label": "para",
                  "text": "The Dublin Core Metadata Initiative began in 1995 to develop conventions for finding,\nsharing, and managing information. The Dublin Core metadata elements represent a\nbroad, interdisciplinary consensus about the core set of elements that are likely to be\nwidely useful to support resource discovery. The Dublin Core consists of 15 metadata\nelements, where each element is optional and repeatable: Title, Creator, Subject, De-\nscription, Publisher, Contributor, Date, Type, Format, Identifier, Source, Language,\nRelation, Coverage, and Rights. This metadata set can be used to describe resources\nthat exist in digital or traditional formats.",
                  "level": -1,
                  "page": 457,
                  "reading_order": 4,
                  "bbox": [
                    97,
                    322,
                    585,
                    452
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_457_order_5",
                  "label": "para",
                  "text": "The Open Archives Initiative (OAI) provides a common framework across digital re-\npositories of scholarly materials, regardless of their type, including documents, data,\nsoftware, recordings, physical artifacts, digital surrogates, and so forth. Each repository\nconsists of a network-accessible server offering public access to archived items. Each\nitem has a unique identifier, and is associated with a Dublin Core metadata record (and\npossibly additional records in other formats). The OAI defines a protocol for metadata\nsearch services to “harvest” the contents of repositories.",
                  "level": -1,
                  "page": 457,
                  "reading_order": 5,
                  "bbox": [
                    97,
                    456,
                    585,
                    575
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            },
            {
              "id": "page_457_order_6",
              "label": "sub_sub_sec",
              "text": "OLAC: Open Language Archives Community",
              "level": 3,
              "page": 457,
              "reading_order": 6,
              "bbox": [
                97,
                591,
                386,
                610
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_457_order_7",
                  "label": "para",
                  "text": "The Open Language Archives Community, or OLAC, is an international partnership\nof institutions and individuals who are creating a worldwide virtual library of language\nresources by: (i) developing consensus on best current practices for the digital archiving\nof language resources, and (ii) developing a network of interoperating repositories and\nservices for housing and accessing such resources. OLAC's home on the Web is at http:\n//www.language-archives.org/ .",
                  "level": -1,
                  "page": 457,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    618,
                    585,
                    716
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_457_order_8",
                  "label": "para",
                  "text": "OLAC Metadata is a standard for describing language resources. Uniform description\nacross repositories is ensured by limiting the values of certain metadata elements to the\nuse of terms from controlled vocabularies. OLAC metadata can be used to describe\ndata and tools, in both physical and digital formats. OLAC metadata extends the",
                  "level": -1,
                  "page": 457,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    724,
                    585,
                    788
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_457_order_9",
                  "label": "foot",
                  "text": "11.6 Describing Language Resources Using OLAC Metadata | 435",
                  "level": -1,
                  "page": 457,
                  "reading_order": 9,
                  "bbox": [
                    315,
                    824,
                    585,
                    842
                  ],
                  "section_number": "11.6",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_458_order_0",
                  "label": "para",
                  "text": "Dublin Core Metadata Set, a widely accepted standard for describing resources of all\ntypes. To this core set, OLAC adds descriptors to cover fundamental properties of\nlanguage resources, such as subject language and linguistic type. Here’s an example of\na complete OLAC record:",
                  "level": -1,
                  "page": 458,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    584,
                    143
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_458_order_2",
                  "label": "para",
                  "text": "Participating language archives publish their catalogs in an XML format, and these\nrecords are regularly “ harvested ” by OLAC services using the OAI protocol. In addition\nto this software infrastructure, OLAC has documented a series of best practices for\ndescribing language resources, through a process that involved extended consultation\nwith the language resources community (e.g., see http://www.language-archives.org/\nREC/bpr.html ).",
                  "level": -1,
                  "page": 458,
                  "reading_order": 2,
                  "bbox": [
                    97,
                    429,
                    585,
                    528
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_458_order_3",
                  "label": "para",
                  "text": "OLAC repositories can be searched using a query engine on the OLAC website. Search-\ning for “German lexicon” finds the following resources, among others:",
                  "level": -1,
                  "page": 458,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    535,
                    584,
                    566
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_458_order_4",
                  "label": "list_group",
                  "text": "CALLHOME German Lexicon, at http://www.language-archives.org/item/oai5\nwww.ldc.upenn.edu:LDC97L18\nMULTILEX multilingual lexicon, at http://www.language-archives.org/item/oai:el\nca.icp.inpg.fr:M0001",
                  "level": -1,
                  "page": 458,
                  "reading_order": 4,
                  "bbox": [
                    122,
                    573,
                    584,
                    609
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": [],
                  "merged_elements": [
                    {
                      "label": "list",
                      "text": "CALLHOME German Lexicon, at http://www.language-archives.org/item/oai5\nwww.ldc.upenn.edu:LDC97L18",
                      "bbox": [
                        122,
                        573,
                        584,
                        609
                      ],
                      "page": 458,
                      "reading_order": 4
                    },
                    {
                      "label": "list",
                      "text": "MULTILEX multilingual lexicon, at http://www.language-archives.org/item/oai:el\nca.icp.inpg.fr:M0001",
                      "bbox": [
                        125,
                        614,
                        584,
                        645
                      ],
                      "page": 458,
                      "reading_order": 5
                    },
                    {
                      "label": "list",
                      "text": "Slelex Siemens Phonetic lexicon, at http://www.language-archives.org/item/oai:elra\n.icp.inpg.fr:S0048",
                      "bbox": [
                        122,
                        651,
                        584,
                        682
                      ],
                      "page": 458,
                      "reading_order": 6
                    }
                  ],
                  "is_merged": true
                },
                {
                  "id": "page_458_order_7",
                  "label": "para",
                  "text": "Searching for “Korean” finds a newswire corpus, and a treebank, a lexicon, a child-\nlanguage corpus, and interlinear glossed texts. It also finds software, including a syn-\ntactic analyzer and a morphological analyzer.",
                  "level": -1,
                  "page": 458,
                  "reading_order": 7,
                  "bbox": [
                    97,
                    689,
                    584,
                    743
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_458_order_8",
                  "label": "para",
                  "text": "Observe that the previous URLs include a substring of the form:\noai:www.ldc.upenn.edu:LDC97L18 . This is an OAI identifier, using a URI scheme regis-\ntered with ICANN (the Internet Corporation for Assigned Names and Numbers). These",
                  "level": -1,
                  "page": 458,
                  "reading_order": 8,
                  "bbox": [
                    97,
                    743,
                    585,
                    797
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_458_order_9",
                  "label": "foot",
                  "text": "436 | Chapter11: Managing Linguistic Data",
                  "level": -1,
                  "page": 458,
                  "reading_order": 9,
                  "bbox": [
                    97,
                    824,
                    288,
                    842
                  ],
                  "section_number": "436",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_459_order_0",
                  "label": "para",
                  "text": "identifiers have the format oai:archive:local_id, where oai is the name of the URI\nscheme, archive is an archive identifier, such as www.ldc.upenn.edu, and local_id is the\nresource identifier assigned by the archive, e.g., LDC97L18.",
                  "level": -1,
                  "page": 459,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    585,
                    125
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_459_order_1",
                  "label": "para",
                  "text": "Given an OAI identifier for an OLAC resource, it is possible to retrieve the complete\nXML record for the resource using a URL of the following form: http://www.language-\narchives.org/static-records/oai:archive:local_id.",
                  "level": -1,
                  "page": 459,
                  "reading_order": 1,
                  "bbox": [
                    97,
                    125,
                    584,
                    179
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": [
            {
              "id": "page_457_order_1",
              "label": "para",
              "text": "Members of the NLP community have a common need for discovering language re-\nsources with high precision and recall. The solution which has been developed by the\nDigital Libraries community involves metadata aggregation.",
              "level": -1,
              "page": 457,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                161
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_459_order_2",
          "label": "sub_sec",
          "text": "11.7 Summary",
          "level": 2,
          "page": 459,
          "reading_order": 2,
          "bbox": [
            98,
            205,
            217,
            228
          ],
          "section_number": "11.7",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_459_order_3",
              "label": "list_group",
              "text": "• Fundamental data types, present in most corpora, are annotated texts and lexicons.\nTexts have a temporal structure, whereas lexicons have a record structure.\n• The life cycle of a corpus includes data collection, annotation, quality control, and\npublication. The life cycle continues after publication as the corpus is modified\nand enriched during the course of research.",
              "level": -1,
              "page": 459,
              "reading_order": 3,
              "bbox": [
                100,
                232,
                584,
                269
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [],
              "merged_elements": [
                {
                  "label": "list",
                  "text": "• Fundamental data types, present in most corpora, are annotated texts and lexicons.\nTexts have a temporal structure, whereas lexicons have a record structure.",
                  "bbox": [
                    100,
                    232,
                    584,
                    269
                  ],
                  "page": 459,
                  "reading_order": 3
                },
                {
                  "label": "list",
                  "text": "• The life cycle of a corpus includes data collection, annotation, quality control, and\npublication. The life cycle continues after publication as the corpus is modified\nand enriched during the course of research.",
                  "bbox": [
                    100,
                    269,
                    585,
                    323
                  ],
                  "page": 459,
                  "reading_order": 4
                },
                {
                  "label": "list",
                  "text": "• Corpus development involves a balance between capturing a representative sample\nof language usage, and capturing enough material from any one source or genre to\nbe useful; multiplying out the dimensions of variability is usually not feasible be-\ncause of resource limitations.",
                  "bbox": [
                    100,
                    323,
                    585,
                    394
                  ],
                  "page": 459,
                  "reading_order": 5
                },
                {
                  "label": "list",
                  "text": "• XML provides a useful format for the storage and interchange of linguistic data,\nbut provides no shortcuts for solving pervasive data modeling problems.",
                  "bbox": [
                    100,
                    394,
                    584,
                    430
                  ],
                  "page": 459,
                  "reading_order": 6
                },
                {
                  "label": "list",
                  "text": "• Toolbox format is widely used in language documentation projects; we can write\nprograms to support the curation of Toolbox files, and to convert them to XML.",
                  "bbox": [
                    100,
                    430,
                    585,
                    467
                  ],
                  "page": 459,
                  "reading_order": 7
                },
                {
                  "label": "list",
                  "text": "• The Open Language Archives Community (OLAC) provides an infrastructure for\ndocumenting and discovering language resources.",
                  "bbox": [
                    100,
                    467,
                    585,
                    510
                  ],
                  "page": 459,
                  "reading_order": 8
                }
              ],
              "is_merged": true
            }
          ]
        },
        {
          "id": "page_459_order_9",
          "label": "sub_sec",
          "text": "11.8 Further Reading",
          "level": 2,
          "page": 459,
          "reading_order": 9,
          "bbox": [
            98,
            528,
            270,
            555
          ],
          "section_number": "11.8",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_459_order_10",
              "label": "para",
              "text": "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web.",
              "level": -1,
              "page": 459,
              "reading_order": 10,
              "bbox": [
                98,
                555,
                584,
                591
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_459_order_11",
              "label": "para",
              "text": "The primary sources of linguistic corpora are the Linguistic Data Consortium and the\nEuropean Language Resources Agency , both with extensive online catalogs. More de-\ntails concerning the major corpora mentioned in the chapter are available: American\nNational Corpus (Reppen, Ide & Suderman, 2005) , British National Corpus (BNC,\n1999) , Thesaurus Linguae Graecae (TLG, 1999) , Child Language Data Exchange Sys-\ntem (CHILDES) (MacWhinney, 1995) , and TIMIT (Garofolo et al., 1986) .",
              "level": -1,
              "page": 459,
              "reading_order": 11,
              "bbox": [
                97,
                600,
                585,
                699
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_459_order_12",
              "label": "para",
              "text": "Two special interest groups of the Association for Computational Linguistics that or-\nganize regular workshops with published proceedings are SIGWAC, which promotes\nthe use of the Web as a corpus and has sponsored the CLEANEVAL task for removing\nHTML markup, and SIGANN, which is encouraging efforts toward interoperability of",
              "level": -1,
              "page": 459,
              "reading_order": 12,
              "bbox": [
                97,
                707,
                586,
                773
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_459_order_13",
              "label": "foot",
              "text": "11.8 Further Reading | 437",
              "level": -1,
              "page": 459,
              "reading_order": 13,
              "bbox": [
                465,
                824,
                585,
                842
              ],
              "section_number": "11.8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_0",
              "label": "para",
              "text": "linguistic annotations. An extended discussion of web crawling is provided by (Croft,\nMetzler & Strohman, 2009).",
              "level": -1,
              "page": 460,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                584,
                107
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_1",
              "label": "para",
              "text": "Full details of the Toolbox data format are provided with the distribution (Buseman,\nBuseman & Early, 1996), and with the latest distribution freely available from http://\nwww.sil.org/computing/toolbox/. For guidelines on the process of constructing a Tool-\nbox lexicon, see http://www.sil.org/computing/ddp/. More examples of our efforts with\nthe Toolbox are documented in (Bird, 1999) and (Robinson, Aumann & Bird, 2007).\nDozens of other tools for linguistic data management are available, some surveyed by\n(Bird & Simons, 2003). See also the proceedings of the LaTeCH workshops on language\ntechnology for cultural heritage data.",
              "level": -1,
              "page": 460,
              "reading_order": 1,
              "bbox": [
                97,
                107,
                585,
                250
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_2",
              "label": "para",
              "text": "There are many excellent resources for XML (e.g., http://zvon.org/) and for writing\nPython programs to work with XML http://www.python.org/doc/lib/markup.html.\nMany editors have XML modes. XML formats for lexical information include OLIF\n(http://www.olif.net/) and LIFT (http://code.google.com/p/lift-standard/).",
              "level": -1,
              "page": 460,
              "reading_order": 2,
              "bbox": [
                97,
                250,
                585,
                322
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_3",
              "label": "para",
              "text": "For a survey of linguistic annotation software, see the Linguistic Annotation Page at\nhttp://www.ldc.upenn.edu/annotation/ . The initial proposal for standoff annotation was\n(Thompson & McKelvie, 1997) . An abstract data model for linguistic annotations,\ncalled “ annotation graphs, ” was proposed in (Bird & Liberman, 2001) . A general-\npurpose ontology for linguistic description (GOLD) is documented at http://www.lin\nguistics-ontology.org/ .",
              "level": -1,
              "page": 460,
              "reading_order": 3,
              "bbox": [
                96,
                322,
                585,
                430
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_4",
              "label": "para",
              "text": "For guidance on planning and constructing a corpus, see (Meyer, 2002) and (Farghaly,\n2003). More details of methods for scoring inter-annotator agreement are available in\n(Artstein & Poesio, 2008) and (Pevzner & Hearst, 2002).",
              "level": -1,
              "page": 460,
              "reading_order": 4,
              "bbox": [
                97,
                430,
                584,
                483
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_5",
              "label": "para",
              "text": "Rotokas data was provided by Stuart Robinson, and Iu Mien data was provided by Greg\nAumann.",
              "level": -1,
              "page": 460,
              "reading_order": 5,
              "bbox": [
                97,
                483,
                585,
                519
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_6",
              "label": "para",
              "text": "For more information about the Open Language Archives Community, visit http://www\n.language-archives.org/, or see (Simons & Bird, 2003).",
              "level": -1,
              "page": 460,
              "reading_order": 6,
              "bbox": [
                97,
                528,
                583,
                564
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        },
        {
          "id": "page_460_order_7",
          "label": "sub_sec",
          "text": "11.9 Exercises",
          "level": 2,
          "page": 460,
          "reading_order": 7,
          "bbox": [
            98,
            582,
            211,
            609
          ],
          "section_number": "11.9",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_460_order_8",
              "label": "para",
              "text": "1. o In Example 11-2 the new field appeared at the bottom of the entry. Modify this\nprogram so that it inserts the new subelement right after the lx field. (Hint: create\nthe new cv field using Element('cv'), assign a text value to it, then use the\ninsert() method of the parent element.)",
              "level": -1,
              "page": 460,
              "reading_order": 8,
              "bbox": [
                100,
                618,
                585,
                684
              ],
              "section_number": "1",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_9",
              "label": "para",
              "text": "2. o Write a function that deletes a specified field from a lexical entry. (We could use\nthis to sanitize our lexical data before giving it to others, e.g., by removing fields\ncontaining irrelevant or uncertain content.)",
              "level": -1,
              "page": 460,
              "reading_order": 9,
              "bbox": [
                100,
                689,
                585,
                743
              ],
              "section_number": "2",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_10",
              "label": "para",
              "text": "3. o Write a program that scans an HTML dictionary file to find entries having an\nillegal part-of-speech field, and then reports the headword for each entry.",
              "level": -1,
              "page": 460,
              "reading_order": 10,
              "bbox": [
                100,
                743,
                584,
                775
              ],
              "section_number": "3",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_460_order_11",
              "label": "foot",
              "text": "438 | Chapter 11: Managing Linguistic Data",
              "level": -1,
              "page": 460,
              "reading_order": 11,
              "bbox": [
                97,
                824,
                288,
                842
              ],
              "section_number": "438",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_0",
              "label": "para",
              "text": "4. o Write a program to find any parts-of-speech (ps field) that occurred less than 10\ntimes. Perhaps these are typing mistakes?",
              "level": -1,
              "page": 461,
              "reading_order": 0,
              "bbox": [
                100,
                71,
                585,
                107
              ],
              "section_number": "4",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_1",
              "label": "para",
              "text": "5. o We saw a method for adding a cv field (Section 11.5). There is an interesting\nissue with keeping this up-to-date when someone modifies the content of the lx\nfield on which it is based. Write a version of this program to add a cv field, replacing\nany existing cv field.",
              "level": -1,
              "page": 461,
              "reading_order": 1,
              "bbox": [
                100,
                107,
                585,
                179
              ],
              "section_number": "5",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_2",
              "label": "para",
              "text": "6. o Write a function to add a new field syl which gives a count of the number of\nsyllables in the word.",
              "level": -1,
              "page": 461,
              "reading_order": 2,
              "bbox": [
                100,
                179,
                584,
                215
              ],
              "section_number": "6",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_3",
              "label": "para",
              "text": "7. o Write a function which displays the complete entry for a lexeme. When the\nlexeme is incorrectly spelled, it should display the entry for the most similarly\nspelled lexeme.",
              "level": -1,
              "page": 461,
              "reading_order": 3,
              "bbox": [
                100,
                215,
                585,
                268
              ],
              "section_number": "7",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_4",
              "label": "para",
              "text": "8. o Write a function that takes a lexicon and finds which pairs of consecutive fields\nare most frequent (e.g., ps is often followed by pt). (This might help us to discover\nsome of the structure of a lexical entry.)",
              "level": -1,
              "page": 461,
              "reading_order": 4,
              "bbox": [
                100,
                268,
                585,
                322
              ],
              "section_number": "8",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_5",
              "label": "para",
              "text": "9. o Create a spreadsheet using office software, containing one lexical entry per row,\nconsisting of a headword, a part of speech, and a gloss. Save the spreadsheet in\nCSV format. Write Python code to read the CSV file and print it in Toolbox format,\nusing lx for the headword, ps for the part of speech, and gl for the gloss.",
              "level": -1,
              "page": 461,
              "reading_order": 5,
              "bbox": [
                100,
                322,
                584,
                394
              ],
              "section_number": "9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_6",
              "label": "para",
              "text": "10. o Index the words of Shakespeare’s plays, with the help of nltk.Index. The result-\ning data structure should permit lookup on individual words, such as music, re-\nturning a list of references to acts, scenes, and speeches, of the form [(3, 2, 9),\n(5, 1, 23), ...], where (3, 2, 9) indicates Act 3 Scene 2 Speech 9.",
              "level": -1,
              "page": 461,
              "reading_order": 6,
              "bbox": [
                100,
                394,
                584,
                465
              ],
              "section_number": "10",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_7",
              "label": "para",
              "text": "11. o Construct a conditional frequency distribution which records the word length\nfor each speech in The Merchant of Venice, conditioned on the name of the char-\nacter; e.g., cfd['PORTIA'][12] would give us the number of speeches by Portia\nconsisting of 12 words.",
              "level": -1,
              "page": 461,
              "reading_order": 7,
              "bbox": [
                100,
                465,
                584,
                537
              ],
              "section_number": "11",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_8",
              "label": "para",
              "text": "12. o Write a recursive function to convert an arbitrary NLTK tree into an XML coun-\nterpart, with non-terminals represented as XML elements, and leaves represented\nas text content, e.g.:",
              "level": -1,
              "page": 461,
              "reading_order": 8,
              "bbox": [
                100,
                537,
                585,
                586
              ],
              "section_number": "12",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_10",
              "label": "para",
              "text": "13. • Obtain a comparative wordlist in CSV format, and write a program that prints\nthose cognates having an edit-distance of at least three from each other.",
              "level": -1,
              "page": 461,
              "reading_order": 10,
              "bbox": [
                100,
                689,
                585,
                725
              ],
              "section_number": "13",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_11",
              "label": "para",
              "text": "14. • Build an index of those lexemes which appear in example sentences. Suppose\nthe lexeme for a given entry is w. Then, add a single cross-reference field xrf to this\nentry, referencing the headwords of other entries having example sentences con-\ntaining w. Do this for all entries and save the result as a Toolbox-format file.",
              "level": -1,
              "page": 461,
              "reading_order": 11,
              "bbox": [
                100,
                725,
                585,
                792
              ],
              "section_number": "14",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_461_order_12",
              "label": "foot",
              "text": "11.9 Exercises | 439",
              "level": -1,
              "page": 461,
              "reading_order": 12,
              "bbox": [
                494,
                824,
                585,
                842
              ],
              "section_number": "11.9",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_462_order_0",
              "label": "para",
              "text": "_",
              "level": -1,
              "page": 462,
              "reading_order": 0,
              "bbox": [
                153,
                161,
                494,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": [
        {
          "id": "page_454_order_7",
          "label": "para",
          "text": "Many lexicons in Toolbox format do not conform to any particular schema. Some\nentries may include extra fields, or may order existing fields in a new way. Manually\ninspecting thousands of lexical entries is not practicable. However, we can easily iden-\ntify frequent versus exceptional field sequences, with the help of a FreqDist:",
          "level": -1,
          "page": 454,
          "reading_order": 7,
          "bbox": [
            97,
            672,
            585,
            737
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_454_order_9",
          "label": "foot",
          "text": "432 | Chapter11: Managing Linguistic Data",
          "level": -1,
          "page": 454,
          "reading_order": 9,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "432",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_455_order_1",
          "label": "para",
          "text": "After inspecting the high-frequency field sequences, we could devise a context-free\ngrammar for lexical entries. The grammar in Example 11 - 3 uses the CFG format we\nsaw in Chapter 8. Such a grammar models the implicit nested structure of Toolbox\nentries, building a tree structure, where the leaves of the tree are individual field names.\nWe iterate over the entries and report their conformance with the grammar, as shown\nin Example 11 - 3 . Those that are accepted by the grammar are prefixed with a '+' ❶ ,\nand those that are rejected are prefixed with a '-' ❷ . During the process of developing\nsuch a grammar, it helps to filter out some of the tags ❸ .",
          "level": -1,
          "page": 455,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_455_order_2",
          "label": "para",
          "text": "Example 11-3. Validating Toolbox entries using a context-free grammar.",
          "level": -1,
          "page": 455,
          "reading_order": 2,
          "bbox": [
            97,
            250,
            458,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_455_order_6",
          "label": "foot",
          "text": "11.5 Working with Toolbox Data | 433",
          "level": -1,
          "page": 455,
          "reading_order": 6,
          "bbox": [
            421,
            824,
            584,
            842
          ],
          "section_number": "11.5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_456_order_0",
          "label": "para",
          "text": "Another approach would be to use a chunk parser ( Chapter 7 ), since these are much\nmore effective at identifying partial structures and can report the partial structures that\nhave been identified. In Example 11-4 we set up a chunk grammar for the entries of a\nlexicon, then parse each entry. A sample of the output from this program is shown in\nFigure 11-7 .",
          "level": -1,
          "page": 456,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_456_order_1",
          "label": "figure",
          "text": "Figure 11-7. XML representation of a lexical entry, resulting from chunk parsing a Toolbox record. [IMAGE: ![Figure](figures/NLTK_page_456_figure_001.png)]",
          "level": -1,
          "page": 456,
          "reading_order": 1,
          "bbox": [
            100,
            179,
            583,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "fig",
              "text": "![Figure](figures/NLTK_page_456_figure_001.png)",
              "bbox": [
                100,
                179,
                583,
                474
              ],
              "page": 456,
              "reading_order": 1
            },
            {
              "label": "cap",
              "text": "Figure 11-7. XML representation of a lexical entry, resulting from chunk parsing a Toolbox record.",
              "bbox": [
                97,
                483,
                583,
                501
              ],
              "page": 456,
              "reading_order": 2
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_456_order_3",
          "label": "para",
          "text": "Example 11-4. Chunking a Toolbox lexicon: A chunk grammar describing the structure of entries for\na lexicon for Iu Mien, a language of China.",
          "level": -1,
          "page": 456,
          "reading_order": 3,
          "bbox": [
            97,
            528,
            583,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_456_order_6",
          "label": "foot",
          "text": "434 | Chapter11: Managing Linguistic Data",
          "level": -1,
          "page": 456,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            288,
            842
          ],
          "section_number": "434",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_463_order_0",
      "label": "sec",
      "text": "Afterword: The Language Challenge",
      "level": 1,
      "page": 463,
      "reading_order": 0,
      "bbox": [
        125,
        107,
        584,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_463_order_1",
          "label": "para",
          "text": "Natural language throws up some interesting computational challenges. We've ex-\nplored many of these in the preceding chapters, including tokenization, tagging, clas-\nsification, information extraction, and building syntactic and semantic representations.\nYou should now be equipped to work with large datasets, to create robust models of\nlinguistic phenomena, and to extend them into components for practical language\ntechnologies. We hope that the Natural Language Toolkit (NLTK) has served to open\nup the exciting endeavor of practical natural language processing to a broader audience\nthan before.",
          "level": -1,
          "page": 463,
          "reading_order": 1,
          "bbox": [
            97,
            286,
            586,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_463_order_2",
          "label": "para",
          "text": "In spite of all that has come before, language presents us with far more than a temporary\nchallenge for computation. Consider the following sentences which attest to the riches\nof language:",
          "level": -1,
          "page": 463,
          "reading_order": 2,
          "bbox": [
            97,
            429,
            585,
            476
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_463_order_3",
          "label": "list_group",
          "text": "1. Overhead the day drives level and grey, hiding the sun by a flight of grey spears.\n(William Faulkner, As I Lay Dying, 1935)\n2. When using the toaster please ensure that the exhaust fan is turned on. (sign in\ndormitory kitchen)",
          "level": -1,
          "page": 463,
          "reading_order": 3,
          "bbox": [
            100,
            483,
            584,
            519
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [],
          "merged_elements": [
            {
              "label": "list",
              "text": "1. Overhead the day drives level and grey, hiding the sun by a flight of grey spears.\n(William Faulkner, As I Lay Dying, 1935)",
              "bbox": [
                100,
                483,
                584,
                519
              ],
              "page": 463,
              "reading_order": 3
            },
            {
              "label": "list",
              "text": "2. When using the toaster please ensure that the exhaust fan is turned on. (sign in\ndormitory kitchen)",
              "bbox": [
                100,
                519,
                584,
                555
              ],
              "page": 463,
              "reading_order": 4
            },
            {
              "label": "list",
              "text": "3. Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated activi-\nties with Ki values of 45.1­271.6 μM (Medline, PMID: 10718780)",
              "bbox": [
                100,
                555,
                584,
                592
              ],
              "page": 463,
              "reading_order": 5
            },
            {
              "label": "list",
              "text": "4. Iraqi Head Seeks Arms (spoof news headline)",
              "bbox": [
                100,
                592,
                386,
                612
              ],
              "page": 463,
              "reading_order": 6
            },
            {
              "label": "list",
              "text": "5. The earnest prayer of a righteous man has great power and wonderful results.\n(James 5:16b)",
              "bbox": [
                100,
                618,
                584,
                654
              ],
              "page": 463,
              "reading_order": 7
            },
            {
              "label": "list",
              "text": "6. Twas brillig, and the slithy toves did gyre and gimble in the wabe (Lewis Carroll,\nJabberwocky, 1872)",
              "bbox": [
                100,
                654,
                584,
                689
              ],
              "page": 463,
              "reading_order": 8
            },
            {
              "label": "list",
              "text": "7. There are two ways to do this, AFAIK :smile: (Internet discussion archive)",
              "bbox": [
                100,
                689,
                548,
                707
              ],
              "page": 463,
              "reading_order": 9
            }
          ],
          "is_merged": true
        },
        {
          "id": "page_463_order_10",
          "label": "para",
          "text": "Other evidence for the riches of language is the vast array of disciplines whose work\ncenters on language. Some obvious disciplines include translation, literary criticism,\nphilosophy, anthropology, and psychology. Many less obvious disciplines investigate\nlanguage use, including law, hermeneutics, forensics, telephony, pedagogy, archaeol-\nogy, cryptanalysis, and speech pathology. Each applies distinct methodologies to gather",
          "level": -1,
          "page": 463,
          "reading_order": 10,
          "bbox": [
            97,
            716,
            585,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_463_order_11",
          "label": "foot",
          "text": "441",
          "level": -1,
          "page": 463,
          "reading_order": 11,
          "bbox": [
            566,
            824,
            584,
            837
          ],
          "section_number": "441",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_464_order_0",
          "label": "para",
          "text": "observations, develop theories, and test hypotheses. All serve to deepen our under-\nstanding of language and of the intellect that is manifested in language.",
          "level": -1,
          "page": 464,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_464_order_1",
          "label": "para",
          "text": "In view of the complexity of language and the broad range of interest in studying it\nfrom different angles, it's clear that we have barely scratched the surface here. Addi-\ntionally, within NLP itself, there are many important methods and applications that\nwe haven’t mentioned.",
          "level": -1,
          "page": 464,
          "reading_order": 1,
          "bbox": [
            97,
            115,
            584,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_464_order_2",
          "label": "para",
          "text": "In our closing remarks we will take a broader view of NLP, including its foundations\nand the further directions you might want to explore. Some of the topics are not well\nsupported by NLTK, and you might like to rectify that problem by contributing new\nsoftware and data to the toolkit.",
          "level": -1,
          "page": 464,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            585,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_464_order_3",
      "label": "sec",
      "text": "Language Processing Versus Symbol Processing",
      "level": 1,
      "page": 464,
      "reading_order": 3,
      "bbox": [
        98,
        277,
        474,
        304
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_464_order_4",
          "label": "para",
          "text": "The very notion that natural language could be treated in a computational manner grew\nout of a research program, dating back to the early 1900s, to reconstruct mathematical\nreasoning using logic, most clearly manifested in work by Frege, Russell, Wittgenstein,\nTarski, Lambek, and Carnap. This work led to the notion of language as a formal system\namenable to automatic processing. Three later developments laid the foundation for\nnatural language processing. The first was formal language theory. This defined a\nlanguage as a set of strings accepted by a class of automata, such as context-free lan-\nguages and pushdown automata, and provided the underpinnings for computational\nsyntax.",
          "level": -1,
          "page": 464,
          "reading_order": 4,
          "bbox": [
            97,
            304,
            585,
            456
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_464_order_5",
          "label": "para",
          "text": "The second development was symbolic logic . This provided a formal method for cap-\nturing selected aspects of natural language that are relevant for expressing logical\nproofs. A formal calculus in symbolic logic provides the syntax of a language, together\nwith rules of inference and, possibly, rules of interpretation in a set-theoretic model;\nexamples are propositional logic and first-order logic. Given such a calculus, with a\nwell-defined syntax and semantics, it becomes possible to associate meanings with\nexpressions of natural language by translating them into expressions of the formal cal-\nculus. For example, if we translate John saw Mary into a formula saw(j, m) , we (im-\nplicitly or explicitly) interpret the English verb saw as a binary relation, and John and\nMary as denoting individuals. More general statements like All birds fly require quan-\ntifiers, in this case $\\forall$ , meaning for all : $\\forall x$ ( bird(x) → fly(x) ). This use of logic provided\nthe technical machinery to perform inferences that are an important part of language\nunderstanding.",
          "level": -1,
          "page": 464,
          "reading_order": 5,
          "bbox": [
            97,
            465,
            585,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_464_order_6",
          "label": "para",
          "text": "A closely related development was the principle of compositionality , namely that\nthe meaning of a complex expression is composed from the meaning of its parts and\ntheir mode of combination ( Chapter 10 ). This principle provided a useful corre-\nspondence between syntax and semantics, namely that the meaning of a complex ex-\npression could be computed recursively. Consider the sentence It is not true that p ,\nwhere p is a proposition. We can represent the meaning of this sentence as not ( p ).",
          "level": -1,
          "page": 464,
          "reading_order": 6,
          "bbox": [
            97,
            688,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_464_order_7",
          "label": "foot",
          "text": "442 | Afterword: The Language Challeng",
          "level": -1,
          "page": 464,
          "reading_order": 7,
          "bbox": [
            97,
            824,
            270,
            842
          ],
          "section_number": "442",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_465_order_0",
          "label": "para",
          "text": "Similarly, we can represent the meaning of John saw Mary as saw(j, m). Now we can\ncompute the interpretation of It is not true that John saw Mary recursively, using the\nforegoing information, to get not(saw(j,m)).",
          "level": -1,
          "page": 465,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_465_order_1",
          "label": "para",
          "text": "The approaches just outlined share the premise that computing with natural language\ncrucially relies on rules for manipulating symbolic representations. For a certain period\nin the development of NLP, particularly during the 1980s, this premise provided a\ncommon starting point for both linguists and practitioners of NLP, leading to a family\nof grammar formalisms known as unification-based (or feature-based) grammar (see\nChapter 9 ), and to NLP applications implemented in the Prolog programming lan-\nguage. Although grammar-based NLP is still a significant area of research, it has become\nsomewhat eclipsed in the last 15 – 20 years due to a variety of factors. One significant\ninfluence came from automatic speech recognition. Although early work in speech\nprocessing adopted a model that emulated the kind of rule-based phonological pho-\nnology processing typified by the Sound Pattern of English (Chomsky & Halle, 1968) ,\nthis turned out to be hopelessly inadequate in dealing with the hard problem of rec-\nognizing actual speech in anything like real time. By contrast, systems which involved\nlearning patterns from large bodies of speech data were significantly more accurate,\nefficient, and robust. In addition, the speech community found that progress in building\nbetter systems was hugely assisted by the construction of shared resources for quanti-\ntatively measuring performance against common test data. Eventually, much of the\nNLP community embraced a data-intensive orientation to language processing, cou-\npled with a growing use of machine-learning techniques and evaluation-led\nmethodology.",
          "level": -1,
          "page": 465,
          "reading_order": 1,
          "bbox": [
            97,
            132,
            585,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_465_order_2",
      "label": "sec",
      "text": "Contemporary Philosophical Divides",
      "level": 1,
      "page": 465,
      "reading_order": 2,
      "bbox": [
        97,
        483,
        386,
        510
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_465_order_3",
          "label": "para",
          "text": "The contrasting approaches to NLP described in the preceding section relate back to\nearly metaphysical debates about rationalism versus empiricism and realism versus\nidealism that occurred in the Enlightenment period of Western philosophy. These\ndebates took place against a backdrop of orthodox thinking in which the source of all\nknowledge was believed to be divine revelation. During this period of the 17th and 18th\ncenturies, philosophers argued that human reason or sensory experience has priority\nover revelation. Descartes and Leibniz, among others, took the rationalist position,\nasserting that all truth has its origins in human thought, and in the existence of “ innate\nideas ” implanted in our minds from birth. For example, they argued that the principles\nof Euclidean geometry were developed using human reason, and were not the result of\nsupernatural revelation or sensory experience. In contrast, Locke and others took the\nempiricist view, that our primary source of knowledge is the experience of our faculties,\nand that human reason plays a secondary role in reflecting on that experience. Often-\ncited evidence for this position was Galileo's discovery—based on careful observation\nof the motion of the planets—that the solar system is heliocentric and not geocentric.\nIn the context of linguistics, this debate leads to the following question: to what extent\ndoes human linguistic experience, versus our innate “ language faculty, ” provide the",
          "level": -1,
          "page": 465,
          "reading_order": 3,
          "bbox": [
            97,
            519,
            586,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_465_order_4",
          "label": "foot",
          "text": "Afterword: The Language Challenge | 443",
          "level": -1,
          "page": 465,
          "reading_order": 4,
          "bbox": [
            404,
            824,
            584,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_466_order_0",
          "label": "para",
          "text": "basis for our knowledge of language? In NLP this issue surfaces in debates about the\npriority of corpus data versus linguistic introspection in the construction of computa-\ntional models.",
          "level": -1,
          "page": 466,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_466_order_1",
          "label": "para",
          "text": "A further concern, enshrined in the debate between realism and idealism, was the\nmetaphysical status of the constructs of a theory. Kant argued for a distinction between\nphenomena, the manifestations we can experience, and “things in themselves” which\ncan never been known directly. A linguistic realist would take a theoretical construct\nlike noun phrase to be a real-world entity that exists independently of human percep-\ntion and reason, and which actually causes the observed linguistic phenomena. A lin-\nguistic idealist, on the other hand, would argue that noun phrases, along with more\nabstract constructs, like semantic representations, are intrinsically unobservable, and\nsimply play the role of useful fictions. The way linguists write about theories often\nbetrays a realist position, whereas NLP practitioners occupy neutral territory or else\nlean toward the idealist position. Thus, in NLP, it is often enough if a theoretical ab-\nstraction leads to a useful result; it does not matter whether this result sheds any light\non human linguistic processing.",
          "level": -1,
          "page": 466,
          "reading_order": 1,
          "bbox": [
            97,
            125,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_466_order_2",
          "label": "para",
          "text": "These issues are still alive today, and show up in the distinctions between symbolic\nversus statistical methods, deep versus shallow processing, binary versus gradient clas-\nsifications, and scientific versus engineering goals. However, such contrasts are now\nhighly nuanced, and the debate is no longer as polarized as it once was. In fact, most\nof the discussions—and most of the advances, even—involve a “balancing act.” For\nexample, one intermediate position is to assume that humans are innately endowed\nwith analogical and memory-based learning methods (weak rationalism), and use these\nmethods to identify meaningful patterns in their sensory language experience (empiri-\ncism).",
          "level": -1,
          "page": 466,
          "reading_order": 2,
          "bbox": [
            97,
            349,
            585,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_466_order_3",
          "label": "para",
          "text": "We have seen many examples of this methodology throughout this book. Statistical\nmethods inform symbolic models anytime corpus statistics guide the selection of pro-\nductions in a context-free grammar, i.e., “grammar engineering.” Symbolic methods\ninform statistical models anytime a corpus that was created using rule-based methods\nis used as a source of features for training a statistical language model, i.e., “grammatical\ninference.” The circle is closed.",
          "level": -1,
          "page": 466,
          "reading_order": 3,
          "bbox": [
            97,
            510,
            585,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_466_order_4",
      "label": "sec",
      "text": "NLTK Roadmap",
      "level": 1,
      "page": 466,
      "reading_order": 4,
      "bbox": [
        98,
        627,
        218,
        656
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_466_order_5",
          "label": "para",
          "text": "The Natural Language Toolkit is a work in progress, and is being continually expanded\nas people contribute code. Some areas of NLP and linguistics are not (yet) well sup-\nported in NLTK, and contributions in these areas are especially welcome. Check http:\n//www.nltk.org/ for news about developments after the publication date of this book.\nContributions in the following areas are particularly encouraged:",
          "level": -1,
          "page": 466,
          "reading_order": 5,
          "bbox": [
            97,
            663,
            584,
            745
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_466_order_6",
          "label": "foot",
          "text": "444 | Afterword: The Language Challeng",
          "level": -1,
          "page": 466,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            270,
            842
          ],
          "section_number": "444",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_467_order_0",
      "label": "sec",
      "text": "Phonology and morphology",
      "level": 1,
      "page": 467,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        252,
        89
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_467_order_1",
          "label": "para",
          "text": "Computational approaches to the study of sound patterns and word structures\ntypically use a finite-state toolkit. Phenomena such as suppletion and non-concat-\nenative morphology are difficult to address using the string-processing methods\nwe have been studying. The technical challenge is not only to link NLTK to a high-\nperformance finite-state toolkit, but to avoid duplication of lexical data and to link\nthe morphosyntactic features needed by morph analyzers and syntactic parsers.",
          "level": -1,
          "page": 467,
          "reading_order": 1,
          "bbox": [
            122,
            89,
            585,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_467_order_2",
      "label": "sec",
      "text": "High-performance components",
      "level": 1,
      "page": 467,
      "reading_order": 2,
      "bbox": [
        100,
        195,
        270,
        209
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_467_order_3",
          "label": "para",
          "text": "Some NLP tasks are too computationally intensive for pure Python implementa-\ntions to be feasible. However, in some cases the expense arises only when training\nmodels, not when using them to label inputs. NLTK's package system provides a\nconvenient way to distribute trained models, even models trained using corpora\nthat cannot be freely distributed. Alternatives are to develop Python interfaces to\nhigh-performance machine learning tools, or to expand the reach of Python by\nusing parallel programming techniques such as MapReduce.",
          "level": -1,
          "page": 467,
          "reading_order": 3,
          "bbox": [
            122,
            212,
            585,
            325
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_467_order_4",
      "label": "sec",
      "text": "Lexical semantics\nThis is a vibra-",
      "level": 1,
      "page": 467,
      "reading_order": 4,
      "bbox": [
        98,
        331,
        198,
        359
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_467_order_5",
          "label": "para",
          "text": "lexicon, ontologies, multiword expressions, etc., mostly outside the scope of NLTK\nas it stands. A conservative goal would be to access lexical information from rich\nexternal stores in support of tasks in word sense disambiguation, parsing, and\nsemantic interpretation.",
          "level": -1,
          "page": 467,
          "reading_order": 5,
          "bbox": [
            121,
            359,
            585,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_467_order_6",
      "label": "sec",
      "text": "Natural language generation",
      "level": 1,
      "page": 467,
      "reading_order": 6,
      "bbox": [
        97,
        430,
        261,
        449
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_467_order_7",
          "label": "para",
          "text": "Producing coherent text from underlying representations of meaning is an impor-\ntant part of NLP; a unification-based approach to NLG has been developed in\nNLTK, and there is scope for more contributions in this area.",
          "level": -1,
          "page": 467,
          "reading_order": 7,
          "bbox": [
            122,
            449,
            584,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_467_order_8",
      "label": "sec",
      "text": "Linguistic fieldwork",
      "level": 1,
      "page": 467,
      "reading_order": 8,
      "bbox": [
        98,
        501,
        207,
        520
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_467_order_9",
          "label": "para",
          "text": "A major challenge faced by linguists is to document thousands of endangered lan-\nguages, work which generates heterogeneous and rapidly evolving data in large\nquantities. More fieldwork data formats, including interlinear text formats and\nlexicon interchange formats, could be supported in NLTK, helping linguists to\ncurate and analyze this data, while liberating them to spend as much time as pos-\nsible on data elicitation.",
          "level": -1,
          "page": 467,
          "reading_order": 9,
          "bbox": [
            121,
            520,
            585,
            618
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_467_order_10",
      "label": "sec",
      "text": "Other languages",
      "level": 1,
      "page": 467,
      "reading_order": 10,
      "bbox": [
        98,
        626,
        189,
        640
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_467_order_11",
          "label": "para",
          "text": "Improved support for NLP in languages other than English could involve work in\ntwo areas: obtaining permission to distribute more corpora with NLTK's data col-\nlection; and writing language-specific HOWTOs for posting at http://www.nltk\n.org/howto , illustrating the use of NLTK and discussing language-specific problems\nfor NLP, including character encodings, word segmentation, and morphology.\nNLP researchers with expertise in a particular language could arrange to translate\nthis book and host a copy on the NLTK website; this would go beyond translating\nthe discussions to providing equivalent worked examples using data in the target\nlanguage, a non-trivial undertaking.",
          "level": -1,
          "page": 467,
          "reading_order": 11,
          "bbox": [
            122,
            640,
            585,
            789
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_467_order_12",
          "label": "foot",
          "text": "Afterword: The Language Challenge | 445",
          "level": -1,
          "page": 467,
          "reading_order": 12,
          "bbox": [
            404,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_468_order_0",
      "label": "sec",
      "text": "NLTK-Contrib",
      "level": 1,
      "page": 468,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        181,
        89
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_468_order_1",
          "label": "para",
          "text": "Many of NLTK's core components were contributed by members of the NLP com-\nmunity, and were initially housed in NLTK's “Contrib” package, nltk_contrib.\nThe only requirement for software to be added to this package is that it must be\nwritten in Python, relevant to NLP, and given the same open source license as the\nrest of NLTK. Imperfect software is welcome, and will probably be improved over\ntime by other members of the NLP community.",
          "level": -1,
          "page": 468,
          "reading_order": 1,
          "bbox": [
            118,
            89,
            585,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_468_order_2",
      "label": "sec",
      "text": "Teaching materials",
      "level": 1,
      "page": 468,
      "reading_order": 2,
      "bbox": [
        99,
        195,
        207,
        209
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_468_order_3",
          "label": "para",
          "text": "Since the earliest days of NLTK development, teaching materials have accompa-\nnied the software, materials that have gradually expanded to fill this book, plus a\nsubstantial quantity of online materials as well. We hope that instructors who\nsupplement these materials with presentation slides, problem sets, solution sets,\nand more detailed treatments of the topics we have covered will make them avail-\nable, and will notify the authors so we can link them from http://www.nltk.org/. Of\nparticular value are materials that help NLP become a mainstream course in the\nundergraduate programs of computer science and linguistics departments, or that\nmake NLP accessible at the secondary level, where there is significant scope for\nincluding computational content in the language, literature, computer science, and\ninformation technology curricula.",
          "level": -1,
          "page": 468,
          "reading_order": 3,
          "bbox": [
            122,
            209,
            586,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_468_order_4",
      "label": "sec",
      "text": "Only a toolkit",
      "level": 1,
      "page": 468,
      "reading_order": 4,
      "bbox": [
        98,
        394,
        180,
        412
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_468_order_5",
          "label": "para",
          "text": "As stated in the preface, NLTK is a toolkit, not a system. Many problems will be\ntackled with a combination of NLTK, Python, other Python libraries, and interfaces\nto external NLP tools and formats.",
          "level": -1,
          "page": 468,
          "reading_order": 5,
          "bbox": [
            118,
            412,
            585,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_468_order_6",
          "label": "foot",
          "text": "446 | Afterword: The Language Challeng",
          "level": -1,
          "page": 468,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            270,
            842
          ],
          "section_number": "446",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_469_order_0",
      "label": "sec",
      "text": "Envoi...",
      "level": 1,
      "page": 469,
      "reading_order": 0,
      "bbox": [
        98,
        71,
        156,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_469_order_1",
          "label": "para",
          "text": "Linguists are sometimes asked how many languages they speak, and have to explain\nthat this field actually concerns the study of abstract structures that are shared by lan-\nguages, a study which is more profound and elusive than learning to speak as many\nlanguages as possible. Similarly, computer scientists are sometimes asked how many\nprogramming languages they know, and have to explain that computer science actually\nconcerns the study of data structures and algorithms that can be implemented in any\nprogramming language, a study which is more profound and elusive than striving for\nfluency in as many programming languages as possible.",
          "level": -1,
          "page": 469,
          "reading_order": 1,
          "bbox": [
            97,
            107,
            585,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_469_order_2",
          "label": "para",
          "text": "This book has covered many topics in the field of Natural Language Processing. Most\nof the examples have used Python and English. However, it would be unfortunate if\nreaders concluded that NLP is about how to write Python programs to manipulate\nEnglish text, or more broadly, about how to write programs (in any programming lan-\nguage) to manipulate text (in any natural language). Our selection of Python and Eng-\nlish was expedient, nothing more. Even our focus on programming itself was only a\nmeans to an end: as a way to understand data structures and algorithms for representing\nand manipulating collections of linguistically annotated text, as a way to build new\nlanguage technologies to better serve the needs of the information society, and ulti-\nmately as a pathway into deeper understanding of the vast riches of human language.",
          "level": -1,
          "page": 469,
          "reading_order": 2,
          "bbox": [
            97,
            248,
            586,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_469_order_3",
          "label": "para",
          "text": "But for the present: happy hacking!",
          "level": -1,
          "page": 469,
          "reading_order": 3,
          "bbox": [
            97,
            421,
            297,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_469_order_4",
          "label": "foot",
          "text": "Afterword: The Language Challenge | 447",
          "level": -1,
          "page": 469,
          "reading_order": 4,
          "bbox": [
            404,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_470_order_0",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 470,
          "reading_order": 0,
          "bbox": [
            153,
            161,
            494,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_471_order_0",
      "label": "sec",
      "text": "Bibliography",
      "level": 1,
      "page": 471,
      "reading_order": 0,
      "bbox": [
        413,
        107,
        585,
        143
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_471_order_1",
          "label": "para",
          "text": "[Abney, 1989] Steven P. Abney. A computational model of human parsing. Journal of\nPsycholinguistic Research, 18:129–144, 1989.",
          "level": -1,
          "page": 471,
          "reading_order": 1,
          "bbox": [
            98,
            302,
            584,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_2",
          "label": "para",
          "text": "[Abney, 1991] Steven P. Abney. Parsing by chunks. In Robert C. Berwick, Steven P.\nAbney, and Carol Tenny, editors, Principle-Based Parsing: Computation and Psycho-\nlinguistics , volume 44 of Studies in Linguistics and Philosophy . Kluwer Academic Pub-\nlishers, Dordrecht, 1991.",
          "level": -1,
          "page": 471,
          "reading_order": 2,
          "bbox": [
            97,
            340,
            584,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_3",
          "label": "para",
          "text": "[Abney, 1996a] Steven Abney. Part-of-speech tagging and partial parsing . In Ken\nChurch, Steve Young, and Gerrit Bloodhooft, editors, Corpus-Based Methods in Lan-\nguage and Speech . Kluwer Academic Publishers, Dordrecht, 1996.",
          "level": -1,
          "page": 471,
          "reading_order": 3,
          "bbox": [
            96,
            412,
            584,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_4",
          "label": "para",
          "text": "[Abney, 1996b] Steven Abney. Statistical methods and linguistics. In Judith Klavans\nand Philip Resnik, editors, The Balancing Act: Combining Symbolic and Statistical Ap-\nproaches to Language. MIT Press, 1996.",
          "level": -1,
          "page": 471,
          "reading_order": 4,
          "bbox": [
            97,
            465,
            585,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_5",
          "label": "para",
          "text": "[Abney, 2008] Steven Abney. Semisupervised Learning for Computational Linguistics .\nChapman and Hall, 2008.",
          "level": -1,
          "page": 471,
          "reading_order": 5,
          "bbox": [
            97,
            519,
            584,
            559
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_6",
          "label": "para",
          "text": "[Agirre and Edmonds, 2007] Eneko Agirre and Philip Edmonds. Word Sense Disam-\nbiguation: Algorithms and Applications . Springer, 2007.",
          "level": -1,
          "page": 471,
          "reading_order": 6,
          "bbox": [
            97,
            564,
            584,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_7",
          "label": "para",
          "text": "Alpaydin, 2004] Ethem Alpaydin. Introduction to Machine Learning. MIT Press, 2004.",
          "level": -1,
          "page": 471,
          "reading_order": 7,
          "bbox": [
            100,
            600,
            584,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_8",
          "label": "para",
          "text": "[Ananiadou and McNaught, 2006] Sophia Ananiadou and John McNaught, editors.\nText Mining for Biology and Biomedicine . Artech House, 2006.",
          "level": -1,
          "page": 471,
          "reading_order": 8,
          "bbox": [
            98,
            627,
            584,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_9",
          "label": "para",
          "text": "[Androutsopoulos et al., 1995] Ion Androutsopoulos, Graeme Ritchie, and Peter Tha-\nnisch. Natural language interfaces to databases—an introduction. Journal of Natural\nLanguage Engineering , 1:29–81, 1995.",
          "level": -1,
          "page": 471,
          "reading_order": 9,
          "bbox": [
            97,
            663,
            585,
            719
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_10",
          "label": "para",
          "text": "[Artstein and Poesio, 2008] Ron Artstein and Massimo Poesio. Inter-coder agreement\nfor computational linguistics. Computational Linguistics , pages 555–596, 2008.",
          "level": -1,
          "page": 471,
          "reading_order": 10,
          "bbox": [
            98,
            725,
            584,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_11",
          "label": "para",
          "text": "[Baayen, 2008] Harald Baayen. Analyzing Linguistic Data: A Practical Introduction to\nStatistics Using R. Cambridge University Press, 2008.",
          "level": -1,
          "page": 471,
          "reading_order": 11,
          "bbox": [
            97,
            761,
            584,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_471_order_12",
          "label": "foot",
          "text": "449",
          "level": -1,
          "page": 471,
          "reading_order": 12,
          "bbox": [
            566,
            824,
            585,
            837
          ],
          "section_number": "449",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_0",
          "label": "para",
          "text": "[Bachenko and Fitzpatrick, 1990] J. Bachenko and E. Fitzpatrick. A computational\ngrammar of discourse-neutral prosodic phrasing in English. Computational Linguis-\ntics , 16:155–170, 1990.",
          "level": -1,
          "page": 472,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_1",
          "label": "para",
          "text": "[Baldwin & Kim, 2010] Timothy Baldwin and Su Nam Kim. Multiword Expressions.\nIn Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Pro-\ncessing , second edition. Morgan and Claypool, 2010.",
          "level": -1,
          "page": 472,
          "reading_order": 1,
          "bbox": [
            97,
            125,
            584,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_2",
          "label": "para",
          "text": "[Beazley, 2006] David M. Beazley. Python Essential Reference . Developer’s Library.\nSams Publishing, third edition, 2006.",
          "level": -1,
          "page": 472,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            584,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_3",
          "label": "para",
          "text": "[Biber et al., 1998] Douglas Biber, Susan Conrad, and Randi Reppen. Corpus Linguis-\ntics: Investigating Language Structure and Use . Cambridge University Press, 1998.",
          "level": -1,
          "page": 472,
          "reading_order": 3,
          "bbox": [
            97,
            224,
            584,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_4",
          "label": "para",
          "text": "[Bird, 1999] Steven Bird. Multidimensional exploration of online linguistic field data.\nIn Pius Tamanji, Masako Hirotani, and Nancy Hall, editors, Proceedings of the 29th\nAnnual Meeting of the Northeast Linguistics Society , pages 33–47. GLSA, University of\nMassachussetts at Amherst, 1999.",
          "level": -1,
          "page": 472,
          "reading_order": 4,
          "bbox": [
            97,
            268,
            584,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_5",
          "label": "para",
          "text": "Bird and Liberman, 2001] Steven Bird and Mark Liberman. A formal framework for\ninguistic annotation . Speech Communication , 33:23–60, 2001.",
          "level": -1,
          "page": 472,
          "reading_order": 5,
          "bbox": [
            100,
            340,
            585,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_6",
          "label": "para",
          "text": "[Bird and Simons, 2003] Steven Bird and Gary Simons. Seven dimensions of portability\nfor language documentation and description. Language , 79:557–582, 2003.",
          "level": -1,
          "page": 472,
          "reading_order": 6,
          "bbox": [
            98,
            376,
            585,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_7",
          "label": "para",
          "text": "[Blackburn and Bos, 2005] Patrick Blackburn and Johan Bos. Representation and In-\nference for Natural Language: A First Course in Computational Semantics . CSLI Publi-\ncations, Stanford, CA, 2005.",
          "level": -1,
          "page": 472,
          "reading_order": 7,
          "bbox": [
            96,
            421,
            584,
            466
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_8",
          "label": "para",
          "text": "BNC, 1999] BNC. British National Corpus, 1999. [http://info.ox.ac.uk/bnc/].",
          "level": -1,
          "page": 472,
          "reading_order": 8,
          "bbox": [
            100,
            474,
            548,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_9",
          "label": "para",
          "text": "[Brent and Cartwright, 1995] Michael Brent and Timothy Cartwright. Distributional\nregularity and phonotactic constraints are useful for segmentation. In Michael Brent,\neditor, Computational Approaches to Language Acquisition . MIT Press, 1995.",
          "level": -1,
          "page": 472,
          "reading_order": 9,
          "bbox": [
            97,
            501,
            584,
            548
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_10",
          "label": "para",
          "text": "[Bresnan and Hay, 2006] Joan Bresnan and Jennifer Hay. Gradient grammar: An effect\nof animacy on the syntax of give in New Zealand and American English. Lingua 118:\n254–59, 2008.",
          "level": -1,
          "page": 472,
          "reading_order": 10,
          "bbox": [
            97,
            555,
            585,
            601
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_11",
          "label": "para",
          "text": "[Budanitsky and Hirst, 2006] Alexander Budanitsky and Graeme Hirst. Evaluating\nwordnet-based measures of lexical semantic relatedness. Computational Linguistics ,\n32:13–48, 2006.",
          "level": -1,
          "page": 472,
          "reading_order": 11,
          "bbox": [
            97,
            609,
            585,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_12",
          "label": "para",
          "text": "Burton-Roberts, 1997] Noel Burton-Roberts. Analysing Sentences. Longman, 1997.",
          "level": -1,
          "page": 472,
          "reading_order": 12,
          "bbox": [
            100,
            670,
            574,
            684
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_13",
          "label": "para",
          "text": "[Buseman et al., 1996] Alan Buseman, Karen Buseman, and Rod Early. The Linguist’\ns\nShoebox: Integrated Data Management and Analysis for the Field Linguist. Waxhaw NC:\nSIL, 1996.",
          "level": -1,
          "page": 472,
          "reading_order": 13,
          "bbox": [
            97,
            689,
            584,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_14",
          "label": "para",
          "text": "[Carpenter, 1992] Bob Carpenter. The Logic of Typed Feature Structures . Cambridge\nUniversity Press, 1992.",
          "level": -1,
          "page": 472,
          "reading_order": 14,
          "bbox": [
            98,
            750,
            585,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_472_order_15",
          "label": "foot",
          "text": "450 | Bibliography",
          "level": -1,
          "page": 472,
          "reading_order": 15,
          "bbox": [
            97,
            824,
            183,
            842
          ],
          "section_number": "450",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_0",
          "label": "para",
          "text": "Carpenter, 1997] Bob Carpenter. Type-Logical Semantics. MIT Press, 1997.",
          "level": -1,
          "page": 473,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            531,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_1",
          "label": "para",
          "text": "[Chierchia and McConnell-Ginet, 1990] Gennaro Chierchia and Sally McConnell-Gi-\nnet. Meaning and Grammar: An Introduction to Meaning . MIT Press, Cambridge, MA,\n1990.",
          "level": -1,
          "page": 473,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_2",
          "label": "para",
          "text": "[Chomsky, 1965] Noam Chomsky. Aspects of the Theory of Syntax. MIT Press, Cam-\nbridge, MA, 1965.",
          "level": -1,
          "page": 473,
          "reading_order": 2,
          "bbox": [
            100,
            152,
            584,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_3",
          "label": "para",
          "text": "[Chomsky, 1970] Noam Chomsky. Remarks on nominalization. In R. Jacobs and P.\nRosenbaum, editors, Readings in English Transformational Grammar . Blaisdell, Wal-\ntham, MA, 1970.",
          "level": -1,
          "page": 473,
          "reading_order": 3,
          "bbox": [
            97,
            195,
            584,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_4",
          "label": "para",
          "text": "[Chomsky and Halle, 1968] Noam Chomsky and Morris Halle. The Sound Pattern of\nEnglish. New York: Harper and Row, 1968.",
          "level": -1,
          "page": 473,
          "reading_order": 4,
          "bbox": [
            98,
            250,
            584,
            282
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_5",
          "label": "para",
          "text": "[Church and Patil, 1982] Kenneth Church and Ramesh Patil. Coping with syntactic\nambiguity or how to put the block in the box on the table. American Journal of Com-\nputational Linguistics , 8:139–149, 1982.",
          "level": -1,
          "page": 473,
          "reading_order": 5,
          "bbox": [
            97,
            286,
            584,
            340
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_6",
          "label": "para",
          "text": "[Cohen and Hunter, 2004] K. Bretonnel Cohen and Lawrence Hunter. Natural lan-\nguage processing and systems biology. In Werner Dubitzky and Francisco Azuaje, ed-\nitors, Artificial Intelligence Methods and Tools for Systems Biology , page 147–174\nSpringer Verlag, 2004.",
          "level": -1,
          "page": 473,
          "reading_order": 6,
          "bbox": [
            97,
            348,
            585,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_7",
          "label": "para",
          "text": "[Cole, 1997] Ronald Cole, editor. Survey of the State of the Art in Human Language\nTechnology . Studies in Natural Language Processing. Cambridge University Press,\n1997.",
          "level": -1,
          "page": 473,
          "reading_order": 7,
          "bbox": [
            98,
            421,
            585,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_8",
          "label": "para",
          "text": "[Copestake, 2002] Ann Copestake. Implementing Typed Feature Structure Grammars .\nCSLI Publications, Stanford, CA, 2002.",
          "level": -1,
          "page": 473,
          "reading_order": 8,
          "bbox": [
            97,
            474,
            584,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_9",
          "label": "para",
          "text": "Corbett, 2006] Greville G. Corbett. Agreement. Cambridge University Press, 2006.",
          "level": -1,
          "page": 473,
          "reading_order": 9,
          "bbox": [
            100,
            517,
            574,
            531
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_10",
          "label": "para",
          "text": "[Croft et al., 2009] Bruce Croft, Donald Metzler, and Trevor Strohman. Search Engines:\nInformation Retrieval in Practice. Addison Wesley, 2009.",
          "level": -1,
          "page": 473,
          "reading_order": 10,
          "bbox": [
            98,
            537,
            584,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_11",
          "label": "para",
          "text": "[Daelemans and van den Bosch, 2005] Walter Daelemans and Antal van den Bosch.\nMemory-Based Language Processing . Cambridge University Press, 2005.",
          "level": -1,
          "page": 473,
          "reading_order": 11,
          "bbox": [
            97,
            581,
            584,
            611
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_12",
          "label": "para",
          "text": "[Dagan et al., 2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL\nrecognising textual entailment challenge. In J. Quinonero-Candela, I. Dagan, B. Mag-\nnini, and F. d’Alché Buc, editors, Machine Learning Challenges , volume 3944 of Lecture\nNotes in Computer Science , pages 177–190. Springer, 2006.",
          "level": -1,
          "page": 473,
          "reading_order": 12,
          "bbox": [
            97,
            618,
            585,
            684
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_13",
          "label": "para",
          "text": "[Dale et al., 2000] Robert Dale, Hermann Moisl, and Harold Somers, editors. Handbook\nof Natural Language Processing. Marcel Dekker, 2000.",
          "level": -1,
          "page": 473,
          "reading_order": 13,
          "bbox": [
            100,
            689,
            584,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_14",
          "label": "para",
          "text": "(Dalrymple, 2001] Mary Dalrymple. Lexical Functional Grammar, volume 34 of Syntax\nand Semantics. Academic Press, New York, 2001.",
          "level": -1,
          "page": 473,
          "reading_order": 14,
          "bbox": [
            100,
            733,
            585,
            762
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_473_order_15",
          "label": "foot",
          "text": "Bibliography |451",
          "level": -1,
          "page": 473,
          "reading_order": 15,
          "bbox": [
            494,
            824,
            584,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_0",
          "label": "para",
          "text": "[Dalrymple et al., 1999] Mary Dalrymple, V. Gupta, John Lamping, and V. Saraswat.\nRelating resource-based semantics to categorial semantics. In Mary Dalrymple, editor,\nSemantics and Syntax in Lexical Functional Grammar: The Resource Logic Approach ,\npages 261–280. MIT Press, Cambridge, MA, 1999.",
          "level": -1,
          "page": 474,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_1",
          "label": "para",
          "text": "[Dowty et al., 1981] David R. Dowty, Robert E. Wall, and Stanley Peters. Introduction\nto Montague Semantics . Kluwer Academic Publishers, 1981.",
          "level": -1,
          "page": 474,
          "reading_order": 1,
          "bbox": [
            97,
            143,
            584,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_2",
          "label": "para",
          "text": "[Earley, 1970] Jay Earley. An efficient context-free parsing algorithm. Communications\nof the Association for Computing Machinery, 13:94–102, 1970.",
          "level": -1,
          "page": 474,
          "reading_order": 2,
          "bbox": [
            100,
            188,
            585,
            224
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_3",
          "label": "para",
          "text": "[Emele and Zajac, 1990] Martin C. Emele and Rémi Zajac. Typed unification gram-\nmars. In Proceedings of the 13th Conference on Computational Linguistics , pages 293–\n298. Association for Computational Linguistics, Morristown, NJ, 1990.",
          "level": -1,
          "page": 474,
          "reading_order": 3,
          "bbox": [
            97,
            224,
            585,
            277
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_4",
          "label": "para",
          "text": "[Farghaly, 2003] Ali Farghaly, editor. Handbook for Language Engineers . CSLI Publi-\ncations, Stanford, CA, 2003.",
          "level": -1,
          "page": 474,
          "reading_order": 4,
          "bbox": [
            97,
            277,
            584,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_5",
          "label": "para",
          "text": "[ Feldman and Sanger, 2007 ] Ronen Feldman and James Sanger. The Text Mining\nHandbook: Advanced Approaches in Analyzing Unstructured Data . Cambridge Univer-\nsity Press, 2007.",
          "level": -1,
          "page": 474,
          "reading_order": 5,
          "bbox": [
            97,
            322,
            585,
            372
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_6",
          "label": "para",
          "text": "[Fellbaum, 1998] Christiane Fellbaum, editor. WordNet: An Electronic Lexical Data-\nbase. MIT Press, 1998. http://wordnet.princeton.edu/.",
          "level": -1,
          "page": 474,
          "reading_order": 6,
          "bbox": [
            97,
            376,
            584,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_7",
          "label": "para",
          "text": "[Finegan, 2007] Edward Finegan. Language: Its Structure and Use . Wadsworth, Fifth\nedition, 2007.",
          "level": -1,
          "page": 474,
          "reading_order": 7,
          "bbox": [
            97,
            421,
            584,
            449
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_8",
          "label": "para",
          "text": "Forsyth and Martell, 2007] Eric N. Forsyth and Craig H. Martell. Lexical and discourse\nanalysis of online chat dialog. In Proceedings of the First IEEE International Conference\non Semantic Computing , pages 19–26, 2007.",
          "level": -1,
          "page": 474,
          "reading_order": 8,
          "bbox": [
            100,
            456,
            585,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_9",
          "label": "para",
          "text": "Friedl, 2002] Jeffrey E. F. Friedl. Mastering Regular Expressions. O'Reilly, second ed-\ntion, 2002.",
          "level": -1,
          "page": 474,
          "reading_order": 9,
          "bbox": [
            100,
            510,
            585,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_10",
          "label": "para",
          "text": "[Gamut, 1991a] L. T. F. Gamut. Intensional Logic and Logical Grammar , volume 2 of\nLogic, Language and Meaning . University of Chicago Press, Chicago, 1991.",
          "level": -1,
          "page": 474,
          "reading_order": 10,
          "bbox": [
            98,
            555,
            584,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_11",
          "label": "para",
          "text": "[Gamut, 1991b] L. T. F. Gamut. Introduction to Logic, volume 1 of Logic, Language\nand Meaning. University of Chicago Press, 1991.",
          "level": -1,
          "page": 474,
          "reading_order": 11,
          "bbox": [
            100,
            591,
            585,
            628
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_12",
          "label": "para",
          "text": "[Garofolo et al., 1986] John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathon\nG. Fiscus, David S. Pallett, and Nancy L. Dahlgren. The DARPA TIMIT Acoustic-\nPhonetic Continuous Speech Corpus CDROM . NIST, 1986.",
          "level": -1,
          "page": 474,
          "reading_order": 12,
          "bbox": [
            97,
            636,
            584,
            684
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_13",
          "label": "para",
          "text": "[Gazdar et al., 1985] Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag (1985).\nGeneralized Phrase Structure Grammar. Basil Blackwell, 1985.",
          "level": -1,
          "page": 474,
          "reading_order": 13,
          "bbox": [
            97,
            689,
            584,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_14",
          "label": "para",
          "text": "[Gomes et al., 2006] Bruce Gomes, William Hayes, and Raf Podowski. Text mining.\nIn Darryl Leon and Scott Markel, editors, In Silico Technologies in Drug Target Identi-\nfication and Validation , Taylor & Francis, 2006.",
          "level": -1,
          "page": 474,
          "reading_order": 14,
          "bbox": [
            97,
            733,
            584,
            781
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_474_order_15",
          "label": "foot",
          "text": "452 | Bibliography",
          "level": -1,
          "page": 474,
          "reading_order": 15,
          "bbox": [
            97,
            824,
            183,
            842
          ],
          "section_number": "452",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_0",
          "label": "para",
          "text": "[Gries, 2009] Stefan Gries. Quantitative Corpus Linguistics with R: A Practical Intro-\nduction . Routledge, 2009.",
          "level": -1,
          "page": 475,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_1",
          "label": "para",
          "text": "[Guzdial, 2005] Mark Guzdial. Introduction to Computing and Programming in Python:\nA Multimedia Approach. Prentice Hall, 2005.",
          "level": -1,
          "page": 475,
          "reading_order": 1,
          "bbox": [
            97,
            115,
            584,
            144
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_2",
          "label": "para",
          "text": "[Harel, 2004] David Harel. Algorithmics: The Spirit of Computing . Addison Wesley,\n2004.",
          "level": -1,
          "page": 475,
          "reading_order": 2,
          "bbox": [
            98,
            152,
            584,
            182
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_3",
          "label": "para",
          "text": "[Hastie et al., 2009] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The El-\nements of Statistical Learning: Data Mining, Inference, and Prediction . Springer, second\nedition, 2009.",
          "level": -1,
          "page": 475,
          "reading_order": 3,
          "bbox": [
            97,
            195,
            584,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_4",
          "label": "para",
          "text": "[Hearst, 1992] Marti Hearst. Automatic acquisition of hyponyms from large text cor-\npora. In Proceedings of the 14th Conference on Computational Linguistics (COLING) ,\npages 539–545, 1992.",
          "level": -1,
          "page": 475,
          "reading_order": 4,
          "bbox": [
            100,
            250,
            584,
            298
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_5",
          "label": "para",
          "text": "[Heim and Kratzer, 1998] Irene Heim and Angelika Kratzer. Semantics in Generative\nGrammar . Blackwell, 1998.",
          "level": -1,
          "page": 475,
          "reading_order": 5,
          "bbox": [
            97,
            304,
            585,
            340
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_6",
          "label": "para",
          "text": "[Hirschman et al., 2005] Lynette Hirschman, Alexander Yeh, Christian Blaschke, and\nAlfonso Valencia. Overview of BioCreAtIvE: critical assessment of information extrac-\ntion for biology . BMC Bioinformatics , 6, May 2005. Supplement 1.",
          "level": -1,
          "page": 475,
          "reading_order": 6,
          "bbox": [
            97,
            348,
            585,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_7",
          "label": "para",
          "text": "Hodges, 1977] Wilfred Hodges. Logic. Penguin Books, Harmondsworth, 1977.",
          "level": -1,
          "page": 475,
          "reading_order": 7,
          "bbox": [
            100,
            403,
            557,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_8",
          "label": "para",
          "text": "[Huddleston and Pullum, 2002] Rodney D. Huddleston and Geoffrey K. Pullum. The\nCambridge Grammar of the English Language. Cambridge University Press, 2002.",
          "level": -1,
          "page": 475,
          "reading_order": 8,
          "bbox": [
            98,
            428,
            585,
            458
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_9",
          "label": "para",
          "text": "[Hunt and Thomas, 2000] Andrew Hunt and David Thomas. The Pragmatic Program-\nmer: From Journeyman to Master. Addison Wesley, 2000.",
          "level": -1,
          "page": 475,
          "reading_order": 9,
          "bbox": [
            100,
            465,
            584,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_10",
          "label": "para",
          "text": "[Indurkhya and Damerau, 2010] Nitin Indurkhya and Fred Damerau, editors. Hand-\nbook of Natural Language Processing . CRC Press, Taylor and Francis Group, second\nedition, 2010.",
          "level": -1,
          "page": 475,
          "reading_order": 10,
          "bbox": [
            97,
            507,
            584,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_11",
          "label": "para",
          "text": "[Jackendoff, 1977] Ray Jackendoff. X-Syntax: a Study of Phrase Structure. Number 2 in\nLinguistic Inquiry Monograph. MIT Press, Cambridge, MA, 1977.",
          "level": -1,
          "page": 475,
          "reading_order": 11,
          "bbox": [
            98,
            564,
            584,
            595
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_12",
          "label": "para",
          "text": "[Johnson, 1988] Mark Johnson. Attribute Value Logic and Theory of Grammar. CSLI\nLecture Notes Series. University of Chicago Press, 1988.",
          "level": -1,
          "page": 475,
          "reading_order": 12,
          "bbox": [
            98,
            600,
            584,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_13",
          "label": "para",
          "text": "[Jurafsky and Martin, 2008] Daniel Jurafsky and James H. Martin. Speech and\nLanguage Processing . Prentice Hall, second edition, 2008.",
          "level": -1,
          "page": 475,
          "reading_order": 13,
          "bbox": [
            98,
            644,
            584,
            674
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_14",
          "label": "para",
          "text": "[Kamp and Reyle, 1993] Hans Kamp and Uwe Reyle. From Discourse to the Lexicon:\nIntroduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Dis-\ncourse Representation Theory. Kluwer Academic Publishers, 1993.",
          "level": -1,
          "page": 475,
          "reading_order": 14,
          "bbox": [
            97,
            680,
            584,
            731
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_15",
          "label": "para",
          "text": "[Kaplan, 1989] Ronald Kaplan. The formal architecture of lexical-functional grammar.\nIn Chu-Ren Huang and Keh-Jiann Chen, editors, Proceedings of ROCLING II , pages\n1–18. CSLI, 1989. Reprinted in Dalrymple, Kaplan, Maxwell, and Zaenen (eds), Formal",
          "level": -1,
          "page": 475,
          "reading_order": 15,
          "bbox": [
            98,
            740,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_475_order_16",
          "label": "foot",
          "text": "Bibliography | 453",
          "level": -1,
          "page": 475,
          "reading_order": 16,
          "bbox": [
            494,
            824,
            584,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_0",
          "label": "para",
          "text": "Issues in Lexical-Functional Grammar , pages 7–27. CSLI Publications, Stanford, CA,\n1995.",
          "level": -1,
          "page": 476,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_1",
          "label": "para",
          "text": "[Kaplan and Bresnan, 1982] Ronald Kaplan and Joan Bresnan. Lexical-functional\ngrammar: A formal system for grammatical representation. In Joan Bresnan, editor,\nThe Mental Representation of Grammatical Relations , pages 173–281. MIT Press, Cam-\nbridge, MA, 1982.",
          "level": -1,
          "page": 476,
          "reading_order": 1,
          "bbox": [
            97,
            115,
            584,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_2",
          "label": "para",
          "text": "[Kasper and Rounds, 1986] Robert T. Kasper and William C. Rounds. A logical se-\nmantics for feature structures. In Proceedings of the 24th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 257–266. Association for Computational\nLinguistics, 1986.",
          "level": -1,
          "page": 476,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            584,
            252
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_3",
          "label": "para",
          "text": "[Kathol, 1999] Andreas Kathol. Agreement and the syntax-morphology interface in\nHPSG. In Robert D. Levine and Georgia M. Green, editors, Studies in Contemporary\nPhrase Structure Grammar , pages 223–274. Cambridge University Press, 1999.",
          "level": -1,
          "page": 476,
          "reading_order": 3,
          "bbox": [
            98,
            259,
            585,
            308
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_4",
          "label": "para",
          "text": "[Kay, 1985] Martin Kay. Unification in grammar. In Verónica Dahl and Patrick Saint-\nDizier, editors, Natural Language Understanding and Logic Programming , pages 233–\n240. North-Holland, 1985. Proceedings of the First International Workshop on Natural\nLanguage Understanding and Logic Programming.",
          "level": -1,
          "page": 476,
          "reading_order": 4,
          "bbox": [
            97,
            313,
            584,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_5",
          "label": "para",
          "text": "[",
          "level": -1,
          "page": 476,
          "reading_order": 5,
          "bbox": [
            100,
            385,
            585,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_6",
          "label": "para",
          "text": "[Kiusalaas, 2005] Jaan Kiusalaas. Numerical Methods in Engineering with Python. Cam-\nbridge University Press, 2005.",
          "level": -1,
          "page": 476,
          "reading_order": 6,
          "bbox": [
            100,
            430,
            584,
            461
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_7",
          "label": "para",
          "text": "[Klein and Manning, 2003] Dan Klein and Christopher D. Manning. A* parsing: Fast\nexact viterbi parse selection. In Proceedings of HLT-NAACL 03 , 2003.",
          "level": -1,
          "page": 476,
          "reading_order": 7,
          "bbox": [
            97,
            465,
            584,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_8",
          "label": "para",
          "text": "[Knuth, 2006] Donald E. Knuth. The Art of Computer Programming, Volume 4: Gen-\nerating All Trees. Addison Wesley, 2006.",
          "level": -1,
          "page": 476,
          "reading_order": 8,
          "bbox": [
            100,
            510,
            584,
            541
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_9",
          "label": "para",
          "text": "[Lappin, 1996] Shalom Lappin, editor. The Handbook of Contemporary Semantic\nTheory. Blackwell Publishers, Oxford, 1996.",
          "level": -1,
          "page": 476,
          "reading_order": 9,
          "bbox": [
            98,
            546,
            583,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_10",
          "label": "para",
          "text": "[Larson and Segal, 1995] Richard Larson and Gabriel Segal. Knowledge of Meaning: An\nIntroduction to Semantic Theory. MIT Press, Cambridge, MA, 1995.",
          "level": -1,
          "page": 476,
          "reading_order": 10,
          "bbox": [
            98,
            590,
            584,
            621
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_11",
          "label": "para",
          "text": "[Levin, 1993] Beth Levin. English Verb Classes and Alternations. University of Chicago\nPress, 1993.",
          "level": -1,
          "page": 476,
          "reading_order": 11,
          "bbox": [
            98,
            627,
            584,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_12",
          "label": "para",
          "text": "[Levitin, 2004] Anany Levitin. The Design and Analysis of Algorithms. Addison Wesley,\n2004.",
          "level": -1,
          "page": 476,
          "reading_order": 12,
          "bbox": [
            97,
            670,
            584,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_13",
          "label": "para",
          "text": "[Lutz and Ascher, 2003] Mark Lutz and David Ascher. Learning Python. O’Reilly, sec-\nond edition, 2003.",
          "level": -1,
          "page": 476,
          "reading_order": 13,
          "bbox": [
            100,
            707,
            584,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_14",
          "label": "para",
          "text": "[MacWhinney, 1995] Brian MacWhinney. The CHILDES Project: Tools for Analyzing\nTalk. Mahwah, NJ: Lawrence Erlbaum, second edition, 1995. [http://childes.psy.cmu\n.edu/].",
          "level": -1,
          "page": 476,
          "reading_order": 14,
          "bbox": [
            97,
            750,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_476_order_15",
          "label": "foot",
          "text": "454 | Bibliography",
          "level": -1,
          "page": 476,
          "reading_order": 15,
          "bbox": [
            97,
            824,
            183,
            842
          ],
          "section_number": "454",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_0",
          "label": "para",
          "text": "[Madnani, 2007] Nitin Madnani. Getting started on natural language processing with\nPython. ACM Crossroads , 13(4), 2007.",
          "level": -1,
          "page": 477,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_1",
          "label": "para",
          "text": "[ Manning, 2003 ] Christopher Manning. Probabilistic syntax. In Probabilistic Linguis-\ntics , pages 289–341. MIT Press, Cambridge, MA, 2003.",
          "level": -1,
          "page": 477,
          "reading_order": 1,
          "bbox": [
            97,
            115,
            584,
            146
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_2",
          "label": "para",
          "text": "(Manning and Schütze, 1999) Christopher Manning and Hinrich Schütze. Foundations\nof Statistical Natural Language Processing. MIT Press, Cambridge, MA, 1999.",
          "level": -1,
          "page": 477,
          "reading_order": 2,
          "bbox": [
            100,
            152,
            585,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_3",
          "label": "para",
          "text": "[Manning et al., 2008] Christopher Manning, Prabhakar Raghavan, and Hinrich Schü-\ntze. Introduction to Information Retrieval . Cambridge University Press, 2008.",
          "level": -1,
          "page": 477,
          "reading_order": 3,
          "bbox": [
            97,
            195,
            585,
            225
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_4",
          "label": "para",
          "text": "(McCawley, 1998] James McCawley. The Syntactic Phenomena of English. University\nof Chicago Press, 1998.",
          "level": -1,
          "page": 477,
          "reading_order": 4,
          "bbox": [
            100,
            232,
            584,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_5",
          "label": "para",
          "text": "[McConnell, 2004] Steve McConnell. Code Complete: A Practical Handbook of Software\nConstruction. Microsoft Press, 2004.",
          "level": -1,
          "page": 477,
          "reading_order": 5,
          "bbox": [
            98,
            275,
            583,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_6",
          "label": "para",
          "text": "[McCune, 2008] William McCune. Prover9: Automated theorem prover for first-order\nand equational logic, 2008.",
          "level": -1,
          "page": 477,
          "reading_order": 6,
          "bbox": [
            100,
            313,
            585,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_7",
          "label": "para",
          "text": "[McEnery, 2006] Anthony McEnery. Corpus-Based Language Studies: An Advanced\nResource Book . Routledge, 2006.",
          "level": -1,
          "page": 477,
          "reading_order": 7,
          "bbox": [
            98,
            355,
            584,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_8",
          "label": "para",
          "text": "[Melamed, 2001] Dan Melamed. Empirical Methods for Exploiting Parallel Texts. MIT\nPress, 2001.",
          "level": -1,
          "page": 477,
          "reading_order": 8,
          "bbox": [
            98,
            394,
            583,
            423
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_9",
          "label": "para",
          "text": "[Mertz, 2003] David Mertz. Text Processing in Python . Addison-Wesley, Boston, MA,\n2003.",
          "level": -1,
          "page": 477,
          "reading_order": 9,
          "bbox": [
            97,
            430,
            584,
            465
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_10",
          "label": "para",
          "text": "[Meyer, 2002] Charles Meyer. English Corpus Linguistics: An Introduction . Cambridge\nUniversity Press, 2002.",
          "level": -1,
          "page": 477,
          "reading_order": 10,
          "bbox": [
            98,
            474,
            585,
            503
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_11",
          "label": "para",
          "text": "[Miller and Charles, 1998] George Miller and Walter Charles. Contextual correlates of\nsemantic similarity. Language and Cognitive Processes , 6:1–28, 1998.",
          "level": -1,
          "page": 477,
          "reading_order": 11,
          "bbox": [
            97,
            510,
            584,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_12",
          "label": "para",
          "text": "Mitkov, 2002a] Ruslan Mitkov. Anaphora Resolution. Longman, 2002.",
          "level": -1,
          "page": 477,
          "reading_order": 12,
          "bbox": [
            100,
            554,
            505,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_13",
          "label": "para",
          "text": "[Mitkov, 2002b] Ruslan Mitkov, editor. Oxford Handbook of Computational Linguis-\ntics . Oxford University Press, 2002.",
          "level": -1,
          "page": 477,
          "reading_order": 13,
          "bbox": [
            97,
            573,
            584,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_14",
          "label": "para",
          "text": "[Müller, 2002] Stefan Müller. Complex Predicates: Verbal Complexes, Resultative Con-\nstructions, and Particle Verbs in German . Number 13 in Studies in Constraint-Based\nLexicalism. Center for the Study of Language and Information, Stanford, 2002. http://\nwww.dfki.de/~stefan/Pub/complex.html .",
          "level": -1,
          "page": 477,
          "reading_order": 14,
          "bbox": [
            97,
            617,
            585,
            681
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_15",
          "label": "para",
          "text": "[Nerbonne et al., 1994] John Nerbonne, Klaus Netter, and Carl Pollard. German in\nHead-Driven Phrase Structure Grammar . CSLI Publications, Stanford, CA, 1994.",
          "level": -1,
          "page": 477,
          "reading_order": 15,
          "bbox": [
            98,
            689,
            584,
            719
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_16",
          "label": "para",
          "text": "[Nespor and Vogel, 1986] Marina Nespor and Irene Vogel. Prosodic Phonology. Num-\nber 28 in Studies in Generative Grammar. Foris Publications, Dordrecht, 1986.",
          "level": -1,
          "page": 477,
          "reading_order": 16,
          "bbox": [
            100,
            725,
            585,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_477_order_17",
          "label": "foot",
          "text": "Bibliography | 455",
          "level": -1,
          "page": 477,
          "reading_order": 17,
          "bbox": [
            494,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_0",
          "label": "para",
          "text": "[Nivre et al., 2006] J. Nivre, J. Hall, and J. Nilsson. Maltparser: A data-driven parser-\ngenerator for dependency parsing . In Proceedings of LREC , pages 2216–2219, 2006.",
          "level": -1,
          "page": 478,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_1",
          "label": "para",
          "text": "[Niyogi, 2006] Partha Niyogi. The Computational Nature of Language Learning and\nEvolution . MIT Press, 2006.",
          "level": -1,
          "page": 478,
          "reading_order": 1,
          "bbox": [
            98,
            115,
            584,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_2",
          "label": "para",
          "text": "[O’Grady et al., 2004] William O’Grady, John Archibald, Mark Aronoff, and Janie\nRees-Miller. Contemporary Linguistics: An Introduction . St. Martin's Press, fifth edition,\n2004.",
          "level": -1,
          "page": 478,
          "reading_order": 2,
          "bbox": [
            98,
            152,
            585,
            199
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_3",
          "label": "para",
          "text": "[OSU, 2007] OSU, editor. Language Files: Materials for an Introduction to Language\nand Linguistics . Ohio State University Press, tenth edition, 2007.",
          "level": -1,
          "page": 478,
          "reading_order": 3,
          "bbox": [
            100,
            206,
            585,
            242
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_4",
          "label": "para",
          "text": "[Partee, 1995] Barbara Partee. Lexical semantics and compositionality. In L. R. Gleit-\nman and M. Liberman, editors, An Invitation to Cognitive Science: Language , volume\n1, pages 311–360. MIT Press, 1995.",
          "level": -1,
          "page": 478,
          "reading_order": 4,
          "bbox": [
            97,
            250,
            584,
            298
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_5",
          "label": "para",
          "text": "[Pasca, 2003] Marius Pasca. Open-Domain Question Answering from Large Text Col-\nlections . CSLI Publications, Stanford, CA, 2003.",
          "level": -1,
          "page": 478,
          "reading_order": 5,
          "bbox": [
            97,
            304,
            584,
            340
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_6",
          "label": "para",
          "text": "[Pevzner and Hearst, 2002] L. Pevzner and M. Hearst. A critique and improvement of\nan evaluation metric for text segmentation. Computational Linguistics , 28:19–36, 2002.",
          "level": -1,
          "page": 478,
          "reading_order": 6,
          "bbox": [
            100,
            348,
            585,
            378
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_7",
          "label": "para",
          "text": "Pullum, 2005] Geoffrey K. Pullum. Fossilized prejudices about \"however\", 2005",
          "level": -1,
          "page": 478,
          "reading_order": 7,
          "bbox": [
            100,
            385,
            559,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_8",
          "label": "para",
          "text": "(Radford, 1988] Andrew Radford. Transformational Grammar: An Introduction. Cam-\nbridge University Press, 1988.",
          "level": -1,
          "page": 478,
          "reading_order": 8,
          "bbox": [
            100,
            411,
            584,
            442
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_9",
          "label": "para",
          "text": "[Ramshaw and Marcus, 1995] Lance A. Ramshaw and Mitchell P. Marcus. Text chunk-\ning using transformation-based learning. In Proceedings of the Third ACL Workshop on\nVery Large Corpora , pages 82–94, 1995.",
          "level": -1,
          "page": 478,
          "reading_order": 9,
          "bbox": [
            97,
            448,
            584,
            501
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_10",
          "label": "para",
          "text": "[Reppen et al., 2005] Randi Reppen, Nancy Ide, and Keith Suderman. American Na-\ntional Corpus . Linguistic Data Consortium, 2005.",
          "level": -1,
          "page": 478,
          "reading_order": 10,
          "bbox": [
            97,
            501,
            584,
            538
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_11",
          "label": "para",
          "text": "[Robinson et al., 2007] Stuart Robinson, Greg Aumann, and Steven Bird. Managing\nfieldwork data with toolbox and the natural language toolkit . Language Documentation\nand Conservation , 1:44–57, 2007.",
          "level": -1,
          "page": 478,
          "reading_order": 11,
          "bbox": [
            100,
            546,
            585,
            592
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_12",
          "label": "para",
          "text": "[Sag and Wasow, 1999] Ivan A. Sag and Thomas Wasow. Syntactic Theory: A Formal\nIntroduction. CSLI Publications, Stanford, CA, 1999.",
          "level": -1,
          "page": 478,
          "reading_order": 12,
          "bbox": [
            98,
            600,
            585,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_13",
          "label": "para",
          "text": "[Sampson and McCarthy, 2005] Geoffrey Sampson and Diana McCarthy. Corpus Lin-\nguistics: Readings in a Widening Discipline . Continuum, 2005.",
          "level": -1,
          "page": 478,
          "reading_order": 13,
          "bbox": [
            96,
            644,
            584,
            674
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_14",
          "label": "para",
          "text": "[Scott and Tribble, 2006] Mike Scott and Christopher Tribble. Textual Patterns: Key\nWords and Corpus Analysis in Language Education . John Benjamins, 2006.",
          "level": -1,
          "page": 478,
          "reading_order": 14,
          "bbox": [
            98,
            680,
            585,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_15",
          "label": "para",
          "text": "'Segaran, 2007] Toby Segaran. Collective Intelligence. O’Reilly Media, 2007.",
          "level": -1,
          "page": 478,
          "reading_order": 15,
          "bbox": [
            100,
            724,
            530,
            738
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_16",
          "label": "para",
          "text": "[Shatkay and Feldman, 2004] Hagit Shatkay and R. Feldman. Mining the biomedical\nliterature in the genomic era: An overview. Journal of Computational Biology , 10:821–\n855, 2004.",
          "level": -1,
          "page": 478,
          "reading_order": 16,
          "bbox": [
            97,
            743,
            585,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_478_order_17",
          "label": "foot",
          "text": "456 | Bibliography",
          "level": -1,
          "page": 478,
          "reading_order": 17,
          "bbox": [
            97,
            824,
            183,
            842
          ],
          "section_number": "456",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_0",
          "label": "para",
          "text": "[Shieber, 1986] Stuart M. Shieber. An Introduction to Unification-Based Approaches to\nGrammar , volume 4 of CSLI Lecture Notes Series .CSLI Publications, Stanford, CA,\n1986.",
          "level": -1,
          "page": 479,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            584,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_1",
          "label": "para",
          "text": "[Shieber et al., 1983] Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane Robinson,\nand Mabry Tyson. The formalism and implementation of PATR-II. In Barbara J. Grosz\nand Mark Stickel, editors, Research on Interactive Acquisition and Use of Knowledge,\ntechreport 4, pages 39–79. SRI International, Menlo Park, CA, November 1983. (http:\n//www.eecs.harvard.edu/ shieber/Biblio/Papers/Shieber-83-FIP.pdf)",
          "level": -1,
          "page": 479,
          "reading_order": 1,
          "bbox": [
            97,
            132,
            585,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_2",
          "label": "para",
          "text": "[Simons and Bird, 2003] Gary Simons and Steven Bird. The Open Language Archives\nCommunity: An infrastructure for distributed archiving of language resources. Literary\nand Linguistic Computing , 18:117–128, 2003.",
          "level": -1,
          "page": 479,
          "reading_order": 2,
          "bbox": [
            97,
            215,
            585,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_3",
          "label": "para",
          "text": "[Sproat et al., 2001] Richard Sproat, Alan Black, Stanley Chen, Shankar Kumar, Mari\nOstendorf, and Christopher Richards. Normalization of non-standard words. Com-\nputer Speech and Language , 15:287–333, 2001.",
          "level": -1,
          "page": 479,
          "reading_order": 3,
          "bbox": [
            97,
            277,
            584,
            325
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_4",
          "label": "para",
          "text": "[Strunk and White, 1999] William Strunk and E. B. White. The Elements of Style. Bos-\nton, Allyn and Bacon, 1999.",
          "level": -1,
          "page": 479,
          "reading_order": 4,
          "bbox": [
            97,
            331,
            585,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_5",
          "label": "para",
          "text": "[Thompson and McKelvie, 1997] Henry S. Thompson and David McKelvie. Hyperlink\nsemantics for standoff markup of read-only documents. In SGML Europe ’97, 1997.\nhttp://www.ltg.ed.ac.uk/~ht/sgmleu97.html.",
          "level": -1,
          "page": 479,
          "reading_order": 5,
          "bbox": [
            97,
            374,
            585,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_6",
          "label": "para",
          "text": "TLG, 1999] TLG. Thesaurus Linguae Graecae, 1999.",
          "level": -1,
          "page": 479,
          "reading_order": 6,
          "bbox": [
            100,
            430,
            404,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_7",
          "label": "para",
          "text": "[Turing, 1950] Alan M. Turing. Computing machinery and intelligence. Mind, 59(236):\n433–460, 1950.",
          "level": -1,
          "page": 479,
          "reading_order": 7,
          "bbox": [
            97,
            448,
            584,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_8",
          "label": "para",
          "text": "[van Benthem and ter Meulen, 1997] Johan van Benthem and Alice ter Meulen, editors.\nHandbook of Logic and Language. MIT Press, Cambridge, MA, 1997.",
          "level": -1,
          "page": 479,
          "reading_order": 8,
          "bbox": [
            98,
            492,
            584,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_9",
          "label": "para",
          "text": "[van Rossum and Drake, 2006a] Guido van Rossum and Fred L. Drake. An Introduction\nto Python—The Python Tutorial. Network Theory Ltd, Bristol, 2006.",
          "level": -1,
          "page": 479,
          "reading_order": 9,
          "bbox": [
            97,
            528,
            584,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_10",
          "label": "para",
          "text": "[van Rossum and Drake, 2006b] Guido van Rossum and Fred L. Drake. The Python\nLanguage Reference Manual . Network Theory Ltd, Bristol, 2006.",
          "level": -1,
          "page": 479,
          "reading_order": 10,
          "bbox": [
            98,
            573,
            584,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_11",
          "label": "para",
          "text": "[Warren and Pereira, 1982] David H. D. Warren and Fernando C. N. Pereira. An effi-\ncient easily adaptable system for interpreting natural language queries. American Jour-\nnal of Computational Linguistics , 8(3-4):110–122, 1982.",
          "level": -1,
          "page": 479,
          "reading_order": 11,
          "bbox": [
            97,
            609,
            585,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_12",
          "label": "para",
          "text": "[Wechsler and Zlatic, 2003] Stephen Mark Wechsler and Larisa Zlatic. The Many Faces\nof Agreement . Stanford Monographs in Linguistics. CSLI Publications, Stanford, CA,\n2003.",
          "level": -1,
          "page": 479,
          "reading_order": 12,
          "bbox": [
            97,
            670,
            585,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_13",
          "label": "para",
          "text": "[Weiss et al., 2004] Sholom Weiss, Nitin Indurkhya, Tong Zhang, and Fred Damerau.\nText Mining: Predictive Methods for Analyzing Unstructured Information . Springer,\n2004.",
          "level": -1,
          "page": 479,
          "reading_order": 13,
          "bbox": [
            97,
            725,
            584,
            771
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_479_order_14",
          "label": "foot",
          "text": "Bibliography | 457",
          "level": -1,
          "page": 479,
          "reading_order": 14,
          "bbox": [
            494,
            824,
            585,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_480_order_0",
          "label": "para",
          "text": "[Woods et al., 1986] Anthony Woods, Paul Fletcher, and Arthur Hughes. Statistics in\nLanguage Studies . Cambridge University Press, 1986.",
          "level": -1,
          "page": 480,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            584,
            107
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_480_order_1",
          "label": "para",
          "text": "[Zhao and Zobel, 2007] Y. Zhao and J. Zobel. Search with style: Authorship attribution\nin classic literature. In Proceedings of the Thirtieth Australasian Computer Science Con-\nference. Association for Computing Machinery, 2007.",
          "level": -1,
          "page": 480,
          "reading_order": 1,
          "bbox": [
            96,
            115,
            584,
            162
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_480_order_2",
          "label": "foot",
          "text": "458 | Bibliography",
          "level": -1,
          "page": 480,
          "reading_order": 2,
          "bbox": [
            97,
            824,
            183,
            842
          ],
          "section_number": "458",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_481_order_0",
      "label": "sec",
      "text": "NLTK Index",
      "level": 1,
      "page": 481,
      "reading_order": 0,
      "bbox": [
        440,
        80,
        584,
        107
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_481_order_1",
          "label": "sub_sec",
          "text": "Symbols",
          "level": 2,
          "page": 481,
          "reading_order": 1,
          "bbox": [
            97,
            267,
            153,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [
            {
              "id": "page_481_order_2",
              "label": "sub_sub_sec",
              "text": "A",
              "level": 3,
              "page": 481,
              "reading_order": 2,
              "bbox": [
                97,
                304,
                109,
                322
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_481_order_3",
                  "label": "para",
                  "text": "abspath, 50\naccuracy, 119, 149, 217\nAnaphoraResolutionException, 401\nAndExpression, 369\nappend, 11, 86, 127, 197\nApplicationExpression, 405\napply, 10\napply_features, 224\nAssignment, 378\nassumptions, 383",
                  "level": -1,
                  "page": 481,
                  "reading_order": 3,
                  "bbox": [
                    97,
                    322,
                    270,
                    456
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            },
            {
              "id": "page_481_order_4",
              "label": "sub_sub_sec",
              "text": "B",
              "level": 3,
              "page": 481,
              "reading_order": 4,
              "bbox": [
                98,
                474,
                106,
                488
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_481_order_6",
              "label": "sub_sub_sec",
              "text": "с",
              "level": 3,
              "page": 481,
              "reading_order": 6,
              "bbox": [
                97,
                627,
                105,
                645
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_481_order_9",
              "label": "sub_sub_sec",
              "text": "D",
              "level": 3,
              "page": 481,
              "reading_order": 9,
              "bbox": [
                350,
                501,
                358,
                519
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_481_order_11",
              "label": "sub_sub_sec",
              "text": "日",
              "level": 3,
              "page": 481,
              "reading_order": 11,
              "bbox": [
                350,
                645,
                356,
                663
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": [
                {
                  "id": "page_481_order_13",
                  "label": "foot",
                  "text": "We\n’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.",
                  "level": -1,
                  "page": 481,
                  "reading_order": 13,
                  "bbox": [
                    97,
                    797,
                    530,
                    815
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_481_order_14",
                  "label": "foot",
                  "text": "459",
                  "level": -1,
                  "page": 481,
                  "reading_order": 14,
                  "bbox": [
                    566,
                    824,
                    585,
                    835
                  ],
                  "section_number": "459",
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                },
                {
                  "id": "page_482_order_0",
                  "label": "para",
                  "text": "entry, 63, 316, 418, 419, 425, 426, 427, 431,\n432, 433",
                  "level": -1,
                  "page": 482,
                  "reading_order": 0,
                  "bbox": [
                    97,
                    71,
                    324,
                    98
                  ],
                  "section_number": null,
                  "summary": null,
                  "embeddings": [],
                  "children": [],
                  "content_elements": []
                }
              ]
            }
          ],
          "content_elements": []
        }
      ],
      "content_elements": []
    },
    {
      "id": "page_482_order_2",
      "label": "sec",
      "text": "民主党",
      "level": 1,
      "page": 482,
      "reading_order": 2,
      "bbox": [
        100,
        169,
        104,
        183
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_482_order_3",
          "label": "para",
          "text": "FeatStruct, 337",
          "level": -1,
          "page": 482,
          "reading_order": 3,
          "bbox": [
            97,
            188,
            180,
            198
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_4",
          "label": "para",
          "text": "feed, 83",
          "level": -1,
          "page": 482,
          "reading_order": 4,
          "bbox": [
            97,
            198,
            137,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_5",
          "label": "para",
          "text": "fileid, 40, 41, 42, 45, 46, 50, 54, 62, 227, 28",
          "level": -1,
          "page": 482,
          "reading_order": 5,
          "bbox": [
            97,
            215,
            325,
            225
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_6",
          "label": "para",
          "text": "filename, 125, 289",
          "level": -1,
          "page": 482,
          "reading_order": 6,
          "bbox": [
            97,
            225,
            191,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_7",
          "label": "para",
          "text": "findall ,  105, 127, 430",
          "level": -1,
          "page": 482,
          "reading_order": 7,
          "bbox": [
            97,
            241,
            208,
            252
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_8",
          "label": "para",
          "text": "fol, 399",
          "level": -1,
          "page": 482,
          "reading_order": 8,
          "bbox": [
            97,
            252,
            139,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_9",
          "label": "para",
          "text": "format, 117, 120, 121, 157, 419, 436",
          "level": -1,
          "page": 482,
          "reading_order": 9,
          "bbox": [
            97,
            268,
            281,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_10",
          "label": "para",
          "text": "freq ,  17, 21, 213\nFreqDist ,  17, 18,",
          "level": -1,
          "page": 482,
          "reading_order": 10,
          "bbox": [
            97,
            278,
            182,
            306
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_11",
          "label": "para",
          "text": "135, 147, 153, 177, 185, 432",
          "level": -1,
          "page": 482,
          "reading_order": 11,
          "bbox": [
            97,
            306,
            324,
            318
          ],
          "section_number": "135",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_12",
          "label": "para",
          "text": "Freqdist, 61, 147, 148, 153, 244",
          "level": -1,
          "page": 482,
          "reading_order": 12,
          "bbox": [
            100,
            321,
            261,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_482_order_13",
      "label": "sec",
      "text": "G",
      "level": 1,
      "page": 482,
      "reading_order": 13,
      "bbox": [
        97,
        349,
        106,
        367
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_482_order_14",
          "label": "para",
          "text": "generate, 6, 7\nget, 68, 185,",
          "level": -1,
          "page": 482,
          "reading_order": 14,
          "bbox": [
            97,
            367,
            166,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_16",
          "label": "para",
          "text": "getchildren, 427, 428",
          "level": -1,
          "page": 482,
          "reading_order": 16,
          "bbox": [
            97,
            396,
            208,
            408
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_17",
          "label": "para",
          "text": "grammar, 265, 267, 269, 272, 278, 308, 311, 317,\n320, 321, 396, 433, 434, 436",
          "level": -1,
          "page": 482,
          "reading_order": 17,
          "bbox": [
            97,
            409,
            333,
            433
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_18",
          "label": "para",
          "text": "Grammar, 320, 334, 351, 354, 436",
          "level": -1,
          "page": 482,
          "reading_order": 18,
          "bbox": [
            98,
            436,
            261,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_482_order_19",
      "label": "sec",
      "text": "H",
      "level": 1,
      "page": 482,
      "reading_order": 19,
      "bbox": [
        98,
        465,
        109,
        479
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_482_order_20",
          "label": "para",
          "text": "role, 99",
          "level": -1,
          "page": 482,
          "reading_order": 20,
          "bbox": [
            100,
            483,
            138,
            494
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_21",
          "label": "para",
          "text": "ур_extra, 236",
          "level": -1,
          "page": 482,
          "reading_order": 21,
          "bbox": [
            100,
            498,
            171,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_23",
          "label": "para",
          "text": "ic, 176",
          "level": -1,
          "page": 482,
          "reading_order": 23,
          "bbox": [
            97,
            546,
            135,
            556
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_24",
          "label": "para",
          "text": "ieer, 284",
          "level": -1,
          "page": 482,
          "reading_order": 24,
          "bbox": [
            97,
            556,
            144,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_25",
          "label": "para",
          "text": "IffExpression, 369",
          "level": -1,
          "page": 482,
          "reading_order": 25,
          "bbox": [
            97,
            573,
            194,
            585
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_26",
          "label": "para",
          "text": "index, 13, 14, 16, 24, 90, 127, 134, 308\ninference, 370",
          "level": -1,
          "page": 482,
          "reading_order": 26,
          "bbox": [
            97,
            585,
            297,
            610
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_482_order_27",
      "label": "sec",
      "text": "]",
      "level": 1,
      "page": 482,
      "reading_order": 27,
      "bbox": [
        96,
        627,
        102,
        645
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_482_order_28",
          "label": "para",
          "text": "jaccard_distance, 155",
          "level": -1,
          "page": 482,
          "reading_order": 28,
          "bbox": [
            97,
            645,
            210,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_482_order_29",
      "label": "sec",
      "text": "K",
      "level": 1,
      "page": 482,
      "reading_order": 29,
      "bbox": [
        100,
        678,
        105,
        692
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_482_order_30",
          "label": "para",
          "text": "keys, 17, 192",
          "level": -1,
          "page": 482,
          "reading_order": 30,
          "bbox": [
            97,
            698,
            163,
            710
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_482_order_31",
      "label": "sec",
      "text": "L",
      "level": 1,
      "page": 482,
      "reading_order": 31,
      "bbox": [
        98,
        725,
        104,
        743
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_482_order_32",
          "label": "para",
          "text": "LambdaExpression, 387",
          "level": -1,
          "page": 482,
          "reading_order": 32,
          "bbox": [
            98,
            743,
            210,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_33",
          "label": "para",
          "text": "lancaster, 107",
          "level": -1,
          "page": 482,
          "reading_order": 33,
          "bbox": [
            97,
            761,
            171,
            771
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_34",
          "label": "para",
          "text": "leaves, 51, 71, 422\nLemma, 68, 71",
          "level": -1,
          "page": 482,
          "reading_order": 34,
          "bbox": [
            97,
            771,
            193,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_36",
          "label": "para",
          "text": "lemma, 68, 69, 214",
          "level": -1,
          "page": 482,
          "reading_order": 36,
          "bbox": [
            349,
            71,
            440,
            84
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_37",
          "label": "para",
          "text": "lemmas, 68",
          "level": -1,
          "page": 482,
          "reading_order": 37,
          "bbox": [
            349,
            87,
            404,
            98
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_38",
          "label": "para",
          "text": "length, 25, 61, 136, 149",
          "level": -1,
          "page": 482,
          "reading_order": 38,
          "bbox": [
            349,
            98,
            470,
            112
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_39",
          "label": "para",
          "text": "load, 124, 206",
          "level": -1,
          "page": 482,
          "reading_order": 39,
          "bbox": [
            349,
            114,
            422,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_40",
          "label": "para",
          "text": "load_corpus, 147",
          "level": -1,
          "page": 482,
          "reading_order": 40,
          "bbox": [
            349,
            125,
            440,
            139
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_41",
          "label": "para",
          "text": "load_earley, 335, 352, 355, 363, 392, 406",
          "level": -1,
          "page": 482,
          "reading_order": 41,
          "bbox": [
            349,
            140,
            557,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_42",
          "label": "para",
          "text": "load_parser, 334",
          "level": -1,
          "page": 482,
          "reading_order": 42,
          "bbox": [
            349,
            152,
            440,
            165
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_43",
          "label": "para",
          "text": "logic , 376, 389",
          "level": -1,
          "page": 482,
          "reading_order": 43,
          "bbox": [
            349,
            165,
            427,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_44",
          "label": "para",
          "text": "LogicParser, 369, 370, 373, 375, 388, 400,\n404",
          "level": -1,
          "page": 482,
          "reading_order": 44,
          "bbox": [
            350,
            179,
            566,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_482_order_45",
      "label": "sec",
      "text": "N",
      "level": 1,
      "page": 482,
      "reading_order": 45,
      "bbox": [
        349,
        223,
        359,
        241
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_482_order_46",
          "label": "para",
          "text": "Mace, 383",
          "level": -1,
          "page": 482,
          "reading_order": 46,
          "bbox": [
            349,
            241,
            396,
            252
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_47",
          "label": "para",
          "text": "MaceCommand, 383",
          "level": -1,
          "page": 482,
          "reading_order": 47,
          "bbox": [
            349,
            252,
            440,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_48",
          "label": "para",
          "text": "maxent, 275",
          "level": -1,
          "page": 482,
          "reading_order": 48,
          "bbox": [
            349,
            268,
            413,
            279
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_49",
          "label": "para",
          "text": "megam, 275",
          "level": -1,
          "page": 482,
          "reading_order": 49,
          "bbox": [
            349,
            279,
            413,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_50",
          "label": "para",
          "text": "member_holonyms, 70, 74",
          "level": -1,
          "page": 482,
          "reading_order": 50,
          "bbox": [
            349,
            295,
            469,
            308
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_51",
          "label": "para",
          "text": "member_meronyms, 74",
          "level": -1,
          "page": 482,
          "reading_order": 51,
          "bbox": [
            350,
            309,
            450,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_52",
          "label": "para",
          "text": "metrics, 154, 155",
          "level": -1,
          "page": 482,
          "reading_order": 52,
          "bbox": [
            349,
            322,
            440,
            332
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_53",
          "label": "para",
          "text": "model , 200, 201\nModel , 201, 382",
          "level": -1,
          "page": 482,
          "reading_order": 53,
          "bbox": [
            349,
            332,
            426,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_482_order_55",
      "label": "sec",
      "text": "N",
      "level": 1,
      "page": 482,
      "reading_order": 55,
      "bbox": [
        350,
        376,
        359,
        394
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_482_order_56",
          "label": "para",
          "text": "nbest_parse, 334",
          "level": -1,
          "page": 482,
          "reading_order": 56,
          "bbox": [
            349,
            394,
            440,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_57",
          "label": "para",
          "text": "ne, 236, 237, 283",
          "level": -1,
          "page": 482,
          "reading_order": 57,
          "bbox": [
            349,
            412,
            440,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_58",
          "label": "para",
          "text": "NegatedExpression, 369\nngrams, 141",
          "level": -1,
          "page": 482,
          "reading_order": 58,
          "bbox": [
            349,
            421,
            467,
            450
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_60",
          "label": "para",
          "text": "NgramTagger, 204\nnltk.chat.chatbot",
          "level": -1,
          "page": 482,
          "reading_order": 60,
          "bbox": [
            349,
            451,
            440,
            473
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_62",
          "label": "para",
          "text": "nltk.classify, 224\nnltk.classify.rte_",
          "level": -1,
          "page": 482,
          "reading_order": 62,
          "bbox": [
            349,
            474,
            445,
            503
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_64",
          "label": "para",
          "text": "nltk.cluster, 172",
          "level": -1,
          "page": 482,
          "reading_order": 64,
          "bbox": [
            349,
            504,
            449,
            514
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_65",
          "label": "para",
          "text": "nltk.corpus, 40, 42, 43, 44, 45, 48, 51, 53, 54,\n60, 65, 67, 90, 105, 106, 119, 162,\n170, 184, 188, 195, 198, 203, 223,\n227, 258, 259, 271, 272, 285, 315,\n316, 422, 430, 431",
          "level": -1,
          "page": 482,
          "reading_order": 65,
          "bbox": [
            349,
            517,
            583,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_66",
          "label": "para",
          "text": "nltk.data.find, 85, 94, 427, 434",
          "level": -1,
          "page": 482,
          "reading_order": 66,
          "bbox": [
            349,
            582,
            514,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_67",
          "label": "para",
          "text": "nltk.data.load, 112, 300, 334",
          "level": -1,
          "page": 482,
          "reading_order": 67,
          "bbox": [
            349,
            593,
            503,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_68",
          "label": "para",
          "text": "nltk.data.show_cfg, 334, 351, 354, 363",
          "level": -1,
          "page": 482,
          "reading_order": 68,
          "bbox": [
            349,
            609,
            548,
            621
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_69",
          "label": "para",
          "text": "nltk.downloader, 316",
          "level": -1,
          "page": 482,
          "reading_order": 69,
          "bbox": [
            349,
            621,
            458,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_70",
          "label": "para",
          "text": "nltk.draw.tree, 324",
          "level": -1,
          "page": 482,
          "reading_order": 70,
          "bbox": [
            349,
            636,
            451,
            646
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_71",
          "label": "para",
          "text": "nltk.etree.ElementTree, 427, 430, 432, 434\nnltk.grammar, 298",
          "level": -1,
          "page": 482,
          "reading_order": 71,
          "bbox": [
            349,
            646,
            574,
            674
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_73",
          "label": "para",
          "text": "nltk.help.brown_tagset, 214\nnltk.help.upenn_tagset, 180",
          "level": -1,
          "page": 482,
          "reading_order": 73,
          "bbox": [
            349,
            675,
            495,
            700
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_75",
          "label": "para",
          "text": "nltk.inference.discourse , 400",
          "level": -1,
          "page": 482,
          "reading_order": 75,
          "bbox": [
            349,
            701,
            503,
            711
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_76",
          "label": "para",
          "text": "nltk.metrics.agreement, 414",
          "level": -1,
          "page": 482,
          "reading_order": 76,
          "bbox": [
            349,
            715,
            495,
            727
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_77",
          "label": "para",
          "text": "nltk.metrics.distance, 155",
          "level": -1,
          "page": 482,
          "reading_order": 77,
          "bbox": [
            349,
            727,
            494,
            737
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_78",
          "label": "para",
          "text": "nltk.parse , 335, 352, 363, 392, 400",
          "level": -1,
          "page": 482,
          "reading_order": 78,
          "bbox": [
            349,
            741,
            530,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_79",
          "label": "para",
          "text": "nltk.probability, 219",
          "level": -1,
          "page": 482,
          "reading_order": 79,
          "bbox": [
            349,
            752,
            467,
            766
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_80",
          "label": "para",
          "text": "nltk.sem, 363, 396",
          "level": -1,
          "page": 482,
          "reading_order": 80,
          "bbox": [
            349,
            767,
            443,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_81",
          "label": "para",
          "text": "nltk.sem.cooper_storage, 396",
          "level": -1,
          "page": 482,
          "reading_order": 81,
          "bbox": [
            349,
            779,
            503,
            792
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_482_order_82",
          "label": "foot",
          "text": "460 | NLTK Index",
          "level": -1,
          "page": 482,
          "reading_order": 82,
          "bbox": [
            97,
            824,
            180,
            838
          ],
          "section_number": "460",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_483_order_0",
          "label": "para",
          "text": "_\nnltk.sem.drt_resolve_anaphora, 399\nhltk.tag, 401\nhltk.tag.brill.demo, 210, 218\nhltk.text.Text, 81\nnode, 170\nnps_chat, 42, 105, 235",
          "level": -1,
          "page": 483,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            281,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_483_order_1",
      "label": "sec",
      "text": "0",
      "level": 1,
      "page": 483,
      "reading_order": 1,
      "bbox": [
        97,
        170,
        109,
        184
      ],
      "section_number": "0",
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_483_order_2",
          "label": "para",
          "text": "olac, 436\nOrExpression, 369",
          "level": -1,
          "page": 483,
          "reading_order": 2,
          "bbox": [
            97,
            188,
            189,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_483_order_3",
      "label": "sec",
      "text": "P",
      "level": 1,
      "page": 483,
      "reading_order": 3,
      "bbox": [
        98,
        232,
        105,
        250
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_483_order_4",
          "label": "para",
          "text": "packages, 154\nparse, 273, 275, 320, 375, 398, 427\nparsed, 51, 373\nParseI, 326\nparse_valuation, 378\npart_holonyms, 74\npart_meronyms, 70, 74\npath, 85, 94, 95, 96\npath_similarity, 72\nphones, 408\nphonetic, 408, 409\nPlaintextCorpusReader, 51\nporter, 107, 108\nposts, 65, 235\nppattach, 259\nPPAttachment, 258, 259\nproductions, 308, 311, 320, 334\nprove, 376\nProver9, 376\npunkt, 112",
          "level": -1,
          "page": 483,
          "reading_order": 4,
          "bbox": [
            97,
            250,
            315,
            515
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_483_order_6",
          "label": "para",
          "text": "RecursiveDescentParser, 302, 304\nregexp, 102, 103, 105, 122\nRegexpChunk, 287\nRegexpParser, 266, 286\nRegexpTagger, 217, 219, 401\nregexp_tokenize, 111\nresolve_anaphora, 399\nreverse, 195\nrte_features, 236",
          "level": -1,
          "page": 483,
          "reading_order": 6,
          "bbox": [
            97,
            553,
            324,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_483_order_7",
      "label": "sec",
      "text": "S",
      "level": 1,
      "page": 483,
      "reading_order": 7,
      "bbox": [
        97,
        688,
        104,
        702
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_483_order_8",
          "label": "para",
          "text": "samples, 22, 44, 54, 55, 56\nsatisfiers, 380, 382\nsatisfy, 155\nscore, 115, 272, 273, 274, 276, 277\nsearch, 177\nSEM, 362, 363, 385, 386, 390, 393, 395, 396,\n403",
          "level": -1,
          "page": 483,
          "reading_order": 8,
          "bbox": [
            97,
            707,
            324,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_483_order_9",
          "label": "para",
          "text": "sem, 363, 396, 400\nsem.evaluate, 406\nSenseval, 257\nsenseval, 258\nShiftReduceParser, 305\nshow_clause, 285\nshow_most_informative_features, 228\nshow_raw_rtuple, 285\nsimilar, 5, 6, 21, 319\nsimplify, 388\nsort, 12, 136, 192\nSpeakerInfo, 409\nsr, 65\nState, 20, 187\nstem, 104, 105\nstr2tuple, 181\nSubElement, 432\nsubstance_holonyms, 74\nsubstance_meronyms, 70, 74\nSynset, 67, 68, 69, 70, 71, 72\nsynset, 68, 69, 70, 71, 425, 426\ns retrieve, 396",
          "level": -1,
          "page": 483,
          "reading_order": 9,
          "bbox": [
            349,
            71,
            539,
            360
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_483_order_11",
          "label": "para",
          "text": "tabulate, 54, 55, 119\ntag, 146, 164, 181, 184, 185, 186, 187, 188, 189,\n195, 196, 198, 207, 210, 226, 231,\n232, 233, 241, 273, 275",
          "level": -1,
          "page": 483,
          "reading_order": 11,
          "bbox": [
            350,
            401,
            584,
            450
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_483_order_12",
          "label": "para",
          "text": "tagged_sents, 183, 231, 233, 238, 241, 275\ntagged_words, 182, 187, 229\ntags, 135, 164, 188, 198, 210, 277, 433\nText, 4, 284, 436\ntoken, 26, 105, 139, 319, 421\ntokenize, 263\ntokens, 16, 80, 81, 82, 86, 105, 107, 108, 11\n139, 140, 153, 198, 206, 234, 308,\n309, 317, 328, 335, 352, 353, 355,\n392\ntoolbox, 66, 67, 430, 431, 434, 438\ntoolbox.ToolboxData, 434\ntrain, 112, 225\ntranslate, 66, 74\ntree, 268, 294, 298, 300, 301, 311, 316, 317\n319, 335, 352, 353, 355, 393, 430,\n434",
          "level": -1,
          "page": 483,
          "reading_order": 12,
          "bbox": [
            350,
            454,
            572,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_483_order_13",
          "label": "para",
          "text": "Tree, 315, 322\nTree.productions, 325\ntree2conlltags, 273\ntreebank, 51, 315, 316\ntrees, 294, 311, 334, 335, 363, 392, 393, 396,\n400\ntrigrams, 141\nTrigramTagger, 205\ntuples, 192",
          "level": -1,
          "page": 483,
          "reading_order": 13,
          "bbox": [
            350,
            673,
            583,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_483_order_14",
          "label": "foot",
          "text": "NLTK Index | 461",
          "level": -1,
          "page": 483,
          "reading_order": 14,
          "bbox": [
            503,
            824,
            584,
            842
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_484_order_0",
          "label": "para",
          "text": "turns, 12\nType, 2, 4, 169",
          "level": -1,
          "page": 484,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            171,
            99
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_484_order_1",
      "label": "sec",
      "text": "U",
      "level": 1,
      "page": 484,
      "reading_order": 1,
      "bbox": [
        98,
        116,
        109,
        134
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_484_order_2",
          "label": "para",
          "text": "Undefined, 379\nunify, 342",
          "level": -1,
          "page": 484,
          "reading_order": 2,
          "bbox": [
            97,
            134,
            172,
            163
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_484_order_4",
          "label": "para",
          "text": "UnigramTagger, 200, 203, 219, 274\nurl, 80, 82, 147, 148",
          "level": -1,
          "page": 484,
          "reading_order": 4,
          "bbox": [
            97,
            164,
            270,
            187
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_484_order_6",
      "label": "sec",
      "text": "V",
      "level": 1,
      "page": 484,
      "reading_order": 6,
      "bbox": [
        97,
        206,
        109,
        224
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_484_order_7",
          "label": "para",
          "text": "/aluation, 371, 378",
          "level": -1,
          "page": 484,
          "reading_order": 7,
          "bbox": [
            100,
            224,
            198,
            237
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_484_order_8",
          "label": "para",
          "text": "values, 149, 192",
          "level": -1,
          "page": 484,
          "reading_order": 8,
          "bbox": [
            97,
            241,
            180,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_484_order_9",
          "label": "para",
          "text": "/ariable, 375",
          "level": -1,
          "page": 484,
          "reading_order": 9,
          "bbox": [
            100,
            250,
            166,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_484_order_10",
          "label": "para",
          "text": "VariableBinderExpression, 389",
          "level": -1,
          "page": 484,
          "reading_order": 10,
          "bbox": [
            97,
            267,
            254,
            279
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_484_order_11",
      "label": "sec",
      "text": "W",
      "level": 1,
      "page": 484,
      "reading_order": 11,
      "bbox": [
        97,
        295,
        110,
        313
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_484_order_12",
          "label": "para",
          "text": "wordlist, 61, 64, 98, 99, 111, 201, 424\nwordnet, 67, 162, 170",
          "level": -1,
          "page": 484,
          "reading_order": 12,
          "bbox": [
            97,
            313,
            297,
            341
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_484_order_13",
      "label": "sec",
      "text": "X",
      "level": 1,
      "page": 484,
      "reading_order": 13,
      "bbox": [
        97,
        358,
        105,
        376
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_484_order_14",
          "label": "para",
          "text": "xml, 427, 436",
          "level": -1,
          "page": 484,
          "reading_order": 14,
          "bbox": [
            97,
            376,
            164,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_484_order_15",
          "label": "para",
          "text": "xml_posts, 235",
          "level": -1,
          "page": 484,
          "reading_order": 15,
          "bbox": [
            97,
            394,
            171,
            406
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_484_order_16",
          "label": "foot",
          "text": "462 | NLTK Index",
          "level": -1,
          "page": 484,
          "reading_order": 16,
          "bbox": [
            97,
            824,
            180,
            838
          ],
          "section_number": "462",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_485_order_0",
      "label": "sec",
      "text": "General Index",
      "level": 1,
      "page": 485,
      "reading_order": 0,
      "bbox": [
        404,
        80,
        584,
        107
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [
        {
          "id": "page_485_order_1",
          "label": "sub_sec",
          "text": "Symbols",
          "level": 2,
          "page": 485,
          "reading_order": 1,
          "bbox": [
            97,
            267,
            153,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": [
            {
              "id": "page_485_order_3",
              "label": "para",
              "text": "- (minus sign), negation operator, 368",
              "level": -1,
              "page": 485,
              "reading_order": 3,
              "bbox": [
                349,
                259,
                540,
                277
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_4",
              "label": "para",
              "text": "-> (implication) operator, 368\n. (dot) wildcard character in re",
              "level": -1,
              "page": 485,
              "reading_order": 4,
              "bbox": [
                349,
                277,
                503,
                305
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_5",
              "label": "para",
              "text": "_",
              "level": -1,
              "page": 485,
              "reading_order": 5,
              "bbox": [
                350,
                305,
                530,
                322
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_6",
              "label": "para",
              "text": "/ (slash),\ndivision operator, 2\n: (colon), ending Python statements, 26",
              "level": -1,
              "page": 485,
              "reading_order": 6,
              "bbox": [
                350,
                322,
                548,
                367
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_7",
              "label": "para",
              "text": "< (less than) operator, 22",
              "level": -1,
              "page": 485,
              "reading_order": 7,
              "bbox": [
                349,
                367,
                477,
                380
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_8",
              "label": "para",
              "text": "<-> (equivalence) operator, 368",
              "level": -1,
              "page": 485,
              "reading_order": 8,
              "bbox": [
                349,
                380,
                512,
                394
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_9",
              "label": "para",
              "text": "<= (less than or equal to) operator, 22\n= (equals sign)",
              "level": -1,
              "page": 485,
              "reading_order": 9,
              "bbox": [
                349,
                394,
                541,
                424
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_11",
              "label": "para",
              "text": "== (equal to) operator, 22",
              "level": -1,
              "page": 485,
              "reading_order": 11,
              "bbox": [
                359,
                425,
                496,
                439
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_12",
              "label": "para",
              "text": "== (identity) operator, 132",
              "level": -1,
              "page": 485,
              "reading_order": 12,
              "bbox": [
                359,
                439,
                503,
                456
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_13",
              "label": "para",
              "text": "assignment operator, 14, 130",
              "level": -1,
              "page": 485,
              "reading_order": 13,
              "bbox": [
                359,
                456,
                512,
                468
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_14",
              "label": "para",
              "text": "equality operator, 376",
              "level": -1,
              "page": 485,
              "reading_order": 14,
              "bbox": [
                359,
                468,
                477,
                483
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_15",
              "label": "para",
              "text": "> (greater than) operator, 22\n>= (greater than or equal to)",
              "level": -1,
              "page": 485,
              "reading_order": 15,
              "bbox": [
                350,
                483,
                494,
                512
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_17",
              "label": "para",
              "text": "? (question mark) in regular expressions, 99,",
              "level": -1,
              "page": 485,
              "reading_order": 17,
              "bbox": [
                350,
                513,
                574,
                528
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_18",
              "label": "para",
              "text": "101",
              "level": -1,
              "page": 485,
              "reading_order": 18,
              "bbox": [
                395,
                528,
                415,
                538
              ],
              "section_number": "101",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_19",
              "label": "para",
              "text": "[ ] (brackets)\nenclosing keys in dictionary, 65\nindexing lists, 12\nomitting in list comprehension used as\nfunction parameter, 55\nregular expression character classes, 99",
              "level": -1,
              "page": 485,
              "reading_order": 19,
              "bbox": [
                350,
                538,
                563,
                629
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_20",
              "label": "para",
              "text": "(backslash)\nending broken line of code, 139\nescaping string literals, 87\nin regular expressions, 100, 101\nuse with multiline strings, 88\n(caret)\ncharacter class negation in regular\nexpressions, 100\nend of string matching in regular\nexpressions, 99",
              "level": -1,
              "page": 485,
              "reading_order": 20,
              "bbox": [
                357,
                629,
                557,
                775
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_21",
              "label": "foot",
              "text": "We\n’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.",
              "level": -1,
              "page": 485,
              "reading_order": 21,
              "bbox": [
                97,
                797,
                530,
                815
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_485_order_22",
              "label": "foot",
              "text": "463",
              "level": -1,
              "page": 485,
              "reading_order": 22,
              "bbox": [
                566,
                824,
                584,
                835
              ],
              "section_number": "463",
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            },
            {
              "id": "page_486_order_0",
              "label": "para",
              "text": "regular expression metacharacter, 101\n{} (curly braces) in regular expressions, 100\n| (pipe character)\nalternation in regular expressions, 100, 10\nor operator, 368\nα-conversion, 389\nα-equivalents, 389\nβ-reduction, 388\nλ (lambda operator), 386–390",
              "level": -1,
              "page": 486,
              "reading_order": 0,
              "bbox": [
                97,
                71,
                325,
                206
              ],
              "section_number": null,
              "summary": null,
              "embeddings": [],
              "children": [],
              "content_elements": []
            }
          ]
        }
      ],
      "content_elements": []
    },
    {
      "id": "page_486_order_1",
      "label": "sec",
      "text": "A",
      "level": 1,
      "page": 486,
      "reading_order": 1,
      "bbox": [
        98,
        223,
        105,
        237
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_486_order_2",
          "label": "para",
          "text": "accumulative functions, 150\naccuracy of classification, 239\nACL (Association for Computational\nLinguistics), 34",
          "level": -1,
          "page": 486,
          "reading_order": 2,
          "bbox": [
            97,
            241,
            324,
            300
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_3",
          "label": "para",
          "text": "Special Interest Group on Web as Corpus\n(SIGWAC), 416",
          "level": -1,
          "page": 486,
          "reading_order": 3,
          "bbox": [
            113,
            300,
            324,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_4",
          "label": "para",
          "text": "adjectives, categorizing and tagging, 186",
          "level": -1,
          "page": 486,
          "reading_order": 4,
          "bbox": [
            100,
            331,
            301,
            344
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_5",
          "label": "para",
          "text": "adjuncts of lexical head, 347",
          "level": -1,
          "page": 486,
          "reading_order": 5,
          "bbox": [
            100,
            346,
            243,
            359
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_6",
          "label": "para",
          "text": "adverbs, categorizing and tagging, 186",
          "level": -1,
          "page": 486,
          "reading_order": 6,
          "bbox": [
            100,
            359,
            290,
            373
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_7",
          "label": "para",
          "text": "agreement, 329–331",
          "level": -1,
          "page": 486,
          "reading_order": 7,
          "bbox": [
            100,
            376,
            198,
            388
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_8",
          "label": "para",
          "text": "esources for further reading, 357",
          "level": -1,
          "page": 486,
          "reading_order": 8,
          "bbox": [
            118,
            388,
            282,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_9",
          "label": "para",
          "text": "algorithm design, 160-167",
          "level": -1,
          "page": 486,
          "reading_order": 9,
          "bbox": [
            100,
            403,
            234,
            417
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_10",
          "label": "para",
          "text": "dynamic programming, 165\necursion, 161",
          "level": -1,
          "page": 486,
          "reading_order": 10,
          "bbox": [
            118,
            420,
            253,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_11",
          "label": "para",
          "text": "esources for further information, 173",
          "level": -1,
          "page": 486,
          "reading_order": 11,
          "bbox": [
            118,
            448,
            306,
            459
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_12",
          "label": "para",
          "text": "all operator, 376",
          "level": -1,
          "page": 486,
          "reading_order": 12,
          "bbox": [
            100,
            463,
            180,
            476
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_13",
          "label": "para",
          "text": "alphabetic variants, 389",
          "level": -1,
          "page": 486,
          "reading_order": 13,
          "bbox": [
            100,
            476,
            218,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_14",
          "label": "para",
          "text": "ambiguity",
          "level": -1,
          "page": 486,
          "reading_order": 14,
          "bbox": [
            100,
            492,
            153,
            505
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_15",
          "label": "para",
          "text": "broad-coverage grammars and, 317\ncapturing structural ambiguity with\ndependency parser, 311",
          "level": -1,
          "page": 486,
          "reading_order": 15,
          "bbox": [
            113,
            507,
            297,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_16",
          "label": "para",
          "text": "quantifier scope, 381, 394-397",
          "level": -1,
          "page": 486,
          "reading_order": 16,
          "bbox": [
            109,
            549,
            270,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_17",
          "label": "para",
          "text": "cope of modifier, 314",
          "level": -1,
          "page": 486,
          "reading_order": 17,
          "bbox": [
            118,
            564,
            226,
            578
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_18",
          "label": "para",
          "text": "tructurally ambiguous sentences, 300",
          "level": -1,
          "page": 486,
          "reading_order": 18,
          "bbox": [
            118,
            580,
            306,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_19",
          "label": "para",
          "text": "1biquitous ambiguity in sentence structure,\n293",
          "level": -1,
          "page": 486,
          "reading_order": 19,
          "bbox": [
            117,
            593,
            333,
            619
          ],
          "section_number": "1",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_20",
          "label": "para",
          "text": "anagram dictionary, creating, 196",
          "level": -1,
          "page": 486,
          "reading_order": 20,
          "bbox": [
            100,
            624,
            270,
            637
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_21",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 486,
          "reading_order": 21,
          "bbox": [
            100,
            637,
            218,
            666
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_23",
          "label": "para",
          "text": "AND (in SQL), 365",
          "level": -1,
          "page": 486,
          "reading_order": 23,
          "bbox": [
            97,
            669,
            198,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_24",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 486,
          "reading_order": 24,
          "bbox": [
            100,
            680,
            181,
            707
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_26",
          "label": "para",
          "text": "annotation layer",
          "level": -1,
          "page": 486,
          "reading_order": 26,
          "bbox": [
            100,
            710,
            180,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_27",
          "label": "para",
          "text": "creating, 412",
          "level": -1,
          "page": 486,
          "reading_order": 27,
          "bbox": [
            117,
            725,
            180,
            739
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_28",
          "label": "para",
          "text": "deciding which to include when acquiring\ndata, 420",
          "level": -1,
          "page": 486,
          "reading_order": 28,
          "bbox": [
            118,
            741,
            325,
            765
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_29",
          "label": "para",
          "text": "quality control for, 413",
          "level": -1,
          "page": 486,
          "reading_order": 29,
          "bbox": [
            117,
            770,
            234,
            780
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_30",
          "label": "para",
          "text": "urvey of annotation software, 438",
          "level": -1,
          "page": 486,
          "reading_order": 30,
          "bbox": [
            118,
            785,
            288,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_31",
          "label": "para",
          "text": "annotation, inline, 421",
          "level": -1,
          "page": 486,
          "reading_order": 31,
          "bbox": [
            349,
            71,
            467,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_32",
          "label": "para",
          "text": "antecedent, 28",
          "level": -1,
          "page": 486,
          "reading_order": 32,
          "bbox": [
            349,
            89,
            423,
            99
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_33",
          "label": "para",
          "text": "antonymy, /",
          "level": -1,
          "page": 486,
          "reading_order": 33,
          "bbox": [
            349,
            107,
            413,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_34",
          "label": "para",
          "text": "apostrophes in tokenization, 110\nappending, 11",
          "level": -1,
          "page": 486,
          "reading_order": 34,
          "bbox": [
            349,
            117,
            512,
            146
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_36",
          "label": "para",
          "text": "arguments",
          "level": -1,
          "page": 486,
          "reading_order": 36,
          "bbox": [
            349,
            151,
            404,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_37",
          "label": "para",
          "text": "functions as, 149",
          "level": -1,
          "page": 486,
          "reading_order": 37,
          "bbox": [
            359,
            161,
            451,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_38",
          "label": "para",
          "text": "named, 152",
          "level": -1,
          "page": 486,
          "reading_order": 38,
          "bbox": [
            359,
            177,
            425,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_39",
          "label": "para",
          "text": "passing to functions (example), 143",
          "level": -1,
          "page": 486,
          "reading_order": 39,
          "bbox": [
            359,
            188,
            548,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_40",
          "label": "para",
          "text": "arguments in logic, 369, 372",
          "level": -1,
          "page": 486,
          "reading_order": 40,
          "bbox": [
            349,
            206,
            494,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_41",
          "label": "para",
          "text": "arity, 378",
          "level": -1,
          "page": 486,
          "reading_order": 41,
          "bbox": [
            349,
            222,
            404,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_42",
          "label": "para",
          "text": "articles, 186",
          "level": -1,
          "page": 486,
          "reading_order": 42,
          "bbox": [
            349,
            234,
            413,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_43",
          "label": "para",
          "text": "assert statements",
          "level": -1,
          "page": 486,
          "reading_order": 43,
          "bbox": [
            349,
            250,
            440,
            260
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_44",
          "label": "para",
          "text": "using in defensive programming, 159",
          "level": -1,
          "page": 486,
          "reading_order": 44,
          "bbox": [
            359,
            260,
            557,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_45",
          "label": "para",
          "text": "using to find logical errors, 146",
          "level": -1,
          "page": 486,
          "reading_order": 45,
          "bbox": [
            359,
            278,
            530,
            295
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_46",
          "label": "para",
          "text": "assignment, 130, 378",
          "level": -1,
          "page": 486,
          "reading_order": 46,
          "bbox": [
            349,
            295,
            458,
            307
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_47",
          "label": "para",
          "text": "defined, 14",
          "level": -1,
          "page": 486,
          "reading_order": 47,
          "bbox": [
            359,
            307,
            422,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_48",
          "label": "para",
          "text": "to list index values, 13",
          "level": -1,
          "page": 486,
          "reading_order": 48,
          "bbox": [
            359,
            322,
            478,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_49",
          "label": "para",
          "text": "Association for Computational Linguistics (see\nACL)",
          "level": -1,
          "page": 486,
          "reading_order": 49,
          "bbox": [
            349,
            338,
            585,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_50",
          "label": "para",
          "text": "associative arrays, 189",
          "level": -1,
          "page": 486,
          "reading_order": 50,
          "bbox": [
            349,
            367,
            467,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_51",
          "label": "para",
          "text": "assumptions, 369",
          "level": -1,
          "page": 486,
          "reading_order": 51,
          "bbox": [
            349,
            383,
            440,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_52",
          "label": "para",
          "text": "atomic values, 336\nattribute value may",
          "level": -1,
          "page": 486,
          "reading_order": 52,
          "bbox": [
            349,
            395,
            443,
            421
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_54",
          "label": "para",
          "text": "attribute-value pairs (Toolbox lexicon), 6",
          "level": -1,
          "page": 486,
          "reading_order": 54,
          "bbox": [
            349,
            422,
            557,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_55",
          "label": "para",
          "text": "attributes, XML, 42",
          "level": -1,
          "page": 486,
          "reading_order": 55,
          "bbox": [
            349,
            439,
            449,
            451
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_56",
          "label": "para",
          "text": "auxiliaries, 348",
          "level": -1,
          "page": 486,
          "reading_order": 56,
          "bbox": [
            349,
            455,
            426,
            466
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_57",
          "label": "para",
          "text": "auxiliary verbs, 336",
          "level": -1,
          "page": 486,
          "reading_order": 57,
          "bbox": [
            349,
            466,
            449,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_58",
          "label": "para",
          "text": "inversion and, 348",
          "level": -1,
          "page": 486,
          "reading_order": 58,
          "bbox": [
            359,
            483,
            459,
            495
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_486_order_59",
      "label": "sec",
      "text": "B",
      "level": 1,
      "page": 486,
      "reading_order": 59,
      "bbox": [
        350,
        510,
        358,
        530
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_486_order_60",
          "label": "para",
          "text": "b word boundary in regular expressions, 110",
          "level": -1,
          "page": 486,
          "reading_order": 60,
          "bbox": [
            350,
            536,
            583,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_61",
          "label": "para",
          "text": "backoff, 200",
          "level": -1,
          "page": 486,
          "reading_order": 61,
          "bbox": [
            349,
            549,
            413,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_62",
          "label": "para",
          "text": "backtracking, 303",
          "level": -1,
          "page": 486,
          "reading_order": 62,
          "bbox": [
            349,
            564,
            440,
            578
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_63",
          "label": "para",
          "text": "bar charts, 168",
          "level": -1,
          "page": 486,
          "reading_order": 63,
          "bbox": [
            350,
            580,
            424,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_64",
          "label": "para",
          "text": "base case, 161",
          "level": -1,
          "page": 486,
          "reading_order": 64,
          "bbox": [
            349,
            591,
            422,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_65",
          "label": "para",
          "text": "basic types, 373",
          "level": -1,
          "page": 486,
          "reading_order": 65,
          "bbox": [
            349,
            609,
            431,
            622
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_66",
          "label": "para",
          "text": "Bayes classifier (see naive Bayes classifier)",
          "level": -1,
          "page": 486,
          "reading_order": 66,
          "bbox": [
            350,
            624,
            558,
            637
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_67",
          "label": "para",
          "text": "bigram taggers, 204\nbigrams, 20",
          "level": -1,
          "page": 486,
          "reading_order": 67,
          "bbox": [
            349,
            637,
            449,
            666
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_69",
          "label": "para",
          "text": "generating random text with, 55",
          "level": -1,
          "page": 486,
          "reading_order": 69,
          "bbox": [
            359,
            667,
            530,
            681
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_70",
          "label": "para",
          "text": "binary formats, text, 85",
          "level": -1,
          "page": 486,
          "reading_order": 70,
          "bbox": [
            349,
            681,
            467,
            693
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_71",
          "label": "para",
          "text": "binary predicate, 372",
          "level": -1,
          "page": 486,
          "reading_order": 71,
          "bbox": [
            350,
            697,
            458,
            710
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_72",
          "label": "para",
          "text": "binary search, 160",
          "level": -1,
          "page": 486,
          "reading_order": 72,
          "bbox": [
            349,
            710,
            442,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_73",
          "label": "para",
          "text": "binding variables, 374",
          "level": -1,
          "page": 486,
          "reading_order": 73,
          "bbox": [
            349,
            725,
            460,
            739
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_74",
          "label": "para",
          "text": "binning, 249",
          "level": -1,
          "page": 486,
          "reading_order": 74,
          "bbox": [
            350,
            742,
            413,
            754
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_75",
          "label": "para",
          "text": "BIO Format, 286",
          "level": -1,
          "page": 486,
          "reading_order": 75,
          "bbox": [
            350,
            754,
            440,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_76",
          "label": "para",
          "text": "book module (NLTK), downloading, 3",
          "level": -1,
          "page": 486,
          "reading_order": 76,
          "bbox": [
            349,
            770,
            548,
            783
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_77",
          "label": "para",
          "text": "Boolean operators, 368",
          "level": -1,
          "page": 486,
          "reading_order": 77,
          "bbox": [
            350,
            785,
            467,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_486_order_78",
          "label": "foot",
          "text": "464 | General Inde",
          "level": -1,
          "page": 486,
          "reading_order": 78,
          "bbox": [
            97,
            824,
            182,
            838
          ],
          "section_number": "464",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_0",
          "label": "para",
          "text": "in propositional logic, truth conditions for,\n368",
          "level": -1,
          "page": 487,
          "reading_order": 0,
          "bbox": [
            109,
            71,
            333,
            99
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_1",
          "label": "para",
          "text": "Boolean values, 336",
          "level": -1,
          "page": 487,
          "reading_order": 1,
          "bbox": [
            100,
            104,
            198,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_2",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 487,
          "reading_order": 2,
          "bbox": [
            100,
            116,
            270,
            146
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_3",
          "label": "para",
          "text": "bottom-up parsing, 304",
          "level": -1,
          "page": 487,
          "reading_order": 3,
          "bbox": [
            100,
            146,
            217,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_4",
          "label": "para",
          "text": "sound, 374, 375",
          "level": -1,
          "page": 487,
          "reading_order": 4,
          "bbox": [
            100,
            161,
            180,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_5",
          "label": "para",
          "text": "5",
          "level": -1,
          "page": 487,
          "reading_order": 5,
          "bbox": [
            100,
            177,
            182,
            190
          ],
          "section_number": "5",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_6",
          "label": "para",
          "text": "Brill tagging, 208\ndemonstration of NLTK Brill tagger, 209\nsteps in, 209",
          "level": -1,
          "page": 487,
          "reading_order": 6,
          "bbox": [
            100,
            190,
            324,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_7",
          "label": "para",
          "text": "Brown Corpus, 42-44",
          "level": -1,
          "page": 487,
          "reading_order": 7,
          "bbox": [
            100,
            234,
            209,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_8",
          "label": "para",
          "text": "156",
          "level": -1,
          "page": 487,
          "reading_order": 8,
          "bbox": [
            100,
            250,
            146,
            263
          ],
          "section_number": "156",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_9",
          "label": "para",
          "text": "c",
          "level": -1,
          "page": 487,
          "reading_order": 9,
          "bbox": [
            97,
            277,
            109,
            296
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_10",
          "label": "para",
          "text": "call structure, 165",
          "level": -1,
          "page": 487,
          "reading_order": 10,
          "bbox": [
            97,
            302,
            189,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_11",
          "label": "para",
          "text": "call-by-value, 14",
          "level": -1,
          "page": 487,
          "reading_order": 11,
          "bbox": [
            97,
            313,
            180,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_12",
          "label": "para",
          "text": "carriage return and linefeed characters, 80",
          "level": -1,
          "page": 487,
          "reading_order": 12,
          "bbox": [
            97,
            331,
            306,
            344
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_13",
          "label": "para",
          "text": "case in German, 353-356",
          "level": -1,
          "page": 487,
          "reading_order": 13,
          "bbox": [
            97,
            347,
            225,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_14",
          "label": "para",
          "text": "Catalan numbers, 317",
          "level": -1,
          "page": 487,
          "reading_order": 14,
          "bbox": [
            97,
            358,
            209,
            371
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_15",
          "label": "para",
          "text": "categorical grammar, 346",
          "level": -1,
          "page": 487,
          "reading_order": 15,
          "bbox": [
            97,
            375,
            225,
            388
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_16",
          "label": "para",
          "text": "categorizing and tagging words, 179–219\nadjectives and adverbs, 186\nautomatically adding POS tags to text, 198\n203",
          "level": -1,
          "page": 487,
          "reading_order": 16,
          "bbox": [
            97,
            390,
            326,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_17",
          "label": "para",
          "text": "determining word category, 210-213",
          "level": -1,
          "page": 487,
          "reading_order": 17,
          "bbox": [
            109,
            448,
            300,
            461
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_18",
          "label": "para",
          "text": "differences in POS tagsets, 213",
          "level": -1,
          "page": 487,
          "reading_order": 18,
          "bbox": [
            109,
            463,
            270,
            476
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_19",
          "label": "para",
          "text": "exploring tagged corpora using POS tags,\n187-189",
          "level": -1,
          "page": 487,
          "reading_order": 19,
          "bbox": [
            109,
            476,
            324,
            502
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_20",
          "label": "para",
          "text": "mapping words to properties using Python\ndictionaries, 189–198",
          "level": -1,
          "page": 487,
          "reading_order": 20,
          "bbox": [
            109,
            507,
            325,
            531
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_21",
          "label": "para",
          "text": "n-gram tagging, 203–20",
          "level": -1,
          "page": 487,
          "reading_order": 21,
          "bbox": [
            109,
            537,
            234,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_22",
          "label": "para",
          "text": "nouns, 184",
          "level": -1,
          "page": 487,
          "reading_order": 22,
          "bbox": [
            109,
            549,
            171,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_23",
          "label": "para",
          "text": "resources for further reading, 214",
          "level": -1,
          "page": 487,
          "reading_order": 23,
          "bbox": [
            109,
            564,
            282,
            578
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_24",
          "label": "para",
          "text": "tagged corpora, 181-189",
          "level": -1,
          "page": 487,
          "reading_order": 24,
          "bbox": [
            109,
            580,
            243,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_25",
          "label": "para",
          "text": "transformation-based tagging, 208-210",
          "level": -1,
          "page": 487,
          "reading_order": 25,
          "bbox": [
            109,
            593,
            315,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_26",
          "label": "para",
          "text": "using POS (part-of-speech) tagger, 179",
          "level": -1,
          "page": 487,
          "reading_order": 26,
          "bbox": [
            109,
            609,
            309,
            622
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_27",
          "label": "para",
          "text": "using unsimplified POS tags, 187",
          "level": -1,
          "page": 487,
          "reading_order": 27,
          "bbox": [
            109,
            624,
            279,
            637
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_28",
          "label": "para",
          "text": "verbs, 185",
          "level": -1,
          "page": 487,
          "reading_order": 28,
          "bbox": [
            109,
            637,
            165,
            648
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_29",
          "label": "para",
          "text": "character class symbols in regular expressions,\n110",
          "level": -1,
          "page": 487,
          "reading_order": 29,
          "bbox": [
            97,
            653,
            333,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_30",
          "label": "para",
          "text": "character encodings, 48, 54, 94",
          "level": -1,
          "page": 487,
          "reading_order": 30,
          "bbox": [
            100,
            680,
            254,
            695
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_31",
          "label": "para",
          "text": "see also Unicode)",
          "level": -1,
          "page": 487,
          "reading_order": 31,
          "bbox": [
            117,
            697,
            207,
            708
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_32",
          "label": "para",
          "text": "using your local encoding in Python, 97",
          "level": -1,
          "page": 487,
          "reading_order": 32,
          "bbox": [
            109,
            708,
            315,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_33",
          "label": "para",
          "text": "characteristic function, 377",
          "level": -1,
          "page": 487,
          "reading_order": 33,
          "bbox": [
            100,
            725,
            235,
            736
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_34",
          "label": "para",
          "text": "chart, 307",
          "level": -1,
          "page": 487,
          "reading_order": 34,
          "bbox": [
            97,
            741,
            153,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_35",
          "label": "para",
          "text": "chart parsing, 30",
          "level": -1,
          "page": 487,
          "reading_order": 35,
          "bbox": [
            97,
            752,
            182,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_36",
          "label": "para",
          "text": "Earley chart parser, 334",
          "level": -1,
          "page": 487,
          "reading_order": 36,
          "bbox": [
            109,
            770,
            234,
            783
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_37",
          "label": "para",
          "text": "charts, displaying information in, 168",
          "level": -1,
          "page": 487,
          "reading_order": 37,
          "bbox": [
            97,
            785,
            288,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_38",
          "label": "para",
          "text": "chat text, 42",
          "level": -1,
          "page": 487,
          "reading_order": 38,
          "bbox": [
            349,
            71,
            413,
            84
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_39",
          "label": "para",
          "text": "chatbots, 31",
          "level": -1,
          "page": 487,
          "reading_order": 39,
          "bbox": [
            349,
            89,
            413,
            99
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_40",
          "label": "para",
          "text": "child nodes, 279",
          "level": -1,
          "page": 487,
          "reading_order": 40,
          "bbox": [
            349,
            104,
            431,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_41",
          "label": "para",
          "text": "chink, 268, 286",
          "level": -1,
          "page": 487,
          "reading_order": 41,
          "bbox": [
            349,
            116,
            431,
            129
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_42",
          "label": "para",
          "text": "chinking, 268",
          "level": -1,
          "page": 487,
          "reading_order": 42,
          "bbox": [
            349,
            133,
            418,
            146
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_43",
          "label": "para",
          "text": "chunk grammar, 265",
          "level": -1,
          "page": 487,
          "reading_order": 43,
          "bbox": [
            349,
            148,
            458,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_44",
          "label": "para",
          "text": "chunking, 214, 264-270",
          "level": -1,
          "page": 487,
          "reading_order": 44,
          "bbox": [
            349,
            161,
            469,
            175
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_45",
          "label": "para",
          "text": "building nested structure with cascaded\nchunkers, 278–279",
          "level": -1,
          "page": 487,
          "reading_order": 45,
          "bbox": [
            359,
            177,
            566,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_46",
          "label": "para",
          "text": "chinking, 268",
          "level": -1,
          "page": 487,
          "reading_order": 46,
          "bbox": [
            359,
            206,
            434,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_47",
          "label": "para",
          "text": "developing and evaluating chunkers, 270–\n278",
          "level": -1,
          "page": 487,
          "reading_order": 47,
          "bbox": [
            359,
            221,
            583,
            245
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_48",
          "label": "para",
          "text": "eading IOB format and CoNLL 2000\ncorpus, 270–272",
          "level": -1,
          "page": 487,
          "reading_order": 48,
          "bbox": [
            386,
            250,
            574,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_49",
          "label": "para",
          "text": "imple evaluation and baselines, 272–\n274",
          "level": -1,
          "page": 487,
          "reading_order": 49,
          "bbox": [
            386,
            278,
            574,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_50",
          "label": "para",
          "text": "raining classifier-based chunkers, 274–\n278",
          "level": -1,
          "page": 487,
          "reading_order": 50,
          "bbox": [
            386,
            304,
            583,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_51",
          "label": "para",
          "text": "exploring text corpora with NP chunker\n267",
          "level": -1,
          "page": 487,
          "reading_order": 51,
          "bbox": [
            359,
            338,
            566,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_52",
          "label": "para",
          "text": "noun phrase (NP), 264",
          "level": -1,
          "page": 487,
          "reading_order": 52,
          "bbox": [
            359,
            367,
            481,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_53",
          "label": "para",
          "text": "representing chunks, tags versus trees, 269\nresources for further reading, 286",
          "level": -1,
          "page": 487,
          "reading_order": 53,
          "bbox": [
            359,
            382,
            583,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_54",
          "label": "para",
          "text": "tag patterns, 266",
          "level": -1,
          "page": 487,
          "reading_order": 54,
          "bbox": [
            359,
            412,
            450,
            424
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_55",
          "label": "para",
          "text": "Toolbox lexicon, 43",
          "level": -1,
          "page": 487,
          "reading_order": 55,
          "bbox": [
            359,
            426,
            467,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_56",
          "label": "para",
          "text": "using regular expressions, 266",
          "level": -1,
          "page": 487,
          "reading_order": 56,
          "bbox": [
            359,
            439,
            521,
            466
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_58",
          "label": "para",
          "text": "class labels, 221",
          "level": -1,
          "page": 487,
          "reading_order": 58,
          "bbox": [
            349,
            467,
            431,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_59",
          "label": "para",
          "text": "classification, 221-259",
          "level": -1,
          "page": 487,
          "reading_order": 59,
          "bbox": [
            349,
            483,
            467,
            494
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_60",
          "label": "para",
          "text": "classifier trained to recognize named\nentities, 283",
          "level": -1,
          "page": 487,
          "reading_order": 60,
          "bbox": [
            359,
            499,
            557,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_61",
          "label": "para",
          "text": "decision trees, 242-245",
          "level": -1,
          "page": 487,
          "reading_order": 61,
          "bbox": [
            359,
            528,
            485,
            539
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_62",
          "label": "para",
          "text": "defined, 221",
          "level": -1,
          "page": 487,
          "reading_order": 62,
          "bbox": [
            359,
            543,
            431,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_63",
          "label": "para",
          "text": "evaluating models, 237-241",
          "level": -1,
          "page": 487,
          "reading_order": 63,
          "bbox": [
            359,
            555,
            503,
            570
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_64",
          "label": "para",
          "text": "accuracy, 239",
          "level": -1,
          "page": 487,
          "reading_order": 64,
          "bbox": [
            384,
            573,
            451,
            585
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_65",
          "label": "para",
          "text": "confusion matrices, 240",
          "level": -1,
          "page": 487,
          "reading_order": 65,
          "bbox": [
            382,
            585,
            503,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_66",
          "label": "para",
          "text": "cross-validation, 241",
          "level": -1,
          "page": 487,
          "reading_order": 66,
          "bbox": [
            385,
            600,
            485,
            612
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_67",
          "label": "para",
          "text": "precision and recall, 239",
          "level": -1,
          "page": 487,
          "reading_order": 67,
          "bbox": [
            385,
            616,
            505,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_68",
          "label": "para",
          "text": "est set, 238",
          "level": -1,
          "page": 487,
          "reading_order": 68,
          "bbox": [
            386,
            627,
            441,
            641
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_69",
          "label": "para",
          "text": "generative versus conditional, 254",
          "level": -1,
          "page": 487,
          "reading_order": 69,
          "bbox": [
            359,
            645,
            539,
            658
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_70",
          "label": "para",
          "text": "Maximum Entropy classifiers, 251-25",
          "level": -1,
          "page": 487,
          "reading_order": 70,
          "bbox": [
            359,
            660,
            557,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_71",
          "label": "para",
          "text": "modelling linguistic patterns, 255",
          "level": -1,
          "page": 487,
          "reading_order": 71,
          "bbox": [
            359,
            673,
            534,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_72",
          "label": "para",
          "text": "naive Bayes classifiers, 246-250",
          "level": -1,
          "page": 487,
          "reading_order": 72,
          "bbox": [
            359,
            689,
            530,
            700
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_73",
          "label": "para",
          "text": "supervised (see supervised classification",
          "level": -1,
          "page": 487,
          "reading_order": 73,
          "bbox": [
            359,
            704,
            566,
            717
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_74",
          "label": "para",
          "text": "classifier-based chunkers, 274–278",
          "level": -1,
          "page": 487,
          "reading_order": 74,
          "bbox": [
            349,
            717,
            530,
            729
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_75",
          "label": "para",
          "text": "closed class, 212",
          "level": -1,
          "page": 487,
          "reading_order": 75,
          "bbox": [
            349,
            733,
            432,
            744
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_76",
          "label": "para",
          "text": "closed formula, 375",
          "level": -1,
          "page": 487,
          "reading_order": 76,
          "bbox": [
            349,
            744,
            449,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_77",
          "label": "para",
          "text": "closures (+ and *), 100",
          "level": -1,
          "page": 487,
          "reading_order": 77,
          "bbox": [
            349,
            761,
            467,
            773
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_78",
          "label": "para",
          "text": "clustering package (nltk.cluster), 17",
          "level": -1,
          "page": 487,
          "reading_order": 78,
          "bbox": [
            349,
            777,
            530,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_487_order_79",
          "label": "foot",
          "text": "General Index | 465",
          "level": -1,
          "page": 487,
          "reading_order": 79,
          "bbox": [
            494,
            824,
            585,
            838
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_488_order_0",
          "label": "para",
          "text": "CMU Pronouncing Dictionary for U.S.\nEnglish, 63\ncode blocks, nested, 25\ncode examples, downloading, 57\ncode points, 94\ncodecs module, 95\ncoindex (in feature structure), 340\ncollocations, 20, 81\ncomma operator (.), 133\ncomparative wordlists, 65\ncomparison operators\nnumerical, 22\nfor words, 23\ncomplements of lexical head, 347\ncomplements of verbs, 313\ncomplex types, 373\ncomplex values, 336\ncomponents, language understanding, 31\ncomputational linguistics, challenges of natural\nlanguage, 441\ncomputer understanding of sentence meaning,\n368\nconcatenation, 11, 88\nlists and strings, 87\nstrings, 16\nconclusions in logic, 369\nconcordances\ncreating, 40\ngraphical POS-concordance tool, 184\nconditional classifiers, 254\nconditional expressions, 25\nconditional frequency distributions, 44, 52–56\ncombining with regular expressions, 103\ncondition and event pairs, 52\ncounting words by genre, 52\ngenerating random text with bigrams, 55\nmale and female names ending in each\nalphabet letter, 62\nplotting and tabulating distributions, 53\nusing to find minimally contrasting set of\nwords, 64\nConditionalFreqDist, 52\ncommonly used methods, 56\nconditionals, 22, 133\nconfusion matrix, 207, 240\nconsecutive classification, 232\nnon phrase chunking with consecutive\nclassifier, 275\nconsistent, 366",
          "level": -1,
          "page": 488,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            333,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_488_order_1",
          "label": "para",
          "text": "constituent structure, 296\nconstituents, 297\ncontext\nexploiting in part-of-speech classifier, 230\nfor taggers, 203\ncontext-free grammar, 298, 300\n(see also grammars)\nprobabilistic context-free grammar, 320\ncontractions in tokenization, 112\ncontrol, 22\ncontrol structures, 26\nconversion specifiers, 118\nconversions of data formats, 419\ncoordinate structures, 295\ncoreferential, 373\ncorpora, 39–52\nannotated text corpora, 46–48\nBrown Corpus, 42–44\ncreating and accessing, resources for further\nreading, 438\ndefined, 39\ndifferences in corpus access methods, 50\nexploring text corpora using a chunker,\n267\nGutenberg Corpus, 39–42\nInaugural Address Corpus, 45\nfrom languages other than English, 48\nloading your own corpus, 51\nobtaining from Web, 416\nReuters Corpus, 44\nsources of, 73\ntagged, 181–189\ntext corpus structure, 49–51\nweb and chat text, 42\nwordlists, 60–63\ncorpora, included with NLTK, 46\ncorpus\ncase study, structure of TIMIT, 407–412\ncorpus HOWTOs, 122\nlife cycle of, 412–416\ncreation scenarios, 412\ncuration versus evolution, 415\nquality control, 413\nwidely-used format for, 421\ncounters, legitimate uses of, 141\ncross-validation, 241\nCSV (comma-separated value) format, 418\nCSV (comma-separated-value) format, 170",
          "level": -1,
          "page": 488,
          "reading_order": 1,
          "bbox": [
            349,
            71,
            583,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_488_order_2",
          "label": "foot",
          "text": "466 | General Inde",
          "level": -1,
          "page": 488,
          "reading_order": 2,
          "bbox": [
            97,
            824,
            182,
            838
          ],
          "section_number": "466",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_0",
          "label": "para",
          "text": "D",
          "level": -1,
          "page": 489,
          "reading_order": 0,
          "bbox": [
            100,
            80,
            109,
            90
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_1",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 489,
          "reading_order": 1,
          "bbox": [
            100,
            98,
            324,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_3",
          "label": "para",
          "text": "data types",
          "level": -1,
          "page": 489,
          "reading_order": 3,
          "bbox": [
            97,
            156,
            153,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_4",
          "label": "para",
          "text": "dictionary, 190",
          "level": -1,
          "page": 489,
          "reading_order": 4,
          "bbox": [
            117,
            170,
            190,
            184
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_5",
          "label": "para",
          "text": "documentation for Python standard types\n173",
          "level": -1,
          "page": 489,
          "reading_order": 5,
          "bbox": [
            118,
            186,
            324,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_6",
          "label": "para",
          "text": "inding type of Python objects, 86",
          "level": -1,
          "page": 489,
          "reading_order": 6,
          "bbox": [
            118,
            215,
            283,
            228
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_7",
          "label": "para",
          "text": "unction parameter, 146",
          "level": -1,
          "page": 489,
          "reading_order": 7,
          "bbox": [
            118,
            231,
            235,
            243
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_8",
          "label": "para",
          "text": "operations on objects, 86",
          "level": -1,
          "page": 489,
          "reading_order": 8,
          "bbox": [
            117,
            243,
            243,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_9",
          "label": "para",
          "text": "database query via natural language, 361-365",
          "level": -1,
          "page": 489,
          "reading_order": 9,
          "bbox": [
            97,
            259,
            327,
            272
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_10",
          "label": "para",
          "text": "databases, obtaining data from, 418",
          "level": -1,
          "page": 489,
          "reading_order": 10,
          "bbox": [
            97,
            272,
            279,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_11",
          "label": "para",
          "text": "debugger (Python), 15\ndebugging techniques",
          "level": -1,
          "page": 489,
          "reading_order": 11,
          "bbox": [
            97,
            286,
            209,
            316
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_13",
          "label": "para",
          "text": "decimal integers, formatting, 119",
          "level": -1,
          "page": 489,
          "reading_order": 13,
          "bbox": [
            97,
            317,
            270,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_14",
          "label": "para",
          "text": "decision nodes, 24",
          "level": -1,
          "page": 489,
          "reading_order": 14,
          "bbox": [
            100,
            331,
            190,
            343
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_15",
          "label": "para",
          "text": "decision stumps, 243",
          "level": -1,
          "page": 489,
          "reading_order": 15,
          "bbox": [
            97,
            347,
            207,
            360
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_16",
          "label": "para",
          "text": "decision trees, 242-24",
          "level": -1,
          "page": 489,
          "reading_order": 16,
          "bbox": [
            97,
            360,
            209,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_17",
          "label": "para",
          "text": "entropy and information gain, 243",
          "level": -1,
          "page": 489,
          "reading_order": 17,
          "bbox": [
            118,
            376,
            288,
            389
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_18",
          "label": "para",
          "text": "decision-tree classifier, 229",
          "level": -1,
          "page": 489,
          "reading_order": 18,
          "bbox": [
            97,
            390,
            234,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_19",
          "label": "para",
          "text": "declarative style, 140\ndecoding, 94",
          "level": -1,
          "page": 489,
          "reading_order": 19,
          "bbox": [
            97,
            403,
            207,
            433
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_21",
          "label": "para",
          "text": "def keyword, 9",
          "level": -1,
          "page": 489,
          "reading_order": 21,
          "bbox": [
            97,
            434,
            172,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_22",
          "label": "para",
          "text": "defaultdict, 193",
          "level": -1,
          "page": 489,
          "reading_order": 22,
          "bbox": [
            100,
            448,
            180,
            460
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_23",
          "label": "para",
          "text": "defensive programming, 159",
          "level": -1,
          "page": 489,
          "reading_order": 23,
          "bbox": [
            97,
            464,
            243,
            477
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_24",
          "label": "para",
          "text": "demonstratives, agreement with noun, 329",
          "level": -1,
          "page": 489,
          "reading_order": 24,
          "bbox": [
            97,
            477,
            315,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_25",
          "label": "para",
          "text": "dependencies, 310",
          "level": -1,
          "page": 489,
          "reading_order": 25,
          "bbox": [
            100,
            492,
            191,
            506
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_26",
          "label": "para",
          "text": "criteria for, 312",
          "level": -1,
          "page": 489,
          "reading_order": 26,
          "bbox": [
            113,
            507,
            191,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_27",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 489,
          "reading_order": 27,
          "bbox": [
            118,
            519,
            299,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_28",
          "label": "para",
          "text": "XML, 42",
          "level": -1,
          "page": 489,
          "reading_order": 28,
          "bbox": [
            153,
            537,
            207,
            547
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_29",
          "label": "para",
          "text": "non-projective, 312",
          "level": -1,
          "page": 489,
          "reading_order": 29,
          "bbox": [
            109,
            547,
            212,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_30",
          "label": "para",
          "text": "projective, 31\nunbounded d",
          "level": -1,
          "page": 489,
          "reading_order": 30,
          "bbox": [
            109,
            564,
            181,
            591
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_31",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 489,
          "reading_order": 31,
          "bbox": [
            117,
            591,
            307,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_32",
          "label": "para",
          "text": "dependency grammars, 310–315\nvalency and the lexicon, 312",
          "level": -1,
          "page": 489,
          "reading_order": 32,
          "bbox": [
            97,
            609,
            261,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_34",
          "label": "para",
          "text": "dependents, 310",
          "level": -1,
          "page": 489,
          "reading_order": 34,
          "bbox": [
            97,
            637,
            180,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_35",
          "label": "para",
          "text": "descriptive models, 255",
          "level": -1,
          "page": 489,
          "reading_order": 35,
          "bbox": [
            97,
            654,
            216,
            667
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_36",
          "label": "para",
          "text": "determiners, 186",
          "level": -1,
          "page": 489,
          "reading_order": 36,
          "bbox": [
            97,
            668,
            182,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_37",
          "label": "para",
          "text": "agreement with nouns, 333",
          "level": -1,
          "page": 489,
          "reading_order": 37,
          "bbox": [
            116,
            680,
            252,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_38",
          "label": "para",
          "text": "deve-test set, 225",
          "level": -1,
          "page": 489,
          "reading_order": 38,
          "bbox": [
            97,
            698,
            184,
            708
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_39",
          "label": "para",
          "text": "development set, 225",
          "level": -1,
          "page": 489,
          "reading_order": 39,
          "bbox": [
            97,
            712,
            207,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_40",
          "label": "para",
          "text": "similarity to test set, 238",
          "level": -1,
          "page": 489,
          "reading_order": 40,
          "bbox": [
            118,
            725,
            243,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_41",
          "label": "para",
          "text": "dialogue act tagging, 214",
          "level": -1,
          "page": 489,
          "reading_order": 41,
          "bbox": [
            97,
            743,
            225,
            755
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_42",
          "label": "para",
          "text": "dialogue acts, identifying types, 235",
          "level": -1,
          "page": 489,
          "reading_order": 42,
          "bbox": [
            97,
            755,
            279,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_43",
          "label": "para",
          "text": "dialogue systems (see spoken dialogue systems)\ndictionaries",
          "level": -1,
          "page": 489,
          "reading_order": 43,
          "bbox": [
            97,
            770,
            333,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_44",
          "label": "para",
          "text": "feature set, 223",
          "level": -1,
          "page": 489,
          "reading_order": 44,
          "bbox": [
            359,
            71,
            442,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_45",
          "label": "para",
          "text": "feature structures as, 337",
          "level": -1,
          "page": 489,
          "reading_order": 45,
          "bbox": [
            359,
            89,
            494,
            100
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_46",
          "label": "para",
          "text": "pronouncing dictionary, 63-65",
          "level": -1,
          "page": 489,
          "reading_order": 46,
          "bbox": [
            359,
            104,
            522,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_47",
          "label": "para",
          "text": "Python, 189-198\ndefault, 193",
          "level": -1,
          "page": 489,
          "reading_order": 47,
          "bbox": [
            359,
            117,
            452,
            144
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_49",
          "label": "para",
          "text": "defining, 19",
          "level": -1,
          "page": 489,
          "reading_order": 49,
          "bbox": [
            382,
            145,
            442,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_50",
          "label": "para",
          "text": "dictionary data type, 190\nfinding key given a value,",
          "level": -1,
          "page": 489,
          "reading_order": 50,
          "bbox": [
            382,
            161,
            512,
            190
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_52",
          "label": "para",
          "text": "ndexing lists versus, 189",
          "level": -1,
          "page": 489,
          "reading_order": 52,
          "bbox": [
            385,
            191,
            512,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_53",
          "label": "para",
          "text": "ummary of dictionary methods, 197",
          "level": -1,
          "page": 489,
          "reading_order": 53,
          "bbox": [
            386,
            206,
            568,
            216
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_54",
          "label": "para",
          "text": "updating incrementally, 195",
          "level": -1,
          "page": 489,
          "reading_order": 54,
          "bbox": [
            368,
            221,
            523,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_55",
          "label": "para",
          "text": "storing features and values, 327",
          "level": -1,
          "page": 489,
          "reading_order": 55,
          "bbox": [
            359,
            234,
            530,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_56",
          "label": "para",
          "text": "translation, 66",
          "level": -1,
          "page": 489,
          "reading_order": 56,
          "bbox": [
            359,
            250,
            440,
            261
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_57",
          "label": "para",
          "text": "dictionary",
          "level": -1,
          "page": 489,
          "reading_order": 57,
          "bbox": [
            349,
            265,
            404,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_58",
          "label": "para",
          "text": "methods, 197",
          "level": -1,
          "page": 489,
          "reading_order": 58,
          "bbox": [
            350,
            278,
            440,
            289
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_59",
          "label": "para",
          "text": "dictionary data structure (Python), 65",
          "level": -1,
          "page": 489,
          "reading_order": 59,
          "bbox": [
            349,
            294,
            539,
            305
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_60",
          "label": "para",
          "text": "directed acyclic graphs (DAGs), 338",
          "level": -1,
          "page": 489,
          "reading_order": 60,
          "bbox": [
            349,
            305,
            531,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_61",
          "label": "para",
          "text": "discourse module, 401",
          "level": -1,
          "page": 489,
          "reading_order": 61,
          "bbox": [
            349,
            322,
            467,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_62",
          "label": "para",
          "text": "discourse semantics, 397-402",
          "level": -1,
          "page": 489,
          "reading_order": 62,
          "bbox": [
            349,
            338,
            503,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_63",
          "label": "para",
          "text": "discourse processing, 400-402",
          "level": -1,
          "page": 489,
          "reading_order": 63,
          "bbox": [
            359,
            349,
            521,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_64",
          "label": "para",
          "text": "discourse referents, 397",
          "level": -1,
          "page": 489,
          "reading_order": 64,
          "bbox": [
            359,
            367,
            485,
            378
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_65",
          "label": "para",
          "text": "discourse representation structure (DRS),\n397",
          "level": -1,
          "page": 489,
          "reading_order": 65,
          "bbox": [
            359,
            382,
            574,
            406
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_66",
          "label": "para",
          "text": "Discourse Representation Theory (DRT),\n397–400",
          "level": -1,
          "page": 489,
          "reading_order": 66,
          "bbox": [
            359,
            411,
            574,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_67",
          "label": "para",
          "text": "dispersion plot, 6\ndivide-and-conquer",
          "level": -1,
          "page": 489,
          "reading_order": 67,
          "bbox": [
            349,
            439,
            440,
            468
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_69",
          "label": "para",
          "text": "docstrings, 14:",
          "level": -1,
          "page": 489,
          "reading_order": 69,
          "bbox": [
            349,
            469,
            422,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_70",
          "label": "para",
          "text": "contents and structure of, 148",
          "level": -1,
          "page": 489,
          "reading_order": 70,
          "bbox": [
            359,
            483,
            521,
            495
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_71",
          "label": "para",
          "text": "example of complete docstring, 148",
          "level": -1,
          "page": 489,
          "reading_order": 71,
          "bbox": [
            359,
            499,
            548,
            512
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_72",
          "label": "para",
          "text": "module-level, 155",
          "level": -1,
          "page": 489,
          "reading_order": 72,
          "bbox": [
            359,
            512,
            458,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_73",
          "label": "para",
          "text": "doctest block, 148",
          "level": -1,
          "page": 489,
          "reading_order": 73,
          "bbox": [
            349,
            528,
            441,
            539
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_74",
          "label": "para",
          "text": "doctest module, 160",
          "level": -1,
          "page": 489,
          "reading_order": 74,
          "bbox": [
            349,
            543,
            449,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_75",
          "label": "para",
          "text": "document classification, 227",
          "level": -1,
          "page": 489,
          "reading_order": 75,
          "bbox": [
            349,
            555,
            494,
            567
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_76",
          "label": "para",
          "text": "documentation",
          "level": -1,
          "page": 489,
          "reading_order": 76,
          "bbox": [
            349,
            572,
            431,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_77",
          "label": "para",
          "text": "functions, 148",
          "level": -1,
          "page": 489,
          "reading_order": 77,
          "bbox": [
            359,
            582,
            440,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_78",
          "label": "para",
          "text": "online Python documentation, version\nand, 173",
          "level": -1,
          "page": 489,
          "reading_order": 78,
          "bbox": [
            359,
            600,
            559,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_79",
          "label": "para",
          "text": "Python, resources for further information,\n173",
          "level": -1,
          "page": 489,
          "reading_order": 79,
          "bbox": [
            359,
            627,
            583,
            655
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_80",
          "label": "para",
          "text": "docutils module, 148",
          "level": -1,
          "page": 489,
          "reading_order": 80,
          "bbox": [
            349,
            660,
            458,
            672
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_81",
          "label": "para",
          "text": "domain (of a model), 377",
          "level": -1,
          "page": 489,
          "reading_order": 81,
          "bbox": [
            349,
            672,
            477,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_82",
          "label": "para",
          "text": "DRS (discourse representation structure), 397",
          "level": -1,
          "page": 489,
          "reading_order": 82,
          "bbox": [
            350,
            689,
            583,
            702
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_83",
          "label": "para",
          "text": "DRS conditions, 39°",
          "level": -1,
          "page": 489,
          "reading_order": 83,
          "bbox": [
            350,
            704,
            449,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_84",
          "label": "para",
          "text": "DRT (Discourse Representation Theory), 397–\n400",
          "level": -1,
          "page": 489,
          "reading_order": 84,
          "bbox": [
            350,
            716,
            583,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_85",
          "label": "para",
          "text": "Dublin Core Metadata initiative, 43",
          "level": -1,
          "page": 489,
          "reading_order": 85,
          "bbox": [
            350,
            743,
            530,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_86",
          "label": "para",
          "text": "duck typing, 281",
          "level": -1,
          "page": 489,
          "reading_order": 86,
          "bbox": [
            349,
            761,
            432,
            775
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_87",
          "label": "para",
          "text": "dynamic programming, 165",
          "level": -1,
          "page": 489,
          "reading_order": 87,
          "bbox": [
            349,
            777,
            494,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_489_order_88",
          "label": "foot",
          "text": "General Index | 467",
          "level": -1,
          "page": 489,
          "reading_order": 88,
          "bbox": [
            494,
            824,
            585,
            838
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_0",
          "label": "para",
          "text": "application to parsing with context-free\ngrammar, 307\ndifferent approaches to, 167",
          "level": -1,
          "page": 490,
          "reading_order": 0,
          "bbox": [
            113,
            71,
            315,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_490_order_1",
      "label": "sec",
      "text": "日",
      "level": 1,
      "page": 490,
      "reading_order": 1,
      "bbox": [
        98,
        134,
        104,
        152
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_490_order_2",
          "label": "para",
          "text": "Earley chart parser, 334",
          "level": -1,
          "page": 490,
          "reading_order": 2,
          "bbox": [
            97,
            152,
            217,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_3",
          "label": "para",
          "text": "electronic books, 80",
          "level": -1,
          "page": 490,
          "reading_order": 3,
          "bbox": [
            97,
            170,
            199,
            180
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_4",
          "label": "para",
          "text": "elements, XML, 425",
          "level": -1,
          "page": 490,
          "reading_order": 4,
          "bbox": [
            97,
            185,
            199,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_5",
          "label": "para",
          "text": "ElementTree interface, 427–429\nusing to access Toolbox data,",
          "level": -1,
          "page": 490,
          "reading_order": 5,
          "bbox": [
            97,
            197,
            261,
            227
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_7",
          "label": "para",
          "text": "elif clause, if ... elif statement, 13.",
          "level": -1,
          "page": 490,
          "reading_order": 7,
          "bbox": [
            97,
            228,
            270,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_8",
          "label": "para",
          "text": "elif statements, 26\nelse statements, 26",
          "level": -1,
          "page": 490,
          "reading_order": 8,
          "bbox": [
            97,
            241,
            189,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_10",
          "label": "para",
          "text": "encoding, 94",
          "level": -1,
          "page": 490,
          "reading_order": 10,
          "bbox": [
            97,
            269,
            162,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_11",
          "label": "para",
          "text": "encoding features, 223",
          "level": -1,
          "page": 490,
          "reading_order": 11,
          "bbox": [
            100,
            286,
            216,
            300
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_12",
          "label": "para",
          "text": "encoding parameters, codecs module, 9",
          "level": -1,
          "page": 490,
          "reading_order": 12,
          "bbox": [
            97,
            302,
            297,
            315
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_13",
          "label": "para",
          "text": "endangered languages, special considerations\nwith, 423–424",
          "level": -1,
          "page": 490,
          "reading_order": 13,
          "bbox": [
            97,
            315,
            326,
            342
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_14",
          "label": "para",
          "text": "entities, 37",
          "level": -1,
          "page": 490,
          "reading_order": 14,
          "bbox": [
            97,
            347,
            153,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_15",
          "label": "para",
          "text": "entity detection, using chunking, 264-270",
          "level": -1,
          "page": 490,
          "reading_order": 15,
          "bbox": [
            97,
            358,
            307,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_16",
          "label": "para",
          "text": "entrie",
          "level": -1,
          "page": 490,
          "reading_order": 16,
          "bbox": [
            97,
            376,
            126,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_17",
          "label": "para",
          "text": "adding field to, in Toolbox, 43",
          "level": -1,
          "page": 490,
          "reading_order": 17,
          "bbox": [
            109,
            385,
            270,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_18",
          "label": "para",
          "text": "contents of, 60",
          "level": -1,
          "page": 490,
          "reading_order": 18,
          "bbox": [
            109,
            403,
            189,
            415
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_19",
          "label": "para",
          "text": "converting data formats, 419",
          "level": -1,
          "page": 490,
          "reading_order": 19,
          "bbox": [
            109,
            419,
            261,
            432
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_20",
          "label": "para",
          "text": "formatting in XML, 430",
          "level": -1,
          "page": 490,
          "reading_order": 20,
          "bbox": [
            109,
            432,
            234,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_21",
          "label": "para",
          "text": "entropy, 251",
          "level": -1,
          "page": 490,
          "reading_order": 21,
          "bbox": [
            100,
            448,
            162,
            461
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_22",
          "label": "para",
          "text": "(see also Maximum Entropy classifiers)\ncalculating for gender prediction task, 243\nmaximizing in Maximum Entropy\nclassifier, 252",
          "level": -1,
          "page": 490,
          "reading_order": 22,
          "bbox": [
            109,
            463,
            326,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_23",
          "label": "para",
          "text": "epytext markup language, 148\nequality, 132, 372",
          "level": -1,
          "page": 490,
          "reading_order": 23,
          "bbox": [
            97,
            519,
            252,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_25",
          "label": "para",
          "text": "equivalence (<->) operator, 368",
          "level": -1,
          "page": 490,
          "reading_order": 25,
          "bbox": [
            97,
            550,
            261,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_26",
          "label": "para",
          "text": "equivalent, 340\nerror analysis, 22",
          "level": -1,
          "page": 490,
          "reading_order": 26,
          "bbox": [
            97,
            564,
            182,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_28",
          "label": "para",
          "text": "errors",
          "level": -1,
          "page": 490,
          "reading_order": 28,
          "bbox": [
            97,
            594,
            127,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_29",
          "label": "para",
          "text": "runtime, 13",
          "level": -1,
          "page": 490,
          "reading_order": 29,
          "bbox": [
            109,
            609,
            172,
            620
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_30",
          "label": "para",
          "text": "sources of, 156",
          "level": -1,
          "page": 490,
          "reading_order": 30,
          "bbox": [
            109,
            624,
            189,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_31",
          "label": "para",
          "text": "syntax,",
          "level": -1,
          "page": 490,
          "reading_order": 31,
          "bbox": [
            109,
            636,
            153,
            656
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_32",
          "label": "para",
          "text": "evaluation sets, 238",
          "level": -1,
          "page": 490,
          "reading_order": 32,
          "bbox": [
            97,
            656,
            198,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_33",
          "label": "para",
          "text": "events, pairing with conditions in conditiona\nfrequency distribution, 52",
          "level": -1,
          "page": 490,
          "reading_order": 33,
          "bbox": [
            97,
            668,
            324,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_34",
          "label": "para",
          "text": "exceptions, 158",
          "level": -1,
          "page": 490,
          "reading_order": 34,
          "bbox": [
            97,
            698,
            180,
            710
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_35",
          "label": "para",
          "text": "existential quantifier, 374",
          "level": -1,
          "page": 490,
          "reading_order": 35,
          "bbox": [
            97,
            710,
            226,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_36",
          "label": "para",
          "text": "exists operator, 376",
          "level": -1,
          "page": 490,
          "reading_order": 36,
          "bbox": [
            100,
            725,
            198,
            739
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_37",
          "label": "para",
          "text": "Expected Likelihood Estimation, 249",
          "level": -1,
          "page": 490,
          "reading_order": 37,
          "bbox": [
            99,
            741,
            288,
            754
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_38",
          "label": "para",
          "text": "exporting data, 117",
          "level": -1,
          "page": 490,
          "reading_order": 38,
          "bbox": [
            97,
            754,
            198,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_40",
          "label": "para",
          "text": "f-structure, 357",
          "level": -1,
          "page": 490,
          "reading_order": 40,
          "bbox": [
            349,
            98,
            431,
            109
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_41",
          "label": "para",
          "text": "feature extractors",
          "level": -1,
          "page": 490,
          "reading_order": 41,
          "bbox": [
            349,
            115,
            440,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_42",
          "label": "para",
          "text": "defining for dialogue acts, 235",
          "level": -1,
          "page": 490,
          "reading_order": 42,
          "bbox": [
            359,
            125,
            521,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_44",
          "label": "para",
          "text": "defining for noun phrase (NP) chunker,\n276–278",
          "level": -1,
          "page": 490,
          "reading_order": 44,
          "bbox": [
            359,
            156,
            566,
            181
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_45",
          "label": "para",
          "text": "defining for punctuation, 234",
          "level": -1,
          "page": 490,
          "reading_order": 45,
          "bbox": [
            359,
            186,
            512,
            199
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_46",
          "label": "para",
          "text": "defining for suffix checking, 229",
          "level": -1,
          "page": 490,
          "reading_order": 46,
          "bbox": [
            359,
            199,
            530,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_47",
          "label": "para",
          "text": "Recognizing Textual Entailment (RTE),\n236",
          "level": -1,
          "page": 490,
          "reading_order": 47,
          "bbox": [
            359,
            215,
            566,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_48",
          "label": "para",
          "text": "selecting relevant features, 224-227",
          "level": -1,
          "page": 490,
          "reading_order": 48,
          "bbox": [
            359,
            241,
            548,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_49",
          "label": "para",
          "text": "feature paths, 339",
          "level": -1,
          "page": 490,
          "reading_order": 49,
          "bbox": [
            349,
            259,
            440,
            272
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_50",
          "label": "para",
          "text": "feature sets, 223",
          "level": -1,
          "page": 490,
          "reading_order": 50,
          "bbox": [
            349,
            272,
            431,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_51",
          "label": "para",
          "text": "feature structures, 328\norder of features, 33",
          "level": -1,
          "page": 490,
          "reading_order": 51,
          "bbox": [
            349,
            286,
            467,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_53",
          "label": "para",
          "text": "resources for further reading, 357",
          "level": -1,
          "page": 490,
          "reading_order": 53,
          "bbox": [
            359,
            314,
            534,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_54",
          "label": "para",
          "text": "feature-based grammars, 327–360",
          "level": -1,
          "page": 490,
          "reading_order": 54,
          "bbox": [
            350,
            331,
            521,
            345
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_55",
          "label": "para",
          "text": "auxiliary verbs and inversion, 348",
          "level": -1,
          "page": 490,
          "reading_order": 55,
          "bbox": [
            359,
            347,
            539,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_56",
          "label": "para",
          "text": "case and gender in German, 353",
          "level": -1,
          "page": 490,
          "reading_order": 56,
          "bbox": [
            359,
            358,
            530,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_57",
          "label": "para",
          "text": "example grammar, 333",
          "level": -1,
          "page": 490,
          "reading_order": 57,
          "bbox": [
            359,
            376,
            485,
            389
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_58",
          "label": "para",
          "text": "extending, 344-356",
          "level": -1,
          "page": 490,
          "reading_order": 58,
          "bbox": [
            359,
            389,
            467,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_59",
          "label": "para",
          "text": "lexical heads, 347",
          "level": -1,
          "page": 490,
          "reading_order": 59,
          "bbox": [
            359,
            403,
            458,
            416
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_60",
          "label": "para",
          "text": "parsing using Earley chart parser, 334",
          "level": -1,
          "page": 490,
          "reading_order": 60,
          "bbox": [
            359,
            420,
            557,
            433
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_61",
          "label": "para",
          "text": "processing feature structures, 337-344",
          "level": -1,
          "page": 490,
          "reading_order": 61,
          "bbox": [
            359,
            433,
            560,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_62",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 490,
          "reading_order": 62,
          "bbox": [
            386,
            448,
            583,
            477
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_64",
          "label": "para",
          "text": "subcategorization, 344-347",
          "level": -1,
          "page": 490,
          "reading_order": 64,
          "bbox": [
            359,
            478,
            506,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_65",
          "label": "para",
          "text": "syntactic agreement, 329-331",
          "level": -1,
          "page": 490,
          "reading_order": 65,
          "bbox": [
            359,
            492,
            514,
            506
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_66",
          "label": "para",
          "text": "terminology, 336",
          "level": -1,
          "page": 490,
          "reading_order": 66,
          "bbox": [
            359,
            507,
            452,
            520
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_67",
          "label": "para",
          "text": "translating from English to SQL, 362",
          "level": -1,
          "page": 490,
          "reading_order": 67,
          "bbox": [
            359,
            520,
            557,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_68",
          "label": "para",
          "text": "unbounded dependency constructions\n349–353",
          "level": -1,
          "page": 490,
          "reading_order": 68,
          "bbox": [
            359,
            537,
            559,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_69",
          "label": "para",
          "text": "using attributes and constraints, 331–336\nfeatures, 223",
          "level": -1,
          "page": 490,
          "reading_order": 69,
          "bbox": [
            349,
            564,
            575,
            592
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_70",
          "label": "para",
          "text": "non-binary features in naive Bayes",
          "level": -1,
          "page": 490,
          "reading_order": 70,
          "bbox": [
            359,
            592,
            539,
            621
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_71",
          "label": "para",
          "text": "fields, 136",
          "level": -1,
          "page": 490,
          "reading_order": 71,
          "bbox": [
            349,
            625,
            404,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_72",
          "label": "para",
          "text": "file formats, libraries for, 172",
          "level": -1,
          "page": 490,
          "reading_order": 72,
          "bbox": [
            349,
            636,
            496,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_73",
          "label": "para",
          "text": "files",
          "level": -1,
          "page": 490,
          "reading_order": 73,
          "bbox": [
            349,
            654,
            369,
            664
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_74",
          "label": "para",
          "text": "opening and reading local files, 84",
          "level": -1,
          "page": 490,
          "reading_order": 74,
          "bbox": [
            359,
            664,
            539,
            681
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_75",
          "label": "para",
          "text": "writing program output to, 120",
          "level": -1,
          "page": 490,
          "reading_order": 75,
          "bbox": [
            359,
            681,
            530,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_76",
          "label": "para",
          "text": "fillers, 349",
          "level": -1,
          "page": 490,
          "reading_order": 76,
          "bbox": [
            349,
            698,
            404,
            709
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_77",
          "label": "para",
          "text": "first-order logic, 372-385",
          "level": -1,
          "page": 490,
          "reading_order": 77,
          "bbox": [
            349,
            709,
            477,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_78",
          "label": "para",
          "text": "individual variables and assignments, 378",
          "level": -1,
          "page": 490,
          "reading_order": 78,
          "bbox": [
            359,
            725,
            575,
            740
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_79",
          "label": "para",
          "text": "model building, 383",
          "level": -1,
          "page": 490,
          "reading_order": 79,
          "bbox": [
            359,
            742,
            467,
            755
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_80",
          "label": "para",
          "text": "quantifier scope ambiguity, 381",
          "level": -1,
          "page": 490,
          "reading_order": 80,
          "bbox": [
            359,
            755,
            530,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_81",
          "label": "para",
          "text": "summary of language, 376",
          "level": -1,
          "page": 490,
          "reading_order": 81,
          "bbox": [
            359,
            770,
            503,
            784
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_82",
          "label": "para",
          "text": "syntax, 372–37",
          "level": -1,
          "page": 490,
          "reading_order": 82,
          "bbox": [
            359,
            786,
            441,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_490_order_83",
          "label": "foot",
          "text": "468 | General Inde",
          "level": -1,
          "page": 490,
          "reading_order": 83,
          "bbox": [
            97,
            824,
            182,
            838
          ],
          "section_number": "468",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_0",
          "label": "para",
          "text": "heorem proving, 37",
          "level": -1,
          "page": 491,
          "reading_order": 0,
          "bbox": [
            118,
            71,
            216,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_1",
          "label": "para",
          "text": "ruth in model, 377",
          "level": -1,
          "page": 491,
          "reading_order": 1,
          "bbox": [
            118,
            89,
            212,
            100
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_2",
          "label": "para",
          "text": "floating-point numbers, formatting, 119\nfolds, 241",
          "level": -1,
          "page": 491,
          "reading_order": 2,
          "bbox": [
            97,
            100,
            299,
            129
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_3",
          "label": "para",
          "text": "or statements, 2",
          "level": -1,
          "page": 491,
          "reading_order": 3,
          "bbox": [
            100,
            134,
            181,
            144
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_4",
          "label": "para",
          "text": "combining with if statements, 26",
          "level": -1,
          "page": 491,
          "reading_order": 4,
          "bbox": [
            113,
            144,
            279,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_5",
          "label": "para",
          "text": "nside a list comprehension, 63",
          "level": -1,
          "page": 491,
          "reading_order": 5,
          "bbox": [
            117,
            161,
            270,
            175
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_6",
          "label": "para",
          "text": "terating over characters in strings, 9",
          "level": -1,
          "page": 491,
          "reading_order": 6,
          "bbox": [
            117,
            177,
            297,
            190
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_7",
          "label": "para",
          "text": "format strings, 118",
          "level": -1,
          "page": 491,
          "reading_order": 7,
          "bbox": [
            97,
            190,
            193,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_8",
          "label": "para",
          "text": "formatting program output, 116-121",
          "level": -1,
          "page": 491,
          "reading_order": 8,
          "bbox": [
            97,
            206,
            288,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_9",
          "label": "para",
          "text": "converting from lists to strings, 116",
          "level": -1,
          "page": 491,
          "reading_order": 9,
          "bbox": [
            113,
            221,
            297,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_10",
          "label": "para",
          "text": "trings and formats, 117-118",
          "level": -1,
          "page": 491,
          "reading_order": 10,
          "bbox": [
            118,
            234,
            261,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_11",
          "label": "para",
          "text": "ext wrapping, 120",
          "level": -1,
          "page": 491,
          "reading_order": 11,
          "bbox": [
            117,
            250,
            209,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_12",
          "label": "para",
          "text": "writing results to file, 120",
          "level": -1,
          "page": 491,
          "reading_order": 12,
          "bbox": [
            117,
            265,
            243,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_13",
          "label": "para",
          "text": "formulas of propositional logic, 368",
          "level": -1,
          "page": 491,
          "reading_order": 13,
          "bbox": [
            98,
            278,
            279,
            292
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_14",
          "label": "para",
          "text": "formulas, type (t), 373",
          "level": -1,
          "page": 491,
          "reading_order": 14,
          "bbox": [
            100,
            294,
            210,
            307
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_15",
          "label": "para",
          "text": "free, 375",
          "level": -1,
          "page": 491,
          "reading_order": 15,
          "bbox": [
            97,
            307,
            144,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_16",
          "label": "para",
          "text": "Frege's Principle, 385",
          "level": -1,
          "page": 491,
          "reading_order": 16,
          "bbox": [
            100,
            322,
            207,
            336
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_17",
          "label": "para",
          "text": "frequency distributions, 17, 22",
          "level": -1,
          "page": 491,
          "reading_order": 17,
          "bbox": [
            100,
            338,
            252,
            351
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_18",
          "label": "para",
          "text": "conditional (see conditional frequency\ndistributions)",
          "level": -1,
          "page": 491,
          "reading_order": 18,
          "bbox": [
            113,
            351,
            307,
            378
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_19",
          "label": "para",
          "text": "unctions defined for, 22",
          "level": -1,
          "page": 491,
          "reading_order": 19,
          "bbox": [
            118,
            382,
            238,
            394
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_20",
          "label": "para",
          "text": "eters, occurrence in strings, 90",
          "level": -1,
          "page": 491,
          "reading_order": 20,
          "bbox": [
            117,
            394,
            271,
            412
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_21",
          "label": "para",
          "text": "functions, 142-154",
          "level": -1,
          "page": 491,
          "reading_order": 21,
          "bbox": [
            100,
            412,
            198,
            422
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_22",
          "label": "para",
          "text": "abstraction provided by, 147",
          "level": -1,
          "page": 491,
          "reading_order": 22,
          "bbox": [
            116,
            426,
            261,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_23",
          "label": "para",
          "text": "accumulative, 150",
          "level": -1,
          "page": 491,
          "reading_order": 23,
          "bbox": [
            118,
            439,
            207,
            450
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_24",
          "label": "para",
          "text": "as arguments to another function, 149",
          "level": -1,
          "page": 491,
          "reading_order": 24,
          "bbox": [
            116,
            455,
            306,
            468
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_25",
          "label": "para",
          "text": "call-by-value parameter passing, 144",
          "level": -1,
          "page": 491,
          "reading_order": 25,
          "bbox": [
            113,
            468,
            297,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_26",
          "label": "para",
          "text": "checking parameter types, 146\ndefined, 9, 57",
          "level": -1,
          "page": 491,
          "reading_order": 26,
          "bbox": [
            113,
            483,
            270,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_28",
          "label": "para",
          "text": "documentation for Python built-in\nfunctions, 173",
          "level": -1,
          "page": 491,
          "reading_order": 28,
          "bbox": [
            117,
            511,
            288,
            539
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_29",
          "label": "para",
          "text": "locumenting, 148",
          "level": -1,
          "page": 491,
          "reading_order": 29,
          "bbox": [
            118,
            544,
            207,
            556
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_30",
          "label": "para",
          "text": "errors from, 157",
          "level": -1,
          "page": 491,
          "reading_order": 30,
          "bbox": [
            118,
            556,
            198,
            567
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_31",
          "label": "para",
          "text": "or frequency distributions, 22",
          "level": -1,
          "page": 491,
          "reading_order": 31,
          "bbox": [
            117,
            572,
            270,
            585
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_32",
          "label": "para",
          "text": "for iteration over sequences, 134",
          "level": -1,
          "page": 491,
          "reading_order": 32,
          "bbox": [
            116,
            585,
            279,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_33",
          "label": "para",
          "text": "generating plurals of nouns (example), 58\nhigher-order, 151",
          "level": -1,
          "page": 491,
          "reading_order": 33,
          "bbox": [
            113,
            600,
            324,
            629
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_35",
          "label": "para",
          "text": "inputs and outputs, 143",
          "level": -1,
          "page": 491,
          "reading_order": 35,
          "bbox": [
            117,
            630,
            234,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_36",
          "label": "para",
          "text": "named arguments, 152",
          "level": -1,
          "page": 491,
          "reading_order": 36,
          "bbox": [
            117,
            645,
            234,
            658
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_37",
          "label": "para",
          "text": "naming, 142",
          "level": -1,
          "page": 491,
          "reading_order": 37,
          "bbox": [
            117,
            661,
            180,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_38",
          "label": "para",
          "text": "poorly-designed, 147",
          "level": -1,
          "page": 491,
          "reading_order": 38,
          "bbox": [
            117,
            673,
            220,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_39",
          "label": "para",
          "text": "ecursive, call structure, 165",
          "level": -1,
          "page": 491,
          "reading_order": 39,
          "bbox": [
            118,
            689,
            255,
            700
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_40",
          "label": "para",
          "text": "aving in modules, 59",
          "level": -1,
          "page": 491,
          "reading_order": 40,
          "bbox": [
            118,
            704,
            225,
            717
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_41",
          "label": "para",
          "text": "variable scope, 145",
          "level": -1,
          "page": 491,
          "reading_order": 41,
          "bbox": [
            117,
            717,
            209,
            731
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_42",
          "label": "para",
          "text": "well-designed, 147",
          "level": -1,
          "page": 491,
          "reading_order": 42,
          "bbox": [
            117,
            733,
            207,
            746
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_43",
          "label": "para",
          "text": "G",
          "level": -1,
          "page": 491,
          "reading_order": 43,
          "bbox": [
            97,
            761,
            106,
            779
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_44",
          "label": "para",
          "text": "gaps, 349",
          "level": -1,
          "page": 491,
          "reading_order": 44,
          "bbox": [
            97,
            786,
            144,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_45",
          "label": "para",
          "text": "gazetteer, 28",
          "level": -1,
          "page": 491,
          "reading_order": 45,
          "bbox": [
            349,
            71,
            413,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_46",
          "label": "para",
          "text": "gender identification, 222",
          "level": -1,
          "page": 491,
          "reading_order": 46,
          "bbox": [
            349,
            89,
            479,
            102
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_47",
          "label": "para",
          "text": "Decision Tree model for, 242",
          "level": -1,
          "page": 491,
          "reading_order": 47,
          "bbox": [
            350,
            104,
            513,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_48",
          "label": "para",
          "text": "gender in German, 353-356",
          "level": -1,
          "page": 491,
          "reading_order": 48,
          "bbox": [
            349,
            116,
            494,
            131
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_49",
          "label": "para",
          "text": "Generalized Phrase Structure Grammar\n(GPSG), 345",
          "level": -1,
          "page": 491,
          "reading_order": 49,
          "bbox": [
            349,
            133,
            548,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_50",
          "label": "para",
          "text": "generate_model () function, 55\ngeneration of language output,",
          "level": -1,
          "page": 491,
          "reading_order": 50,
          "bbox": [
            349,
            161,
            508,
            190
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_52",
          "label": "para",
          "text": "generative classifiers, 254",
          "level": -1,
          "page": 491,
          "reading_order": 52,
          "bbox": [
            349,
            191,
            477,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_53",
          "label": "para",
          "text": "generator expressions, 138",
          "level": -1,
          "page": 491,
          "reading_order": 53,
          "bbox": [
            349,
            206,
            485,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_54",
          "label": "para",
          "text": "[1]",
          "level": -1,
          "page": 491,
          "reading_order": 54,
          "bbox": [
            368,
            221,
            503,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_55",
          "label": "para",
          "text": "genres, systematic differences between, 42-44",
          "level": -1,
          "page": 491,
          "reading_order": 55,
          "bbox": [
            349,
            234,
            583,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_56",
          "label": "para",
          "text": "German, case and gender in, 353-356",
          "level": -1,
          "page": 491,
          "reading_order": 56,
          "bbox": [
            349,
            250,
            540,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_57",
          "label": "para",
          "text": "gerunds, 211",
          "level": -1,
          "page": 491,
          "reading_order": 57,
          "bbox": [
            349,
            265,
            413,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_58",
          "label": "para",
          "text": "glyphs, 9",
          "level": -1,
          "page": 491,
          "reading_order": 58,
          "bbox": [
            349,
            278,
            395,
            292
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_59",
          "label": "para",
          "text": "gold standard, 201",
          "level": -1,
          "page": 491,
          "reading_order": 59,
          "bbox": [
            349,
            294,
            442,
            307
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_60",
          "label": "para",
          "text": "government-sponsored challenges to machine\nlearning application in NLP, 257",
          "level": -1,
          "page": 491,
          "reading_order": 60,
          "bbox": [
            349,
            307,
            583,
            336
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_61",
          "label": "para",
          "text": "gradient (grammaticality), 318",
          "level": -1,
          "page": 491,
          "reading_order": 61,
          "bbox": [
            349,
            338,
            503,
            351
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_62",
          "label": "para",
          "text": "grammars, 327",
          "level": -1,
          "page": 491,
          "reading_order": 62,
          "bbox": [
            349,
            351,
            425,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_63",
          "label": "para",
          "text": "(see also feature-based grammars",
          "level": -1,
          "page": 491,
          "reading_order": 63,
          "bbox": [
            366,
            367,
            532,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_64",
          "label": "para",
          "text": "chunk grammar, 265",
          "level": -1,
          "page": 491,
          "reading_order": 64,
          "bbox": [
            365,
            382,
            471,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_65",
          "label": "para",
          "text": "context-free, 298–30\nparsing with, 302",
          "level": -1,
          "page": 491,
          "reading_order": 65,
          "bbox": [
            365,
            395,
            467,
            424
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_67",
          "label": "para",
          "text": "validating Toolbox entries with, 433",
          "level": -1,
          "page": 491,
          "reading_order": 67,
          "bbox": [
            385,
            425,
            566,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_68",
          "label": "para",
          "text": "writing your own, 300\nendency, 310–315",
          "level": -1,
          "page": 491,
          "reading_order": 68,
          "bbox": [
            385,
            439,
            494,
            466
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_70",
          "label": "para",
          "text": "development, 315-32",
          "level": -1,
          "page": 491,
          "reading_order": 70,
          "bbox": [
            359,
            468,
            476,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_71",
          "label": "para",
          "text": "problems with ambiguity, 317",
          "level": -1,
          "page": 491,
          "reading_order": 71,
          "bbox": [
            385,
            483,
            534,
            497
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_72",
          "label": "para",
          "text": "reebanks and grammars, 315–317",
          "level": -1,
          "page": 491,
          "reading_order": 72,
          "bbox": [
            386,
            499,
            557,
            512
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_73",
          "label": "para",
          "text": "weighted grammar, 318-321",
          "level": -1,
          "page": 491,
          "reading_order": 73,
          "bbox": [
            385,
            512,
            530,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_74",
          "label": "para",
          "text": "dilemmas in sentence structure analysis,\n292–295",
          "level": -1,
          "page": 491,
          "reading_order": 74,
          "bbox": [
            359,
            528,
            568,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_75",
          "label": "para",
          "text": "resources for further reading, 321",
          "level": -1,
          "page": 491,
          "reading_order": 75,
          "bbox": [
            368,
            555,
            530,
            570
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_76",
          "label": "para",
          "text": "scaling up, 315",
          "level": -1,
          "page": 491,
          "reading_order": 76,
          "bbox": [
            365,
            572,
            441,
            585
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_77",
          "label": "para",
          "text": "grammatical category, 328",
          "level": -1,
          "page": 491,
          "reading_order": 77,
          "bbox": [
            349,
            585,
            485,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_78",
          "label": "para",
          "text": "graphical displays of da",
          "level": -1,
          "page": 491,
          "reading_order": 78,
          "bbox": [
            349,
            600,
            467,
            614
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_79",
          "label": "para",
          "text": "conditional frequency distributions, 56",
          "level": -1,
          "page": 491,
          "reading_order": 79,
          "bbox": [
            365,
            616,
            562,
            629
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_80",
          "label": "para",
          "text": "Matplotlib, 168–170",
          "level": -1,
          "page": 491,
          "reading_order": 80,
          "bbox": [
            366,
            629,
            467,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_81",
          "label": "para",
          "text": "graphs",
          "level": -1,
          "page": 491,
          "reading_order": 81,
          "bbox": [
            349,
            645,
            386,
            658
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_82",
          "label": "para",
          "text": "defining and manipulating, 170",
          "level": -1,
          "page": 491,
          "reading_order": 82,
          "bbox": [
            365,
            660,
            530,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_83",
          "label": "para",
          "text": "directed acyclic graphs, 338",
          "level": -1,
          "page": 491,
          "reading_order": 83,
          "bbox": [
            365,
            673,
            505,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_84",
          "label": "para",
          "text": "greedy sequence classification, 232",
          "level": -1,
          "page": 491,
          "reading_order": 84,
          "bbox": [
            349,
            689,
            530,
            702
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_85",
          "label": "para",
          "text": "Gutenberg Corpus, 40-42, 80",
          "level": -1,
          "page": 491,
          "reading_order": 85,
          "bbox": [
            349,
            704,
            503,
            717
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_86",
          "label": "para",
          "text": "н",
          "level": -1,
          "page": 491,
          "reading_order": 86,
          "bbox": [
            350,
            734,
            358,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_87",
          "label": "para",
          "text": "hapaxes, 19",
          "level": -1,
          "page": 491,
          "reading_order": 87,
          "bbox": [
            349,
            752,
            413,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_88",
          "label": "para",
          "text": "hash arrays, 189, 190",
          "level": -1,
          "page": 491,
          "reading_order": 88,
          "bbox": [
            349,
            770,
            458,
            780
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_89",
          "label": "para",
          "text": "(see also dictionaries",
          "level": -1,
          "page": 491,
          "reading_order": 89,
          "bbox": [
            366,
            785,
            467,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_491_order_90",
          "label": "foot",
          "text": "General Index | 469",
          "level": -1,
          "page": 491,
          "reading_order": 90,
          "bbox": [
            494,
            824,
            585,
            838
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_0",
          "label": "para",
          "text": "read of a sentence, 310",
          "level": -1,
          "page": 492,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            216,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_1",
          "label": "para",
          "text": "criteria for head and dependencies, 312\nneads, lexical, 347",
          "level": -1,
          "page": 492,
          "reading_order": 1,
          "bbox": [
            100,
            89,
            315,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_2",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 492,
          "reading_order": 2,
          "bbox": [
            100,
            116,
            209,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_4",
          "label": "para",
          "text": "hexadecimal notation for Unicode string\nliteral, 95",
          "level": -1,
          "page": 492,
          "reading_order": 4,
          "bbox": [
            100,
            148,
            302,
            173
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_5",
          "label": "para",
          "text": "Hidden Markov Models, 233",
          "level": -1,
          "page": 492,
          "reading_order": 5,
          "bbox": [
            100,
            177,
            243,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_6",
          "label": "para",
          "text": "higher-order functions, 151",
          "level": -1,
          "page": 492,
          "reading_order": 6,
          "bbox": [
            100,
            188,
            234,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_7",
          "label": "para",
          "text": "2019年",
          "level": -1,
          "page": 492,
          "reading_order": 7,
          "bbox": [
            100,
            206,
            163,
            216
          ],
          "section_number": "2019",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_8",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 492,
          "reading_order": 8,
          "bbox": [
            100,
            222,
            171,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_9",
          "label": "para",
          "text": "HTML documents, 82",
          "level": -1,
          "page": 492,
          "reading_order": 9,
          "bbox": [
            100,
            232,
            210,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_10",
          "label": "para",
          "text": "HTML markup, stripping out, 418",
          "level": -1,
          "page": 492,
          "reading_order": 10,
          "bbox": [
            100,
            250,
            272,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_11",
          "label": "para",
          "text": "hypernyms, 70",
          "level": -1,
          "page": 492,
          "reading_order": 11,
          "bbox": [
            100,
            266,
            172,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_12",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 492,
          "reading_order": 12,
          "bbox": [
            118,
            278,
            246,
            305
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_14",
          "label": "para",
          "text": "nyphens in tokenization, 110",
          "level": -1,
          "page": 492,
          "reading_order": 14,
          "bbox": [
            100,
            306,
            244,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_15",
          "label": "para",
          "text": "nyponyms, 69",
          "level": -1,
          "page": 492,
          "reading_order": 15,
          "bbox": [
            100,
            322,
            171,
            336
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_16",
          "label": "para",
          "text": "dentifiers for variables, 15",
          "level": -1,
          "page": 492,
          "reading_order": 16,
          "bbox": [
            100,
            375,
            234,
            386
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_17",
          "label": "para",
          "text": "dioms, Python, 24",
          "level": -1,
          "page": 492,
          "reading_order": 17,
          "bbox": [
            100,
            390,
            191,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_18",
          "label": "para",
          "text": ":DLE (Interactive DeveLopment\nEnvironment), 2",
          "level": -1,
          "page": 492,
          "reading_order": 18,
          "bbox": [
            100,
            403,
            261,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_19",
          "label": "para",
          "text": "f... elif statements, 133",
          "level": -1,
          "page": 492,
          "reading_order": 19,
          "bbox": [
            100,
            430,
            225,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_20",
          "label": "para",
          "text": "f statements, 25",
          "level": -1,
          "page": 492,
          "reading_order": 20,
          "bbox": [
            100,
            448,
            180,
            458
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_21",
          "label": "para",
          "text": "combining with for statements, 26\nconditions in, 133",
          "level": -1,
          "page": 492,
          "reading_order": 21,
          "bbox": [
            113,
            463,
            288,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_22",
          "label": "para",
          "text": "immediate constituents, 297",
          "level": -1,
          "page": 492,
          "reading_order": 22,
          "bbox": [
            100,
            492,
            243,
            502
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_23",
          "label": "para",
          "text": "mmutable, 93",
          "level": -1,
          "page": 492,
          "reading_order": 23,
          "bbox": [
            100,
            507,
            171,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_24",
          "label": "para",
          "text": "mplication (->) operator, 36",
          "level": -1,
          "page": 492,
          "reading_order": 24,
          "bbox": [
            100,
            519,
            243,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_25",
          "label": "para",
          "text": "n operator, 91",
          "level": -1,
          "page": 492,
          "reading_order": 25,
          "bbox": [
            100,
            537,
            171,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_26",
          "label": "para",
          "text": "inaugural Address Corpus, 4",
          "level": -1,
          "page": 492,
          "reading_order": 26,
          "bbox": [
            100,
            549,
            243,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_27",
          "label": "para",
          "text": "nconsistent, 366",
          "level": -1,
          "page": 492,
          "reading_order": 27,
          "bbox": [
            100,
            564,
            183,
            575
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_28",
          "label": "para",
          "text": "indenting code, 138",
          "level": -1,
          "page": 492,
          "reading_order": 28,
          "bbox": [
            100,
            580,
            198,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_29",
          "label": "para",
          "text": "independence assumption, 248",
          "level": -1,
          "page": 492,
          "reading_order": 29,
          "bbox": [
            100,
            593,
            254,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_30",
          "label": "para",
          "text": "naivete of, 24",
          "level": -1,
          "page": 492,
          "reading_order": 30,
          "bbox": [
            117,
            609,
            181,
            619
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_31",
          "label": "para",
          "text": "ndexes",
          "level": -1,
          "page": 492,
          "reading_order": 31,
          "bbox": [
            100,
            624,
            135,
            636
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_32",
          "label": "para",
          "text": "counting from zero (0), 12\nlist, 12 – 14",
          "level": -1,
          "page": 492,
          "reading_order": 32,
          "bbox": [
            113,
            636,
            246,
            664
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_34",
          "label": "para",
          "text": "mapping dictionary definition to lexeme,\n419",
          "level": -1,
          "page": 492,
          "reading_order": 34,
          "bbox": [
            117,
            665,
            324,
            692
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_35",
          "label": "para",
          "text": "peeding up program by using, 163",
          "level": -1,
          "page": 492,
          "reading_order": 35,
          "bbox": [
            118,
            697,
            297,
            710
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_36",
          "label": "para",
          "text": "tring, 15, 89, 91",
          "level": -1,
          "page": 492,
          "reading_order": 36,
          "bbox": [
            118,
            710,
            198,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_37",
          "label": "para",
          "text": "ext index created using a stemmer, 107",
          "level": -1,
          "page": 492,
          "reading_order": 37,
          "bbox": [
            117,
            725,
            315,
            739
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_38",
          "label": "para",
          "text": "words containing a given consonant-vowe\npair, 103",
          "level": -1,
          "page": 492,
          "reading_order": 38,
          "bbox": [
            117,
            741,
            325,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_39",
          "label": "para",
          "text": "nference, 369",
          "level": -1,
          "page": 492,
          "reading_order": 39,
          "bbox": [
            100,
            770,
            171,
            780
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_40",
          "label": "para",
          "text": "nformation extraction, 261-289",
          "level": -1,
          "page": 492,
          "reading_order": 40,
          "bbox": [
            100,
            785,
            261,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_41",
          "label": "para",
          "text": "architecture of system, 26",
          "level": -1,
          "page": 492,
          "reading_order": 41,
          "bbox": [
            359,
            71,
            495,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_42",
          "label": "para",
          "text": "chunking, 264-270",
          "level": -1,
          "page": 492,
          "reading_order": 42,
          "bbox": [
            365,
            89,
            467,
            102
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_43",
          "label": "para",
          "text": "defined, 262",
          "level": -1,
          "page": 492,
          "reading_order": 43,
          "bbox": [
            359,
            104,
            431,
            116
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_44",
          "label": "para",
          "text": "developing and evaluating chunkers, 270–\n278",
          "level": -1,
          "page": 492,
          "reading_order": 44,
          "bbox": [
            359,
            116,
            583,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_45",
          "label": "para",
          "text": "named entity recognition, 281-284",
          "level": -1,
          "page": 492,
          "reading_order": 45,
          "bbox": [
            368,
            148,
            548,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_46",
          "label": "para",
          "text": "recursion in linguistic structure, 278-281",
          "level": -1,
          "page": 492,
          "reading_order": 46,
          "bbox": [
            359,
            161,
            574,
            175
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_47",
          "label": "para",
          "text": "relation extraction, 284",
          "level": -1,
          "page": 492,
          "reading_order": 47,
          "bbox": [
            359,
            177,
            485,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_48",
          "label": "para",
          "text": "resources for further reading, 286",
          "level": -1,
          "page": 492,
          "reading_order": 48,
          "bbox": [
            359,
            188,
            534,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_49",
          "label": "para",
          "text": "information gain, 243",
          "level": -1,
          "page": 492,
          "reading_order": 49,
          "bbox": [
            350,
            206,
            458,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_50",
          "label": "para",
          "text": "inside, outside, begin tags (see IOB tags)",
          "level": -1,
          "page": 492,
          "reading_order": 50,
          "bbox": [
            350,
            221,
            557,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_51",
          "label": "para",
          "text": "integer ordinal, finding for character, 95",
          "level": -1,
          "page": 492,
          "reading_order": 51,
          "bbox": [
            350,
            234,
            557,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_52",
          "label": "para",
          "text": "interpreter",
          "level": -1,
          "page": 492,
          "reading_order": 52,
          "bbox": [
            350,
            250,
            404,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_53",
          "label": "para",
          "text": ">>> prompt, 2",
          "level": -1,
          "page": 492,
          "reading_order": 53,
          "bbox": [
            366,
            266,
            442,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_54",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 492,
          "reading_order": 54,
          "bbox": [
            368,
            278,
            424,
            292
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_55",
          "label": "para",
          "text": "using text editor instead of to write\nprograms, 56",
          "level": -1,
          "page": 492,
          "reading_order": 55,
          "bbox": [
            359,
            294,
            548,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_56",
          "label": "para",
          "text": "inverted clauses, 348",
          "level": -1,
          "page": 492,
          "reading_order": 56,
          "bbox": [
            350,
            322,
            458,
            333
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_57",
          "label": "para",
          "text": "IOB tags, 269, 286",
          "level": -1,
          "page": 492,
          "reading_order": 57,
          "bbox": [
            350,
            339,
            443,
            351
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_58",
          "label": "para",
          "text": "reading, 270–272",
          "level": -1,
          "page": 492,
          "reading_order": 58,
          "bbox": [
            368,
            351,
            453,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_59",
          "label": "para",
          "text": "is operator, 145",
          "level": -1,
          "page": 492,
          "reading_order": 59,
          "bbox": [
            350,
            367,
            431,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_60",
          "label": "para",
          "text": "testing for object identity, 132",
          "level": -1,
          "page": 492,
          "reading_order": 60,
          "bbox": [
            368,
            380,
            521,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_61",
          "label": "para",
          "text": "ISO 639 language codes, 65\niterative optimization techn-",
          "level": -1,
          "page": 492,
          "reading_order": 61,
          "bbox": [
            350,
            395,
            488,
            424
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_492_order_63",
      "label": "sec",
      "text": "]",
      "level": 1,
      "page": 492,
      "reading_order": 63,
      "bbox": [
        348,
        439,
        354,
        457
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_492_order_64",
          "label": "para",
          "text": "joint classifier models, 231",
          "level": -1,
          "page": 492,
          "reading_order": 64,
          "bbox": [
            350,
            463,
            485,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_65",
          "label": "para",
          "text": "joint-features (maximum entropy model), 252",
          "level": -1,
          "page": 492,
          "reading_order": 65,
          "bbox": [
            350,
            474,
            583,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_67",
          "label": "para",
          "text": "Kappa coefficient (k), 414\nkeys, 65, 191",
          "level": -1,
          "page": 492,
          "reading_order": 67,
          "bbox": [
            349,
            528,
            480,
            554
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_69",
          "label": "para",
          "text": "complex, 196",
          "level": -1,
          "page": 492,
          "reading_order": 69,
          "bbox": [
            365,
            555,
            440,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_70",
          "label": "para",
          "text": "keyword arguments, 15",
          "level": -1,
          "page": 492,
          "reading_order": 70,
          "bbox": [
            349,
            573,
            468,
            586
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_71",
          "label": "para",
          "text": "Kleene closures, 100",
          "level": -1,
          "page": 492,
          "reading_order": 71,
          "bbox": [
            350,
            588,
            449,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_492_order_72",
      "label": "sec",
      "text": "L",
      "level": 1,
      "page": 492,
      "reading_order": 72,
      "bbox": [
        350,
        618,
        356,
        636
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_492_order_73",
          "label": "para",
          "text": "lambda expressions, 150, 386-390",
          "level": -1,
          "page": 492,
          "reading_order": 73,
          "bbox": [
            349,
            636,
            530,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_74",
          "label": "para",
          "text": "example, 152",
          "level": -1,
          "page": 492,
          "reading_order": 74,
          "bbox": [
            365,
            654,
            433,
            667
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_75",
          "label": "para",
          "text": "lambda operator (λ), 386",
          "level": -1,
          "page": 492,
          "reading_order": 75,
          "bbox": [
            349,
            669,
            476,
            682
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_76",
          "label": "para",
          "text": "Lancaster stemmer, 107",
          "level": -1,
          "page": 492,
          "reading_order": 76,
          "bbox": [
            350,
            682,
            469,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_77",
          "label": "para",
          "text": "language codes, 65",
          "level": -1,
          "page": 492,
          "reading_order": 77,
          "bbox": [
            349,
            698,
            444,
            711
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_78",
          "label": "para",
          "text": "language output, generating, 29",
          "level": -1,
          "page": 492,
          "reading_order": 78,
          "bbox": [
            349,
            713,
            512,
            726
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_79",
          "label": "para",
          "text": "language processing, symbol processing\nversus, 442",
          "level": -1,
          "page": 492,
          "reading_order": 79,
          "bbox": [
            349,
            726,
            557,
            752
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_80",
          "label": "para",
          "text": "language resource",
          "level": -1,
          "page": 492,
          "reading_order": 80,
          "bbox": [
            349,
            752,
            440,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_81",
          "label": "para",
          "text": "describing using OLAC metadata, 435-437",
          "level": -1,
          "page": 492,
          "reading_order": 81,
          "bbox": [
            368,
            770,
            584,
            784
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_82",
          "label": "para",
          "text": "LanguageLog (linguistics blog), 35",
          "level": -1,
          "page": 492,
          "reading_order": 82,
          "bbox": [
            350,
            786,
            522,
            799
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_492_order_83",
          "label": "foot",
          "text": "470 | General Inde",
          "level": -1,
          "page": 492,
          "reading_order": 83,
          "bbox": [
            97,
            824,
            182,
            838
          ],
          "section_number": "470",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_493_order_0",
          "label": "para",
          "text": "latent semantic analysis, 171\nLatin-2 character encoding, 94\nleaf nodes, 242\nleft-corner parser, 306\nleft-recursive, 302\nlemmas, 60\nlexical relationships between, 71\npairing of synset with a word, 68\nlemmatization, 107\nexample of, 108\nlength of a text, 7\nletter trie, 162\nlexical categories, 179\nlexical entry, 60\nlexical relations, 70\nlexical resources\ncomparative wordlists, 65\npronouncing dictionary, 63–65\nShoebox and Toolbox lexicons, 66\nwordlist corpora, 60–63\nlexicon, 60\n(see also lexical resources)\nchunking Toolbox lexicon, 434\ndefined, 60\nvalidating in Toolbox, 432–435\nLGB rule of name resolution, 145\nlicensed, 350\nlikelihood ratios, 224\nLinear-Chain Conditional Random Field\nModels, 233\nlinguistic objects, mappings from keys to\nvalues, 190\nlinguistic patterns, modeling, 255\nlinguistics and NLP-related concepts, resources\nfor, 34\nlist comprehensions, 24\nfor statement in, 63\nfunction invoked in, 64\nused as function parameters, 55\nlists, 10\nappending item to, 11\nconcatenating, using + operator, 11\nconverting to strings, 116\nindexing, 12–14\nindexing, dictionaries versus, 189\nnormalizing and sorting, 86\nPython list type, 86\nsorted, 14\nstrings versus, 92",
          "level": -1,
          "page": 493,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            333,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_493_order_1",
          "label": "para",
          "text": "tuples versus, 136\nlocal variables, 58\nlogic\nfirst-order, 372–385\nnatural language, semantics, and, 365–368\npropositional, 368–371\nresources for further reading, 404\nlogical constants, 372\nlogical form, 368\nlogical proofs, 370\nloops, 26\nlooping with conditions, 26\nlowercase, converting text to, 45, 107",
          "level": -1,
          "page": 493,
          "reading_order": 1,
          "bbox": [
            349,
            71,
            583,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_493_order_2",
      "label": "sec",
      "text": "M",
      "level": 1,
      "page": 493,
      "reading_order": 2,
      "bbox": [
        349,
        277,
        361,
        296
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_493_order_3",
          "label": "para",
          "text": "machine learning\napplication to NLP, web pages for\ngovernment challenges, 257\ndecision trees, 242–245\nMaximum Entropy classifiers, 251–254\nnaive Bayes classifiers, 246–250\npackages, 237\nresources for further reading, 257\nsupervised classification, 221–237\nmachine translation (MT)\nlimitations of, 30\nusing NLTK's babelizer, 30\nmapping, 189\nMatplotlib package, 168–170\nmaximal projection, 347\nMaximum Entropy classifiers, 251–254\nMaximum Entropy Markov Models, 233\nMaximum Entropy principle, 253\nmemoization, 167\nmeronyms, 70\nmetadata, 435\nOLAC (Open Language Archives\nCommunity), 435\nmodals, 186\nmodel building, 383\nmodel checking, 379\nmodels\ninterpretation of sentences of logical\nlanguage, 371\nof linguistic patterns, 255\nrepresentation using set theory, 367\ntruth-conditional semantics in first-order\nlogic, 377",
          "level": -1,
          "page": 493,
          "reading_order": 3,
          "bbox": [
            350,
            302,
            574,
            783
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_493_order_4",
          "label": "foot",
          "text": "General Index | 471",
          "level": -1,
          "page": 493,
          "reading_order": 4,
          "bbox": [
            494,
            824,
            584,
            838
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_494_order_0",
          "label": "para",
          "text": "what can be learned from models of\nlanguage, 255\nmodifiers, 314\nmodules\ndefined, 59\nmultimodule programs, 156\nstructure of Python module, 154\nmorphological analysis, 213\nmorphological cues to word category, 211\nmorphological tagging, 214\nmorphosyntactic information in tagsets, 212\nMSWord, text from, 85\nmutable, 93",
          "level": -1,
          "page": 494,
          "reading_order": 0,
          "bbox": [
            98,
            71,
            324,
            261
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_494_order_1",
      "label": "sec",
      "text": "N",
      "level": 1,
      "page": 494,
      "reading_order": 1,
      "bbox": [
        98,
        277,
        106,
        296
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_494_order_2",
          "label": "para",
          "text": "On newline character in regular expressions,\n111\nn-gram tagging, 203–208\nacross sentence boundaries, 208\ncombining taggers, 205\nn-gram tagger as generalization of unigran\ntagger, 203\nperformance limitations, 206\nseparating training and test data, 203\nstoring taggers, 206\nunigram tagging, 203\nunknown words, 206\nnaive Bayes assumption, 248\nnaive Bayes classifier, 246–250\ndeveloping for gender identification task,\n223\ndouble-counting problem, 250\nas generative classifier, 254\nnaivete of independence assumption, 249\nnon-binary features, 249\nunderlying probabilistic model, 248\nzero counts and smoothing, 248\nname resolution, LGB rule for, 145\nnamed arguments, 152\nnamed entities\ncommonly used types of, 281\nrelations between, 284\nnamed entity recognition (NER), 281–284\nNames Corpus, 61\nnegative lookahead assertion, 284\nNER (see named entity recognition)\nnested code blocks, 25\nNetworkX package, 170\nnew words in languages, 212",
          "level": -1,
          "page": 494,
          "reading_order": 2,
          "bbox": [
            97,
            302,
            326,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_494_order_3",
          "label": "para",
          "text": "newlines, 84\nmatching in regular expressions, 109\nprinting with print statement, 90\nresources for further information, 122\nnon-logical constants, 372\nnon-standard words, 108\nnormalizing text, 107–108\nlemmatization, 108\nusing stemmers, 107\nnoun phrase (NP), 297\nnoun phrase (NP) chunking, 264\nregular expression–based NP chunker, 267\nusing unigram tagger, 272\nnoun phrases, quantified, 390\nnouns\ncategorizing and tagging, 184\nprogram to find most frequent noun tags,\n187\nsyntactic agreement, 329\nnumerically intense algorithms in Python,\nincreasing efficiency of, 257\nNumPy package, 171",
          "level": -1,
          "page": 494,
          "reading_order": 3,
          "bbox": [
            349,
            71,
            583,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_494_order_4",
      "label": "sec",
      "text": "0",
      "level": 1,
      "page": 494,
      "reading_order": 4,
      "bbox": [
        349,
        412,
        358,
        430
      ],
      "section_number": "0",
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_494_order_5",
          "label": "para",
          "text": "object references, 130\ncopying, 132\nobjective function, 114\nobjects, finding data type for, 86\nOLAC metadata, 74, 435\ndefinition of metadata, 435\nOpen Language Archives Community, 435\nOpen Archives Initiative (OAI), 435\nopen class, 212\nopen formula, 374\nOpen Language Archives Community\n(OLAC), 435\noperators, 369\n(see also names of individual operators)\naddition and multiplication, 88\nBoolean, 368\nnumerical comparison, 22\nscope of, 157\nword comparison, 23\nor operator, 24\northography, 328\nout-of-vocabulary items, 206\noverfitting, 225, 245",
          "level": -1,
          "page": 494,
          "reading_order": 5,
          "bbox": [
            349,
            430,
            583,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_494_order_6",
          "label": "foot",
          "text": "472 | General Inde",
          "level": -1,
          "page": 494,
          "reading_order": 6,
          "bbox": [
            97,
            824,
            182,
            842
          ],
          "section_number": "472",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_1",
          "label": "para",
          "text": "packages, 59",
          "level": -1,
          "page": 495,
          "reading_order": 1,
          "bbox": [
            100,
            98,
            162,
            111
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_2",
          "label": "para",
          "text": "parameters, 57",
          "level": -1,
          "page": 495,
          "reading_order": 2,
          "bbox": [
            100,
            116,
            171,
            125
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_3",
          "label": "para",
          "text": "call-by-value parameter passing, 144\nchecking types of, 146",
          "level": -1,
          "page": 495,
          "reading_order": 3,
          "bbox": [
            113,
            125,
            297,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_5",
          "label": "para",
          "text": "defined, 9",
          "level": -1,
          "page": 495,
          "reading_order": 5,
          "bbox": [
            117,
            156,
            164,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_6",
          "label": "para",
          "text": "defining for functions, 14.",
          "level": -1,
          "page": 495,
          "reading_order": 6,
          "bbox": [
            117,
            170,
            243,
            184
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_7",
          "label": "para",
          "text": "parent nodes, 279",
          "level": -1,
          "page": 495,
          "reading_order": 7,
          "bbox": [
            100,
            186,
            189,
            197
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_8",
          "label": "para",
          "text": "parsing, 31",
          "level": -1,
          "page": 495,
          "reading_order": 8,
          "bbox": [
            100,
            197,
            153,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_9",
          "label": "para",
          "text": "see also grammars",
          "level": -1,
          "page": 495,
          "reading_order": 9,
          "bbox": [
            117,
            215,
            210,
            228
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_10",
          "label": "para",
          "text": "with context-free gramma",
          "level": -1,
          "page": 495,
          "reading_order": 10,
          "bbox": [
            117,
            230,
            244,
            243
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_11",
          "label": "para",
          "text": "left-corner parser, 306",
          "level": -1,
          "page": 495,
          "reading_order": 11,
          "bbox": [
            126,
            243,
            243,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_12",
          "label": "para",
          "text": "recursive descent parsing, 303",
          "level": -1,
          "page": 495,
          "reading_order": 12,
          "bbox": [
            126,
            259,
            281,
            272
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_13",
          "label": "para",
          "text": "shift-reduce parsing, 304",
          "level": -1,
          "page": 495,
          "reading_order": 13,
          "bbox": [
            126,
            272,
            256,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_14",
          "label": "para",
          "text": "well-formed substring tables, 307-310",
          "level": -1,
          "page": 495,
          "reading_order": 14,
          "bbox": [
            126,
            286,
            324,
            301
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_15",
          "label": "para",
          "text": "$$\n\\begin{array} { c } { { \\mathrm { E a r l e y ~ c h a r t ~ p a r s e r , ~ p a r s i n g ~ f e a t u r e - b a s e d } } } \\\\ { { \\mathrm { g r a m m a r s , ~ 3 3 4 } } } \\end{array}\n$$",
          "level": -1,
          "page": 495,
          "reading_order": 15,
          "bbox": [
            117,
            303,
            324,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_16",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 495,
          "reading_order": 16,
          "bbox": [
            117,
            331,
            173,
            342
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_17",
          "label": "para",
          "text": "objective dependency parser, 311",
          "level": -1,
          "page": 495,
          "reading_order": 17,
          "bbox": [
            124,
            347,
            288,
            360
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_18",
          "label": "para",
          "text": "part-of-speech tagging (see POS tagging",
          "level": -1,
          "page": 495,
          "reading_order": 18,
          "bbox": [
            100,
            360,
            298,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_19",
          "label": "para",
          "text": "partial information, 341",
          "level": -1,
          "page": 495,
          "reading_order": 19,
          "bbox": [
            100,
            376,
            217,
            386
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_20",
          "label": "para",
          "text": "parts of speech, 179",
          "level": -1,
          "page": 495,
          "reading_order": 20,
          "bbox": [
            100,
            390,
            198,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_21",
          "label": "para",
          "text": "$^{2}$",
          "level": -1,
          "page": 495,
          "reading_order": 21,
          "bbox": [
            100,
            403,
            154,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_23",
          "label": "para",
          "text": "personal pronouns, 186",
          "level": -1,
          "page": 495,
          "reading_order": 23,
          "bbox": [
            100,
            433,
            217,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_24",
          "label": "para",
          "text": "philosophical divides in contemporary NLP,\n444",
          "level": -1,
          "page": 495,
          "reading_order": 24,
          "bbox": [
            100,
            448,
            324,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_25",
          "label": "para",
          "text": ">honetics",
          "level": -1,
          "page": 495,
          "reading_order": 25,
          "bbox": [
            100,
            474,
            146,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_26",
          "label": "para",
          "text": "computer-readable phonetic alphabe\n(SAMPA), 137",
          "level": -1,
          "page": 495,
          "reading_order": 26,
          "bbox": [
            113,
            492,
            299,
            519
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_27",
          "label": "para",
          "text": "hones, 63",
          "level": -1,
          "page": 495,
          "reading_order": 27,
          "bbox": [
            117,
            519,
            171,
            537
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_28",
          "label": "para",
          "text": "esources for further information, 74",
          "level": -1,
          "page": 495,
          "reading_order": 28,
          "bbox": [
            118,
            537,
            298,
            548
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_29",
          "label": "para",
          "text": "phrasal level, 347",
          "level": -1,
          "page": 495,
          "reading_order": 29,
          "bbox": [
            100,
            548,
            185,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_30",
          "label": "para",
          "text": "347\npipeline for NLP, 31",
          "level": -1,
          "page": 495,
          "reading_order": 30,
          "bbox": [
            100,
            564,
            218,
            594
          ],
          "section_number": "347",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_32",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 495,
          "reading_order": 32,
          "bbox": [
            100,
            595,
            185,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_33",
          "label": "para",
          "text": "plotting functions, Matplotlib, 168",
          "level": -1,
          "page": 495,
          "reading_order": 33,
          "bbox": [
            100,
            609,
            273,
            623
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_34",
          "label": "para",
          "text": "2orter stemmer, 107",
          "level": -1,
          "page": 495,
          "reading_order": 34,
          "bbox": [
            100,
            626,
            199,
            636
          ],
          "section_number": "2",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_35",
          "label": "para",
          "text": "POS (part-of-speech) tagging, 179, 208, 229",
          "level": -1,
          "page": 495,
          "reading_order": 35,
          "bbox": [
            100,
            636,
            324,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_36",
          "label": "para",
          "text": "see also tagging)",
          "level": -1,
          "page": 495,
          "reading_order": 36,
          "bbox": [
            117,
            654,
            199,
            667
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_37",
          "label": "para",
          "text": "lifferences in POS tagsets, 213",
          "level": -1,
          "page": 495,
          "reading_order": 37,
          "bbox": [
            118,
            668,
            270,
            681
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_38",
          "label": "para",
          "text": "examining word context, 230",
          "level": -1,
          "page": 495,
          "reading_order": 38,
          "bbox": [
            118,
            681,
            262,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_39",
          "label": "para",
          "text": "inding IOB chunk tag for word's POS tag\n272",
          "level": -1,
          "page": 495,
          "reading_order": 39,
          "bbox": [
            118,
            698,
            324,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_40",
          "label": "para",
          "text": "n information retrieval, 263",
          "level": -1,
          "page": 495,
          "reading_order": 40,
          "bbox": [
            117,
            725,
            252,
            738
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_41",
          "label": "para",
          "text": "morphology in POS tagsets, 212",
          "level": -1,
          "page": 495,
          "reading_order": 41,
          "bbox": [
            117,
            742,
            275,
            755
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_42",
          "label": "para",
          "text": "esources for further reading, 214",
          "level": -1,
          "page": 495,
          "reading_order": 42,
          "bbox": [
            118,
            755,
            282,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_43",
          "label": "para",
          "text": "implified tagset, 18",
          "level": -1,
          "page": 495,
          "reading_order": 43,
          "bbox": [
            118,
            770,
            216,
            784
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_44",
          "label": "para",
          "text": "toring POS tags in tagged corpora, 181",
          "level": -1,
          "page": 495,
          "reading_order": 44,
          "bbox": [
            118,
            785,
            315,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_45",
          "label": "para",
          "text": "tagged data from four Indian languages,",
          "level": -1,
          "page": 495,
          "reading_order": 45,
          "bbox": [
            359,
            71,
            566,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_46",
          "label": "para",
          "text": "182",
          "level": -1,
          "page": 495,
          "reading_order": 46,
          "bbox": [
            413,
            89,
            433,
            99
          ],
          "section_number": "182",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_47",
          "label": "para",
          "text": "unsimplified tags, 18",
          "level": -1,
          "page": 495,
          "reading_order": 47,
          "bbox": [
            359,
            104,
            467,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_48",
          "label": "para",
          "text": "use in noun phrase chunking, 265\nusing consecutive classifier, 231",
          "level": -1,
          "page": 495,
          "reading_order": 48,
          "bbox": [
            359,
            117,
            539,
            146
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_50",
          "label": "para",
          "text": "pre-sorting, 160",
          "level": -1,
          "page": 495,
          "reading_order": 50,
          "bbox": [
            349,
            149,
            431,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_51",
          "label": "para",
          "text": "precision, evaluating search tasks for, 239\nprecision/recall trade-off in information",
          "level": -1,
          "page": 495,
          "reading_order": 51,
          "bbox": [
            349,
            161,
            560,
            190
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_52",
          "label": "para",
          "text": "这是一篇與中國人物相關的小作品。 你可以编辑或修订扩充其内容。",
          "level": -1,
          "page": 495,
          "reading_order": 52,
          "bbox": [
            349,
            190,
            549,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_53",
          "label": "para",
          "text": "predicates (first-order logic), 372",
          "level": -1,
          "page": 495,
          "reading_order": 53,
          "bbox": [
            349,
            206,
            514,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_54",
          "label": "para",
          "text": "prepositional phrase (PP), 297",
          "level": -1,
          "page": 495,
          "reading_order": 54,
          "bbox": [
            349,
            221,
            503,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_55",
          "label": "para",
          "text": "prepositional phrase attachment ambiguity\n300",
          "level": -1,
          "page": 495,
          "reading_order": 55,
          "bbox": [
            349,
            234,
            566,
            260
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_56",
          "label": "para",
          "text": "Prepositional Phrase Attachment Corpus, 316",
          "level": -1,
          "page": 495,
          "reading_order": 56,
          "bbox": [
            350,
            260,
            583,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_57",
          "label": "para",
          "text": "prepositions, 186",
          "level": -1,
          "page": 495,
          "reading_order": 57,
          "bbox": [
            349,
            278,
            440,
            292
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_58",
          "label": "para",
          "text": "present participles, 211",
          "level": -1,
          "page": 495,
          "reading_order": 58,
          "bbox": [
            349,
            294,
            467,
            307
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_59",
          "label": "para",
          "text": "Principle of Compositionality, 385, 443",
          "level": -1,
          "page": 495,
          "reading_order": 59,
          "bbox": [
            349,
            307,
            548,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_60",
          "label": "para",
          "text": "print statements, 89",
          "level": -1,
          "page": 495,
          "reading_order": 60,
          "bbox": [
            349,
            322,
            450,
            336
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_61",
          "label": "para",
          "text": "newline at end, 90",
          "level": -1,
          "page": 495,
          "reading_order": 61,
          "bbox": [
            359,
            338,
            458,
            349
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_62",
          "label": "para",
          "text": "string formats and, 117",
          "level": -1,
          "page": 495,
          "reading_order": 62,
          "bbox": [
            359,
            349,
            485,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_63",
          "label": "para",
          "text": "prior probability, 246",
          "level": -1,
          "page": 495,
          "reading_order": 63,
          "bbox": [
            349,
            367,
            458,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_64",
          "label": "para",
          "text": "probabilistic context-free grammar (PCFG)\n320",
          "level": -1,
          "page": 495,
          "reading_order": 64,
          "bbox": [
            349,
            382,
            566,
            406
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_65",
          "label": "para",
          "text": "probabilistic model, naive Bayes classifier, 248",
          "level": -1,
          "page": 495,
          "reading_order": 65,
          "bbox": [
            349,
            411,
            583,
            424
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_66",
          "label": "para",
          "text": "probabilistic parsing, 318",
          "level": -1,
          "page": 495,
          "reading_order": 66,
          "bbox": [
            349,
            426,
            477,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_67",
          "label": "para",
          "text": "procedural style, 139",
          "level": -1,
          "page": 495,
          "reading_order": 67,
          "bbox": [
            349,
            439,
            458,
            453
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_68",
          "label": "para",
          "text": "processing pipeline (NLP), 86",
          "level": -1,
          "page": 495,
          "reading_order": 68,
          "bbox": [
            349,
            455,
            503,
            468
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_69",
          "label": "para",
          "text": "productions in grammars, 293",
          "level": -1,
          "page": 495,
          "reading_order": 69,
          "bbox": [
            349,
            468,
            503,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_70",
          "label": "para",
          "text": "rules for writing CFGs for parsing in\nNLTK, 301",
          "level": -1,
          "page": 495,
          "reading_order": 70,
          "bbox": [
            359,
            483,
            548,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_71",
          "label": "para",
          "text": "program development, 154-160",
          "level": -1,
          "page": 495,
          "reading_order": 71,
          "bbox": [
            349,
            510,
            512,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_72",
          "label": "para",
          "text": "debugging techniques, 158",
          "level": -1,
          "page": 495,
          "reading_order": 72,
          "bbox": [
            359,
            528,
            503,
            541
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_73",
          "label": "para",
          "text": "defensive programming, 159",
          "level": -1,
          "page": 495,
          "reading_order": 73,
          "bbox": [
            359,
            543,
            512,
            556
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_74",
          "label": "para",
          "text": "multimodule programs, 156\nPython module structure, 15",
          "level": -1,
          "page": 495,
          "reading_order": 74,
          "bbox": [
            359,
            556,
            512,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_76",
          "label": "para",
          "text": "sources of error, 156",
          "level": -1,
          "page": 495,
          "reading_order": 76,
          "bbox": [
            359,
            583,
            469,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_77",
          "label": "para",
          "text": "programming style, 139",
          "level": -1,
          "page": 495,
          "reading_order": 77,
          "bbox": [
            349,
            600,
            469,
            614
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_78",
          "label": "para",
          "text": "programs, writing, 129-177",
          "level": -1,
          "page": 495,
          "reading_order": 78,
          "bbox": [
            349,
            617,
            494,
            629
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_79",
          "label": "para",
          "text": "advanced features of functions, 149-154",
          "level": -1,
          "page": 495,
          "reading_order": 79,
          "bbox": [
            359,
            629,
            574,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_80",
          "label": "para",
          "text": "algorithm design, 160-167",
          "level": -1,
          "page": 495,
          "reading_order": 80,
          "bbox": [
            359,
            645,
            503,
            658
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_81",
          "label": "para",
          "text": "assignment, 130",
          "level": -1,
          "page": 495,
          "reading_order": 81,
          "bbox": [
            359,
            661,
            449,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_82",
          "label": "para",
          "text": "conditionals, 133",
          "level": -1,
          "page": 495,
          "reading_order": 82,
          "bbox": [
            359,
            673,
            458,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_83",
          "label": "para",
          "text": "equality, 132",
          "level": -1,
          "page": 495,
          "reading_order": 83,
          "bbox": [
            359,
            689,
            431,
            702
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_84",
          "label": "para",
          "text": "functions, 142-149",
          "level": -1,
          "page": 495,
          "reading_order": 84,
          "bbox": [
            359,
            704,
            467,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_85",
          "label": "para",
          "text": "resources for further reading, 173",
          "level": -1,
          "page": 495,
          "reading_order": 85,
          "bbox": [
            359,
            716,
            530,
            731
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_86",
          "label": "para",
          "text": "sequences, 133–138",
          "level": -1,
          "page": 495,
          "reading_order": 86,
          "bbox": [
            359,
            734,
            467,
            743
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_87",
          "label": "para",
          "text": "style considerations, 138-142",
          "level": -1,
          "page": 495,
          "reading_order": 87,
          "bbox": [
            359,
            743,
            548,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_88",
          "label": "para",
          "text": "egitimate uses for counters, 141",
          "level": -1,
          "page": 495,
          "reading_order": 88,
          "bbox": [
            385,
            761,
            548,
            775
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_89",
          "label": "para",
          "text": "procedural versus declarative style, 139",
          "level": -1,
          "page": 495,
          "reading_order": 89,
          "bbox": [
            385,
            777,
            583,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_495_order_90",
          "label": "foot",
          "text": "General Index | 473",
          "level": -1,
          "page": 495,
          "reading_order": 90,
          "bbox": [
            494,
            824,
            584,
            838
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_0",
          "label": "para",
          "text": "Python coding style, 138",
          "level": -1,
          "page": 496,
          "reading_order": 0,
          "bbox": [
            118,
            71,
            254,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_1",
          "label": "para",
          "text": "summary of important points, 172\nusing Python libraries, 167–172",
          "level": -1,
          "page": 496,
          "reading_order": 1,
          "bbox": [
            113,
            89,
            288,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_2",
          "label": "para",
          "text": "Project Gutenberg, 80",
          "level": -1,
          "page": 496,
          "reading_order": 2,
          "bbox": [
            100,
            117,
            208,
            134
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_3",
          "label": "para",
          "text": "projections, 347",
          "level": -1,
          "page": 496,
          "reading_order": 3,
          "bbox": [
            100,
            134,
            180,
            146
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_4",
          "label": "para",
          "text": "projective, 311",
          "level": -1,
          "page": 496,
          "reading_order": 4,
          "bbox": [
            100,
            146,
            171,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_5",
          "label": "para",
          "text": "pronouncing dictionary, 63-65",
          "level": -1,
          "page": 496,
          "reading_order": 5,
          "bbox": [
            100,
            161,
            254,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_6",
          "label": "para",
          "text": "pronouns",
          "level": -1,
          "page": 496,
          "reading_order": 6,
          "bbox": [
            100,
            179,
            146,
            188
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_7",
          "label": "para",
          "text": "anaphoric antecedents, 397",
          "level": -1,
          "page": 496,
          "reading_order": 7,
          "bbox": [
            116,
            188,
            252,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_8",
          "label": "para",
          "text": "interpreting in first-order logic, 373",
          "level": -1,
          "page": 496,
          "reading_order": 8,
          "bbox": [
            109,
            206,
            297,
            219
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_9",
          "label": "para",
          "text": "esolving in discourse processing, 40",
          "level": -1,
          "page": 496,
          "reading_order": 9,
          "bbox": [
            118,
            221,
            298,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_10",
          "label": "para",
          "text": "proof goal, 376",
          "level": -1,
          "page": 496,
          "reading_order": 10,
          "bbox": [
            100,
            234,
            174,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_11",
          "label": "para",
          "text": "properties of linguistic categories, 331",
          "level": -1,
          "page": 496,
          "reading_order": 11,
          "bbox": [
            100,
            250,
            288,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_12",
          "label": "para",
          "text": "propositional logic, 368-371",
          "level": -1,
          "page": 496,
          "reading_order": 12,
          "bbox": [
            100,
            263,
            243,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_13",
          "label": "para",
          "text": "Boolean operators, 368\npositional symbols, 368",
          "level": -1,
          "page": 496,
          "reading_order": 13,
          "bbox": [
            114,
            278,
            234,
            307
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_15",
          "label": "para",
          "text": "pruning decision nodes, 245",
          "level": -1,
          "page": 496,
          "reading_order": 15,
          "bbox": [
            100,
            308,
            243,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_16",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 496,
          "reading_order": 16,
          "bbox": [
            100,
            322,
            252,
            348
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_18",
          "label": "para",
          "text": "carriage return and linefeed characters, 80",
          "level": -1,
          "page": 496,
          "reading_order": 18,
          "bbox": [
            113,
            349,
            325,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_19",
          "label": "para",
          "text": "codecs module, 95",
          "level": -1,
          "page": 496,
          "reading_order": 19,
          "bbox": [
            113,
            367,
            207,
            378
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_20",
          "label": "para",
          "text": "lictionary data structure, 65",
          "level": -1,
          "page": 496,
          "reading_order": 20,
          "bbox": [
            118,
            382,
            256,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_21",
          "label": "para",
          "text": "dictionary methods, summary of, 19\ndocumentation, 173",
          "level": -1,
          "page": 496,
          "reading_order": 21,
          "bbox": [
            117,
            395,
            297,
            422
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_23",
          "label": "para",
          "text": "documentation and information resources\n34",
          "level": -1,
          "page": 496,
          "reading_order": 23,
          "bbox": [
            117,
            423,
            326,
            450
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_24",
          "label": "para",
          "text": "ElementTree module, 427",
          "level": -1,
          "page": 496,
          "reading_order": 24,
          "bbox": [
            117,
            455,
            244,
            466
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_25",
          "label": "para",
          "text": "errors in understanding semantics of, 157",
          "level": -1,
          "page": 496,
          "reading_order": 25,
          "bbox": [
            109,
            466,
            324,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_26",
          "label": "para",
          "text": "anding type of any object, 86\ngetting started, 2",
          "level": -1,
          "page": 496,
          "reading_order": 26,
          "bbox": [
            118,
            483,
            261,
            512
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_28",
          "label": "para",
          "text": "ncreasing efficiency of numerically intens\nalgorithms, 257",
          "level": -1,
          "page": 496,
          "reading_order": 28,
          "bbox": [
            117,
            513,
            324,
            541
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_29",
          "label": "para",
          "text": "libraries, 167-172",
          "level": -1,
          "page": 496,
          "reading_order": 29,
          "bbox": [
            109,
            543,
            207,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_30",
          "label": "para",
          "text": "CSV,170",
          "level": -1,
          "page": 496,
          "reading_order": 30,
          "bbox": [
            126,
            555,
            180,
            567
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_31",
          "label": "para",
          "text": "Matplotlib, 168-170",
          "level": -1,
          "page": 496,
          "reading_order": 31,
          "bbox": [
            126,
            572,
            234,
            585
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_32",
          "label": "para",
          "text": "NetworkX, 170",
          "level": -1,
          "page": 496,
          "reading_order": 32,
          "bbox": [
            126,
            585,
            208,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_33",
          "label": "para",
          "text": "NumPy, 1\nother, 172",
          "level": -1,
          "page": 496,
          "reading_order": 33,
          "bbox": [
            126,
            600,
            180,
            626
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_35",
          "label": "para",
          "text": "reference materials, 122",
          "level": -1,
          "page": 496,
          "reading_order": 35,
          "bbox": [
            109,
            627,
            234,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_36",
          "label": "para",
          "text": "style guide for Python code, 138",
          "level": -1,
          "page": 496,
          "reading_order": 36,
          "bbox": [
            109,
            645,
            275,
            658
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_37",
          "label": "para",
          "text": "extwrap module, 120",
          "level": -1,
          "page": 496,
          "reading_order": 37,
          "bbox": [
            117,
            660,
            225,
            673
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_38",
          "label": "para",
          "text": "Python Package Index, 172",
          "level": -1,
          "page": 496,
          "reading_order": 38,
          "bbox": [
            100,
            673,
            234,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_39",
          "label": "para",
          "text": "(",
          "level": -1,
          "page": 496,
          "reading_order": 39,
          "bbox": [
            97,
            706,
            101,
            720
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_40",
          "label": "para",
          "text": "quality control in corpus creation, 413",
          "level": -1,
          "page": 496,
          "reading_order": 40,
          "bbox": [
            100,
            725,
            297,
            739
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_41",
          "label": "para",
          "text": "quantification",
          "level": -1,
          "page": 496,
          "reading_order": 41,
          "bbox": [
            97,
            741,
            171,
            754
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_42",
          "label": "para",
          "text": "irst-order logic, 373, 380",
          "level": -1,
          "page": 496,
          "reading_order": 42,
          "bbox": [
            118,
            754,
            243,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_43",
          "label": "para",
          "text": "quantified noun phrases, 390",
          "level": -1,
          "page": 496,
          "reading_order": 43,
          "bbox": [
            109,
            770,
            261,
            783
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_44",
          "label": "para",
          "text": "scope ambiguity, 381, 394-397",
          "level": -1,
          "page": 496,
          "reading_order": 44,
          "bbox": [
            109,
            785,
            270,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_45",
          "label": "para",
          "text": "quantified formulas, interpretation of, 38\nquestions, answering, 29",
          "level": -1,
          "page": 496,
          "reading_order": 45,
          "bbox": [
            349,
            71,
            557,
            102
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_46",
          "label": "para",
          "text": "quotation marks in strings, 8",
          "level": -1,
          "page": 496,
          "reading_order": 46,
          "bbox": [
            349,
            102,
            494,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_47",
          "label": "para",
          "text": "R",
          "level": -1,
          "page": 496,
          "reading_order": 47,
          "bbox": [
            350,
            134,
            358,
            152
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_48",
          "label": "para",
          "text": "random text",
          "level": -1,
          "page": 496,
          "reading_order": 48,
          "bbox": [
            350,
            152,
            413,
            170
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_49",
          "label": "para",
          "text": "generating in various styles, 6",
          "level": -1,
          "page": 496,
          "reading_order": 49,
          "bbox": [
            359,
            170,
            514,
            183
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_50",
          "label": "para",
          "text": "generating using bigrams, 55",
          "level": -1,
          "page": 496,
          "reading_order": 50,
          "bbox": [
            359,
            185,
            512,
            198
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_51",
          "label": "para",
          "text": "raster (pixel) images, 169",
          "level": -1,
          "page": 496,
          "reading_order": 51,
          "bbox": [
            350,
            198,
            476,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_52",
          "label": "para",
          "text": "raw strings, 101",
          "level": -1,
          "page": 496,
          "reading_order": 52,
          "bbox": [
            350,
            215,
            431,
            227
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_53",
          "label": "para",
          "text": "raw text, processing, 79–128",
          "level": -1,
          "page": 496,
          "reading_order": 53,
          "bbox": [
            350,
            227,
            494,
            242
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_54",
          "label": "para",
          "text": "capturing user input, 85\ndetecting word patterns",
          "level": -1,
          "page": 496,
          "reading_order": 54,
          "bbox": [
            359,
            242,
            486,
            271
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_55",
          "label": "para",
          "text": "这是一篇與中國人物相關的小作品。 你可以编辑或修订扩充其内容。",
          "level": -1,
          "page": 496,
          "reading_order": 55,
          "bbox": [
            359,
            271,
            557,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_56",
          "label": "para",
          "text": "formatting from lists to strings, 116-121",
          "level": -1,
          "page": 496,
          "reading_order": 56,
          "bbox": [
            359,
            286,
            567,
            300
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_57",
          "label": "para",
          "text": "HTML documents, 82",
          "level": -1,
          "page": 496,
          "reading_order": 57,
          "bbox": [
            359,
            302,
            478,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_58",
          "label": "para",
          "text": "NLP pipeline, 86",
          "level": -1,
          "page": 496,
          "reading_order": 58,
          "bbox": [
            359,
            313,
            451,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_59",
          "label": "para",
          "text": "normalizing text, 107-108",
          "level": -1,
          "page": 496,
          "reading_order": 59,
          "bbox": [
            359,
            331,
            503,
            344
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_60",
          "label": "para",
          "text": "reading local files, 84",
          "level": -1,
          "page": 496,
          "reading_order": 60,
          "bbox": [
            359,
            344,
            472,
            359
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_61",
          "label": "para",
          "text": "regular expressions for tokenizing text, 109–\n112",
          "level": -1,
          "page": 496,
          "reading_order": 61,
          "bbox": [
            359,
            359,
            583,
            385
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_62",
          "label": "para",
          "text": "resources for further reading, 122",
          "level": -1,
          "page": 496,
          "reading_order": 62,
          "bbox": [
            359,
            385,
            534,
            403
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_63",
          "label": "para",
          "text": "RSS feeds, 83",
          "level": -1,
          "page": 496,
          "reading_order": 63,
          "bbox": [
            359,
            403,
            432,
            415
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_64",
          "label": "para",
          "text": "search engine results, 82",
          "level": -1,
          "page": 496,
          "reading_order": 64,
          "bbox": [
            359,
            419,
            488,
            432
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_65",
          "label": "para",
          "text": "segmentation, 112-116",
          "level": -1,
          "page": 496,
          "reading_order": 65,
          "bbox": [
            359,
            432,
            485,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_66",
          "label": "para",
          "text": "strings, lowest level text processing, 87-93",
          "level": -1,
          "page": 496,
          "reading_order": 66,
          "bbox": [
            359,
            448,
            583,
            461
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_67",
          "label": "para",
          "text": "summary of important points, 121",
          "level": -1,
          "page": 496,
          "reading_order": 67,
          "bbox": [
            359,
            463,
            539,
            476
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_68",
          "label": "para",
          "text": "text from web and from disk, 80",
          "level": -1,
          "page": 496,
          "reading_order": 68,
          "bbox": [
            359,
            476,
            530,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_69",
          "label": "para",
          "text": "text in binary formats, 85",
          "level": -1,
          "page": 496,
          "reading_order": 69,
          "bbox": [
            359,
            492,
            494,
            505
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_70",
          "label": "para",
          "text": "useful applications of regular expressions\n102–106",
          "level": -1,
          "page": 496,
          "reading_order": 70,
          "bbox": [
            359,
            507,
            574,
            531
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_71",
          "label": "para",
          "text": "using Unicode, 93-97",
          "level": -1,
          "page": 496,
          "reading_order": 71,
          "bbox": [
            359,
            536,
            476,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_72",
          "label": "para",
          "text": "raw() function, 41",
          "level": -1,
          "page": 496,
          "reading_order": 72,
          "bbox": [
            350,
            549,
            442,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_73",
          "label": "para",
          "text": "re module, 101, 110",
          "level": -1,
          "page": 496,
          "reading_order": 73,
          "bbox": [
            350,
            564,
            451,
            575
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_74",
          "label": "para",
          "text": "recall, evaluating search tasks for, 240",
          "level": -1,
          "page": 496,
          "reading_order": 74,
          "bbox": [
            350,
            580,
            541,
            593
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_75",
          "label": "para",
          "text": "Recognizing Textual Entailment (RTE), 32,\n235",
          "level": -1,
          "page": 496,
          "reading_order": 75,
          "bbox": [
            350,
            593,
            568,
            619
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_76",
          "label": "para",
          "text": "exploiting word context, 230",
          "level": -1,
          "page": 496,
          "reading_order": 76,
          "bbox": [
            359,
            624,
            512,
            637
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_77",
          "label": "para",
          "text": "records, 136",
          "level": -1,
          "page": 496,
          "reading_order": 77,
          "bbox": [
            350,
            637,
            413,
            648
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_78",
          "label": "para",
          "text": "recursion, 161",
          "level": -1,
          "page": 496,
          "reading_order": 78,
          "bbox": [
            350,
            654,
            422,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_79",
          "label": "para",
          "text": "function to compute Sanskrit meter\n(example), 165",
          "level": -1,
          "page": 496,
          "reading_order": 79,
          "bbox": [
            359,
            663,
            548,
            695
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_80",
          "label": "para",
          "text": "in linguistic structure, 278-281",
          "level": -1,
          "page": 496,
          "reading_order": 80,
          "bbox": [
            359,
            697,
            521,
            710
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_81",
          "label": "para",
          "text": "1980年",
          "level": -1,
          "page": 496,
          "reading_order": 81,
          "bbox": [
            386,
            710,
            468,
            725
          ],
          "section_number": "1980",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_82",
          "label": "para",
          "text": "rees, 279-280",
          "level": -1,
          "page": 496,
          "reading_order": 82,
          "bbox": [
            386,
            725,
            458,
            736
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_83",
          "label": "para",
          "text": "performance and, 163",
          "level": -1,
          "page": 496,
          "reading_order": 83,
          "bbox": [
            359,
            741,
            476,
            754
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_84",
          "label": "para",
          "text": "in syntactic structure, 301",
          "level": -1,
          "page": 496,
          "reading_order": 84,
          "bbox": [
            359,
            754,
            495,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_85",
          "label": "para",
          "text": "recursive, 30",
          "level": -1,
          "page": 496,
          "reading_order": 85,
          "bbox": [
            350,
            770,
            413,
            780
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_86",
          "label": "para",
          "text": "recursive descent parsing, 303",
          "level": -1,
          "page": 496,
          "reading_order": 86,
          "bbox": [
            350,
            785,
            503,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_496_order_87",
          "label": "foot",
          "text": "474 | General Inde",
          "level": -1,
          "page": 496,
          "reading_order": 87,
          "bbox": [
            97,
            824,
            182,
            838
          ],
          "section_number": "474",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_0",
          "label": "para",
          "text": "reentrancy, 340",
          "level": -1,
          "page": 497,
          "reading_order": 0,
          "bbox": [
            91,
            71,
            180,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_1",
          "label": "para",
          "text": "References\nregression testing framework, 16",
          "level": -1,
          "page": 497,
          "reading_order": 1,
          "bbox": [
            100,
            89,
            261,
            117
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_3",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 497,
          "reading_order": 3,
          "bbox": [
            100,
            118,
            243,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_5",
          "label": "para",
          "text": "chunker based on, evaluating, 272",
          "level": -1,
          "page": 497,
          "reading_order": 5,
          "bbox": [
            109,
            146,
            288,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_6",
          "label": "para",
          "text": "extracting word pieces, 102\nfinding word stems, 104",
          "level": -1,
          "page": 497,
          "reading_order": 6,
          "bbox": [
            109,
            161,
            252,
            190
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_8",
          "label": "para",
          "text": "matching initial and final vowel sequence:\nand all consonants, 102",
          "level": -1,
          "page": 497,
          "reading_order": 8,
          "bbox": [
            109,
            191,
            324,
            217
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_9",
          "label": "para",
          "text": "metacharacters, 10",
          "level": -1,
          "page": 497,
          "reading_order": 9,
          "bbox": [
            109,
            221,
            209,
            232
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_10",
          "label": "para",
          "text": "metacharacters, summary of, 101",
          "level": -1,
          "page": 497,
          "reading_order": 10,
          "bbox": [
            109,
            232,
            280,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_11",
          "label": "para",
          "text": "noun phrase (NP) chunker based on, 265",
          "level": -1,
          "page": 497,
          "reading_order": 11,
          "bbox": [
            109,
            250,
            324,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_12",
          "label": "para",
          "text": "ranges and closures, 99",
          "level": -1,
          "page": 497,
          "reading_order": 12,
          "bbox": [
            109,
            263,
            234,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_13",
          "label": "para",
          "text": "resources for further information, 12",
          "level": -1,
          "page": 497,
          "reading_order": 13,
          "bbox": [
            109,
            278,
            297,
            290
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_14",
          "label": "para",
          "text": "searching tokenized text, 105",
          "level": -1,
          "page": 497,
          "reading_order": 14,
          "bbox": [
            109,
            294,
            261,
            307
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_15",
          "label": "para",
          "text": "symbols, 110",
          "level": -1,
          "page": 497,
          "reading_order": 15,
          "bbox": [
            109,
            307,
            180,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_16",
          "label": "para",
          "text": "tagger, 199",
          "level": -1,
          "page": 497,
          "reading_order": 16,
          "bbox": [
            109,
            322,
            171,
            336
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_17",
          "label": "para",
          "text": "tokenizing text, 109-112",
          "level": -1,
          "page": 497,
          "reading_order": 17,
          "bbox": [
            109,
            338,
            243,
            351
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_18",
          "label": "para",
          "text": "use in PlaintextCorpusReader, 51",
          "level": -1,
          "page": 497,
          "reading_order": 18,
          "bbox": [
            109,
            351,
            281,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_19",
          "label": "para",
          "text": "using basic metacharacters, 98",
          "level": -1,
          "page": 497,
          "reading_order": 19,
          "bbox": [
            109,
            367,
            270,
            380
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_20",
          "label": "para",
          "text": "using for relation extraction, 284",
          "level": -1,
          "page": 497,
          "reading_order": 20,
          "bbox": [
            109,
            380,
            279,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_21",
          "label": "para",
          "text": "using with conditional frequency\ndistributions, 103",
          "level": -1,
          "page": 497,
          "reading_order": 21,
          "bbox": [
            109,
            395,
            280,
            422
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_22",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 497,
          "reading_order": 22,
          "bbox": [
            100,
            422,
            207,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_23",
          "label": "para",
          "text": "relation extraction, 28\nrelational operators, 2",
          "level": -1,
          "page": 497,
          "reading_order": 23,
          "bbox": [
            100,
            439,
            209,
            468
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_25",
          "label": "para",
          "text": "reserved words, 15",
          "level": -1,
          "page": 497,
          "reading_order": 25,
          "bbox": [
            100,
            469,
            192,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_26",
          "label": "para",
          "text": "return statements, 144",
          "level": -1,
          "page": 497,
          "reading_order": 26,
          "bbox": [
            100,
            483,
            211,
            495
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_27",
          "label": "para",
          "text": "return value, 57",
          "level": -1,
          "page": 497,
          "reading_order": 27,
          "bbox": [
            100,
            499,
            180,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_28",
          "label": "para",
          "text": "-eusing code, 56-59",
          "level": -1,
          "page": 497,
          "reading_order": 28,
          "bbox": [
            100,
            510,
            199,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_29",
          "label": "para",
          "text": "creating programs using a text editor, 56",
          "level": -1,
          "page": 497,
          "reading_order": 29,
          "bbox": [
            109,
            528,
            318,
            541
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_30",
          "label": "para",
          "text": "functions, 57",
          "level": -1,
          "page": 497,
          "reading_order": 30,
          "bbox": [
            109,
            541,
            180,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_31",
          "label": "para",
          "text": "modules, 59",
          "level": -1,
          "page": 497,
          "reading_order": 31,
          "bbox": [
            109,
            555,
            175,
            568
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_32",
          "label": "para",
          "text": "Reuters Corpus, 44",
          "level": -1,
          "page": 497,
          "reading_order": 32,
          "bbox": [
            100,
            573,
            198,
            585
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_33",
          "label": "para",
          "text": "foot element (XML), 427",
          "level": -1,
          "page": 497,
          "reading_order": 33,
          "bbox": [
            100,
            585,
            225,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_34",
          "label": "para",
          "text": "root hypernyms, 70",
          "level": -1,
          "page": 497,
          "reading_order": 34,
          "bbox": [
            100,
            600,
            198,
            614
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_35",
          "label": "para",
          "text": "root node, 242",
          "level": -1,
          "page": 497,
          "reading_order": 35,
          "bbox": [
            100,
            616,
            171,
            627
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_36",
          "label": "para",
          "text": "foot synsets, 69",
          "level": -1,
          "page": 497,
          "reading_order": 36,
          "bbox": [
            100,
            627,
            180,
            645
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_37",
          "label": "para",
          "text": "Rotokas language, 66",
          "level": -1,
          "page": 497,
          "reading_order": 37,
          "bbox": [
            100,
            645,
            207,
            658
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_38",
          "label": "para",
          "text": "extracting all consonant-vowel sequences\nfrom words, 103",
          "level": -1,
          "page": 497,
          "reading_order": 38,
          "bbox": [
            109,
            660,
            324,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_39",
          "label": "para",
          "text": "Toolbox file containing lexicon, 429",
          "level": -1,
          "page": 497,
          "reading_order": 39,
          "bbox": [
            109,
            689,
            297,
            702
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_40",
          "label": "para",
          "text": "RSS feeds, 83",
          "level": -1,
          "page": 497,
          "reading_order": 40,
          "bbox": [
            100,
            704,
            164,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_41",
          "label": "para",
          "text": "feedparser library, 172",
          "level": -1,
          "page": 497,
          "reading_order": 41,
          "bbox": [
            109,
            716,
            227,
            734
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_42",
          "label": "para",
          "text": "RTE (Recognizing Textual Entailment), 32,\n235",
          "level": -1,
          "page": 497,
          "reading_order": 42,
          "bbox": [
            100,
            734,
            316,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_43",
          "label": "para",
          "text": "exploiting word context, 230",
          "level": -1,
          "page": 497,
          "reading_order": 43,
          "bbox": [
            109,
            761,
            261,
            775
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_44",
          "label": "para",
          "text": "-untime errors, 13",
          "level": -1,
          "page": 497,
          "reading_order": 44,
          "bbox": [
            100,
            778,
            189,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_46",
          "label": "para",
          "text": "\\s whitespace characters in regular\nexpressions, 111",
          "level": -1,
          "page": 497,
          "reading_order": 46,
          "bbox": [
            350,
            98,
            521,
            126
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_47",
          "label": "para",
          "text": "\\$ nonwhitespace characters in regular\nexpressions, 111",
          "level": -1,
          "page": 497,
          "reading_order": 47,
          "bbox": [
            350,
            126,
            548,
            155
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_48",
          "label": "para",
          "text": "SAMPA computer-readable phonetic alphabet,\n137",
          "level": -1,
          "page": 497,
          "reading_order": 48,
          "bbox": [
            349,
            155,
            584,
            181
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_49",
          "label": "para",
          "text": "Sanskrit meter, computing, 165",
          "level": -1,
          "page": 497,
          "reading_order": 49,
          "bbox": [
            349,
            186,
            508,
            199
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_50",
          "label": "para",
          "text": "satisfies, 379",
          "level": -1,
          "page": 497,
          "reading_order": 50,
          "bbox": [
            349,
            199,
            413,
            215
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_51",
          "label": "para",
          "text": "scope of quantifiers, 38",
          "level": -1,
          "page": 497,
          "reading_order": 51,
          "bbox": [
            349,
            215,
            467,
            228
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_52",
          "label": "para",
          "text": "scope of variables, 145",
          "level": -1,
          "page": 497,
          "reading_order": 52,
          "bbox": [
            349,
            230,
            467,
            243
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_53",
          "label": "para",
          "text": "searche",
          "level": -1,
          "page": 497,
          "reading_order": 53,
          "bbox": [
            349,
            243,
            386,
            259
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_54",
          "label": "para",
          "text": "binary search, 160",
          "level": -1,
          "page": 497,
          "reading_order": 54,
          "bbox": [
            359,
            259,
            458,
            270
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_55",
          "label": "para",
          "text": "evaluating for precision and recall, 239",
          "level": -1,
          "page": 497,
          "reading_order": 55,
          "bbox": [
            359,
            270,
            561,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_56",
          "label": "para",
          "text": "processing search engine results, 82",
          "level": -1,
          "page": 497,
          "reading_order": 56,
          "bbox": [
            359,
            286,
            548,
            301
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_57",
          "label": "para",
          "text": "using POS tags, 187",
          "level": -1,
          "page": 497,
          "reading_order": 57,
          "bbox": [
            359,
            304,
            467,
            316
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_58",
          "label": "para",
          "text": "segmentation, 112–116",
          "level": -1,
          "page": 497,
          "reading_order": 58,
          "bbox": [
            349,
            316,
            467,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_59",
          "label": "para",
          "text": "in chunking and tokenization, 264",
          "level": -1,
          "page": 497,
          "reading_order": 59,
          "bbox": [
            359,
            331,
            539,
            345
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_60",
          "label": "para",
          "text": "sentence, 112",
          "level": -1,
          "page": 497,
          "reading_order": 60,
          "bbox": [
            359,
            348,
            434,
            358
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_61",
          "label": "para",
          "text": "word, 113-116",
          "level": -1,
          "page": 497,
          "reading_order": 61,
          "bbox": [
            359,
            358,
            442,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_62",
          "label": "para",
          "text": "semantic cues to word category, 211",
          "level": -1,
          "page": 497,
          "reading_order": 62,
          "bbox": [
            349,
            376,
            531,
            389
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_63",
          "label": "para",
          "text": "semantic interpretations, NLTK functions for,\n393",
          "level": -1,
          "page": 497,
          "reading_order": 63,
          "bbox": [
            349,
            389,
            583,
            415
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_64",
          "label": "para",
          "text": "semantic role labeling, 29",
          "level": -1,
          "page": 497,
          "reading_order": 64,
          "bbox": [
            349,
            420,
            478,
            433
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_65",
          "label": "para",
          "text": "semantic:",
          "level": -1,
          "page": 497,
          "reading_order": 65,
          "bbox": [
            349,
            433,
            396,
            448
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_66",
          "label": "para",
          "text": "natural language, logic and, 365-368\nnatural language, resources for",
          "level": -1,
          "page": 497,
          "reading_order": 66,
          "bbox": [
            359,
            448,
            557,
            477
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_67",
          "label": "para",
          "text": "information, 403",
          "level": -1,
          "page": 497,
          "reading_order": 67,
          "bbox": [
            359,
            477,
            521,
            492
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_68",
          "label": "para",
          "text": "semantics of English sentences, 385-397",
          "level": -1,
          "page": 497,
          "reading_order": 68,
          "bbox": [
            349,
            492,
            557,
            506
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_69",
          "label": "para",
          "text": "quantifier ambiguity, 394-397",
          "level": -1,
          "page": 497,
          "reading_order": 69,
          "bbox": [
            359,
            506,
            521,
            520
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_70",
          "label": "para",
          "text": "transitive verbs, 391-394",
          "level": -1,
          "page": 497,
          "reading_order": 70,
          "bbox": [
            359,
            520,
            494,
            533
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_71",
          "label": "para",
          "text": "人-calculus, 386–390",
          "level": -1,
          "page": 497,
          "reading_order": 71,
          "bbox": [
            359,
            537,
            467,
            548
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_72",
          "label": "para",
          "text": "SemCor tagging, 214",
          "level": -1,
          "page": 497,
          "reading_order": 72,
          "bbox": [
            349,
            548,
            458,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_73",
          "label": "para",
          "text": "sentence boundaries, tagging across, 208",
          "level": -1,
          "page": 497,
          "reading_order": 73,
          "bbox": [
            349,
            564,
            557,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_74",
          "label": "para",
          "text": "sentence segmentation, 112, 233",
          "level": -1,
          "page": 497,
          "reading_order": 74,
          "bbox": [
            349,
            582,
            513,
            594
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_75",
          "label": "para",
          "text": "in chunking, 264",
          "level": -1,
          "page": 497,
          "reading_order": 75,
          "bbox": [
            359,
            594,
            451,
            609
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_76",
          "label": "para",
          "text": "in information retrieval process, 263",
          "level": -1,
          "page": 497,
          "reading_order": 76,
          "bbox": [
            359,
            609,
            548,
            623
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_77",
          "label": "para",
          "text": "sentence structure, analyzing, 291-326",
          "level": -1,
          "page": 497,
          "reading_order": 77,
          "bbox": [
            349,
            625,
            548,
            638
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_78",
          "label": "para",
          "text": "context-free grammar, 298-302",
          "level": -1,
          "page": 497,
          "reading_order": 78,
          "bbox": [
            359,
            638,
            530,
            654
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_79",
          "label": "para",
          "text": "dependencies and dependency grammar,\n310–315",
          "level": -1,
          "page": 497,
          "reading_order": 79,
          "bbox": [
            359,
            654,
            574,
            680
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_80",
          "label": "para",
          "text": "grammar development, 315-321",
          "level": -1,
          "page": 497,
          "reading_order": 80,
          "bbox": [
            359,
            680,
            530,
            698
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_81",
          "label": "para",
          "text": "grammatical dilemmas, 292",
          "level": -1,
          "page": 497,
          "reading_order": 81,
          "bbox": [
            359,
            698,
            505,
            711
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_82",
          "label": "para",
          "text": "parsing with context-free grammar, 302–\n310",
          "level": -1,
          "page": 497,
          "reading_order": 82,
          "bbox": [
            359,
            711,
            574,
            737
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_83",
          "label": "para",
          "text": "resources for further reading, 322",
          "level": -1,
          "page": 497,
          "reading_order": 83,
          "bbox": [
            359,
            742,
            534,
            755
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_84",
          "label": "para",
          "text": "summary of important points, 321",
          "level": -1,
          "page": 497,
          "reading_order": 84,
          "bbox": [
            359,
            755,
            539,
            770
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_85",
          "label": "para",
          "text": "syntax, 295–298",
          "level": -1,
          "page": 497,
          "reading_order": 85,
          "bbox": [
            359,
            770,
            449,
            781
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_86",
          "label": "para",
          "text": "sents() function, 41",
          "level": -1,
          "page": 497,
          "reading_order": 86,
          "bbox": [
            349,
            785,
            449,
            797
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_497_order_87",
          "label": "foot",
          "text": "General Index | 475",
          "level": -1,
          "page": 497,
          "reading_order": 87,
          "bbox": [
            494,
            824,
            585,
            838
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_498_order_0",
          "label": "para",
          "text": "sequence classification, 231–233\nother methods, 233\nPOS tagging with consecutive classifier,\n232\nsequence iteration, 134\nsequences, 133–138\ncombining different sequence types, 136\nconverting between sequence types, 135\noperations on sequence types, 134\nprocessing using generator expressions,\n137\nstrings and lists as, 92\nshift operation, 305\nshift-reduce parsing, 304\nShoebox, 66, 412\nsibling nodes, 279\nsignature, 373\nsimilarity, semantic, 71\nSinica Treebank Corpus, 316\nslash categories, 350\nslicing\nlists, 12, 13\nstrings, 15, 90\nsmoothing, 249\nspace-time trade-offs in algorithm design, 163\nspaces, matching in regular expressions, 109\nSpeech Synthesis Markup Language (W3C\nSSML), 214\nspellcheckers, Words Corpus used by, 60\nspoken dialogue systems, 31\nspreadsheets, obtaining data from, 418\nSQL (Structured Query Language), 362\ntranslating English sentence to, 362\nstack trace, 158\nstandards for linguistic data creation, 421\nstandoff annotation, 415, 421\nstart symbol for grammars, 298, 334\nstartswith() function, 45\nstemming, 107\nNLTK HOWTO, 122\nstemmers, 107\nusing regular expressions, 104\nusing stem() function, 105\nstopwords, 60\nstress (in pronunciation), 64\nstring formatting expressions, 117\nstring literals, Unicode string literal in Python\n95\nstrings, 15, 87–93",
          "level": -1,
          "page": 498,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            324,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_498_order_1",
          "label": "para",
          "text": "accessing individual characters, 89\naccessing substrings, 90\nbasic operations with, 87–89\nconverting lists to, 116\nformats, 117–118\nformatting\nlining things up, 118\ntabulating data, 119\nimmutability of, 93\nlists versus, 92\nmethods, 92\nmore operations on, useful string methods,\n92\nprinting, 89\nPython's str data type, 86\nregular expressions as, 101\ntokenizing, 86\nstructurally ambiguous sentences, 300\nstructure sharing, 340\ninteraction with unification, 343\nstructured data, 261\nstyle guide for Python code, 138\nstylistics, 43\nsubcategories of verbs, 314\nsubcategorization, 344–347\nsubstrings (WFST), 307\nsubstrings, accessing, 90\nsubsumes, 341\nsubsumption, 341–344\nsuffixes, classifier for, 229\nsupervised classification, 222–237\nchoosing features, 224–227\ndocuments, 227\nexploiting context, 230\ngender identification, 222\nidentifying dialogue act types, 235\npart-of-speech tagging, 229\nRecognizing Textual Entailment (RTE),\n235\nscaling up to large datasets, 237\nsentence segmentation, 233\nsequence classification, 231–233\nSwadesh wordlists, 65\nsymbol processing, language processing\nversus, 442\nsynonyms, 67\nsynsets, 67\nsemantic similarity, 71\nin WordNet concept hierarchy, 69",
          "level": -1,
          "page": 498,
          "reading_order": 1,
          "bbox": [
            349,
            71,
            583,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_498_order_2",
          "label": "foot",
          "text": "476 | General Inde",
          "level": -1,
          "page": 498,
          "reading_order": 2,
          "bbox": [
            97,
            824,
            182,
            838
          ],
          "section_number": "476",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_0",
          "label": "para",
          "text": "syntactic agreement, 329–331\nsyntactic cues to word category, 211\nsyntactic structure, recursion in, 301\nsyntax, 295–298\nsyntax errors, 3",
          "level": -1,
          "page": 499,
          "reading_order": 0,
          "bbox": [
            99,
            71,
            280,
            144
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_1",
          "label": "para",
          "text": "T",
          "level": -1,
          "page": 499,
          "reading_order": 1,
          "bbox": [
            97,
            161,
            105,
            179
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_2",
          "label": "para",
          "text": "10",
          "level": -1,
          "page": 499,
          "reading_order": 2,
          "bbox": [
            100,
            185,
            333,
            224
          ],
          "section_number": "10",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_3",
          "label": "para",
          "text": "138",
          "level": -1,
          "page": 499,
          "reading_order": 3,
          "bbox": [
            116,
            229,
            299,
            259
          ],
          "section_number": "138",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_4",
          "label": "para",
          "text": "266\nmatching, precedence in, 267",
          "level": -1,
          "page": 499,
          "reading_order": 4,
          "bbox": [
            100,
            259,
            270,
            286
          ],
          "section_number": "266",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_5",
          "label": "para",
          "text": "tagging, 179–219\nadjectives and adverbs, 186\ncombining taggers, 205\ndefault tagger, 198\nevaluating tagger performance, 201\nexploring tagged corpora, 187–189\nlookup tagger, 200–201\nmapping words to tags using Python\ndictionaries, 189–198\nnouns, 184",
          "level": -1,
          "page": 499,
          "reading_order": 5,
          "bbox": [
            100,
            286,
            297,
            430
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_7",
          "label": "para",
          "text": "part-of-speech (POS) tagging, 229\nperformance limitations, 206\nreading tagged corpora, 181\nregular expression tagger, 199\nrepresenting tagged tokens, 181\nresources for further reading, 214\nacross sentence boundaries, 208\nseparating training and testing data, 203\nsimplified part-of-speech tagset, 183\nstoring taggers, 206\ntransformation-based, 208–210\nunigram tagging, 202\nunknown words, 206\nunsimplified POS tags, 187\nusing POS (part-of-speech) tagger, 179\nverbs, 185",
          "level": -1,
          "page": 499,
          "reading_order": 7,
          "bbox": [
            113,
            431,
            316,
            663
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_8",
          "label": "para",
          "text": "269\nXML, 425",
          "level": -1,
          "page": 499,
          "reading_order": 8,
          "bbox": [
            100,
            672,
            315,
            798
          ],
          "section_number": "269",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_9",
          "label": "para",
          "text": "terms (first-order logic), 372",
          "level": -1,
          "page": 499,
          "reading_order": 9,
          "bbox": [
            349,
            71,
            494,
            89
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_10",
          "label": "para",
          "text": "test sets, 44, 223",
          "level": -1,
          "page": 499,
          "reading_order": 10,
          "bbox": [
            349,
            89,
            432,
            100
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_11",
          "label": "para",
          "text": "choosing for classification models, 238\ntesting classifier for document classification,\n228",
          "level": -1,
          "page": 499,
          "reading_order": 11,
          "bbox": [
            349,
            104,
            574,
            143
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_12",
          "label": "para",
          "text": "text, 1",
          "level": -1,
          "page": 499,
          "reading_order": 12,
          "bbox": [
            349,
            149,
            386,
            161
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_13",
          "label": "para",
          "text": "computing statistics from, 16–22\ncounting vocabulary, 7–10",
          "level": -1,
          "page": 499,
          "reading_order": 13,
          "bbox": [
            359,
            161,
            532,
            190
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_15",
          "label": "para",
          "text": "entering on mobile phones (T9 system), 99\nas lists of words, 10-16",
          "level": -1,
          "page": 499,
          "reading_order": 15,
          "bbox": [
            359,
            191,
            583,
            217
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_16",
          "label": "para",
          "text": "searching, 4-7",
          "level": -1,
          "page": 499,
          "reading_order": 16,
          "bbox": [
            359,
            221,
            440,
            234
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_17",
          "label": "para",
          "text": "examining common contexts, 5",
          "level": -1,
          "page": 499,
          "reading_order": 17,
          "bbox": [
            377,
            234,
            540,
            250
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_18",
          "label": "para",
          "text": "text alignment, 30",
          "level": -1,
          "page": 499,
          "reading_order": 18,
          "bbox": [
            349,
            250,
            441,
            263
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_19",
          "label": "para",
          "text": "text editor, creating programs with, 56\ntextonyms, 99",
          "level": -1,
          "page": 499,
          "reading_order": 19,
          "bbox": [
            349,
            265,
            548,
            289
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_20",
          "label": "para",
          "text": "textual entailment, 32",
          "level": -1,
          "page": 499,
          "reading_order": 20,
          "bbox": [
            349,
            294,
            460,
            304
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_21",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 499,
          "reading_order": 21,
          "bbox": [
            349,
            304,
            458,
            322
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_22",
          "label": "para",
          "text": "theorem proving in first order logic, 375\ntimeit module, 164",
          "level": -1,
          "page": 499,
          "reading_order": 22,
          "bbox": [
            349,
            322,
            557,
            348
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_24",
          "label": "para",
          "text": "TIMIT Corpus, 407-41",
          "level": -1,
          "page": 499,
          "reading_order": 24,
          "bbox": [
            349,
            349,
            467,
            367
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_25",
          "label": "para",
          "text": "tokenization, 80",
          "level": -1,
          "page": 499,
          "reading_order": 25,
          "bbox": [
            349,
            367,
            431,
            377
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_26",
          "label": "para",
          "text": "chunking and, 264",
          "level": -1,
          "page": 499,
          "reading_order": 26,
          "bbox": [
            359,
            377,
            460,
            395
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_27",
          "label": "para",
          "text": "in information retrieval, 26\nissues with, 111",
          "level": -1,
          "page": 499,
          "reading_order": 27,
          "bbox": [
            359,
            395,
            503,
            422
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_29",
          "label": "para",
          "text": "list produced from tokenizing string, 86",
          "level": -1,
          "page": 499,
          "reading_order": 29,
          "bbox": [
            359,
            423,
            566,
            439
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_30",
          "label": "para",
          "text": "regular expressions for, 109–112\nrepresenting tagged tokens, 181",
          "level": -1,
          "page": 499,
          "reading_order": 30,
          "bbox": [
            359,
            439,
            530,
            468
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_32",
          "label": "para",
          "text": "segmentation and, 112",
          "level": -1,
          "page": 499,
          "reading_order": 32,
          "bbox": [
            359,
            469,
            480,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_33",
          "label": "para",
          "text": "with Unicode strings as input and output\n97",
          "level": -1,
          "page": 499,
          "reading_order": 33,
          "bbox": [
            359,
            483,
            574,
            510
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_34",
          "label": "para",
          "text": "tokenized text, searching, 10",
          "level": -1,
          "page": 499,
          "reading_order": 34,
          "bbox": [
            349,
            510,
            494,
            528
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_35",
          "label": "para",
          "text": "tokens, 8",
          "level": -1,
          "page": 499,
          "reading_order": 35,
          "bbox": [
            349,
            528,
            395,
            539
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_36",
          "label": "para",
          "text": "Toolbox, 66, 412, 431–435",
          "level": -1,
          "page": 499,
          "reading_order": 36,
          "bbox": [
            350,
            543,
            486,
            555
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_37",
          "label": "para",
          "text": "accessing data from XML, using\nElementTree, 429",
          "level": -1,
          "page": 499,
          "reading_order": 37,
          "bbox": [
            359,
            555,
            530,
            583
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_38",
          "label": "para",
          "text": "adding field to each entry, 431",
          "level": -1,
          "page": 499,
          "reading_order": 38,
          "bbox": [
            359,
            583,
            521,
            600
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_39",
          "label": "para",
          "text": "resources for further reading, 438",
          "level": -1,
          "page": 499,
          "reading_order": 39,
          "bbox": [
            359,
            600,
            534,
            614
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_40",
          "label": "para",
          "text": "validating lexicon, 432-435",
          "level": -1,
          "page": 499,
          "reading_order": 40,
          "bbox": [
            359,
            616,
            505,
            629
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_41",
          "label": "para",
          "text": "tools for creation, publication, and use of\nlinguistic data, 421",
          "level": -1,
          "page": 499,
          "reading_order": 41,
          "bbox": [
            349,
            629,
            558,
            658
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_42",
          "label": "para",
          "text": "top-down approach to dynamic programming,\n167",
          "level": -1,
          "page": 499,
          "reading_order": 42,
          "bbox": [
            349,
            660,
            584,
            689
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_43",
          "label": "para",
          "text": "top-down parsing, 304",
          "level": -1,
          "page": 499,
          "reading_order": 43,
          "bbox": [
            349,
            689,
            467,
            702
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_44",
          "label": "para",
          "text": "total likelihood, 251",
          "level": -1,
          "page": 499,
          "reading_order": 44,
          "bbox": [
            349,
            704,
            450,
            716
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_45",
          "label": "para",
          "text": "training",
          "level": -1,
          "page": 499,
          "reading_order": 45,
          "bbox": [
            349,
            716,
            386,
            729
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_46",
          "label": "para",
          "text": "classifier, 223",
          "level": -1,
          "page": 499,
          "reading_order": 46,
          "bbox": [
            359,
            733,
            434,
            744
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_47",
          "label": "para",
          "text": "classifier for document classification, 228",
          "level": -1,
          "page": 499,
          "reading_order": 47,
          "bbox": [
            359,
            744,
            574,
            761
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_48",
          "label": "para",
          "text": "classifier-based chunkers, 274–278",
          "level": -1,
          "page": 499,
          "reading_order": 48,
          "bbox": [
            359,
            761,
            541,
            773
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_49",
          "label": "para",
          "text": "taggers, 2018",
          "level": -1,
          "page": 499,
          "reading_order": 49,
          "bbox": [
            359,
            778,
            422,
            790
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_499_order_50",
          "label": "foot",
          "text": "General Index | 477",
          "level": -1,
          "page": 499,
          "reading_order": 50,
          "bbox": [
            494,
            824,
            585,
            838
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_0",
          "label": "para",
          "text": "unigram chunker using CoNLL 2000\nChunking Corpus, 273\ntraining sets, 223, 225\ntransformation-based tagging, 208–210\ntransitive verbs, 314, 391–394\ntranslations\ncomparative wordlists, 66\nmachine (see machine translation)\nfreebanks, 315–317\ntrees, 279–281\nrepresenting chunks, 270\ntraversal of, 280\ntrie, 162\ntrigram taggers, 204\ntruth conditions, 368\ntruth-conditional semantics in first-order logic,\n377\ntuples, 133\nlists versus, 136\nparentheses with, 134\nrepresenting tagged tokens, 181\nTuring Test, 31, 368\ntype-raising, 390\ntype-token distinction, 8\nTypeError, 157\ntypes, 8, 86\n(see also data types)\ntypes (first-order logic), 373",
          "level": -1,
          "page": 500,
          "reading_order": 0,
          "bbox": [
            100,
            71,
            333,
            483
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_500_order_1",
      "label": "sec",
      "text": "U",
      "level": 1,
      "page": 500,
      "reading_order": 1,
      "bbox": [
        98,
        501,
        109,
        515
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_500_order_2",
          "label": "para",
          "text": "unary predicate, 372\nunbounded dependency constructions, 349–\n353\ndefined, 350\nunderspecified, 333\nUnicode, 93–97\ndecoding and encoding, 94\ndefinition and description of, 94\nextracting gfrom files, 94\nresources for further information, 122\nusing your local encoding in Python, 97\nunicodedata module, 96\nunification, 342–344\nunigram taggers\nconfusion matrix for, 240\nnoun phrase chunking with, 272\nunigram tagging, 202\nlookup tagger (example), 200\nseparating training and test data, 203",
          "level": -1,
          "page": 500,
          "reading_order": 2,
          "bbox": [
            98,
            519,
            324,
            798
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_3",
          "label": "para",
          "text": "unique beginners, 69\nUniversal Feed Parser, 83\nuniversal quantifier, 374\nunknown words, tagging, 206\nupdating dictionary incrementally, 195\nUS Presidential Inaugural Addresses Corpus,\n45\nuser input, capturing, 85",
          "level": -1,
          "page": 500,
          "reading_order": 3,
          "bbox": [
            342,
            71,
            583,
            190
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_500_order_4",
      "label": "sec",
      "text": "V",
      "level": 1,
      "page": 500,
      "reading_order": 4,
      "bbox": [
        349,
        206,
        358,
        224
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_500_order_5",
          "label": "para",
          "text": "valencies, 313",
          "level": -1,
          "page": 500,
          "reading_order": 5,
          "bbox": [
            349,
            224,
            422,
            241
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_6",
          "label": "para",
          "text": "validity of arguments, 369\nvalidity of XML documents",
          "level": -1,
          "page": 500,
          "reading_order": 6,
          "bbox": [
            349,
            241,
            485,
            268
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_8",
          "label": "para",
          "text": "valuation, 377\nexamining quantifier scope ambiguity, 381\nMace4 model converted to, 384",
          "level": -1,
          "page": 500,
          "reading_order": 8,
          "bbox": [
            349,
            269,
            583,
            313
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_9",
          "label": "para",
          "text": "valuation function, 377",
          "level": -1,
          "page": 500,
          "reading_order": 9,
          "bbox": [
            349,
            313,
            467,
            331
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_10",
          "label": "para",
          "text": "values, 191",
          "level": -1,
          "page": 500,
          "reading_order": 10,
          "bbox": [
            349,
            331,
            431,
            341
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_11",
          "label": "para",
          "text": "complex, 196",
          "level": -1,
          "page": 500,
          "reading_order": 11,
          "bbox": [
            359,
            341,
            434,
            359
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_12",
          "label": "para",
          "text": "variables",
          "level": -1,
          "page": 500,
          "reading_order": 12,
          "bbox": [
            349,
            359,
            395,
            376
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_13",
          "label": "para",
          "text": "arguments of predicates in first-order logic,\n373\nassignment, 378\nbound by quantifiers in first-order logic,\n373\ndefining, 14\nlocal, 58\nnaming, 15\nrelabeling bound variables, 389\nsatisfaction of, using to interpret quantified\nformulas, 380\nscope of, 145",
          "level": -1,
          "page": 500,
          "reading_order": 13,
          "bbox": [
            359,
            376,
            584,
            549
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_14",
          "label": "para",
          "text": "verb phrase (VP), 297",
          "level": -1,
          "page": 500,
          "reading_order": 14,
          "bbox": [
            349,
            549,
            458,
            564
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_15",
          "label": "para",
          "text": "verbs",
          "level": -1,
          "page": 500,
          "reading_order": 15,
          "bbox": [
            349,
            564,
            377,
            575
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_16",
          "label": "para",
          "text": "agreement paradigm for English regular\nverbs, 329\nauxiliary, 336\nauxiliary verbs and inversion of subject and\nverb, 348\ncategorizing and tagging, 185\nexamining for dependency grammar, 312\nhead of sentence and dependencies, 310\npresent participle, 211\ntransitive, 391–394",
          "level": -1,
          "page": 500,
          "reading_order": 16,
          "bbox": [
            359,
            575,
            584,
            725
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_500_order_17",
      "label": "sec",
      "text": "W",
      "level": 1,
      "page": 500,
      "reading_order": 17,
      "bbox": [
        349,
        743,
        362,
        757
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_500_order_18",
          "label": "para",
          "text": "$\\backslash$ W non-word characters in Python, 110, 111\n$\\backslash$ w word characters in Python, 110, 111",
          "level": -1,
          "page": 500,
          "reading_order": 18,
          "bbox": [
            350,
            761,
            574,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_500_order_19",
          "label": "foot",
          "text": "478 | General Inde",
          "level": -1,
          "page": 500,
          "reading_order": 19,
          "bbox": [
            97,
            824,
            182,
            838
          ],
          "section_number": "478",
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_501_order_0",
          "label": "para",
          "text": "web text, 42\nWeb, obtaining data from, 416\nwebsites, obtaining corpora from, 416\nweighted grammars, 318–321\nprobabilistic context-free grammar (PCFG),\n320\nwell-formed (XML), 425\nwell-formed formulas, 368\nwell-formed substring tables (WFST), 307–\n310\nwhitespace\nregular expression characters for, 109\ntokenizing text on, 109\nwildcard symbol (.), 98\nwindowdiff scorer, 414\nword classes, 179\nword comparison operators, 23\nword occurrence, counting in text, 8\nword offset, 45\nword processor files, obtaining data from, 417\nword segmentation, 113–116\nword sense disambiguation, 28\nword sequences, 7\nwordlist corpora, 60–63\nWordNet, 67–73\nconcept hierarchy, 69\nlemmatizer, 108\nmore lexical relations, 70\nsemantic similarity, 71\nvisualization of hypernym hierarchy using\nMatplotlib and NetworkX, 170\nWords Corpus, 60\nwords( ) function, 40\nwrapping text, 120",
          "level": -1,
          "page": 501,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            333,
            573
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_501_order_1",
          "label": "para",
          "text": "X\nXML, 425–431\nElementTree interface, 427–429\nformatting entries, 430\nrepresentation of lexical entry from chunk\nparsing Toolbox record, 434\nresources for further reading, 438\nrole of, in using to represent linguistic\nstructures, 426\nusing ElementTree to access Toolbox data\n429\nusing for linguistic structures, 425\nvalidity of documents, 426",
          "level": -1,
          "page": 501,
          "reading_order": 1,
          "bbox": [
            97,
            589,
            327,
            783
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_501_order_2",
          "label": "para",
          "text": "Z\nzero counts (naive Bayes classifier), 249\nzero projection, 347",
          "level": -1,
          "page": 501,
          "reading_order": 2,
          "bbox": [
            349,
            78,
            548,
            126
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_501_order_3",
          "label": "foot",
          "text": "General Index | 479",
          "level": -1,
          "page": 501,
          "reading_order": 3,
          "bbox": [
            494,
            824,
            585,
            838
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_502_order_0",
          "label": "para",
          "text": "_",
          "level": -1,
          "page": 502,
          "reading_order": 0,
          "bbox": [
            153,
            161,
            494,
            206
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_503_order_0",
      "label": "sec",
      "text": "About the Authors",
      "level": 1,
      "page": 503,
      "reading_order": 0,
      "bbox": [
        97,
        71,
        225,
        98
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_503_order_1",
          "label": "para",
          "text": "Steven Bird is Associate Professor in the Department of Computer Science and Soft-\nware Engineering at the University of Melbourne, and Senior Research Associate in the\nLinguistic Data Consortium at the University of Pennsylvania. He completed a Ph.D.\non computational phonology at the University of Edinburgh in 1990, supervised by\nEwan Klein. He later moved to Cameroon to conduct linguistic fieldwork on the Grass-\nfields Bantu languages under the auspices of the Summer Institute of Linguistics. More\nrecently, he spent several years as Associate Director of the Linguistic Data Consortium,\nwhere he led an R&D team to create models and tools for large databases of annotated\ntext. At Melbourne University, he established a language technology research group\nand has taught at all levels of the undergraduate computer science curriculum. In 2009,\nSteven is President of the Association for Computational Linguistics.",
          "level": -1,
          "page": 503,
          "reading_order": 1,
          "bbox": [
            97,
            98,
            585,
            286
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_503_order_2",
          "label": "para",
          "text": "Ewan Klein is Professor of Language Technology in the School of Informatics at the\nUniversity of Edinburgh. He completed a Ph.D. on formal semantics at the University\nof Cambridge in 1978. After some years working at the Universities of Sussex and\nNewcastle upon Tyne, Ewan took up a teaching position at Edinburgh. He was involved\nin the establishment of Edinburgh's Language Technology Group in 1993, and has\nbeen closely associated with it ever since. From 2000 to 2002, he took leave from the\nUniversity to act as Research Manager for the Edinburgh-based Natural Language Re-\nsearch Group of Edify Corporation, Santa Clara, and was responsible for spoken dia-\nlogue processing. Ewan is a past President of the European Chapter of the Association\nfor Computational Linguistics and was a founding member and Coordinator of the\nEuropean Network of Excellence in Human Language Technologies (ELSNET).",
          "level": -1,
          "page": 503,
          "reading_order": 2,
          "bbox": [
            97,
            294,
            585,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_503_order_3",
          "label": "para",
          "text": "Edward Loper has recently completed a Ph.D. on machine learning for natural lan-\nguage processing at the University of Pennsylvania. Edward was a student in Steven's\ngraduate course on computational linguistics in the fall of 2000, and went on to be a\nTeacher's Assistant and share in the development of NLTK. In addition to NLTK, he\nhas helped develop two packages for documenting and testing Python software,\nepydoc and doctest .",
          "level": -1,
          "page": 503,
          "reading_order": 3,
          "bbox": [
            97,
            483,
            585,
            582
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    },
    {
      "id": "page_503_order_4",
      "label": "sec",
      "text": "Colophon",
      "level": 1,
      "page": 503,
      "reading_order": 4,
      "bbox": [
        97,
        600,
        163,
        627
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": [
        {
          "id": "page_503_order_5",
          "label": "para",
          "text": "The animal on the cover of Natural Language Processing with Python is a right whale,\nthe rarest of all large whales. It is identifiable by its enormous head, which can measure\nup to one-third of its total body length. It lives in temperate and cool seas in both\nhemispheres at the surface of the ocean. It’s believed that the right whale may have\ngotten its name from whalers who thought that it was the “right” whale to kill for oil.\nEven though it has been protected since the 1930s, the right whale is still the most\nendangered of all the great whales.",
          "level": -1,
          "page": 503,
          "reading_order": 5,
          "bbox": [
            97,
            627,
            585,
            745
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_503_order_6",
          "label": "para",
          "text": "The large and bulky right whale is easily distinguished from other whales by the calluses\non its head. It has a broad back without a dorsal fin and a long arching mouth that",
          "level": -1,
          "page": 503,
          "reading_order": 6,
          "bbox": [
            100,
            752,
            585,
            788
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_504_order_0",
          "label": "para",
          "text": "begins above the eye. Its body is black, except for a white patch on its belly. Wounds\nand scars may appear bright orange, often becoming infested with whale lice or\ncyamids. The calluses—which are also found near the blowholes, above the eyes, and\non the chin, and upper lip—are black or gray. It has large flippers that are shaped like\npaddles, and a distinctive V-shaped blow, caused by the widely spaced blowholes on\nthe top of its head, which rises to 16 feet above the ocean’s surface.",
          "level": -1,
          "page": 504,
          "reading_order": 0,
          "bbox": [
            97,
            71,
            585,
            172
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_504_order_1",
          "label": "para",
          "text": "The right whale feeds on planktonic organisms, including shrimp-like krill and cope-\npods. As baleen whales, they have a series of 225–250 fringed overlapping plates hang-\ning from each side of the upper jaw, where teeth would otherwise be located. The plates\nare black and can be as long as 7.2 feet. Right whales are “grazers of the sea,” often\nswimming slowly with their mouths open. As water flows into the mouth and through\nthe baleen, prey is trapped near the tongue.",
          "level": -1,
          "page": 504,
          "reading_order": 1,
          "bbox": [
            97,
            179,
            585,
            278
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_504_order_2",
          "label": "para",
          "text": "Because females are not sexually mature until 10 years of age and they give birth to a\nsingle calf after a year-long pregnancy, populations grow slowly. The young right whale\nstays with its mother for one year.",
          "level": -1,
          "page": 504,
          "reading_order": 2,
          "bbox": [
            97,
            286,
            585,
            332
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_504_order_3",
          "label": "para",
          "text": "Right whales are found worldwide but in very small numbers. A right whale is com-\nmonly found alone or in small groups of 1 to 3, but when courting, they may form\ngroups of up to 30. Like most baleen whales, they are seasonally migratory. They inhabit\ncolder waters for feeding and then migrate to warmer waters for breeding and calving.\nAlthough they may move far out to sea during feeding seasons, right whales give birth\nin coastal areas. Interestingly, many of the females do not return to these coastal breed-\ning areas every year, but visit the area only in calving years. Where they go in other\nyears remains a mystery.",
          "level": -1,
          "page": 504,
          "reading_order": 3,
          "bbox": [
            97,
            340,
            585,
            474
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_504_order_4",
          "label": "para",
          "text": "The right whale’s only predators are orcas and humans. When danger lurks, a group\nof right whales may come together in a circle, with their tails pointing outward, to deter\na predator. This defense is not always successful and calves are occasionally separated\nfrom their mother and killed.",
          "level": -1,
          "page": 504,
          "reading_order": 4,
          "bbox": [
            97,
            483,
            585,
            546
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_504_order_5",
          "label": "para",
          "text": "Right whales are among the slowest swimming whales, although they may reach speeds\nup to 10 mph in short spurts. They can dive to at least 1,000 feet and can stay submerged\nfor up to 40 minutes. The right whale is extremely endangered, even after years of\nprotected status. Only in the past 15 years is there evidence of a population recovery\nin the Southern Hemisphere, and it is still not known if the right whale will survive at\nall in the Northern Hemisphere. Although not presently hunted, current conservation\nproblems include collisions with ships, conflicts with fishing activities, habitat de-\nstruction, oil drilling, and possible competition from other whale species. Right whales\nhave no teeth, so ear bones and, in some cases, eye lenses can be used to estimate the\nage of a right whale at death. It is believed that right whales live at least 50 years, but\nthere is little data on their longevity.",
          "level": -1,
          "page": 504,
          "reading_order": 5,
          "bbox": [
            97,
            555,
            586,
            736
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        },
        {
          "id": "page_504_order_6",
          "label": "para",
          "text": "The cover image is from the Dover Pictorial Archive. The cover font is Adobe ITC\nGaramond. The text font is Linotype Birka; the heading font is Adobe Myriad Con-\ndensed; and the code font is LucasFont’s TheSansMonoCondensed.",
          "level": -1,
          "page": 504,
          "reading_order": 6,
          "bbox": [
            97,
            743,
            585,
            791
          ],
          "section_number": null,
          "summary": null,
          "embeddings": [],
          "children": [],
          "content_elements": []
        }
      ]
    }
  ],
  "content_elements": [
    {
      "id": "page_1_order_0",
      "label": "para",
      "text": "Analyzing Text with tbe Natural Language Toolkit",
      "level": -1,
      "page": 1,
      "reading_order": 0,
      "bbox": [
        109,
        17,
        568,
        40
      ],
      "section_number": null,
      "summary": null,
      "embeddings": [],
      "children": [],
      "content_elements": []
    }
  ]
}
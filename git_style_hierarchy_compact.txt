GIT-STYLE DOCUMENT HIERARCHY (Compact)
==================================================

ğŸ“– Attention Is All You Need (p.1) [22 items]
â”œâ”€ ğŸ“‘ Abstract (p.1) [1 items]
â”œâ”€ ğŸ“‘ 1 Introduction (p.1) [8 items]
â”œâ”€ ğŸ“‘ 2 Background (p.2) [4 items]
â”œâ”€ ğŸ“‘ 3 Model Architecture (p.2) [2 items]
â”‚  â”œâ”€ ğŸ“ 3.1 Encoder and Decoder Stacks (p.2) [5 items]
â”‚  â”œâ”€ ğŸ“ 3.2 Attention (p.3) [1 items]
â”‚  â”‚  â”œâ”€ â€¢ 3.2.1 Scaled Dot-Product Attention (p.3) [6 items]
â”‚  â”‚  â”œâ”€ â€¢ 3.2.2 Multi-Head Attention (p.4) [7 items]
â”‚  â”‚  â””â”€ â€¢ 3.2.3 Applications of Attention in our Mode (p.5) [2 items]
â”‚  â”œâ”€ ğŸ“ 3.3 Position-wise Feed-Forward Networks (p.5) [2 items]
â”‚  â”œâ”€ ğŸ“ 3.4 Embeddings and Softma (p.5) [1 items]
â”‚  â””â”€ ğŸ“ 3.5 Positional Encoding (p.5) [6 items]
â”œâ”€ ğŸ“‘ 4 Why Self-Attention (p.6) [8 items]
â”œâ”€ ğŸ“‘ 5 Training (p.7) [1 items]
â”‚  â”œâ”€ ğŸ“ 5.1 Training Data and Batching (p.7) [1 items]
â”‚  â”œâ”€ ğŸ“ 5.2 Hardware and Schedule (p.7) [1 items]
â”‚  â”œâ”€ ğŸ“ 5.3 Optimizer (p.7) [2 items]
â”‚  â””â”€ ğŸ“ 5.4 Regularization (p.7) [4 items]
â”œâ”€ ğŸ“‘ 6 Results (p.8)
â”‚  â”œâ”€ ğŸ“ 6.1 Machine Translation (p.8) [4 items]
â”‚  â””â”€ ğŸ“ 6.2 Model Variations (p.8) [6 items]
â”œâ”€ ğŸ“‘ 7 Conclusion (p.9) [5 items]
â””â”€ ğŸ“‘ References (p.10) [34 items]
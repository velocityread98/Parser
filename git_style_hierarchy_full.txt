GIT-STYLE DOCUMENT HIERARCHY (Full Detail)
==================================================

ğŸ“– Attention Is All You Need (p.1)
â”œâ”€ ğŸ“‘ Abstract (p.1)
â”‚  â””â”€   Â¶ The dominant sequence transduction models are based on compl...
â”œâ”€ ğŸ“‘ 1 Introduction (p.1)
â”‚  â”œâ”€   Â¶ Recurrent neural networks, long short-term memory [12] and g...
â”‚  â”œâ”€   ğŸ“Œ * Equal contribution. Listing order is random. Jakob propose...
â”‚  â”œâ”€   ğŸ“Œ $^{\dagger}$ Work performed while at Google Brain.
â”‚  â”œâ”€   ğŸ“Œ ${ }^{\ddagger}$ Work performed while at Google Research.
â”‚  â”œâ”€   ğŸ‘‡ 31st Conference on Neural Information Processing Systems (NI...
â”‚  â”œâ”€   Â¶ Recurrent models typically factor computation along the symb...
â”‚  â”œâ”€   Â¶ Attention mechanisms have become an integral part of compell...
â”‚  â””â”€   Â¶ In this work we propose the Transformer, a model architectur...
â”œâ”€ ğŸ“‘ 2 Background (p.2)
â”‚  â”œâ”€   Â¶ The goal of reducing sequential computation also forms the f...
â”‚  â”œâ”€   Â¶ Self-attention, sometimes called intra-attention is an atten...
â”‚  â”œâ”€   Â¶ End-to-end memory networks are based on a recurrent attentio...
â”‚  â””â”€   Â¶ To the best of our knowledge, however, the Transformer is th...
â”œâ”€ ğŸ“‘ 3 Model Architecture (p.2)
â”‚  â”œâ”€ ğŸ“ 3.1 Encoder and Decoder Stacks (p.2)
â”‚  â”‚  â”œâ”€   Â¶ Encoder: The encoder is composed of a stack of $N=6$ identic...
â”‚  â”‚  â”œâ”€   ğŸ‘‡ 2
â”‚  â”‚  â”œâ”€   ğŸ–¼ï¸ Figure 1: The Transformer - model architecture. [IMAGE: ![Fi... [MERGEDÃ—2]
â”‚  â”‚  â”œâ”€   Â¶ wise fully connected feed-forward network. We employ a resid...
â”‚  â”‚  â””â”€   Â¶ The The decoder is also composed of a stack of $N=6$ identic...
â”‚  â”œâ”€ ğŸ“ 3.2 Attention (p.3)
â”‚  â”‚  â”œâ”€ â€¢ 3.2.1 Scaled Dot-Product Attention (p.3)
â”‚  â”‚  â”‚  â”œâ”€   Â¶ We call our particular attention "Scaled Dot-Product Attenti...
â”‚  â”‚  â”‚  â”œâ”€   ğŸ–¼ï¸ Figure 2: (left) Scaled Dot-Product Attention. (right) Multi... [MERGEDÃ—2]
â”‚  â”‚  â”‚  â”œâ”€   Â¶ query with all keys, divide each by $\sqrt{d_k}$ , and apply...
â”‚  â”‚  â”‚  â”œâ”€   Â¶ In practice, we compute the attention function on a set of q...
â”‚  â”‚  â”‚  â”œâ”€   Â¶ The two most commonly used attention functions are additive ...
â”‚  â”‚  â”‚  â””â”€   Â¶ While for small values of $d_k$ the two mechanisms perform s...
â”‚  â”‚  â”œâ”€ â€¢ 3.2.2 Multi-Head Attention (p.4)
â”‚  â”‚  â”‚  â”œâ”€   Â¶ Instead of performing a single attention function with $d_{\...
â”‚  â”‚  â”‚  â”œâ”€   Â¶ Multi-head attention allows the model to jointly attend to i...
â”‚  â”‚  â”‚  â”œâ”€   ğŸ“Œ ${ }^{4}$ To illustrate why the dot products get large, assu...
â”‚  â”‚  â”‚  â”œâ”€   ğŸ‘‡ +
â”‚  â”‚  â”‚  â”œâ”€   Â¶ $$\begin{aligned}
\operatorname{MultiHead}(Q, K, V) & =\text...
â”‚  â”‚  â”‚  â”œâ”€   Â¶ Where the projections are parameter matrices $W_i^Q\in\mathb...
â”‚  â”‚  â”‚  â””â”€   Â¶ In this work we employ $h=8$ parallel attention layers, or h...
â”‚  â”‚  â”œâ”€ â€¢ 3.2.3 Applications of Attention in our Mode (p.5)
â”‚  â”‚  â”‚  â”œâ”€   Â¶ The Transformer uses multi-head attention in three different...
â”‚  â”‚  â”‚  â””â”€   ğŸ“‹ â€¢ In "encoder-decoder attention" layers, the queries come fr... [MERGEDÃ—3]
â”‚  â”‚  â””â”€   Â¶ An attention function can be described as mapping a query an...
â”‚  â”œâ”€ ğŸ“ 3.3 Position-wise Feed-Forward Networks (p.5)
â”‚  â”‚  â”œâ”€   Â¶ In addition to attention sub-layers, each of the layers in o...
â”‚  â”‚  â””â”€   Â¶ While the linear transformations are the same across differe...
â”‚  â”œâ”€ ğŸ“ 3.4 Embeddings and Softma (p.5)
â”‚  â”‚  â””â”€   Â¶ Similarly to other sequence transduction models, we use lear...
â”‚  â”œâ”€ ğŸ“ 3.5 Positional Encoding (p.5)
â”‚  â”‚  â”œâ”€   Â¶ Since our model contains no recurrence and no convolution, i...
â”‚  â”‚  â”œâ”€   ğŸ‘‡ 5
â”‚  â”‚  â”œâ”€   ğŸ“Š Table 1: Maximum path lengths, per-layer complexity and mini... [MERGEDÃ—2]
â”‚  â”‚  â”œâ”€   Â¶ bottoms of the encoder and decoder stacks. The positional en...
â”‚  â”‚  â”œâ”€   Â¶ In this work, we use sine and cosine functions of different ...
â”‚  â”‚  â””â”€   Â¶ We also experimented with using learned positional embedding...
â”‚  â”œâ”€   Â¶ Most competitive neural sequence transduction models have an...
â”‚  â””â”€   Â¶ The Transformer follows this overall architecture using stac...
â”œâ”€ ğŸ“‘ 4 Why Self-Attention (p.6)
â”‚  â”œâ”€   Â¶ In this section we compare various aspects of self-attention...
â”‚  â”œâ”€   Â¶ One is the total computational complexity per layer. Another...
â”‚  â”œâ”€   Â¶ The third is the path length between long-range dependencies...
â”‚  â”œâ”€   Â¶ As noted in Table 1 , a self-attention layer connects all po...
â”‚  â”œâ”€   ğŸ‘‡ çš„
â”‚  â”œâ”€   Â¶ the input sequence centered around the respective output pos...
â”‚  â”œâ”€   Â¶ A single convolutional layer with kernel width $k<n$ does no...
â”‚  â””â”€   Â¶ As side benefit, self-attention could yield more interpretab...
â”œâ”€ ğŸ“‘ 5 Training (p.7)
â”‚  â”œâ”€ ğŸ“ 5.1 Training Data and Batching (p.7)
â”‚  â”‚  â””â”€   Â¶ We trained on the standard WMT 2014 English-German dataset c...
â”‚  â”œâ”€ ğŸ“ 5.2 Hardware and Schedule (p.7)
â”‚  â”‚  â””â”€   Â¶ We trained our models on one machine with 8 NVIDIA P100 GPUs...
â”‚  â”œâ”€ ğŸ“ 5.3 Optimizer (p.7)
â”‚  â”‚  â”œâ”€   Â¶ We used the Adam optimizer with $\beta_1=0.9$ , $\beta_2=0.9...
â”‚  â”‚  â””â”€   Â¶ This corresponds to increasing the learning rate linearly fo...
â”‚  â”œâ”€ ğŸ“ 5.4 Regularization (p.7)
â”‚  â”‚  â”œâ”€   Â¶ We employ three types of regularization during training
â”‚  â”‚  â”œâ”€   Â¶ We Dropout We apply dropout [27] to the output of each sub-l...
â”‚  â”‚  â”œâ”€   ? <table><tr><td rowspan="2">Model</td><td colspan="2">BLEU</t...
â”‚  â”‚  â””â”€   Â¶ During Smoothing During training, we employed label smoothin...
â”‚  â””â”€   Â¶ This section describes the training regime for our models.
â”œâ”€ ğŸ“‘ 6 Results (p.8)
â”‚  â”œâ”€ ğŸ“ 6.1 Machine Translation (p.8)
â”‚  â”‚  â”œâ”€   Â¶ On the WMT 2014 English-to-German translation task, the big ...
â”‚  â”‚  â”œâ”€   Â¶ On the WMT 2014 English-to-French translation task, our big ...
â”‚  â”‚  â”œâ”€   Â¶ For the base models, we used a single model obtained by aver...
â”‚  â”‚  â””â”€   Â¶ Table 2 summarizes our results and compares our translation ...
â”‚  â””â”€ ğŸ“ 6.2 Model Variations (p.8)
â”‚     â”œâ”€   Â¶ To evaluate the importance of different components of the Tr...
â”‚     â”œâ”€   Â¶ In Table 3 rows (A), we vary the number of attention heads a...
â”‚     â”œâ”€   ğŸ“Œ We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40,...
â”‚     â”œâ”€   ğŸ‘‡ 8
â”‚     â”œâ”€   ğŸ“Š Table 3: Variations on the Transformer architecture. Unliste... [MERGEDÃ—2]
â”‚     â””â”€   Â¶ In Table 3 rows (B), we observe that reducing the attention ...
â”œâ”€ ğŸ“‘ 7 Conclusion (p.9)
â”‚  â”œâ”€   Â¶ In this work, we presented the Transformer, the first sequen...
â”‚  â”œâ”€   Â¶ For translation tasks, the Transformer can be trained signif...
â”‚  â”œâ”€   Â¶ We are excited about the future of attention-based models an...
â”‚  â”œâ”€   Â¶ The code we used to train and evaluate our models is availab...
â”‚  â””â”€   Â¶ Acknowledgements We are grateful to Nal Kalchbrenner and Ste...
â”œâ”€ ğŸ“‘ References (p.10)
â”‚  â”œâ”€   Â¶ [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. L...
â”‚  â”œâ”€   Â¶ [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neur...
â”‚  â”œâ”€   Â¶ [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. ...
â”‚  â”œâ”€   Â¶ [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-...
â”‚  â”œâ”€   Â¶ [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fe...
â”‚  â”œâ”€   Â¶ [6] Francois Chollet. Xception: Deep learning with depthwise...
â”‚  â”œâ”€   Â¶ [7] Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yosh...
â”‚  â””â”€   Â¶ [8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarat...
â”‚  â””â”€   ğŸ“„ ... and 26 more content elements
â”œâ”€   ğŸ‘¤ Ashish Vaswani*
â”œâ”€   Â¶ Google Brain
â”œâ”€   Â¶ avaswani@google.com
â”œâ”€   ğŸ‘¤ Noam Shazeer*
Google Brain
â”œâ”€   Â¶ noam@google.com
â”œâ”€   ğŸ‘¤ Niki Parmar*
â”œâ”€   Â¶ Google Research
â””â”€   Â¶ nikip@google.com
â””â”€   ğŸ“„ ... and 14 more content elements
GIT-STYLE DOCUMENT HIERARCHY (Full Detail)
==================================================

📖 Attention Is All You Need (p.1)
├─ 📑 Abstract (p.1)
│  └─   ¶ The dominant sequence transduction models are based on compl...
├─ 📑 1 Introduction (p.1)
│  ├─   ¶ Recurrent neural networks, long short-term memory [12] and g...
│  ├─   📌 * Equal contribution. Listing order is random. Jakob propose...
│  ├─   📌 $^{\dagger}$ Work performed while at Google Brain.
│  ├─   📌 ${ }^{\ddagger}$ Work performed while at Google Research.
│  ├─   👇 31st Conference on Neural Information Processing Systems (NI...
│  ├─   ¶ Recurrent models typically factor computation along the symb...
│  ├─   ¶ Attention mechanisms have become an integral part of compell...
│  └─   ¶ In this work we propose the Transformer, a model architectur...
├─ 📑 2 Background (p.2)
│  ├─   ¶ The goal of reducing sequential computation also forms the f...
│  ├─   ¶ Self-attention, sometimes called intra-attention is an atten...
│  ├─   ¶ End-to-end memory networks are based on a recurrent attentio...
│  └─   ¶ To the best of our knowledge, however, the Transformer is th...
├─ 📑 3 Model Architecture (p.2)
│  ├─ 📝 3.1 Encoder and Decoder Stacks (p.2)
│  │  ├─   ¶ Encoder: The encoder is composed of a stack of $N=6$ identic...
│  │  ├─   👇 2
│  │  ├─   🖼️ Figure 1: The Transformer - model architecture. [IMAGE: ![Fi... [MERGED×2]
│  │  ├─   ¶ wise fully connected feed-forward network. We employ a resid...
│  │  └─   ¶ The The decoder is also composed of a stack of $N=6$ identic...
│  ├─ 📝 3.2 Attention (p.3)
│  │  ├─ • 3.2.1 Scaled Dot-Product Attention (p.3)
│  │  │  ├─   ¶ We call our particular attention "Scaled Dot-Product Attenti...
│  │  │  ├─   🖼️ Figure 2: (left) Scaled Dot-Product Attention. (right) Multi... [MERGED×2]
│  │  │  ├─   ¶ query with all keys, divide each by $\sqrt{d_k}$ , and apply...
│  │  │  ├─   ¶ In practice, we compute the attention function on a set of q...
│  │  │  ├─   ¶ The two most commonly used attention functions are additive ...
│  │  │  └─   ¶ While for small values of $d_k$ the two mechanisms perform s...
│  │  ├─ • 3.2.2 Multi-Head Attention (p.4)
│  │  │  ├─   ¶ Instead of performing a single attention function with $d_{\...
│  │  │  ├─   ¶ Multi-head attention allows the model to jointly attend to i...
│  │  │  ├─   📌 ${ }^{4}$ To illustrate why the dot products get large, assu...
│  │  │  ├─   👇 +
│  │  │  ├─   ¶ $$\begin{aligned}
\operatorname{MultiHead}(Q, K, V) & =\text...
│  │  │  ├─   ¶ Where the projections are parameter matrices $W_i^Q\in\mathb...
│  │  │  └─   ¶ In this work we employ $h=8$ parallel attention layers, or h...
│  │  ├─ • 3.2.3 Applications of Attention in our Mode (p.5)
│  │  │  ├─   ¶ The Transformer uses multi-head attention in three different...
│  │  │  └─   📋 • In "encoder-decoder attention" layers, the queries come fr... [MERGED×3]
│  │  └─   ¶ An attention function can be described as mapping a query an...
│  ├─ 📝 3.3 Position-wise Feed-Forward Networks (p.5)
│  │  ├─   ¶ In addition to attention sub-layers, each of the layers in o...
│  │  └─   ¶ While the linear transformations are the same across differe...
│  ├─ 📝 3.4 Embeddings and Softma (p.5)
│  │  └─   ¶ Similarly to other sequence transduction models, we use lear...
│  ├─ 📝 3.5 Positional Encoding (p.5)
│  │  ├─   ¶ Since our model contains no recurrence and no convolution, i...
│  │  ├─   👇 5
│  │  ├─   📊 Table 1: Maximum path lengths, per-layer complexity and mini... [MERGED×2]
│  │  ├─   ¶ bottoms of the encoder and decoder stacks. The positional en...
│  │  ├─   ¶ In this work, we use sine and cosine functions of different ...
│  │  └─   ¶ We also experimented with using learned positional embedding...
│  ├─   ¶ Most competitive neural sequence transduction models have an...
│  └─   ¶ The Transformer follows this overall architecture using stac...
├─ 📑 4 Why Self-Attention (p.6)
│  ├─   ¶ In this section we compare various aspects of self-attention...
│  ├─   ¶ One is the total computational complexity per layer. Another...
│  ├─   ¶ The third is the path length between long-range dependencies...
│  ├─   ¶ As noted in Table 1 , a self-attention layer connects all po...
│  ├─   👇 的
│  ├─   ¶ the input sequence centered around the respective output pos...
│  ├─   ¶ A single convolutional layer with kernel width $k<n$ does no...
│  └─   ¶ As side benefit, self-attention could yield more interpretab...
├─ 📑 5 Training (p.7)
│  ├─ 📝 5.1 Training Data and Batching (p.7)
│  │  └─   ¶ We trained on the standard WMT 2014 English-German dataset c...
│  ├─ 📝 5.2 Hardware and Schedule (p.7)
│  │  └─   ¶ We trained our models on one machine with 8 NVIDIA P100 GPUs...
│  ├─ 📝 5.3 Optimizer (p.7)
│  │  ├─   ¶ We used the Adam optimizer with $\beta_1=0.9$ , $\beta_2=0.9...
│  │  └─   ¶ This corresponds to increasing the learning rate linearly fo...
│  ├─ 📝 5.4 Regularization (p.7)
│  │  ├─   ¶ We employ three types of regularization during training
│  │  ├─   ¶ We Dropout We apply dropout [27] to the output of each sub-l...
│  │  ├─   ? <table><tr><td rowspan="2">Model</td><td colspan="2">BLEU</t...
│  │  └─   ¶ During Smoothing During training, we employed label smoothin...
│  └─   ¶ This section describes the training regime for our models.
├─ 📑 6 Results (p.8)
│  ├─ 📝 6.1 Machine Translation (p.8)
│  │  ├─   ¶ On the WMT 2014 English-to-German translation task, the big ...
│  │  ├─   ¶ On the WMT 2014 English-to-French translation task, our big ...
│  │  ├─   ¶ For the base models, we used a single model obtained by aver...
│  │  └─   ¶ Table 2 summarizes our results and compares our translation ...
│  └─ 📝 6.2 Model Variations (p.8)
│     ├─   ¶ To evaluate the importance of different components of the Tr...
│     ├─   ¶ In Table 3 rows (A), we vary the number of attention heads a...
│     ├─   📌 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40,...
│     ├─   👇 8
│     ├─   📊 Table 3: Variations on the Transformer architecture. Unliste... [MERGED×2]
│     └─   ¶ In Table 3 rows (B), we observe that reducing the attention ...
├─ 📑 7 Conclusion (p.9)
│  ├─   ¶ In this work, we presented the Transformer, the first sequen...
│  ├─   ¶ For translation tasks, the Transformer can be trained signif...
│  ├─   ¶ We are excited about the future of attention-based models an...
│  ├─   ¶ The code we used to train and evaluate our models is availab...
│  └─   ¶ Acknowledgements We are grateful to Nal Kalchbrenner and Ste...
├─ 📑 References (p.10)
│  ├─   ¶ [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. L...
│  ├─   ¶ [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neur...
│  ├─   ¶ [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. ...
│  ├─   ¶ [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-...
│  ├─   ¶ [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fe...
│  ├─   ¶ [6] Francois Chollet. Xception: Deep learning with depthwise...
│  ├─   ¶ [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yosh...
│  └─   ¶ [8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarat...
│  └─   📄 ... and 26 more content elements
├─   👤 Ashish Vaswani*
├─   ¶ Google Brain
├─   ¶ avaswani@google.com
├─   👤 Noam Shazeer*
Google Brain
├─   ¶ noam@google.com
├─   👤 Niki Parmar*
├─   ¶ Google Research
└─   ¶ nikip@google.com
└─   📄 ... and 14 more content elements